{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes \n",
    "### Required installing Oracle JAVA 8 to get javabridge installed\n",
    "### Then, I was able to install py-causal from https://bd2kccd.github.io/docs/py-causal/\n",
    "### GFCI is slower than RFCI, but more accurate (SPIRTES), GFCI and RFCI account for unobserved variables, FGES assumes no unobserved variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure Learning Performance Guarantees If the assumptions in the previous section hold, then in the large sample limit, the CBN structure output by GFCId will contain an edge of one of four kinds between Xand Y   if and only if Xand Yare not independent conditional on any subset of the other measured variables of less than or equal to a specified size. In addition, there is (1) an arc X->Y   if and only if Xdirectly or indirectly causes Y, and Y   does not directly or indirectly cause X; (2) an edge X <-->Y   if and only if X   is not a direct or indirect cause of Yand Y   is not a direct or indirect cause of X(which can only occur if there are latent confounders of Xand some other variable or Yand some other variable; (3) an edge Xo->Y   only if Yis not a direct or indirect cause of X, but Xmay or may not be an indirect cause of Y; (4) an edge X oâ€“o Y   indicates that Xand Y   are dependent no matter what subset of observed variables is conditioned on, but contains no orientation information (X   may be a direct or indirect cause of Y, and Ymay be an indirect cause of X, or there may be a latent common cause of Xand Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying some various ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512]] ['temp/a0', 'temp/a1', 'temp/a2', 'temp/a3', 'temp/a4', 'temp/a5', 'temp/a6', 'temp/a7', 'temp/a8', 'temp/a9', 'temp/a10', 'temp/a11', 'temp/a12', 'temp/a13', 'temp/a14', 'temp/a15', 'temp/a16', 'temp/a17', 'temp/a18', 'temp/a19', 'temp/a20', 'temp/a21', 'temp/a22', 'temp/a23', 'temp/a24', 'temp/a25', 'temp/a26', 'temp/a27', 'temp/a28', 'temp/a29', 'temp/a30', 'temp/a31', 'temp/a32', 'temp/a33', 'temp/a34', 'temp/a35', 'temp/a36', 'temp/a37', 'temp/a38', 'temp/a39', 'temp/a40', 'temp/a41', 'temp/a42', 'temp/a43', 'temp/a44', 'temp/a45', 'temp/a46', 'temp/a47', 'temp/a48', 'temp/a49', 'temp/a50', 'temp/a51', 'temp/a52', 'temp/a53', 'temp/a54', 'temp/a55', 'temp/a56', 'temp/a57', 'temp/a58', 'temp/a59', 'temp/a60', 'temp/a61', 'temp/a62', 'temp/a63', 'temp/a64', 'temp/a65', 'temp/a66', 'temp/a67', 'temp/a68', 'temp/a69', 'temp/a70', 'temp/a71', 'temp/a72', 'temp/a73', 'temp/a74', 'temp/a75', 'temp/a76', 'temp/a77', 'temp/a78', 'temp/a79', 'temp/a80', 'temp/a81', 'temp/a82', 'temp/a83', 'temp/a84', 'temp/a85', 'temp/a86', 'temp/a87', 'temp/a88', 'temp/a89', 'temp/a90', 'temp/a91', 'temp/a92', 'temp/a93', 'temp/a94', 'temp/a95', 'temp/a96', 'temp/a97', 'temp/a98', 'temp/a99']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-585001.983947746"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from pycausal import search as s\n",
    "import configparser\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, \\\n",
    "                        Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "import keras.optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "import glob, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "# select your GPU Here\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #Comment this line out if you want all GPUS (2 hehe)\n",
    "\n",
    "# python full-display web browser\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "def get_model(dense, dropouts, inputs, target_len):\n",
    "    # dense is an ordered list of the number of dense neurons like [1024, 2048, 1024]\n",
    "    # dropouts is an ordered list of the dropout masks like [0.2, 0.3, 0.4]\n",
    "    inputs = keras.Input(shape = (inputs,))\n",
    "    x = keras.layers.BatchNormalization()(inputs)\n",
    "    x = keras.layers.Dense(dense[0], activation = 'relu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(dropouts[0])(x, training=True)\n",
    "    for den, drop in zip(dense[1:], dropouts[1:]):\n",
    "        x = keras.layers.Dense(den, activation = 'relu')(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Dropout(drop)(x, training=True)\n",
    "    outputs = keras.layers.Dense(target_len, activation = 'linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def discrete_gauss(low, high, samples, std = 20):\n",
    "    x = np.arange(low, high)\n",
    "    xU, xL = x + 0.5, x - 0.5 \n",
    "    prob = ss.norm.cdf(xU, scale = std) - ss.norm.cdf(xL, scale = std)\n",
    "    prob = prob / prob.sum() #normalize the probabilities so their sum is 1\n",
    "    nums = np.random.choice(x, size = samples, p = prob)\n",
    "    return nums\n",
    "\n",
    "\n",
    "\n",
    "def bar_plot(x_ax, val1, val1std, val2, val2std):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ## the data\n",
    "    N = len(x_ax)\n",
    "\n",
    "    ## necessary variables\n",
    "    ind = np.arange(N)                # the x locations for the groups\n",
    "    width = 0.35                      # the width of the bars\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    ## the bars\n",
    "    rects1 = ax.bar(ind, val1, width,\n",
    "                    color='gray',\n",
    "                    yerr=val1std,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='blue'))\n",
    "\n",
    "    rects2 = ax.bar(ind+width, val2, width,\n",
    "                        color='blue',\n",
    "                        #yerr=val2std,\n",
    "                        error_kw=dict(elinewidth=2,ecolor='gray'))\n",
    "\n",
    "    # axes and labels\n",
    "    ax.set_xlim(-width,len(ind)+width)\n",
    "    #ax.set_ylim(0,45)\n",
    "    ax.set_ylabel('Percentage')\n",
    "    ax.set_title('')\n",
    "    plt.xticks(ind + width / 2, x_ax, rotation=75, size = 14)\n",
    "    ## add a legend\n",
    "    ax.legend( (rects1[0], rects2[0]), ('Accuracy', '% Violations') )\n",
    "    fig.savefig(\"violations.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def normalize(x):\n",
    "    #return (x - x.min(0)) / x.ptp(0)\n",
    "    return x\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 2000):\n",
    "    # set bmi to these values real world mean and standard deviation for a certain country.\n",
    "    bmi = np.random.normal(mean, var, SIZE)\n",
    "    estrogen =  2*bmi +  np.random.normal(mean,var, SIZE)\n",
    "    age = np.random.normal(mean,var, SIZE)\n",
    "    genes = 1.1 * age + estrogen +  np.random.normal(mean,var, SIZE)\n",
    "    insomnia = estrogen +  np.random.normal(mean,var, SIZE)\n",
    "    density = estrogen + genes + np.random.normal(mean,var, SIZE)\n",
    "    cancer = 0.1 + density + np.random.normal(mean,var, SIZE)\n",
    "    return pd.DataFrame({'bmi' : bmi,'density' : density, 'age' : age, 'cancer' : cancer, 'estrogen': estrogen, 'genes':genes, 'insomnia': insomnia})\n",
    "\n",
    "\n",
    "def gen_data_perturbed(mean = 2, var = 1.3, SIZE = 2000):\n",
    "    # set bmi to these values real world mean and standard deviation for a certain country.\n",
    "    bmi = np.random.normal(25,5, SIZE)\n",
    "    estrogen =  2*bmi +  np.random.normal(mean,var, SIZE)\n",
    "    age = np.random.normal(55,10, SIZE)\n",
    "    genes = 1.1 * age +   np.random.normal(mean,var, SIZE)\n",
    "    insomnia = estrogen +  np.random.normal(mean,var, SIZE)\n",
    "    density = estrogen + genes + np.random.normal(mean,var + 10, SIZE)\n",
    "    cancer = density + np.random.normal(mean,var, SIZE)\n",
    "    return pd.DataFrame({'bmi' : bmi,'density' : density, 'age' : age, 'cancer' : cancer, 'estrogen': estrogen, 'genes':genes, 'insomnia': insomnia})\n",
    "def get_CG(df, tetrad):\n",
    "    tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "\n",
    "\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from collections import defaultdict\n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '5000M')\n",
    "tetrad = s.tetradrunner()\n",
    "\n",
    "\n",
    "verbosity = 1\n",
    "\n",
    "\n",
    "\n",
    "models = []\n",
    "model_names = []\n",
    "\n",
    "num_models =100\n",
    "model_layers = [1024,512]\n",
    "for i in range(num_models):\n",
    "    models.append(model_layers)\n",
    "    model_names.append('temp/a' + str(i))\n",
    "\n",
    "print(models, model_names)\n",
    "\n",
    "from pycausal import prior as p\n",
    "def get_bic(df, prior):\n",
    "\n",
    "    tetrad.run(algoId = 'fges', dfs = df,  scoreId = 'sem-bic-deterministic', dataType = 'continuous',\n",
    "               structurePrior = 1.0, samplePrior = 1, maxDegree = -1, maxPathLength = -1, priorKnowledge = prior,\n",
    "               completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True,\n",
    "               penaltyDiscount = 2\n",
    "               )\n",
    "    BIC = tetrad.getTetradGraph().getAllAttributes().toString()\n",
    "    BIC = float(BIC.split('=')[-1].split('}')[0])\n",
    "    return BIC #/ len(df)\n",
    "import itertools\n",
    "def get_pairs(lst):\n",
    "    a = set()\n",
    "    for i in itertools.permutations(lst,2):\n",
    "        a.add(i)\n",
    "    return a\n",
    "\n",
    "inputs = ['bmi', 'density', 'age', 'genes', 'insomnia', 'estrogen']\n",
    "target = ['cancer']\n",
    "full_conx = get_pairs(inputs + target)\n",
    "forced_conx = set({('age','genes'), ('bmi', 'estrogen'), ('estrogen', 'genes'),('estrogen', 'insomnia'), ('estrogen', 'density'), ('genes', 'density'), ('density', 'cancer')})\n",
    "restricted_conx = full_conx.difference(forced_conx)   \n",
    "\n",
    "prior = p.knowledge(requiredirect =  list(map(list, forced_conx)),\n",
    "                       forbiddirect = list(map(list, restricted_conx))\n",
    "                       )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = gen_data(SIZE = 50000)\n",
    "\n",
    "X = df[inputs].values\n",
    "X = normalize(X)\n",
    "y = df[target].values\n",
    "\n",
    "\n",
    "val_df = gen_data(SIZE = 5000)\n",
    "\n",
    "x_val = val_df[inputs].values\n",
    "x_val = normalize(x_val)\n",
    "y_val = val_df[target].values\n",
    "\n",
    "get_bic(df,prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp/a0\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.5043 - mean_squared_error: 2.5043 - val_loss: 1.3494 - val_mean_squared_error: 1.3494\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.34935, saving model to temp/a0\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2606 - mean_squared_error: 2.2606 - val_loss: 1.1391 - val_mean_squared_error: 1.1391\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.34935 to 1.13914, saving model to temp/a0\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1733 - mean_squared_error: 2.1733 - val_loss: 1.2624 - val_mean_squared_error: 1.2624\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.13914\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1671 - mean_squared_error: 2.1671 - val_loss: 2.0655 - val_mean_squared_error: 2.0655\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.13914\n",
      "Epoch 00004: early stopping\n",
      "temp/a1\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5563 - mean_squared_error: 2.5563 - val_loss: 1.3052 - val_mean_squared_error: 1.3052\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.30520, saving model to temp/a1\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.2187 - mean_squared_error: 2.2187 - val_loss: 1.4316 - val_mean_squared_error: 1.4316\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.30520\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1629 - mean_squared_error: 2.1629 - val_loss: 1.1964 - val_mean_squared_error: 1.1964\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.30520 to 1.19644, saving model to temp/a1\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1283 - mean_squared_error: 2.1283 - val_loss: 1.2121 - val_mean_squared_error: 1.2121\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.19644\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1051 - mean_squared_error: 2.1051 - val_loss: 1.1274 - val_mean_squared_error: 1.1274\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.19644 to 1.12735, saving model to temp/a1\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0483 - mean_squared_error: 2.0483 - val_loss: 1.1717 - val_mean_squared_error: 1.1717\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.12735\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 2.0120 - mean_squared_error: 2.0120 - val_loss: 1.2957 - val_mean_squared_error: 1.2957\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.12735\n",
      "Epoch 00007: early stopping\n",
      "temp/a2\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.5253 - mean_squared_error: 2.5253 - val_loss: 1.1862 - val_mean_squared_error: 1.1862\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.18620, saving model to temp/a2\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1906 - mean_squared_error: 2.1906 - val_loss: 1.3282 - val_mean_squared_error: 1.3282\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.18620\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1436 - mean_squared_error: 2.1436 - val_loss: 1.1846 - val_mean_squared_error: 1.1846\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.18620 to 1.18465, saving model to temp/a2\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1182 - mean_squared_error: 2.1182 - val_loss: 1.1237 - val_mean_squared_error: 1.1237\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.18465 to 1.12373, saving model to temp/a2\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1391 - mean_squared_error: 2.1391 - val_loss: 1.3184 - val_mean_squared_error: 1.3184\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.12373\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0305 - mean_squared_error: 2.0305 - val_loss: 1.2455 - val_mean_squared_error: 1.2455\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.12373\n",
      "Epoch 00006: early stopping\n",
      "temp/a3\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5081 - mean_squared_error: 2.5081 - val_loss: 2.3540 - val_mean_squared_error: 2.3540\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.35402, saving model to temp/a3\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2325 - mean_squared_error: 2.2325 - val_loss: 1.4194 - val_mean_squared_error: 1.4194\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.35402 to 1.41939, saving model to temp/a3\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1496 - mean_squared_error: 2.1496 - val_loss: 1.2528 - val_mean_squared_error: 1.2528\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.41939 to 1.25276, saving model to temp/a3\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1283 - mean_squared_error: 2.1283 - val_loss: 1.1882 - val_mean_squared_error: 1.1882\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.25276 to 1.18820, saving model to temp/a3\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 2.1389 - mean_squared_error: 2.1389 - val_loss: 1.1140 - val_mean_squared_error: 1.1140\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.18820 to 1.11398, saving model to temp/a3\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1064 - mean_squared_error: 2.1064 - val_loss: 1.4189 - val_mean_squared_error: 1.4189\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.11398\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0089 - mean_squared_error: 2.0089 - val_loss: 1.1667 - val_mean_squared_error: 1.1667\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.11398\n",
      "Epoch 00007: early stopping\n",
      "temp/a4\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5347 - mean_squared_error: 2.5347 - val_loss: 1.6799 - val_mean_squared_error: 1.6799\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.67990, saving model to temp/a4\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1832 - mean_squared_error: 2.1832 - val_loss: 1.4108 - val_mean_squared_error: 1.4108\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.67990 to 1.41083, saving model to temp/a4\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1228 - mean_squared_error: 2.1228 - val_loss: 1.1081 - val_mean_squared_error: 1.1081\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.41083 to 1.10812, saving model to temp/a4\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1742 - mean_squared_error: 2.1742 - val_loss: 1.3794 - val_mean_squared_error: 1.3794\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.10812\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1267 - mean_squared_error: 2.1267 - val_loss: 1.2780 - val_mean_squared_error: 1.2780\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.10812\n",
      "Epoch 00005: early stopping\n",
      "temp/a5\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5488 - mean_squared_error: 2.5488 - val_loss: 1.1990 - val_mean_squared_error: 1.1990\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.19899, saving model to temp/a5\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1906 - mean_squared_error: 2.1906 - val_loss: 1.1684 - val_mean_squared_error: 1.1684\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.19899 to 1.16837, saving model to temp/a5\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1529 - mean_squared_error: 2.1529 - val_loss: 1.1482 - val_mean_squared_error: 1.1482\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.16837 to 1.14822, saving model to temp/a5\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1336 - mean_squared_error: 2.1336 - val_loss: 1.3834 - val_mean_squared_error: 1.3834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss did not improve from 1.14822\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1010 - mean_squared_error: 2.1010 - val_loss: 1.1617 - val_mean_squared_error: 1.1617\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.14822\n",
      "Epoch 00005: early stopping\n",
      "temp/a6\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 220us/step - loss: 2.5660 - mean_squared_error: 2.5660 - val_loss: 1.3560 - val_mean_squared_error: 1.3560\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.35604, saving model to temp/a6\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2190 - mean_squared_error: 2.2190 - val_loss: 1.2447 - val_mean_squared_error: 1.2447\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.35604 to 1.24467, saving model to temp/a6\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 200us/step - loss: 2.1534 - mean_squared_error: 2.1534 - val_loss: 1.2615 - val_mean_squared_error: 1.2615\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.24467\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1397 - mean_squared_error: 2.1397 - val_loss: 1.1389 - val_mean_squared_error: 1.1389\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.24467 to 1.13894, saving model to temp/a6\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1215 - mean_squared_error: 2.1215 - val_loss: 1.0996 - val_mean_squared_error: 1.0996\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.13894 to 1.09960, saving model to temp/a6\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0989 - mean_squared_error: 2.0989 - val_loss: 1.1024 - val_mean_squared_error: 1.1024\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.09960\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1175 - mean_squared_error: 2.1175 - val_loss: 1.1413 - val_mean_squared_error: 1.1413\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.09960\n",
      "Epoch 00007: early stopping\n",
      "temp/a7\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 220us/step - loss: 2.4968 - mean_squared_error: 2.4968 - val_loss: 1.3260 - val_mean_squared_error: 1.3260\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.32602, saving model to temp/a7\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1911 - mean_squared_error: 2.1911 - val_loss: 1.5987 - val_mean_squared_error: 1.5987\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.32602\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1925 - mean_squared_error: 2.1925 - val_loss: 1.1768 - val_mean_squared_error: 1.1768\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.32602 to 1.17676, saving model to temp/a7\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1547 - mean_squared_error: 2.1547 - val_loss: 1.3851 - val_mean_squared_error: 1.3851\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.17676\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1143 - mean_squared_error: 2.1143 - val_loss: 1.1441 - val_mean_squared_error: 1.1441\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.17676 to 1.14407, saving model to temp/a7\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0752 - mean_squared_error: 2.0752 - val_loss: 1.1890 - val_mean_squared_error: 1.1890\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.14407\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.0500 - mean_squared_error: 2.0500 - val_loss: 1.2803 - val_mean_squared_error: 1.2803\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.14407\n",
      "Epoch 00007: early stopping\n",
      "temp/a8\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.6592 - mean_squared_error: 2.6592 - val_loss: 1.2404 - val_mean_squared_error: 1.2404\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.24041, saving model to temp/a8\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2147 - mean_squared_error: 2.2147 - val_loss: 1.1607 - val_mean_squared_error: 1.1607\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.24041 to 1.16067, saving model to temp/a8\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1456 - mean_squared_error: 2.1456 - val_loss: 1.3951 - val_mean_squared_error: 1.3951\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.16067\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1291 - mean_squared_error: 2.1291 - val_loss: 1.3445 - val_mean_squared_error: 1.3445\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.16067\n",
      "Epoch 00004: early stopping\n",
      "temp/a9\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.5618 - mean_squared_error: 2.5618 - val_loss: 1.3891 - val_mean_squared_error: 1.3891\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.38913, saving model to temp/a9\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2030 - mean_squared_error: 2.2030 - val_loss: 1.2912 - val_mean_squared_error: 1.2912\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.38913 to 1.29124, saving model to temp/a9\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1498 - mean_squared_error: 2.1498 - val_loss: 1.1512 - val_mean_squared_error: 1.1512\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.29124 to 1.15125, saving model to temp/a9\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1440 - mean_squared_error: 2.1440 - val_loss: 1.2203 - val_mean_squared_error: 1.2203\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.15125\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1393 - mean_squared_error: 2.1393 - val_loss: 1.6621 - val_mean_squared_error: 1.6621\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.15125\n",
      "Epoch 00005: early stopping\n",
      "temp/a10\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 2.6353 - mean_squared_error: 2.6353 - val_loss: 1.2551 - val_mean_squared_error: 1.2551\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.25514, saving model to temp/a10\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2210 - mean_squared_error: 2.2210 - val_loss: 1.2310 - val_mean_squared_error: 1.2310\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.25514 to 1.23096, saving model to temp/a10\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1727 - mean_squared_error: 2.1727 - val_loss: 1.1248 - val_mean_squared_error: 1.1248\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.23096 to 1.12475, saving model to temp/a10\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1435 - mean_squared_error: 2.1435 - val_loss: 1.3877 - val_mean_squared_error: 1.3877\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.12475\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1429 - mean_squared_error: 2.1429 - val_loss: 1.1308 - val_mean_squared_error: 1.1308\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.12475\n",
      "Epoch 00005: early stopping\n",
      "temp/a11\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 2.5524 - mean_squared_error: 2.5524 - val_loss: 1.2542 - val_mean_squared_error: 1.2542\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.25418, saving model to temp/a11\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2007 - mean_squared_error: 2.2007 - val_loss: 1.2109 - val_mean_squared_error: 1.2109\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.25418 to 1.21087, saving model to temp/a11\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1467 - mean_squared_error: 2.1467 - val_loss: 1.2098 - val_mean_squared_error: 1.2098\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.21087 to 1.20975, saving model to temp/a11\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0961 - mean_squared_error: 2.0961 - val_loss: 1.1623 - val_mean_squared_error: 1.1623\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.20975 to 1.16230, saving model to temp/a11\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1048 - mean_squared_error: 2.1048 - val_loss: 1.3600 - val_mean_squared_error: 1.3600\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.16230\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1144 - mean_squared_error: 2.1144 - val_loss: 1.1432 - val_mean_squared_error: 1.1432\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.16230 to 1.14321, saving model to temp/a11\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.0746 - mean_squared_error: 2.0746 - val_loss: 1.1278 - val_mean_squared_error: 1.1278\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.14321 to 1.12779, saving model to temp/a11\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0938 - mean_squared_error: 2.0938 - val_loss: 1.1047 - val_mean_squared_error: 1.1047\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.12779 to 1.10473, saving model to temp/a11\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0389 - mean_squared_error: 2.0389 - val_loss: 1.2014 - val_mean_squared_error: 1.2014\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.10473\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0299 - mean_squared_error: 2.0299 - val_loss: 1.1330 - val_mean_squared_error: 1.1330\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.10473\n",
      "Epoch 00010: early stopping\n",
      "temp/a12\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.5792 - mean_squared_error: 2.5792 - val_loss: 1.1955 - val_mean_squared_error: 1.1955\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.19548, saving model to temp/a12\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1831 - mean_squared_error: 2.1831 - val_loss: 1.3413 - val_mean_squared_error: 1.3413\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.19548\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1181 - mean_squared_error: 2.1181 - val_loss: 1.3436 - val_mean_squared_error: 1.3436\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.19548\n",
      "Epoch 00003: early stopping\n",
      "temp/a13\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.6195 - mean_squared_error: 2.6195 - val_loss: 1.3539 - val_mean_squared_error: 1.3539\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.35393, saving model to temp/a13\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2042 - mean_squared_error: 2.2042 - val_loss: 1.3178 - val_mean_squared_error: 1.3178\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.35393 to 1.31783, saving model to temp/a13\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1354 - mean_squared_error: 2.1354 - val_loss: 1.1509 - val_mean_squared_error: 1.1509\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.31783 to 1.15093, saving model to temp/a13\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1196 - mean_squared_error: 2.1196 - val_loss: 1.2911 - val_mean_squared_error: 1.2911\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.15093\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1160 - mean_squared_error: 2.1160 - val_loss: 1.1552 - val_mean_squared_error: 1.1552\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.15093\n",
      "Epoch 00005: early stopping\n",
      "temp/a14\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 221us/step - loss: 2.5955 - mean_squared_error: 2.5955 - val_loss: 1.1911 - val_mean_squared_error: 1.1911\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.19106, saving model to temp/a14\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2119 - mean_squared_error: 2.2119 - val_loss: 1.1682 - val_mean_squared_error: 1.1682\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.19106 to 1.16816, saving model to temp/a14\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1199 - mean_squared_error: 2.1199 - val_loss: 1.1010 - val_mean_squared_error: 1.1010\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.16816 to 1.10097, saving model to temp/a14\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1140 - mean_squared_error: 2.1140 - val_loss: 1.1768 - val_mean_squared_error: 1.1768\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.10097\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0727 - mean_squared_error: 2.0727 - val_loss: 1.2140 - val_mean_squared_error: 1.2140\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.10097\n",
      "Epoch 00005: early stopping\n",
      "temp/a15\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.5354 - mean_squared_error: 2.5354 - val_loss: 1.1749 - val_mean_squared_error: 1.1749\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.17487, saving model to temp/a15\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2032 - mean_squared_error: 2.2032 - val_loss: 1.3849 - val_mean_squared_error: 1.3849\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.17487\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1745 - mean_squared_error: 2.1745 - val_loss: 1.1402 - val_mean_squared_error: 1.1402\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.17487 to 1.14024, saving model to temp/a15\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1321 - mean_squared_error: 2.1321 - val_loss: 1.1361 - val_mean_squared_error: 1.1361\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.14024 to 1.13611, saving model to temp/a15\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0598 - mean_squared_error: 2.0598 - val_loss: 1.1444 - val_mean_squared_error: 1.1444\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.13611\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0750 - mean_squared_error: 2.0750 - val_loss: 1.4373 - val_mean_squared_error: 1.4373\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.13611\n",
      "Epoch 00006: early stopping\n",
      "temp/a16\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.5504 - mean_squared_error: 2.5504 - val_loss: 1.2844 - val_mean_squared_error: 1.2844\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.28435, saving model to temp/a16\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2541 - mean_squared_error: 2.2541 - val_loss: 1.1167 - val_mean_squared_error: 1.1167\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.28435 to 1.11673, saving model to temp/a16\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1575 - mean_squared_error: 2.1575 - val_loss: 1.2467 - val_mean_squared_error: 1.2467\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.11673\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1575 - mean_squared_error: 2.1575 - val_loss: 1.2202 - val_mean_squared_error: 1.2202\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.11673\n",
      "Epoch 00004: early stopping\n",
      "temp/a17\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5562 - mean_squared_error: 2.5562 - val_loss: 1.2880 - val_mean_squared_error: 1.2880\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.28800, saving model to temp/a17\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.2405 - mean_squared_error: 2.2405 - val_loss: 1.6117 - val_mean_squared_error: 1.6117\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.28800\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1690 - mean_squared_error: 2.1690 - val_loss: 1.3458 - val_mean_squared_error: 1.3458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss did not improve from 1.28800\n",
      "Epoch 00003: early stopping\n",
      "temp/a18\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5652 - mean_squared_error: 2.5652 - val_loss: 1.2625 - val_mean_squared_error: 1.2625\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.26249, saving model to temp/a18\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2535 - mean_squared_error: 2.2535 - val_loss: 1.2088 - val_mean_squared_error: 1.2088\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.26249 to 1.20877, saving model to temp/a18\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1512 - mean_squared_error: 2.1512 - val_loss: 1.1166 - val_mean_squared_error: 1.1166\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.20877 to 1.11657, saving model to temp/a18\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0989 - mean_squared_error: 2.0989 - val_loss: 1.3264 - val_mean_squared_error: 1.3264\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.11657\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0754 - mean_squared_error: 2.0754 - val_loss: 1.1694 - val_mean_squared_error: 1.1694\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.11657\n",
      "Epoch 00005: early stopping\n",
      "temp/a19\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 2.5594 - mean_squared_error: 2.5594 - val_loss: 1.5538 - val_mean_squared_error: 1.5538\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.55384, saving model to temp/a19\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1423 - mean_squared_error: 2.1423 - val_loss: 1.1786 - val_mean_squared_error: 1.1786\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.55384 to 1.17859, saving model to temp/a19\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1668 - mean_squared_error: 2.1668 - val_loss: 1.1340 - val_mean_squared_error: 1.1340\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.17859 to 1.13397, saving model to temp/a19\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1861 - mean_squared_error: 2.1861 - val_loss: 1.1909 - val_mean_squared_error: 1.1909\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.13397\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0545 - mean_squared_error: 2.0545 - val_loss: 1.1495 - val_mean_squared_error: 1.1495\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.13397\n",
      "Epoch 00005: early stopping\n",
      "temp/a20\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5737 - mean_squared_error: 2.5737 - val_loss: 1.9845 - val_mean_squared_error: 1.9845\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.98446, saving model to temp/a20\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2295 - mean_squared_error: 2.2295 - val_loss: 1.3439 - val_mean_squared_error: 1.3439\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.98446 to 1.34391, saving model to temp/a20\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1552 - mean_squared_error: 2.1552 - val_loss: 1.1050 - val_mean_squared_error: 1.1050\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.34391 to 1.10504, saving model to temp/a20\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1421 - mean_squared_error: 2.1421 - val_loss: 1.4096 - val_mean_squared_error: 1.4096\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.10504\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1008 - mean_squared_error: 2.1008 - val_loss: 1.1150 - val_mean_squared_error: 1.1150\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.10504\n",
      "Epoch 00005: early stopping\n",
      "temp/a21\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5786 - mean_squared_error: 2.5786 - val_loss: 1.7362 - val_mean_squared_error: 1.7362\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.73619, saving model to temp/a21\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2240 - mean_squared_error: 2.2240 - val_loss: 1.1395 - val_mean_squared_error: 1.1395\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.73619 to 1.13952, saving model to temp/a21\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 2.1489 - mean_squared_error: 2.1489 - val_loss: 1.1984 - val_mean_squared_error: 1.1984\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.13952\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1325 - mean_squared_error: 2.1325 - val_loss: 1.2269 - val_mean_squared_error: 1.2269\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.13952\n",
      "Epoch 00004: early stopping\n",
      "temp/a22\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 222us/step - loss: 2.4885 - mean_squared_error: 2.4885 - val_loss: 1.2016 - val_mean_squared_error: 1.2016\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.20163, saving model to temp/a22\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1850 - mean_squared_error: 2.1850 - val_loss: 1.3062 - val_mean_squared_error: 1.3062\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.20163\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1223 - mean_squared_error: 2.1223 - val_loss: 1.1395 - val_mean_squared_error: 1.1395\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.20163 to 1.13953, saving model to temp/a22\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1221 - mean_squared_error: 2.1221 - val_loss: 1.5223 - val_mean_squared_error: 1.5223\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.13953\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.0938 - mean_squared_error: 2.0938 - val_loss: 1.1508 - val_mean_squared_error: 1.1508\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.13953\n",
      "Epoch 00005: early stopping\n",
      "temp/a23\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.5558 - mean_squared_error: 2.5558 - val_loss: 1.5394 - val_mean_squared_error: 1.5394\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.53942, saving model to temp/a23\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1861 - mean_squared_error: 2.1861 - val_loss: 1.2748 - val_mean_squared_error: 1.2748\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.53942 to 1.27479, saving model to temp/a23\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1393 - mean_squared_error: 2.1393 - val_loss: 1.2464 - val_mean_squared_error: 1.2464\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.27479 to 1.24642, saving model to temp/a23\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1029 - mean_squared_error: 2.1029 - val_loss: 1.2393 - val_mean_squared_error: 1.2393\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.24642 to 1.23929, saving model to temp/a23\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0844 - mean_squared_error: 2.0844 - val_loss: 1.3397 - val_mean_squared_error: 1.3397\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.23929\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0960 - mean_squared_error: 2.0960 - val_loss: 1.1571 - val_mean_squared_error: 1.1571\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.23929 to 1.15705, saving model to temp/a23\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0959 - mean_squared_error: 2.0959 - val_loss: 1.1508 - val_mean_squared_error: 1.1508\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.15705 to 1.15078, saving model to temp/a23\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0428 - mean_squared_error: 2.0428 - val_loss: 1.1586 - val_mean_squared_error: 1.1586\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.15078\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0253 - mean_squared_error: 2.0253 - val_loss: 1.7940 - val_mean_squared_error: 1.7940\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.15078\n",
      "Epoch 00009: early stopping\n",
      "temp/a24\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.5431 - mean_squared_error: 2.5431 - val_loss: 1.2332 - val_mean_squared_error: 1.2332\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.23321, saving model to temp/a24\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1884 - mean_squared_error: 2.1884 - val_loss: 1.1454 - val_mean_squared_error: 1.1454\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.23321 to 1.14543, saving model to temp/a24\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2134 - mean_squared_error: 2.2134 - val_loss: 1.1375 - val_mean_squared_error: 1.1375\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.14543 to 1.13753, saving model to temp/a24\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1194 - mean_squared_error: 2.1194 - val_loss: 1.2038 - val_mean_squared_error: 1.2038\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.13753\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0736 - mean_squared_error: 2.0736 - val_loss: 1.3146 - val_mean_squared_error: 1.3146\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.13753\n",
      "Epoch 00005: early stopping\n",
      "temp/a25\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.5346 - mean_squared_error: 2.5346 - val_loss: 1.2830 - val_mean_squared_error: 1.2830\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.28303, saving model to temp/a25\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1985 - mean_squared_error: 2.1985 - val_loss: 1.1668 - val_mean_squared_error: 1.1668\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.28303 to 1.16678, saving model to temp/a25\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1139 - mean_squared_error: 2.1139 - val_loss: 1.4334 - val_mean_squared_error: 1.4334\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.16678\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1125 - mean_squared_error: 2.1125 - val_loss: 1.2237 - val_mean_squared_error: 1.2237\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.16678\n",
      "Epoch 00004: early stopping\n",
      "temp/a26\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.6007 - mean_squared_error: 2.6007 - val_loss: 1.3658 - val_mean_squared_error: 1.3658\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.36578, saving model to temp/a26\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2106 - mean_squared_error: 2.2106 - val_loss: 1.3083 - val_mean_squared_error: 1.3083\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.36578 to 1.30831, saving model to temp/a26\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1618 - mean_squared_error: 2.1618 - val_loss: 1.2054 - val_mean_squared_error: 1.2054\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.30831 to 1.20544, saving model to temp/a26\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1056 - mean_squared_error: 2.1056 - val_loss: 1.0930 - val_mean_squared_error: 1.0930\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.20544 to 1.09302, saving model to temp/a26\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1041 - mean_squared_error: 2.1041 - val_loss: 1.1800 - val_mean_squared_error: 1.1800\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.09302\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1278 - mean_squared_error: 2.1278 - val_loss: 1.1094 - val_mean_squared_error: 1.1094\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.09302\n",
      "Epoch 00006: early stopping\n",
      "temp/a27\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.5310 - mean_squared_error: 2.5310 - val_loss: 1.2123 - val_mean_squared_error: 1.2123\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.21235, saving model to temp/a27\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2190 - mean_squared_error: 2.2190 - val_loss: 1.1099 - val_mean_squared_error: 1.1099\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.21235 to 1.10992, saving model to temp/a27\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1423 - mean_squared_error: 2.1423 - val_loss: 1.2400 - val_mean_squared_error: 1.2400\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.10992\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1585 - mean_squared_error: 2.1585 - val_loss: 1.2053 - val_mean_squared_error: 1.2053\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.10992\n",
      "Epoch 00004: early stopping\n",
      "temp/a28\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.5873 - mean_squared_error: 2.5873 - val_loss: 1.2564 - val_mean_squared_error: 1.2564\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.25639, saving model to temp/a28\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2968 - mean_squared_error: 2.2968 - val_loss: 1.1454 - val_mean_squared_error: 1.1454\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.25639 to 1.14545, saving model to temp/a28\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1737 - mean_squared_error: 2.1737 - val_loss: 1.4861 - val_mean_squared_error: 1.4861\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.14545\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1304 - mean_squared_error: 2.1304 - val_loss: 1.2139 - val_mean_squared_error: 1.2139\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.14545\n",
      "Epoch 00004: early stopping\n",
      "temp/a29\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.6127 - mean_squared_error: 2.6127 - val_loss: 1.2961 - val_mean_squared_error: 1.2961\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.29615, saving model to temp/a29\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2087 - mean_squared_error: 2.2087 - val_loss: 1.5021 - val_mean_squared_error: 1.5021\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.29615\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1479 - mean_squared_error: 2.1479 - val_loss: 2.2218 - val_mean_squared_error: 2.2218\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.29615\n",
      "Epoch 00003: early stopping\n",
      "temp/a30\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.5627 - mean_squared_error: 2.5627 - val_loss: 1.6179 - val_mean_squared_error: 1.6179\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.61788, saving model to temp/a30\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1968 - mean_squared_error: 2.1968 - val_loss: 1.3640 - val_mean_squared_error: 1.3640\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.61788 to 1.36399, saving model to temp/a30\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1397 - mean_squared_error: 2.1397 - val_loss: 1.1080 - val_mean_squared_error: 1.1080\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.36399 to 1.10803, saving model to temp/a30\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1350 - mean_squared_error: 2.1350 - val_loss: 1.4634 - val_mean_squared_error: 1.4634\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.10803\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0637 - mean_squared_error: 2.0637 - val_loss: 1.4126 - val_mean_squared_error: 1.4126\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.10803\n",
      "Epoch 00005: early stopping\n",
      "temp/a31\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 11s 220us/step - loss: 2.5762 - mean_squared_error: 2.5762 - val_loss: 1.6264 - val_mean_squared_error: 1.6264\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.62642, saving model to temp/a31\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1868 - mean_squared_error: 2.1868 - val_loss: 1.1811 - val_mean_squared_error: 1.1811\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.62642 to 1.18109, saving model to temp/a31\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1743 - mean_squared_error: 2.1743 - val_loss: 1.4466 - val_mean_squared_error: 1.4466\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.18109\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1199 - mean_squared_error: 2.1199 - val_loss: 1.3109 - val_mean_squared_error: 1.3109\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.18109\n",
      "Epoch 00004: early stopping\n",
      "temp/a32\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.5746 - mean_squared_error: 2.5746 - val_loss: 1.2238 - val_mean_squared_error: 1.2238\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.22381, saving model to temp/a32\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1916 - mean_squared_error: 2.1916 - val_loss: 1.2501 - val_mean_squared_error: 1.2501\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.22381\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 2.1253 - mean_squared_error: 2.1253 - val_loss: 1.2618 - val_mean_squared_error: 1.2618\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.22381\n",
      "Epoch 00003: early stopping\n",
      "temp/a33\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5301 - mean_squared_error: 2.5301 - val_loss: 1.2708 - val_mean_squared_error: 1.2708\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.27078, saving model to temp/a33\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.2266 - mean_squared_error: 2.2266 - val_loss: 1.2610 - val_mean_squared_error: 1.2610\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.27078 to 1.26096, saving model to temp/a33\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1050 - mean_squared_error: 2.1050 - val_loss: 1.4575 - val_mean_squared_error: 1.4575\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.26096\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1373 - mean_squared_error: 2.1373 - val_loss: 1.1179 - val_mean_squared_error: 1.1179\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.26096 to 1.11791, saving model to temp/a33\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1142 - mean_squared_error: 2.1142 - val_loss: 1.1842 - val_mean_squared_error: 1.1842\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.11791\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0646 - mean_squared_error: 2.0646 - val_loss: 1.2265 - val_mean_squared_error: 1.2265\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.11791\n",
      "Epoch 00006: early stopping\n",
      "temp/a34\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.5646 - mean_squared_error: 2.5646 - val_loss: 1.2377 - val_mean_squared_error: 1.2377\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.23765, saving model to temp/a34\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2055 - mean_squared_error: 2.2055 - val_loss: 1.2792 - val_mean_squared_error: 1.2792\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.23765\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1927 - mean_squared_error: 2.1927 - val_loss: 1.3194 - val_mean_squared_error: 1.3194\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.23765\n",
      "Epoch 00003: early stopping\n",
      "temp/a35\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5569 - mean_squared_error: 2.5569 - val_loss: 1.6212 - val_mean_squared_error: 1.6212\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.62125, saving model to temp/a35\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.2107 - mean_squared_error: 2.2107 - val_loss: 1.1131 - val_mean_squared_error: 1.1131\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.62125 to 1.11311, saving model to temp/a35\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 2.1533 - mean_squared_error: 2.1533 - val_loss: 1.2346 - val_mean_squared_error: 1.2346\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.11311\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1536 - mean_squared_error: 2.1536 - val_loss: 1.2274 - val_mean_squared_error: 1.2274\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.11311\n",
      "Epoch 00004: early stopping\n",
      "temp/a36\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.6289 - mean_squared_error: 2.6289 - val_loss: 1.6876 - val_mean_squared_error: 1.6876\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.68764, saving model to temp/a36\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.2578 - mean_squared_error: 2.2578 - val_loss: 1.3287 - val_mean_squared_error: 1.3287\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.68764 to 1.32870, saving model to temp/a36\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1468 - mean_squared_error: 2.1468 - val_loss: 1.1186 - val_mean_squared_error: 1.1186\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.32870 to 1.11860, saving model to temp/a36\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1382 - mean_squared_error: 2.1382 - val_loss: 1.1166 - val_mean_squared_error: 1.1166\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.11860 to 1.11664, saving model to temp/a36\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1118 - mean_squared_error: 2.1118 - val_loss: 1.1215 - val_mean_squared_error: 1.1215\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.11664\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0701 - mean_squared_error: 2.0701 - val_loss: 1.4640 - val_mean_squared_error: 1.4640\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.11664\n",
      "Epoch 00006: early stopping\n",
      "temp/a37\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5835 - mean_squared_error: 2.5835 - val_loss: 1.2433 - val_mean_squared_error: 1.2433\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.24331, saving model to temp/a37\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1989 - mean_squared_error: 2.1989 - val_loss: 1.3159 - val_mean_squared_error: 1.3159\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.24331\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1233 - mean_squared_error: 2.1233 - val_loss: 1.1195 - val_mean_squared_error: 1.1195\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.24331 to 1.11954, saving model to temp/a37\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1174 - mean_squared_error: 2.1174 - val_loss: 1.1628 - val_mean_squared_error: 1.1628\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.11954\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1110 - mean_squared_error: 2.1110 - val_loss: 1.2029 - val_mean_squared_error: 1.2029\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.11954\n",
      "Epoch 00005: early stopping\n",
      "temp/a38\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 220us/step - loss: 2.5383 - mean_squared_error: 2.5383 - val_loss: 1.2196 - val_mean_squared_error: 1.2196\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.21955, saving model to temp/a38\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2404 - mean_squared_error: 2.2404 - val_loss: 1.2232 - val_mean_squared_error: 1.2232\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.21955\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1205 - mean_squared_error: 2.1205 - val_loss: 1.3378 - val_mean_squared_error: 1.3378\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.21955\n",
      "Epoch 00003: early stopping\n",
      "temp/a39\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.5586 - mean_squared_error: 2.5586 - val_loss: 1.1756 - val_mean_squared_error: 1.1756\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.17560, saving model to temp/a39\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1920 - mean_squared_error: 2.1920 - val_loss: 1.2114 - val_mean_squared_error: 1.2114\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.17560\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0818 - mean_squared_error: 2.0818 - val_loss: 1.9785 - val_mean_squared_error: 1.9785\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.17560\n",
      "Epoch 00003: early stopping\n",
      "temp/a40\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.6544 - mean_squared_error: 2.6544 - val_loss: 1.3273 - val_mean_squared_error: 1.3273\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.32730, saving model to temp/a40\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1769 - mean_squared_error: 2.1769 - val_loss: 1.3006 - val_mean_squared_error: 1.3006\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.32730 to 1.30057, saving model to temp/a40\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1281 - mean_squared_error: 2.1281 - val_loss: 1.1358 - val_mean_squared_error: 1.1358\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.30057 to 1.13577, saving model to temp/a40\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1147 - mean_squared_error: 2.1147 - val_loss: 1.3458 - val_mean_squared_error: 1.3458\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.13577\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.0891 - mean_squared_error: 2.0891 - val_loss: 1.1848 - val_mean_squared_error: 1.1848\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.13577\n",
      "Epoch 00005: early stopping\n",
      "temp/a41\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.5201 - mean_squared_error: 2.5201 - val_loss: 1.2863 - val_mean_squared_error: 1.2863\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.28633, saving model to temp/a41\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2004 - mean_squared_error: 2.2004 - val_loss: 1.1322 - val_mean_squared_error: 1.1322\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.28633 to 1.13224, saving model to temp/a41\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1614 - mean_squared_error: 2.1614 - val_loss: 1.2062 - val_mean_squared_error: 1.2062\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.13224\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1548 - mean_squared_error: 2.1548 - val_loss: 1.2088 - val_mean_squared_error: 1.2088\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.13224\n",
      "Epoch 00004: early stopping\n",
      "temp/a42\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.5690 - mean_squared_error: 2.5690 - val_loss: 1.2718 - val_mean_squared_error: 1.2718\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.27181, saving model to temp/a42\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2476 - mean_squared_error: 2.2476 - val_loss: 1.1522 - val_mean_squared_error: 1.1522\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.27181 to 1.15224, saving model to temp/a42\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1816 - mean_squared_error: 2.1816 - val_loss: 1.2051 - val_mean_squared_error: 1.2051\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.15224\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1862 - mean_squared_error: 2.1862 - val_loss: 1.1350 - val_mean_squared_error: 1.1350\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.15224 to 1.13503, saving model to temp/a42\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1440 - mean_squared_error: 2.1440 - val_loss: 1.1568 - val_mean_squared_error: 1.1568\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.13503\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1296 - mean_squared_error: 2.1296 - val_loss: 1.2081 - val_mean_squared_error: 1.2081\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.13503\n",
      "Epoch 00006: early stopping\n",
      "temp/a43\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5731 - mean_squared_error: 2.5731 - val_loss: 1.4582 - val_mean_squared_error: 1.4582\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.45819, saving model to temp/a43\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2377 - mean_squared_error: 2.2377 - val_loss: 1.2348 - val_mean_squared_error: 1.2348\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.45819 to 1.23475, saving model to temp/a43\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1507 - mean_squared_error: 2.1507 - val_loss: 1.8091 - val_mean_squared_error: 1.8091\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.23475\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0983 - mean_squared_error: 2.0983 - val_loss: 1.1892 - val_mean_squared_error: 1.1892\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.23475 to 1.18925, saving model to temp/a43\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0849 - mean_squared_error: 2.0849 - val_loss: 1.1755 - val_mean_squared_error: 1.1755\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.18925 to 1.17551, saving model to temp/a43\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0981 - mean_squared_error: 2.0981 - val_loss: 1.4996 - val_mean_squared_error: 1.4996\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.17551\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0517 - mean_squared_error: 2.0517 - val_loss: 1.1295 - val_mean_squared_error: 1.1295\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.17551 to 1.12950, saving model to temp/a43\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0687 - mean_squared_error: 2.0687 - val_loss: 1.2485 - val_mean_squared_error: 1.2485\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.12950\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0585 - mean_squared_error: 2.0585 - val_loss: 1.1502 - val_mean_squared_error: 1.1502\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.12950\n",
      "Epoch 00009: early stopping\n",
      "temp/a44\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5887 - mean_squared_error: 2.5887 - val_loss: 1.2367 - val_mean_squared_error: 1.2367\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.23671, saving model to temp/a44\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2057 - mean_squared_error: 2.2057 - val_loss: 1.1689 - val_mean_squared_error: 1.1689\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.23671 to 1.16886, saving model to temp/a44\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1746 - mean_squared_error: 2.1746 - val_loss: 1.1737 - val_mean_squared_error: 1.1737\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.16886\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1313 - mean_squared_error: 2.1313 - val_loss: 1.1895 - val_mean_squared_error: 1.1895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss did not improve from 1.16886\n",
      "Epoch 00004: early stopping\n",
      "temp/a45\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.6537 - mean_squared_error: 2.6537 - val_loss: 1.2159 - val_mean_squared_error: 1.2159\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.21593, saving model to temp/a45\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1777 - mean_squared_error: 2.1777 - val_loss: 1.1424 - val_mean_squared_error: 1.1424\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.21593 to 1.14237, saving model to temp/a45\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1466 - mean_squared_error: 2.1466 - val_loss: 1.1488 - val_mean_squared_error: 1.1488\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.14237\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1134 - mean_squared_error: 2.1134 - val_loss: 1.4346 - val_mean_squared_error: 1.4346\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.14237\n",
      "Epoch 00004: early stopping\n",
      "temp/a46\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 220us/step - loss: 2.5603 - mean_squared_error: 2.5603 - val_loss: 1.2521 - val_mean_squared_error: 1.2521\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.25211, saving model to temp/a46\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1904 - mean_squared_error: 2.1904 - val_loss: 1.2583 - val_mean_squared_error: 1.2583\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.25211\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1181 - mean_squared_error: 2.1181 - val_loss: 1.3279 - val_mean_squared_error: 1.3279\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.25211\n",
      "Epoch 00003: early stopping\n",
      "temp/a47\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 221us/step - loss: 2.5073 - mean_squared_error: 2.5073 - val_loss: 1.4242 - val_mean_squared_error: 1.4242\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.42419, saving model to temp/a47\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 2.2172 - mean_squared_error: 2.2172 - val_loss: 1.7516 - val_mean_squared_error: 1.7516\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.42419\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2082 - mean_squared_error: 2.2082 - val_loss: 1.3074 - val_mean_squared_error: 1.3074\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.42419 to 1.30741, saving model to temp/a47\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1271 - mean_squared_error: 2.1271 - val_loss: 1.1730 - val_mean_squared_error: 1.1730\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.30741 to 1.17299, saving model to temp/a47\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0467 - mean_squared_error: 2.0467 - val_loss: 1.3965 - val_mean_squared_error: 1.3965\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.17299\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1070 - mean_squared_error: 2.1070 - val_loss: 1.1251 - val_mean_squared_error: 1.1251\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.17299 to 1.12513, saving model to temp/a47\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0376 - mean_squared_error: 2.0376 - val_loss: 1.3880 - val_mean_squared_error: 1.3880\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.12513\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 1.9803 - mean_squared_error: 1.9803 - val_loss: 1.1387 - val_mean_squared_error: 1.1387\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.12513\n",
      "Epoch 00008: early stopping\n",
      "temp/a48\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 220us/step - loss: 2.5206 - mean_squared_error: 2.5206 - val_loss: 1.3090 - val_mean_squared_error: 1.3090\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.30904, saving model to temp/a48\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2303 - mean_squared_error: 2.2303 - val_loss: 1.4556 - val_mean_squared_error: 1.4556\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.30904\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1268 - mean_squared_error: 2.1268 - val_loss: 1.1534 - val_mean_squared_error: 1.1534\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.30904 to 1.15343, saving model to temp/a48\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.0525 - mean_squared_error: 2.0525 - val_loss: 1.1824 - val_mean_squared_error: 1.1824\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.15343\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0808 - mean_squared_error: 2.0808 - val_loss: 1.1299 - val_mean_squared_error: 1.1299\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.15343 to 1.12995, saving model to temp/a48\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0525 - mean_squared_error: 2.0525 - val_loss: 1.1859 - val_mean_squared_error: 1.1859\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.12995\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0870 - mean_squared_error: 2.0870 - val_loss: 1.4387 - val_mean_squared_error: 1.4387\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.12995\n",
      "Epoch 00007: early stopping\n",
      "temp/a49\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5591 - mean_squared_error: 2.5591 - val_loss: 1.2628 - val_mean_squared_error: 1.2628\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.26280, saving model to temp/a49\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1937 - mean_squared_error: 2.1937 - val_loss: 1.2387 - val_mean_squared_error: 1.2387\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.26280 to 1.23869, saving model to temp/a49\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1684 - mean_squared_error: 2.1684 - val_loss: 1.2214 - val_mean_squared_error: 1.2214\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.23869 to 1.22137, saving model to temp/a49\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1044 - mean_squared_error: 2.1044 - val_loss: 1.7678 - val_mean_squared_error: 1.7678\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.22137\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0921 - mean_squared_error: 2.0921 - val_loss: 1.8512 - val_mean_squared_error: 1.8512\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.22137\n",
      "Epoch 00005: early stopping\n",
      "temp/a50\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.5611 - mean_squared_error: 2.5611 - val_loss: 1.2224 - val_mean_squared_error: 1.2224\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.22236, saving model to temp/a50\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2209 - mean_squared_error: 2.2209 - val_loss: 1.1276 - val_mean_squared_error: 1.1276\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.22236 to 1.12757, saving model to temp/a50\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1627 - mean_squared_error: 2.1627 - val_loss: 1.4397 - val_mean_squared_error: 1.4397\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.12757\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1351 - mean_squared_error: 2.1351 - val_loss: 1.3064 - val_mean_squared_error: 1.3064\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.12757\n",
      "Epoch 00004: early stopping\n",
      "temp/a51\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.5158 - mean_squared_error: 2.5158 - val_loss: 1.2542 - val_mean_squared_error: 1.2542\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.25424, saving model to temp/a51\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1975 - mean_squared_error: 2.1975 - val_loss: 1.1545 - val_mean_squared_error: 1.1545\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.25424 to 1.15450, saving model to temp/a51\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1031 - mean_squared_error: 2.1031 - val_loss: 2.0679 - val_mean_squared_error: 2.0679\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.15450\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1308 - mean_squared_error: 2.1308 - val_loss: 1.2796 - val_mean_squared_error: 1.2796\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.15450\n",
      "Epoch 00004: early stopping\n",
      "temp/a52\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 220us/step - loss: 2.6023 - mean_squared_error: 2.6023 - val_loss: 1.5689 - val_mean_squared_error: 1.5689\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.56894, saving model to temp/a52\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2270 - mean_squared_error: 2.2270 - val_loss: 1.4306 - val_mean_squared_error: 1.4306\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.56894 to 1.43058, saving model to temp/a52\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2021 - mean_squared_error: 2.2021 - val_loss: 1.1209 - val_mean_squared_error: 1.1209\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.43058 to 1.12093, saving model to temp/a52\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1418 - mean_squared_error: 2.1418 - val_loss: 1.1336 - val_mean_squared_error: 1.1336\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.12093\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1074 - mean_squared_error: 2.1074 - val_loss: 1.2640 - val_mean_squared_error: 1.2640\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.12093\n",
      "Epoch 00005: early stopping\n",
      "temp/a53\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5605 - mean_squared_error: 2.5605 - val_loss: 1.2789 - val_mean_squared_error: 1.2789\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.27886, saving model to temp/a53\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2532 - mean_squared_error: 2.2532 - val_loss: 1.4203 - val_mean_squared_error: 1.4203\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.27886\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1508 - mean_squared_error: 2.1508 - val_loss: 1.3243 - val_mean_squared_error: 1.3243\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.27886\n",
      "Epoch 00003: early stopping\n",
      "temp/a54\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.5993 - mean_squared_error: 2.5993 - val_loss: 1.4268 - val_mean_squared_error: 1.4268\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.42676, saving model to temp/a54\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1905 - mean_squared_error: 2.1905 - val_loss: 1.8974 - val_mean_squared_error: 1.8974\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.42676\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1835 - mean_squared_error: 2.1835 - val_loss: 1.4076 - val_mean_squared_error: 1.4076\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.42676 to 1.40757, saving model to temp/a54\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1351 - mean_squared_error: 2.1351 - val_loss: 1.1686 - val_mean_squared_error: 1.1686\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.40757 to 1.16857, saving model to temp/a54\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1185 - mean_squared_error: 2.1185 - val_loss: 1.1563 - val_mean_squared_error: 1.1563\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.16857 to 1.15630, saving model to temp/a54\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0770 - mean_squared_error: 2.0770 - val_loss: 1.1163 - val_mean_squared_error: 1.1163\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.15630 to 1.11628, saving model to temp/a54\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0841 - mean_squared_error: 2.0841 - val_loss: 1.1539 - val_mean_squared_error: 1.1539\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.11628\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0469 - mean_squared_error: 2.0469 - val_loss: 1.5507 - val_mean_squared_error: 1.5507\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.11628\n",
      "Epoch 00008: early stopping\n",
      "temp/a55\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 221us/step - loss: 2.5472 - mean_squared_error: 2.5472 - val_loss: 1.5304 - val_mean_squared_error: 1.5304\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.53045, saving model to temp/a55\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2208 - mean_squared_error: 2.2208 - val_loss: 2.1758 - val_mean_squared_error: 2.1758\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.53045\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1126 - mean_squared_error: 2.1126 - val_loss: 1.1449 - val_mean_squared_error: 1.1449\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.53045 to 1.14488, saving model to temp/a55\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1526 - mean_squared_error: 2.1526 - val_loss: 1.1502 - val_mean_squared_error: 1.1502\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.14488\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1262 - mean_squared_error: 2.1262 - val_loss: 1.5440 - val_mean_squared_error: 1.5440\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.14488\n",
      "Epoch 00005: early stopping\n",
      "temp/a56\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5493 - mean_squared_error: 2.5493 - val_loss: 1.4687 - val_mean_squared_error: 1.4687\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.46865, saving model to temp/a56\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1907 - mean_squared_error: 2.1907 - val_loss: 1.3635 - val_mean_squared_error: 1.3635\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.46865 to 1.36354, saving model to temp/a56\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1491 - mean_squared_error: 2.1491 - val_loss: 1.1269 - val_mean_squared_error: 1.1269\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.36354 to 1.12694, saving model to temp/a56\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1245 - mean_squared_error: 2.1245 - val_loss: 1.9585 - val_mean_squared_error: 1.9585\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.12694\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1069 - mean_squared_error: 2.1069 - val_loss: 1.5601 - val_mean_squared_error: 1.5601\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.12694\n",
      "Epoch 00005: early stopping\n",
      "temp/a57\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.5020 - mean_squared_error: 2.5020 - val_loss: 1.2128 - val_mean_squared_error: 1.2128\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.21277, saving model to temp/a57\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2584 - mean_squared_error: 2.2584 - val_loss: 1.2049 - val_mean_squared_error: 1.2049\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.21277 to 1.20487, saving model to temp/a57\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1526 - mean_squared_error: 2.1526 - val_loss: 1.5934 - val_mean_squared_error: 1.5934\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.20487\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1578 - mean_squared_error: 2.1578 - val_loss: 1.1651 - val_mean_squared_error: 1.1651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss improved from 1.20487 to 1.16510, saving model to temp/a57\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0928 - mean_squared_error: 2.0928 - val_loss: 1.1645 - val_mean_squared_error: 1.1645\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.16510 to 1.16453, saving model to temp/a57\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1100 - mean_squared_error: 2.1100 - val_loss: 1.1360 - val_mean_squared_error: 1.1360\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.16453 to 1.13597, saving model to temp/a57\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0581 - mean_squared_error: 2.0581 - val_loss: 1.7513 - val_mean_squared_error: 1.7513\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.13597\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0485 - mean_squared_error: 2.0485 - val_loss: 1.1719 - val_mean_squared_error: 1.1719\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.13597\n",
      "Epoch 00008: early stopping\n",
      "temp/a58\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.5672 - mean_squared_error: 2.5672 - val_loss: 1.3492 - val_mean_squared_error: 1.3492\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.34924, saving model to temp/a58\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2182 - mean_squared_error: 2.2182 - val_loss: 1.2552 - val_mean_squared_error: 1.2552\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.34924 to 1.25520, saving model to temp/a58\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1089 - mean_squared_error: 2.1089 - val_loss: 1.7221 - val_mean_squared_error: 1.7221\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.25520\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0954 - mean_squared_error: 2.0954 - val_loss: 1.3083 - val_mean_squared_error: 1.3083\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.25520\n",
      "Epoch 00004: early stopping\n",
      "temp/a59\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5587 - mean_squared_error: 2.5587 - val_loss: 1.6916 - val_mean_squared_error: 1.6916\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.69159, saving model to temp/a59\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1634 - mean_squared_error: 2.1634 - val_loss: 1.1706 - val_mean_squared_error: 1.1706\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.69159 to 1.17061, saving model to temp/a59\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1871 - mean_squared_error: 2.1871 - val_loss: 1.3556 - val_mean_squared_error: 1.3556\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.17061\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1153 - mean_squared_error: 2.1153 - val_loss: 1.3132 - val_mean_squared_error: 1.3132\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.17061\n",
      "Epoch 00004: early stopping\n",
      "temp/a60\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.5621 - mean_squared_error: 2.5621 - val_loss: 1.2915 - val_mean_squared_error: 1.2915\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.29149, saving model to temp/a60\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1904 - mean_squared_error: 2.1904 - val_loss: 1.1678 - val_mean_squared_error: 1.1678\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.29149 to 1.16781, saving model to temp/a60\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1344 - mean_squared_error: 2.1344 - val_loss: 1.1935 - val_mean_squared_error: 1.1935\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.16781\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1262 - mean_squared_error: 2.1262 - val_loss: 1.1289 - val_mean_squared_error: 1.1289\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.16781 to 1.12887, saving model to temp/a60\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1262 - mean_squared_error: 2.1262 - val_loss: 1.2351 - val_mean_squared_error: 1.2351\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.12887\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0450 - mean_squared_error: 2.0450 - val_loss: 1.1397 - val_mean_squared_error: 1.1397\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.12887\n",
      "Epoch 00006: early stopping\n",
      "temp/a61\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.5652 - mean_squared_error: 2.5652 - val_loss: 1.6416 - val_mean_squared_error: 1.6416\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.64157, saving model to temp/a61\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.2048 - mean_squared_error: 2.2048 - val_loss: 1.4170 - val_mean_squared_error: 1.4170\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.64157 to 1.41696, saving model to temp/a61\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1550 - mean_squared_error: 2.1550 - val_loss: 1.3897 - val_mean_squared_error: 1.3897\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.41696 to 1.38970, saving model to temp/a61\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1026 - mean_squared_error: 2.1026 - val_loss: 1.3629 - val_mean_squared_error: 1.3629\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.38970 to 1.36290, saving model to temp/a61\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1144 - mean_squared_error: 2.1144 - val_loss: 1.1800 - val_mean_squared_error: 1.1800\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.36290 to 1.18001, saving model to temp/a61\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0665 - mean_squared_error: 2.0665 - val_loss: 1.1342 - val_mean_squared_error: 1.1342\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.18001 to 1.13416, saving model to temp/a61\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0707 - mean_squared_error: 2.0707 - val_loss: 1.4386 - val_mean_squared_error: 1.4386\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.13416\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.0244 - mean_squared_error: 2.0244 - val_loss: 1.5217 - val_mean_squared_error: 1.5217\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.13416\n",
      "Epoch 00008: early stopping\n",
      "temp/a62\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.6253 - mean_squared_error: 2.6253 - val_loss: 1.8285 - val_mean_squared_error: 1.8285\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.82854, saving model to temp/a62\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2355 - mean_squared_error: 2.2355 - val_loss: 1.1736 - val_mean_squared_error: 1.1736\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.82854 to 1.17357, saving model to temp/a62\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1189 - mean_squared_error: 2.1189 - val_loss: 1.2544 - val_mean_squared_error: 1.2544\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.17357\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1313 - mean_squared_error: 2.1313 - val_loss: 1.2792 - val_mean_squared_error: 1.2792\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.17357\n",
      "Epoch 00004: early stopping\n",
      "temp/a63\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.5904 - mean_squared_error: 2.5904 - val_loss: 1.2516 - val_mean_squared_error: 1.2516\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.25158, saving model to temp/a63\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2571 - mean_squared_error: 2.2571 - val_loss: 1.2948 - val_mean_squared_error: 1.2948\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.25158\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1628 - mean_squared_error: 2.1628 - val_loss: 1.1493 - val_mean_squared_error: 1.1493\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.25158 to 1.14932, saving model to temp/a63\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1113 - mean_squared_error: 2.1113 - val_loss: 1.5681 - val_mean_squared_error: 1.5681\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.14932\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0939 - mean_squared_error: 2.0939 - val_loss: 1.1339 - val_mean_squared_error: 1.1339\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.14932 to 1.13389, saving model to temp/a63\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0182 - mean_squared_error: 2.0182 - val_loss: 1.7785 - val_mean_squared_error: 1.7785\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.13389\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.0197 - mean_squared_error: 2.0197 - val_loss: 1.1182 - val_mean_squared_error: 1.1182\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.13389 to 1.11815, saving model to temp/a63\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0122 - mean_squared_error: 2.0122 - val_loss: 1.2542 - val_mean_squared_error: 1.2542\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.11815\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0107 - mean_squared_error: 2.0107 - val_loss: 1.0960 - val_mean_squared_error: 1.0960\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.11815 to 1.09600, saving model to temp/a63\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1018 - mean_squared_error: 2.1018 - val_loss: 1.2293 - val_mean_squared_error: 1.2293\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.09600\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 2.0490 - mean_squared_error: 2.0490 - val_loss: 1.1442 - val_mean_squared_error: 1.1442\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.09600\n",
      "Epoch 00011: early stopping\n",
      "temp/a64\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.5556 - mean_squared_error: 2.5556 - val_loss: 1.1943 - val_mean_squared_error: 1.1943\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.19433, saving model to temp/a64\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2123 - mean_squared_error: 2.2123 - val_loss: 1.4538 - val_mean_squared_error: 1.4538\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.19433\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 2.1454 - mean_squared_error: 2.1454 - val_loss: 1.1391 - val_mean_squared_error: 1.1391\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.19433 to 1.13910, saving model to temp/a64\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1646 - mean_squared_error: 2.1646 - val_loss: 1.1275 - val_mean_squared_error: 1.1275\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.13910 to 1.12749, saving model to temp/a64\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1579 - mean_squared_error: 2.1579 - val_loss: 1.4011 - val_mean_squared_error: 1.4011\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.12749\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0931 - mean_squared_error: 2.0931 - val_loss: 1.3230 - val_mean_squared_error: 1.3230\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.12749\n",
      "Epoch 00006: early stopping\n",
      "temp/a65\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.5793 - mean_squared_error: 2.5793 - val_loss: 1.6475 - val_mean_squared_error: 1.6475\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.64755, saving model to temp/a65\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1928 - mean_squared_error: 2.1928 - val_loss: 1.1443 - val_mean_squared_error: 1.1443\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.64755 to 1.14428, saving model to temp/a65\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1821 - mean_squared_error: 2.1821 - val_loss: 1.1986 - val_mean_squared_error: 1.1986\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.14428\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1297 - mean_squared_error: 2.1297 - val_loss: 1.2557 - val_mean_squared_error: 1.2557\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.14428\n",
      "Epoch 00004: early stopping\n",
      "temp/a66\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 2.5352 - mean_squared_error: 2.5352 - val_loss: 1.2218 - val_mean_squared_error: 1.2218\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.22183, saving model to temp/a66\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1892 - mean_squared_error: 2.1892 - val_loss: 1.1744 - val_mean_squared_error: 1.1744\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.22183 to 1.17436, saving model to temp/a66\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1512 - mean_squared_error: 2.1512 - val_loss: 1.3168 - val_mean_squared_error: 1.3168\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.17436\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1507 - mean_squared_error: 2.1507 - val_loss: 1.4045 - val_mean_squared_error: 1.4045\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.17436\n",
      "Epoch 00004: early stopping\n",
      "temp/a67\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.5728 - mean_squared_error: 2.5728 - val_loss: 1.4332 - val_mean_squared_error: 1.4332\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.43317, saving model to temp/a67\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2331 - mean_squared_error: 2.2331 - val_loss: 1.3615 - val_mean_squared_error: 1.3615\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.43317 to 1.36152, saving model to temp/a67\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1270 - mean_squared_error: 2.1270 - val_loss: 1.1100 - val_mean_squared_error: 1.1100\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.36152 to 1.11004, saving model to temp/a67\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1437 - mean_squared_error: 2.1437 - val_loss: 1.3122 - val_mean_squared_error: 1.3122\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.11004\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1156 - mean_squared_error: 2.1156 - val_loss: 1.1579 - val_mean_squared_error: 1.1579\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.11004\n",
      "Epoch 00005: early stopping\n",
      "temp/a68\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 2.5574 - mean_squared_error: 2.5574 - val_loss: 1.5093 - val_mean_squared_error: 1.5093\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.50931, saving model to temp/a68\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 200us/step - loss: 2.1883 - mean_squared_error: 2.1883 - val_loss: 1.4089 - val_mean_squared_error: 1.4089\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.50931 to 1.40888, saving model to temp/a68\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1361 - mean_squared_error: 2.1361 - val_loss: 1.1176 - val_mean_squared_error: 1.1176\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.40888 to 1.11762, saving model to temp/a68\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1416 - mean_squared_error: 2.1416 - val_loss: 1.2388 - val_mean_squared_error: 1.2388\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.11762\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0734 - mean_squared_error: 2.0734 - val_loss: 1.1511 - val_mean_squared_error: 1.1511\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.11762\n",
      "Epoch 00005: early stopping\n",
      "temp/a69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.5425 - mean_squared_error: 2.5425 - val_loss: 1.3128 - val_mean_squared_error: 1.3128\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.31282, saving model to temp/a69\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2599 - mean_squared_error: 2.2599 - val_loss: 1.1432 - val_mean_squared_error: 1.1432\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.31282 to 1.14322, saving model to temp/a69\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2037 - mean_squared_error: 2.2037 - val_loss: 1.2540 - val_mean_squared_error: 1.2540\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.14322\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1367 - mean_squared_error: 2.1367 - val_loss: 1.2479 - val_mean_squared_error: 1.2479\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.14322\n",
      "Epoch 00004: early stopping\n",
      "temp/a70\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 220us/step - loss: 2.5309 - mean_squared_error: 2.5309 - val_loss: 1.1835 - val_mean_squared_error: 1.1835\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.18347, saving model to temp/a70\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1761 - mean_squared_error: 2.1761 - val_loss: 1.6904 - val_mean_squared_error: 1.6904\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.18347\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1710 - mean_squared_error: 2.1710 - val_loss: 1.3269 - val_mean_squared_error: 1.3269\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.18347\n",
      "Epoch 00003: early stopping\n",
      "temp/a71\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.5586 - mean_squared_error: 2.5586 - val_loss: 2.1133 - val_mean_squared_error: 2.1133\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.11333, saving model to temp/a71\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1799 - mean_squared_error: 2.1799 - val_loss: 1.3976 - val_mean_squared_error: 1.3976\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.11333 to 1.39762, saving model to temp/a71\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1141 - mean_squared_error: 2.1141 - val_loss: 1.1229 - val_mean_squared_error: 1.1229\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.39762 to 1.12294, saving model to temp/a71\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1231 - mean_squared_error: 2.1231 - val_loss: 1.5046 - val_mean_squared_error: 1.5046\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.12294\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1253 - mean_squared_error: 2.1253 - val_loss: 1.3671 - val_mean_squared_error: 1.3671\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.12294\n",
      "Epoch 00005: early stopping\n",
      "temp/a72\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.5253 - mean_squared_error: 2.5253 - val_loss: 1.1831 - val_mean_squared_error: 1.1831\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.18311, saving model to temp/a72\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1653 - mean_squared_error: 2.1653 - val_loss: 1.4067 - val_mean_squared_error: 1.4067\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.18311\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1646 - mean_squared_error: 2.1646 - val_loss: 1.2968 - val_mean_squared_error: 1.2968\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.18311\n",
      "Epoch 00003: early stopping\n",
      "temp/a73\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.5032 - mean_squared_error: 2.5032 - val_loss: 1.1772 - val_mean_squared_error: 1.1772\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.17718, saving model to temp/a73\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.2316 - mean_squared_error: 2.2316 - val_loss: 1.2605 - val_mean_squared_error: 1.2605\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.17718\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1811 - mean_squared_error: 2.1811 - val_loss: 1.1326 - val_mean_squared_error: 1.1326\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.17718 to 1.13256, saving model to temp/a73\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.0780 - mean_squared_error: 2.0780 - val_loss: 1.2339 - val_mean_squared_error: 1.2339\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.13256\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1397 - mean_squared_error: 2.1397 - val_loss: 1.2681 - val_mean_squared_error: 1.2681\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.13256\n",
      "Epoch 00005: early stopping\n",
      "temp/a74\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5510 - mean_squared_error: 2.5510 - val_loss: 1.6031 - val_mean_squared_error: 1.6031\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.60306, saving model to temp/a74\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2508 - mean_squared_error: 2.2508 - val_loss: 1.1342 - val_mean_squared_error: 1.1342\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.60306 to 1.13417, saving model to temp/a74\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1173 - mean_squared_error: 2.1173 - val_loss: 1.1221 - val_mean_squared_error: 1.1221\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.13417 to 1.12212, saving model to temp/a74\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1272 - mean_squared_error: 2.1272 - val_loss: 1.1151 - val_mean_squared_error: 1.1151\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.12212 to 1.11513, saving model to temp/a74\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0984 - mean_squared_error: 2.0984 - val_loss: 1.1003 - val_mean_squared_error: 1.1003\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.11513 to 1.10031, saving model to temp/a74\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0752 - mean_squared_error: 2.0752 - val_loss: 1.2585 - val_mean_squared_error: 1.2585\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.10031\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1111 - mean_squared_error: 2.1111 - val_loss: 1.1236 - val_mean_squared_error: 1.1236\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.10031\n",
      "Epoch 00007: early stopping\n",
      "temp/a75\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5748 - mean_squared_error: 2.5748 - val_loss: 1.1812 - val_mean_squared_error: 1.1812\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.18120, saving model to temp/a75\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2240 - mean_squared_error: 2.2240 - val_loss: 1.3545 - val_mean_squared_error: 1.3545\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.18120\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1337 - mean_squared_error: 2.1337 - val_loss: 1.1204 - val_mean_squared_error: 1.1204\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.18120 to 1.12040, saving model to temp/a75\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1132 - mean_squared_error: 2.1132 - val_loss: 1.1574 - val_mean_squared_error: 1.1574\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.12040\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0646 - mean_squared_error: 2.0646 - val_loss: 1.2897 - val_mean_squared_error: 1.2897\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.12040\n",
      "Epoch 00005: early stopping\n",
      "temp/a76\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5865 - mean_squared_error: 2.5865 - val_loss: 1.3836 - val_mean_squared_error: 1.3836\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.38359, saving model to temp/a76\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1172 - mean_squared_error: 2.1172 - val_loss: 1.1789 - val_mean_squared_error: 1.1789\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.38359 to 1.17888, saving model to temp/a76\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0974 - mean_squared_error: 2.0974 - val_loss: 1.1618 - val_mean_squared_error: 1.1618\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.17888 to 1.16177, saving model to temp/a76\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0575 - mean_squared_error: 2.0575 - val_loss: 1.2068 - val_mean_squared_error: 1.2068\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.16177\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1500 - mean_squared_error: 2.1500 - val_loss: 1.1794 - val_mean_squared_error: 1.1794\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.16177\n",
      "Epoch 00005: early stopping\n",
      "temp/a77\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5651 - mean_squared_error: 2.5651 - val_loss: 1.2410 - val_mean_squared_error: 1.2410\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.24105, saving model to temp/a77\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1895 - mean_squared_error: 2.1895 - val_loss: 1.2458 - val_mean_squared_error: 1.2458\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.24105\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1264 - mean_squared_error: 2.1264 - val_loss: 1.3346 - val_mean_squared_error: 1.3346\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.24105\n",
      "Epoch 00003: early stopping\n",
      "temp/a78\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 221us/step - loss: 2.6240 - mean_squared_error: 2.6240 - val_loss: 1.2349 - val_mean_squared_error: 1.2349\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.23491, saving model to temp/a78\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2154 - mean_squared_error: 2.2154 - val_loss: 1.1442 - val_mean_squared_error: 1.1442\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.23491 to 1.14416, saving model to temp/a78\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1672 - mean_squared_error: 2.1672 - val_loss: 1.8609 - val_mean_squared_error: 1.8609\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.14416\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 2.1020 - mean_squared_error: 2.1020 - val_loss: 1.1346 - val_mean_squared_error: 1.1346\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.14416 to 1.13462, saving model to temp/a78\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1188 - mean_squared_error: 2.1188 - val_loss: 1.1282 - val_mean_squared_error: 1.1282\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.13462 to 1.12822, saving model to temp/a78\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.0519 - mean_squared_error: 2.0519 - val_loss: 1.1234 - val_mean_squared_error: 1.1234\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.12822 to 1.12340, saving model to temp/a78\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0605 - mean_squared_error: 2.0605 - val_loss: 1.2099 - val_mean_squared_error: 1.2099\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.12340\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0077 - mean_squared_error: 2.0077 - val_loss: 1.2452 - val_mean_squared_error: 1.2452\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.12340\n",
      "Epoch 00008: early stopping\n",
      "temp/a79\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 221us/step - loss: 2.5499 - mean_squared_error: 2.5499 - val_loss: 1.2936 - val_mean_squared_error: 1.2936\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.29360, saving model to temp/a79\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1374 - mean_squared_error: 2.1374 - val_loss: 1.1463 - val_mean_squared_error: 1.1463\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.29360 to 1.14633, saving model to temp/a79\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1654 - mean_squared_error: 2.1654 - val_loss: 1.4221 - val_mean_squared_error: 1.4221\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.14633\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0986 - mean_squared_error: 2.0986 - val_loss: 1.4273 - val_mean_squared_error: 1.4273\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.14633\n",
      "Epoch 00004: early stopping\n",
      "temp/a80\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.5276 - mean_squared_error: 2.5276 - val_loss: 1.1971 - val_mean_squared_error: 1.1971\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.19713, saving model to temp/a80\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1952 - mean_squared_error: 2.1952 - val_loss: 1.1834 - val_mean_squared_error: 1.1834\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.19713 to 1.18345, saving model to temp/a80\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1789 - mean_squared_error: 2.1789 - val_loss: 1.1475 - val_mean_squared_error: 1.1475\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.18345 to 1.14754, saving model to temp/a80\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1851 - mean_squared_error: 2.1851 - val_loss: 1.1683 - val_mean_squared_error: 1.1683\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.14754\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0845 - mean_squared_error: 2.0845 - val_loss: 1.1468 - val_mean_squared_error: 1.1468\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.14754 to 1.14679, saving model to temp/a80\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1059 - mean_squared_error: 2.1059 - val_loss: 1.1961 - val_mean_squared_error: 1.1961\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.14679\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0831 - mean_squared_error: 2.0831 - val_loss: 1.1821 - val_mean_squared_error: 1.1821\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.14679\n",
      "Epoch 00007: early stopping\n",
      "temp/a81\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.4981 - mean_squared_error: 2.4981 - val_loss: 1.2113 - val_mean_squared_error: 1.2113\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.21133, saving model to temp/a81\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2334 - mean_squared_error: 2.2334 - val_loss: 1.2281 - val_mean_squared_error: 1.2281\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.21133\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1123 - mean_squared_error: 2.1123 - val_loss: 1.5541 - val_mean_squared_error: 1.5541\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.21133\n",
      "Epoch 00003: early stopping\n",
      "temp/a82\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5505 - mean_squared_error: 2.5505 - val_loss: 1.3311 - val_mean_squared_error: 1.3311\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.33107, saving model to temp/a82\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1917 - mean_squared_error: 2.1917 - val_loss: 1.1715 - val_mean_squared_error: 1.1715\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.33107 to 1.17147, saving model to temp/a82\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1853 - mean_squared_error: 2.1853 - val_loss: 1.2586 - val_mean_squared_error: 1.2586\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.17147\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0471 - mean_squared_error: 2.0471 - val_loss: 1.1164 - val_mean_squared_error: 1.1164\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.17147 to 1.11637, saving model to temp/a82\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0822 - mean_squared_error: 2.0822 - val_loss: 1.1171 - val_mean_squared_error: 1.1171\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.11637\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0634 - mean_squared_error: 2.0634 - val_loss: 1.1831 - val_mean_squared_error: 1.1831\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.11637\n",
      "Epoch 00006: early stopping\n",
      "temp/a83\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5523 - mean_squared_error: 2.5523 - val_loss: 1.4055 - val_mean_squared_error: 1.4055\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.40553, saving model to temp/a83\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2175 - mean_squared_error: 2.2175 - val_loss: 1.4875 - val_mean_squared_error: 1.4875\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.40553\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1286 - mean_squared_error: 2.1286 - val_loss: 1.2447 - val_mean_squared_error: 1.2447\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.40553 to 1.24472, saving model to temp/a83\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1161 - mean_squared_error: 2.1161 - val_loss: 1.2705 - val_mean_squared_error: 1.2705\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.24472\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1344 - mean_squared_error: 2.1344 - val_loss: 1.2031 - val_mean_squared_error: 1.2031\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.24472 to 1.20310, saving model to temp/a83\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.0843 - mean_squared_error: 2.0843 - val_loss: 1.1265 - val_mean_squared_error: 1.1265\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.20310 to 1.12651, saving model to temp/a83\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1473 - mean_squared_error: 2.1473 - val_loss: 1.4556 - val_mean_squared_error: 1.4556\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.12651\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1069 - mean_squared_error: 2.1069 - val_loss: 1.2040 - val_mean_squared_error: 1.2040\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.12651\n",
      "Epoch 00008: early stopping\n",
      "temp/a84\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5309 - mean_squared_error: 2.5309 - val_loss: 1.2989 - val_mean_squared_error: 1.2989\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.29889, saving model to temp/a84\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2003 - mean_squared_error: 2.2003 - val_loss: 1.1651 - val_mean_squared_error: 1.1651\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.29889 to 1.16507, saving model to temp/a84\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1229 - mean_squared_error: 2.1229 - val_loss: 1.1595 - val_mean_squared_error: 1.1595\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.16507 to 1.15945, saving model to temp/a84\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1768 - mean_squared_error: 2.1768 - val_loss: 1.3914 - val_mean_squared_error: 1.3914\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.15945\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1312 - mean_squared_error: 2.1312 - val_loss: 1.3652 - val_mean_squared_error: 1.3652\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.15945\n",
      "Epoch 00005: early stopping\n",
      "temp/a85\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.6173 - mean_squared_error: 2.6173 - val_loss: 1.1714 - val_mean_squared_error: 1.1714\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.17138, saving model to temp/a85\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1766 - mean_squared_error: 2.1766 - val_loss: 1.2156 - val_mean_squared_error: 1.2156\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.17138\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1461 - mean_squared_error: 2.1461 - val_loss: 1.1551 - val_mean_squared_error: 1.1551\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.17138 to 1.15508, saving model to temp/a85\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.0619 - mean_squared_error: 2.0619 - val_loss: 1.2916 - val_mean_squared_error: 1.2916\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.15508\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0886 - mean_squared_error: 2.0886 - val_loss: 1.3863 - val_mean_squared_error: 1.3863\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.15508\n",
      "Epoch 00005: early stopping\n",
      "temp/a86\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.5660 - mean_squared_error: 2.5660 - val_loss: 1.3029 - val_mean_squared_error: 1.3029\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.30290, saving model to temp/a86\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1782 - mean_squared_error: 2.1782 - val_loss: 1.5398 - val_mean_squared_error: 1.5398\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.30290\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1433 - mean_squared_error: 2.1433 - val_loss: 1.1682 - val_mean_squared_error: 1.1682\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.30290 to 1.16815, saving model to temp/a86\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1529 - mean_squared_error: 2.1529 - val_loss: 1.2486 - val_mean_squared_error: 1.2486\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.16815\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0891 - mean_squared_error: 2.0891 - val_loss: 1.1481 - val_mean_squared_error: 1.1481\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.16815 to 1.14813, saving model to temp/a86\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0970 - mean_squared_error: 2.0970 - val_loss: 1.1933 - val_mean_squared_error: 1.1933\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.14813\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.0347 - mean_squared_error: 2.0347 - val_loss: 1.1496 - val_mean_squared_error: 1.1496\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.14813\n",
      "Epoch 00007: early stopping\n",
      "temp/a87\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.6108 - mean_squared_error: 2.6108 - val_loss: 1.4038 - val_mean_squared_error: 1.4038\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.40383, saving model to temp/a87\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2141 - mean_squared_error: 2.2141 - val_loss: 1.1501 - val_mean_squared_error: 1.1501\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.40383 to 1.15007, saving model to temp/a87\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1020 - mean_squared_error: 2.1020 - val_loss: 1.1454 - val_mean_squared_error: 1.1454\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.15007 to 1.14540, saving model to temp/a87\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0883 - mean_squared_error: 2.0883 - val_loss: 1.2063 - val_mean_squared_error: 1.2063\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.14540\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1046 - mean_squared_error: 2.1046 - val_loss: 1.1591 - val_mean_squared_error: 1.1591\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.14540\n",
      "Epoch 00005: early stopping\n",
      "temp/a88\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5942 - mean_squared_error: 2.5942 - val_loss: 1.2154 - val_mean_squared_error: 1.2154\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.21541, saving model to temp/a88\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2302 - mean_squared_error: 2.2302 - val_loss: 1.1683 - val_mean_squared_error: 1.1683\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.21541 to 1.16831, saving model to temp/a88\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1080 - mean_squared_error: 2.1080 - val_loss: 1.1103 - val_mean_squared_error: 1.1103\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.16831 to 1.11028, saving model to temp/a88\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1242 - mean_squared_error: 2.1242 - val_loss: 1.2567 - val_mean_squared_error: 1.2567\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.11028\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.0981 - mean_squared_error: 2.0981 - val_loss: 1.1618 - val_mean_squared_error: 1.1618\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.11028\n",
      "Epoch 00005: early stopping\n",
      "temp/a89\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.5533 - mean_squared_error: 2.5533 - val_loss: 1.2014 - val_mean_squared_error: 1.2014\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.20143, saving model to temp/a89\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1596 - mean_squared_error: 2.1596 - val_loss: 1.1548 - val_mean_squared_error: 1.1548\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.20143 to 1.15481, saving model to temp/a89\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1495 - mean_squared_error: 2.1495 - val_loss: 1.1477 - val_mean_squared_error: 1.1477\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.15481 to 1.14766, saving model to temp/a89\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0898 - mean_squared_error: 2.0898 - val_loss: 1.1484 - val_mean_squared_error: 1.1484\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.14766\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0451 - mean_squared_error: 2.0451 - val_loss: 1.2169 - val_mean_squared_error: 1.2169\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.14766\n",
      "Epoch 00005: early stopping\n",
      "temp/a90\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5818 - mean_squared_error: 2.5818 - val_loss: 1.2032 - val_mean_squared_error: 1.2032\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.20320, saving model to temp/a90\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2282 - mean_squared_error: 2.2282 - val_loss: 1.1863 - val_mean_squared_error: 1.1863\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.20320 to 1.18626, saving model to temp/a90\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1367 - mean_squared_error: 2.1367 - val_loss: 1.2489 - val_mean_squared_error: 1.2489\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.18626\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1165 - mean_squared_error: 2.1165 - val_loss: 1.1657 - val_mean_squared_error: 1.1657\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.18626 to 1.16573, saving model to temp/a90\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1067 - mean_squared_error: 2.1067 - val_loss: 1.1349 - val_mean_squared_error: 1.1349\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.16573 to 1.13492, saving model to temp/a90\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0967 - mean_squared_error: 2.0967 - val_loss: 1.1278 - val_mean_squared_error: 1.1278\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.13492 to 1.12776, saving model to temp/a90\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0765 - mean_squared_error: 2.0765 - val_loss: 1.1322 - val_mean_squared_error: 1.1322\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.12776\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0767 - mean_squared_error: 2.0767 - val_loss: 1.1643 - val_mean_squared_error: 1.1643\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.12776\n",
      "Epoch 00008: early stopping\n",
      "temp/a91\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5465 - mean_squared_error: 2.5465 - val_loss: 1.3378 - val_mean_squared_error: 1.3378\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.33780, saving model to temp/a91\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1660 - mean_squared_error: 2.1660 - val_loss: 1.2268 - val_mean_squared_error: 1.2268\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.33780 to 1.22677, saving model to temp/a91\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1400 - mean_squared_error: 2.1400 - val_loss: 1.1603 - val_mean_squared_error: 1.1603\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.22677 to 1.16032, saving model to temp/a91\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1026 - mean_squared_error: 2.1026 - val_loss: 1.9220 - val_mean_squared_error: 1.9220\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.16032\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0723 - mean_squared_error: 2.0723 - val_loss: 1.1088 - val_mean_squared_error: 1.1088\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.16032 to 1.10883, saving model to temp/a91\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.0643 - mean_squared_error: 2.0643 - val_loss: 1.2923 - val_mean_squared_error: 1.2923\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.10883\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.0907 - mean_squared_error: 2.0907 - val_loss: 1.2824 - val_mean_squared_error: 1.2824\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.10883\n",
      "Epoch 00007: early stopping\n",
      "temp/a92\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.6285 - mean_squared_error: 2.6285 - val_loss: 1.6500 - val_mean_squared_error: 1.6500\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.64997, saving model to temp/a92\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1789 - mean_squared_error: 2.1789 - val_loss: 1.1760 - val_mean_squared_error: 1.1760\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.64997 to 1.17605, saving model to temp/a92\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1673 - mean_squared_error: 2.1673 - val_loss: 1.1645 - val_mean_squared_error: 1.1645\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.17605 to 1.16451, saving model to temp/a92\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1225 - mean_squared_error: 2.1225 - val_loss: 1.1352 - val_mean_squared_error: 1.1352\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.16451 to 1.13518, saving model to temp/a92\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0538 - mean_squared_error: 2.0538 - val_loss: 1.1763 - val_mean_squared_error: 1.1763\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.13518\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0902 - mean_squared_error: 2.0902 - val_loss: 1.8200 - val_mean_squared_error: 1.8200\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.13518\n",
      "Epoch 00006: early stopping\n",
      "temp/a93\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 11s 215us/step - loss: 2.5820 - mean_squared_error: 2.5820 - val_loss: 1.6557 - val_mean_squared_error: 1.6557\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.65570, saving model to temp/a93\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1851 - mean_squared_error: 2.1851 - val_loss: 1.1854 - val_mean_squared_error: 1.1854\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.65570 to 1.18539, saving model to temp/a93\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1595 - mean_squared_error: 2.1595 - val_loss: 1.4630 - val_mean_squared_error: 1.4630\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.18539\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0967 - mean_squared_error: 2.0967 - val_loss: 1.4326 - val_mean_squared_error: 1.4326\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.18539\n",
      "Epoch 00004: early stopping\n",
      "temp/a94\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.5361 - mean_squared_error: 2.5361 - val_loss: 1.2547 - val_mean_squared_error: 1.2547\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.25473, saving model to temp/a94\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1773 - mean_squared_error: 2.1773 - val_loss: 1.1365 - val_mean_squared_error: 1.1365\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.25473 to 1.13653, saving model to temp/a94\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1422 - mean_squared_error: 2.1422 - val_loss: 1.1805 - val_mean_squared_error: 1.1805\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.13653\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1211 - mean_squared_error: 2.1211 - val_loss: 1.1175 - val_mean_squared_error: 1.1175\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.13653 to 1.11748, saving model to temp/a94\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1058 - mean_squared_error: 2.1058 - val_loss: 1.3058 - val_mean_squared_error: 1.3058\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.11748\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1464 - mean_squared_error: 2.1464 - val_loss: 1.3820 - val_mean_squared_error: 1.3820\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.11748\n",
      "Epoch 00006: early stopping\n",
      "temp/a95\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.5970 - mean_squared_error: 2.5970 - val_loss: 1.7460 - val_mean_squared_error: 1.7460\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.74597, saving model to temp/a95\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2007 - mean_squared_error: 2.2007 - val_loss: 1.1329 - val_mean_squared_error: 1.1329\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.74597 to 1.13285, saving model to temp/a95\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1202 - mean_squared_error: 2.1202 - val_loss: 1.1250 - val_mean_squared_error: 1.1250\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.13285 to 1.12496, saving model to temp/a95\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1399 - mean_squared_error: 2.1399 - val_loss: 1.2552 - val_mean_squared_error: 1.2552\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.12496\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0362 - mean_squared_error: 2.0362 - val_loss: 1.1311 - val_mean_squared_error: 1.1311\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.12496\n",
      "Epoch 00005: early stopping\n",
      "temp/a96\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5741 - mean_squared_error: 2.5741 - val_loss: 1.4251 - val_mean_squared_error: 1.4251\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.42508, saving model to temp/a96\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2097 - mean_squared_error: 2.2097 - val_loss: 1.1749 - val_mean_squared_error: 1.1749\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.42508 to 1.17489, saving model to temp/a96\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1357 - mean_squared_error: 2.1357 - val_loss: 1.1990 - val_mean_squared_error: 1.1990\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.17489\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1498 - mean_squared_error: 2.1498 - val_loss: 1.4560 - val_mean_squared_error: 1.4560\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.17489\n",
      "Epoch 00004: early stopping\n",
      "temp/a97\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.5859 - mean_squared_error: 2.5859 - val_loss: 1.2687 - val_mean_squared_error: 1.2687\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.26874, saving model to temp/a97\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1902 - mean_squared_error: 2.1902 - val_loss: 1.1954 - val_mean_squared_error: 1.1954\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.26874 to 1.19536, saving model to temp/a97\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1749 - mean_squared_error: 2.1749 - val_loss: 1.2996 - val_mean_squared_error: 1.2996\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.19536\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1707 - mean_squared_error: 2.1707 - val_loss: 1.1726 - val_mean_squared_error: 1.1726\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.19536 to 1.17258, saving model to temp/a97\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.0895 - mean_squared_error: 2.0895 - val_loss: 1.1086 - val_mean_squared_error: 1.1086\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.17258 to 1.10857, saving model to temp/a97\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0731 - mean_squared_error: 2.0731 - val_loss: 1.6142 - val_mean_squared_error: 1.6142\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.10857\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0871 - mean_squared_error: 2.0871 - val_loss: 1.1700 - val_mean_squared_error: 1.1700\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.10857\n",
      "Epoch 00007: early stopping\n",
      "temp/a98\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.5196 - mean_squared_error: 2.5196 - val_loss: 1.5353 - val_mean_squared_error: 1.5353\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.53526, saving model to temp/a98\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2068 - mean_squared_error: 2.2068 - val_loss: 1.1524 - val_mean_squared_error: 1.1524\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.53526 to 1.15243, saving model to temp/a98\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1913 - mean_squared_error: 2.1913 - val_loss: 1.1722 - val_mean_squared_error: 1.1722\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.15243\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1477 - mean_squared_error: 2.1477 - val_loss: 1.2335 - val_mean_squared_error: 1.2335\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.15243\n",
      "Epoch 00004: early stopping\n",
      "temp/a99\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.5590 - mean_squared_error: 2.5590 - val_loss: 1.2166 - val_mean_squared_error: 1.2166\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.21664, saving model to temp/a99\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1758 - mean_squared_error: 2.1758 - val_loss: 1.5798 - val_mean_squared_error: 1.5798\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.21664\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1595 - mean_squared_error: 2.1595 - val_loss: 1.1476 - val_mean_squared_error: 1.1476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss improved from 1.21664 to 1.14761, saving model to temp/a99\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1210 - mean_squared_error: 2.1210 - val_loss: 1.1543 - val_mean_squared_error: 1.1543\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.14761\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1580 - mean_squared_error: 2.1580 - val_loss: 1.1722 - val_mean_squared_error: 1.1722\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.14761\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "for idx, model_name in enumerate(model_names):\n",
    "    print(model_name)\n",
    "\n",
    "    if type(models[idx]) is list:\n",
    "        #clear session\n",
    "        keras.backend.clear_session() \n",
    "        #get model according to specification\n",
    "        model = get_model(models[idx], [0.2] * len(models), len(inputs), 1)\n",
    "        callbacks = [ModelCheckpoint(model_name, verbose= verbosity, monitor='val_loss',save_best_only=True), \n",
    "                     EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose= verbosity, mode='auto')]\n",
    "        model.compile(optimizer = optimizers.Adam(lr = 0.001), loss='mse', metrics = ['mse'])\n",
    "        #print(len(X), len(y))\n",
    "        model.fit(X, y, epochs = 20, validation_data = (x_val, y_val), callbacks = callbacks, batch_size = 32, verbose = verbosity)\n",
    "    else:\n",
    "        models[idx].fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.172792075618476\n",
      "5.264733541144344\n",
      "5.161942747415382\n",
      "5.5925347936087855\n",
      "5.352842873943664\n",
      "5.10375666276982\n",
      "5.518106593379432\n",
      "5.546679894174773\n",
      "5.579551927647513\n",
      "5.534016690614592\n",
      "5.3214090112834995\n",
      "5.431042600237672\n",
      "5.440615413892341\n",
      "5.131886195342897\n",
      "5.3103050534487615\n",
      "5.609253843641534\n",
      "5.414953794988178\n",
      "4.940759012959619\n",
      "5.5695628601080225\n",
      "5.1553737533575585\n",
      "5.227526437256458\n",
      "5.281887105863864\n",
      "5.145029742348677\n",
      "5.314908249867381\n",
      "5.070970368098917\n",
      "5.290760880803528\n",
      "5.4536796043736215\n",
      "5.420615082505103\n",
      "5.591524715432712\n",
      "5.005736386473976\n",
      "5.351255085599065\n",
      "5.159913986262888\n",
      "5.727391666503391\n",
      "5.432541677155542\n",
      "5.522438096493369\n",
      "5.3767106276637255\n",
      "5.267235812581337\n",
      "5.564773601395825\n",
      "5.521621121839479\n",
      "5.176812926318455\n",
      "5.176718198827694\n",
      "5.220548439890661\n",
      "5.579428183034747\n",
      "5.623690059418364\n",
      "5.7369089092266075\n",
      "5.379180397616009\n",
      "5.294671305079804\n",
      "5.328137521322659\n",
      "5.144550419779682\n",
      "5.770323520944223\n",
      "5.448596000815635\n",
      "5.622693145756821\n",
      "5.321424726522402\n",
      "5.550581719466864\n",
      "5.455191494142873\n",
      "5.616009489232531\n",
      "5.3480018076292435\n",
      "5.654971209162353\n",
      "4.842732655244808\n",
      "5.1450983713065\n",
      "5.179910035204424\n",
      "5.3361373612400715\n",
      "5.110004704929086\n",
      "5.5562023138683765\n",
      "5.269338599904478\n",
      "5.579928660938563\n",
      "5.440719568229969\n",
      "5.438951576072955\n",
      "5.480905212606557\n",
      "5.224255393479264\n",
      "5.32496203671952\n",
      "5.499430107534304\n",
      "5.339133748289105\n",
      "5.236351852974229\n",
      "5.5193363742019494\n",
      "5.148178228927899\n",
      "5.074190440866907\n",
      "5.158769869440193\n",
      "5.381447136954371\n",
      "5.592516302740431\n",
      "5.615325665983835\n",
      "5.471740578563219\n",
      "5.35269428190407\n",
      "5.257148436952972\n",
      "5.718429742250376\n",
      "5.36326280123961\n",
      "5.237488191600342\n",
      "5.337684652957533\n",
      "5.272520785763318\n",
      "5.107644169311033\n",
      "5.289591649931793\n",
      "5.413284479373671\n",
      "5.60865054248074\n",
      "5.164133468981212\n",
      "5.301171644953649\n",
      "5.2874280482115745\n",
      "5.141437597141743\n",
      "5.293719206942586\n",
      "5.101942192870661\n",
      "5.107588322829944\n"
     ]
    }
   ],
   "source": [
    "def gen_data(mean = 0.4, var = 1.4, SIZE = 2000):\n",
    "    # set bmi to these values real world mean and standard deviation for a certain country.\n",
    "    bmi = np.random.normal(mean, var, SIZE)\n",
    "    estrogen =  2*bmi +  np.random.normal(mean,var, SIZE)\n",
    "    age = np.random.normal(mean,var, SIZE)\n",
    "    genes = 1.1 * age +  estrogen +  np.random.normal(mean,var, SIZE)\n",
    "    insomnia = estrogen +  np.random.normal(mean,var, SIZE)\n",
    "    density = estrogen + genes + np.random.normal(mean,var, SIZE)\n",
    "    cancer = 0.1*density + np.random.normal(mean,var, SIZE)\n",
    "    return pd.DataFrame({'bmi' : bmi,'density' : density, 'age' : age, 'cancer' : cancer, 'estrogen': estrogen, 'genes':genes, 'insomnia': insomnia})\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "nb_test = 6000\n",
    "metrics_dicts = []\n",
    "\n",
    "perturbed_df = gen_data()\n",
    "y_test2 = perturbed_df[target]\n",
    "x_test2 = normalize(perturbed_df[inputs].values)\n",
    "for idx, model_name in enumerate(model_names):\n",
    "\n",
    "    if type(models[idx]) is list:\n",
    "        keras.backend.clear_session()\n",
    "        model = load_model(model_name)\n",
    "    else:\n",
    "        model = models[idx]\n",
    "    #y_pred2 = model.predict(x_test2)[:,1]\n",
    "    y_pred2 = model.predict(x_test2)\n",
    "    print(mean_absolute_error(y_test2, y_pred2))\n",
    "    metrics_dicts.append(mean_absolute_error(y_test2, y_pred2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times =  0\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64472.89048974068\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64999.52965575544\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64411.90756818736\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64267.011483022754\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64707.92727012451\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64436.25754946082\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64339.202114062806\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64291.017975005656\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64077.001611885455\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64171.55393919998\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64195.3332399405\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64521.936436028176\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63856.035115698716\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64608.3245031391\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64363.37994184585\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64363.37994184586\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64509.65326905465\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63948.45990962746\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64183.43651208884\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64460.66509850473\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64195.3332399405\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64620.72378180216\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64884.491978794336\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64387.61506282365\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64424.07536860426\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "65167.83284504242\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64267.01148302276\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64327.134656716684\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64783.24432329864\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63651.31278754254\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64006.67803359439\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "65038.14387109163\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64363.37994184585\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64030.06314637602\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64088.77134149819\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64112.35301281391\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64935.47109855691\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64315.08148862305\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63971.70527153562\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64833.749981686204\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64327.134656716684\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64279.00760693442\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64065.2459384312\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64436.257549460875\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64124.16497637529\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64658.00906656705\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64485.1303075241\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64472.89048974068\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64207.24413382651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64707.92727012456\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64871.784279990745\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64135.991039808294\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64424.07536860422\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64608.324503139054\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64112.35301281386\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64986.68813077192\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64436.257549460825\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64583.569599278635\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64633.1376273653\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64645.5660516676\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64124.16497637528\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64509.65326905463\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64720.4434166803\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64231.108464136705\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64961.04982601766\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64720.4434166803\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64695.425773792354\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64171.55393919998\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64485.13030752411\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64546.54620059111\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64534.23407598658\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63821.60468061999\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64783.24432329862\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64375.49033499523\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64620.7237818022\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64219.16920485272\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64112.353012813895\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64231.10846413671\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64608.32450313908\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64521.93643602818\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64472.89048974068\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64986.688130771916\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64231.108464136705\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64770.65474530719\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64351.28387199098\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63867.539619112984\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64583.569599278635\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64546.546200591125\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64112.353012813895\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64595.93977956452\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64485.13030752411\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64521.93643602817\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64147.831214086444\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63787.298638265216\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64833.749981686226\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64315.081488623044\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64267.01148302276\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64327.134656716684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64608.32450313908\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64279.007606934436\n",
      "Times =  1\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64327.13465671669\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64859.091389039255\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64303.04259848082\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64135.991039808236\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64521.93643602818\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64339.20211406281\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64291.01797500565\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64279.007606934436\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63856.035115698796\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64124.16497637529\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63960.07561644651\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64571.213950526435\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63730.397565209336\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64375.49033499515\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64448.45412225547\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64243.0619228141\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64315.08148862308\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63936.85814041241\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64124.164976375294\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64399.7541367484\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64279.00760693443\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64534.23407598658\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64595.93977956452\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64243.0619228141\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64159.68551020854\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64821.10144082883\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64159.68551020857\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64387.61506282362\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64645.5660516676\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63572.89768026238\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64018.36358899586\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64935.47109855691\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64171.55393919998\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63925.27029814424\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63913.69637216853\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64207.24413382649\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64732.974225451835\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64135.99103980824\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64030.06314637602\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64720.44341668038\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64183.436512088854\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64207.24413382649\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63833.06765694451\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64255.029592052844\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63810.15552568613\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64608.32450313915\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64633.13762736529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64351.28387199097\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63971.705271535655\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64633.13762736534\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64695.42577379225\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63948.45990962741\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64267.01148302277\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64608.32450313914\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64065.2459384312\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64620.723781802204\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64303.04259848082\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64243.06192281411\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64583.569599278606\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64783.24432329867\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64006.67803359439\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64279.007606934414\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64645.566051667614\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64231.108464136676\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64859.09138903925\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64521.93643602818\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64411.90756818732\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64053.504310268654\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64243.0619228141\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64448.45412225547\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64436.257549460875\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63925.27029814424\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64633.13762736533\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64363.379941845815\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64472.89048974068\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64088.77134149819\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63983.34888561405\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64065.2459384312\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64424.0753686042\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64460.66509850473\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64207.24413382649\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64871.78427999074\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64100.55513816682\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64645.5660516676\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64183.436512088854\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63753.116707057474\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64485.13030752411\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64485.1303075241\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64018.36358899586\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64375.490334995186\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64327.13465671669\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64351.283871991014\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64088.771341498235\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63764.49691131707\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64707.92727012446\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64100.55513816685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64315.08148862308\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64065.24593843121\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64448.45412225547\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64030.06314637602\n",
      "Times =  2\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63741.75026180045\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64534.234075986555\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63948.45990962727\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63925.27029814421\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64065.245938431224\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63867.539619112984\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63673.839951830894\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63775.89088493028\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63517.29496890961\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63550.61587817866\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63584.05897013054\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64030.06314637602\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63473.05668623156\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63833.06765694459\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63787.29863826528\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63787.29863826528\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64065.245938431224\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63550.61587817865\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63572.89768026242\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64088.77134149825\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63890.59022664883\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64231.10846413678\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64159.68551020854\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63971.70527153561\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63867.53961911299\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64485.1303075241\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63673.839951830894\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63960.07561644653\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63983.34888561401\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63255.094237117\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63584.05897013059\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64546.54620059108\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63775.89088493028\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63451.01862185935\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63550.615878178665\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63561.74998628714\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64363.379941845764\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63707.73337676712\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63473.05668623161\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64291.01797500555\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63606.42237779473\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63673.839951830894\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63418.0626494718\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63890.590226648856\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63374.30970601425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64135.9910398083\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64207.24413382651\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63867.539619112984\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63606.42237779472\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64497.38456342568\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64399.754136748445\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63775.89088493025\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63775.89088493027\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64135.99103980834\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63741.75026180049\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64243.0619228141\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63960.07561644653\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64053.504310268654\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64267.01148302281\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64448.45412225549\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63628.84028994981\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63867.539619112984\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64315.081488623124\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63971.70527153561\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64448.45412225549\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64135.99103980834\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64171.55393919998\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63584.05897013057\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63775.89088493027\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63995.00646938665\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63890.590226648856\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63407.10425249177\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64124.16497637525\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63936.85814041245\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64207.24413382652\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63451.01862185935\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63528.388379474534\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63833.06765694459\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64041.77671652743\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63890.590226648856\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63821.60468062003\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64315.081488623124\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63775.890884930275\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64183.43651208884\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63821.604680620025\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63330.77145107\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63948.459909627265\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63787.29863826528\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63651.3127875425\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63833.06765694459\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64100.55513816682\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64077.00161188539\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63418.062649471845\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63385.22779051955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64387.61506282365\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63936.85814041245\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63925.27029814424\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63844.54446514541\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64135.9910398083\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63685.12405945096\n",
      "Times =  3\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64147.83121408686\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64948.25302134284\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64315.081488622985\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64077.00161188574\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64327.13465671677\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64291.017975006\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63995.0064693863\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64100.55513816685\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63730.39756520927\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64243.06192281413\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63925.270298144314\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64387.61506282356\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63821.60468061974\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64267.011483022696\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64291.01797500599\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64100.55513816685\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64255.0295920524\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63810.15552568619\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64159.68551020868\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64521.93643602787\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64231.10846413633\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64436.25754946112\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64460.66509850473\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64339.202114062806\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64327.13465671678\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "65025.25751339545\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64065.24593843109\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64147.83121408681\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64436.25754946112\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63662.56953126106\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64018.36358899585\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "65038.14387109163\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64183.43651208884\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63890.590226648725\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63867.539619112984\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64018.36358899585\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64658.00906656704\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64100.55513816685\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63719.05860695669\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64658.00906656705\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64207.24413382651\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64147.831214086866\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63810.155525686176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64231.10846413636\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63844.54446514512\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64707.92727012431\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64546.54620059137\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64375.490334994975\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63971.705271535924\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64645.566051667556\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64808.46765892495\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63995.00646938628\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64112.353012814165\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64339.202114062806\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63879.05798591537\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64571.21395052608\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64351.28387199111\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64303.04259848081\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64460.66509850473\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64795.848623795406\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64006.678033594704\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64207.24413382651\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64595.93977956447\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64375.490334994945\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64859.091389039444\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64497.38456342564\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64608.324503139025\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63902.136351866706\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64351.28387199111\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64472.89048974069\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64363.37994184621\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63673.8399518309\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64620.72378180222\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64207.24413382652\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64485.13030752417\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64100.55513816685\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63867.539619112955\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64018.36358899585\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64436.25754946112\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64195.333239940555\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64147.83121408686\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64846.41329368646\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64018.36358899586\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64571.21395052607\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64088.77134149784\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63606.42237779466\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64436.25754946114\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64231.10846413637\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63971.70527153592\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64219.16920485301\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64327.134656716764\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64460.66509850473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63913.6963721685\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63775.89088492985\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64707.92727012429\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64255.029592052444\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64243.06192281412\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "63936.85814041249\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64424.07536860396\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "64112.35301281417\n",
      "Model_name =  temp/a0 Violations =  0.0\n",
      "Average_violations =  64172.40165558617 273.99825303067615\n",
      "MSE =  43.66258288088896 0.5499180106167105\n",
      "Model_name =  temp/a1 Violations =  0.0\n",
      "Average_violations =  64835.27703553102 180.92573306818852\n",
      "MSE =  45.44171102468964 0.7042307909776682\n",
      "Model_name =  temp/a2 Violations =  0.0\n",
      "Average_violations =  64244.62289122961 176.12066244368046\n",
      "MSE =  43.13148705151005 0.5752492917892322\n",
      "Model_name =  temp/a3 Violations =  0.0\n",
      "Average_violations =  64101.31860821524 122.71983342547271\n",
      "MSE =  51.812973928461815 0.7058347978574104\n",
      "Model_name =  temp/a4 Violations =  0.0\n",
      "Average_violations =  64405.561075325175 238.1877599033355\n",
      "MSE =  47.155121098485765 0.6955664504247152\n",
      "Model_name =  temp/a5 Violations =  0.0\n",
      "Average_violations =  64233.50431441065 217.6688301836115\n",
      "MSE =  42.743012964216895 0.6182321502732135\n",
      "Model_name =  temp/a6 Violations =  0.0\n",
      "Average_violations =  64074.766627571415 266.36190321493\n",
      "MSE =  51.17881488103762 0.7192180999503881\n",
      "Model_name =  temp/a7 Violations =  0.0\n",
      "Average_violations =  64111.617901259306 207.98961166764477\n",
      "MSE =  50.633741524169764 0.679729810281667\n",
      "Model_name =  temp/a8 Violations =  0.0\n",
      "Average_violations =  63795.18231542579 202.81988337716382\n",
      "MSE =  51.71115868982831 0.7577968701870447\n",
      "Model_name =  temp/a9 Violations =  0.0\n",
      "Average_violations =  64022.34917914201 275.6242639206763\n",
      "MSE =  51.64222995077346 0.8921461604638229\n",
      "Model_name =  temp/a10 Violations =  0.0\n",
      "Average_violations =  63916.18453116547 218.08276435350828\n",
      "MSE =  46.67246019049417 0.7791712198392644\n",
      "Model_name =  temp/a11 Violations =  0.0\n",
      "Average_violations =  64377.707148938556 211.66098411752859\n",
      "MSE =  48.610480255641356 0.7325477256000883\n",
      "Model_name =  temp/a12 Violations =  0.0\n",
      "Average_violations =  63720.27351193984 149.9315461878336\n",
      "MSE =  49.91134652228084 0.879613538093308\n",
      "Model_name =  temp/a13 Violations =  0.0\n",
      "Average_violations =  64270.97349452539 281.2946378252498\n",
      "MSE =  43.85267894194495 0.641437398880607\n",
      "Model_name =  temp/a14 Violations =  0.0\n",
      "Average_violations =  64222.537669343146 257.3894694123513\n",
      "MSE =  46.77173571786183 0.7132281413496749\n",
      "Model_name =  temp/a15 Violations =  0.0\n",
      "Average_violations =  64123.57391027302 215.28773789563868\n",
      "MSE =  51.204062659184174 0.5690734210009919\n",
      "Model_name =  temp/a16 Violations =  0.0\n",
      "Average_violations =  64286.252572040336 158.5542371212263\n",
      "MSE =  48.52358454464239 0.8623045243762822\n",
      "Model_name =  temp/a17 Violations =  0.0\n",
      "Average_violations =  63811.52236347618 160.1054424761986\n",
      "MSE =  40.58454220994191 0.48965923334610434\n",
      "Model_name =  temp/a18 Violations =  0.0\n",
      "Average_violations =  64010.04616973381 253.26766814782394\n",
      "MSE =  51.242311576347596 0.7414381805981546\n",
      "Model_name =  temp/a19 Violations =  0.0\n",
      "Average_violations =  64367.78175319481 166.77831485275996\n",
      "MSE =  43.318312507398176 0.6110981451980724\n",
      "Model_name =  temp/a20 Violations =  0.0\n",
      "Average_violations =  64149.00988441502 152.12343461776794\n",
      "MSE =  45.14257221306775 0.7426744385876145\n",
      "Model_name =  temp/a21 Violations =  0.0\n",
      "Average_violations =  64455.58096784666 145.1032045141457\n",
      "MSE =  44.89799878469003 0.6286976664667269\n",
      "Model_name =  temp/a22 Violations =  0.0\n",
      "Average_violations =  64525.19559176803 260.7011882019205\n",
      "MSE =  43.338625132301466 0.6704365932811248\n",
      "Model_name =  temp/a23 Violations =  0.0\n",
      "Average_violations =  64235.39609280904 160.88654314016435\n",
      "MSE =  46.50470415375324 0.7039344161326564\n",
      "Model_name =  temp/a24 Violations =  0.0\n",
      "Average_violations =  64194.60878866064 211.19416070377386\n",
      "MSE =  42.43022040632114 0.6996801353820149\n",
      "Model_name =  temp/a25 Violations =  0.0\n",
      "Average_violations =  64874.8305266977 256.5305597655976\n",
      "MSE =  46.0055860962012 0.6879921181061083\n",
      "Model_name =  temp/a26 Violations =  0.0\n",
      "Average_violations =  64041.44572087333 223.92019372154266\n",
      "MSE =  49.457600907926206 0.808622939795123\n",
      "Model_name =  temp/a27 Violations =  0.0\n",
      "Average_violations =  64205.66413751841 166.972748501545\n",
      "MSE =  48.51981563958084 0.6053199627975677\n",
      "Model_name =  temp/a28 Violations =  0.0\n",
      "Average_violations =  64462.104202510345 302.76390562882375\n",
      "MSE =  51.15401969187374 0.6213597663221887\n",
      "Model_name =  temp/a29 Violations =  0.0\n",
      "Average_violations =  63535.46855904575 165.51830859919153\n",
      "MSE =  41.90375826785679 0.6458182548035324\n",
      "Model_name =  temp/a30 Violations =  0.0\n",
      "Average_violations =  63906.866045429175 186.43379876455455\n",
      "MSE =  47.03819395234594 0.6974192223678136\n",
      "Model_name =  temp/a31 Violations =  0.0\n",
      "Average_violations =  64889.57626033281 202.43556258424618\n",
      "MSE =  43.87991311837464 0.6224539500658735\n",
      "Model_name =  temp/a32 Violations =  0.0\n",
      "Average_violations =  64123.56531951624 214.6369218395189\n",
      "MSE =  54.411921366232534 0.6241800058795811\n",
      "Model_name =  temp/a33 Violations =  0.0\n",
      "Average_violations =  63824.23557325709 221.51006654687131\n",
      "MSE =  48.384268484802696 0.6375641503403916\n",
      "Model_name =  temp/a34 Violations =  0.0\n",
      "Average_violations =  63855.155802739595 194.22998145116534\n",
      "MSE =  52.00209053629715 0.6823465413031272\n",
      "Model_name =  temp/a35 Violations =  0.0\n",
      "Average_violations =  63974.92768048085 247.71916639094718\n",
      "MSE =  47.19205974802023 0.7085515762221744\n",
      "Model_name =  temp/a36 Violations =  0.0\n",
      "Average_violations =  64672.458583105385 205.2901125625806\n",
      "MSE =  45.20864236822605 0.6509130242247245\n",
      "Model_name =  temp/a37 Violations =  0.0\n",
      "Average_violations =  64064.840260841316 221.63257011598495\n",
      "MSE =  51.83620131087622 0.6892449144247504\n",
      "Model_name =  temp/a38 Violations =  0.0\n",
      "Average_violations =  63798.47092777498 221.27236546376733\n",
      "MSE =  51.65490919558128 1.0010167367243215\n",
      "Model_name =  temp/a39 Violations =  0.0\n",
      "Average_violations =  64625.805109984794 203.29596609886747\n",
      "MSE =  44.78046108888015 0.6510057642069048\n",
      "Model_name =  temp/a40 Violations =  0.0\n",
      "Average_violations =  64081.0594201067 279.3908529938768\n",
      "MSE =  43.507885745548684 0.71114778557987\n",
      "Model_name =  temp/a41 Violations =  0.0\n",
      "Average_violations =  64076.98072666967 237.34241693093676\n",
      "MSE =  44.98511772084345 0.7337446870728679\n",
      "Model_name =  temp/a42 Violations =  0.0\n",
      "Average_violations =  63781.63294263343 232.42136145118155\n",
      "MSE =  51.61729012186055 0.6761254644714256\n",
      "Model_name =  temp/a43 Violations =  0.0\n",
      "Average_violations =  64203.24645807473 197.17116428674754\n",
      "MSE =  51.571432127211686 0.7629425015366942\n",
      "Model_name =  temp/a44 Violations =  0.0\n",
      "Average_violations =  63788.2936683052 268.2509783635899\n",
      "MSE =  54.817002763664625 0.8680900516084151\n",
      "Model_name =  temp/a45 Violations =  0.0\n",
      "Average_violations =  64527.562969909704 228.80038604532058\n",
      "MSE =  47.03407468402237 0.6765644436409427\n",
      "Model_name =  temp/a46 Violations =  0.0\n",
      "Average_violations =  64468.01456732682 159.47335978942255\n",
      "MSE =  46.00231652125207 0.5657829296354943\n",
      "Model_name =  temp/a47 Violations =  0.0\n",
      "Average_violations =  64266.80107895991 234.9644947916863\n",
      "MSE =  46.44424174220368 0.6196367295619241\n",
      "Model_name =  temp/a48 Violations =  0.0\n",
      "Average_violations =  63939.2692636732 214.8847189104027\n",
      "MSE =  43.66256840826496 0.7598566427265284\n",
      "Model_name =  temp/a49 Violations =  0.0\n",
      "Average_violations =  64621.003878145784 76.79182782562273\n",
      "MSE =  55.04719713089927 0.856047779503922\n",
      "Model_name =  temp/a50 Violations =  0.0\n",
      "Average_violations =  64693.8579623641 181.17165570556696\n",
      "MSE =  48.01585230353933 0.7014267402070204\n",
      "Model_name =  temp/a51 Violations =  0.0\n",
      "Average_violations =  63963.83707593806 128.61643200059243\n",
      "MSE =  52.762218362191526 0.8018840299671234\n",
      "Model_name =  temp/a52 Violations =  0.0\n",
      "Average_violations =  64144.83268734285 239.83177227838783\n",
      "MSE =  46.886479527032414 0.7016748952594027\n",
      "Model_name =  temp/a53 Violations =  0.0\n",
      "Average_violations =  64422.96054003734 198.8005065492759\n",
      "MSE =  50.33215889009298 0.754132963095088\n",
      "Model_name =  temp/a54 Violations =  0.0\n",
      "Average_violations =  63949.60179974023 148.357857614104\n",
      "MSE =  50.84707378487653 0.6550279047676356\n",
      "Model_name =  temp/a55 Violations =  0.0\n",
      "Average_violations =  64605.42194647858 263.66314834798465\n",
      "MSE =  51.82668298255968 0.7630346104402871\n",
      "Model_name =  temp/a56 Violations =  0.0\n",
      "Average_violations =  64262.66490909482 181.0927684914564\n",
      "MSE =  47.116633307111826 0.7130280973998966\n",
      "Model_name =  temp/a57 Violations =  0.0\n",
      "Average_violations =  64295.794607710544 189.96861808801935\n",
      "MSE =  52.52343275664572 0.7980930459808172\n",
      "Model_name =  temp/a58 Violations =  0.0\n",
      "Average_violations =  64486.09595204286 141.21534890420259\n",
      "MSE =  38.255060303927166 0.5258914827124195\n",
      "Model_name =  temp/a59 Violations =  0.0\n",
      "Average_violations =  64668.27828025429 139.93738722347896\n",
      "MSE =  43.12308475311657 0.5968818826029338\n",
      "Model_name =  temp/a60 Violations =  0.0\n",
      "Average_violations =  63941.590333378546 186.8280675975085\n",
      "MSE =  43.6931058821182 0.6244752953564122\n",
      "Model_name =  temp/a61 Violations =  0.0\n",
      "Average_violations =  64215.86115723214 230.0562029641823\n",
      "MSE =  46.67850727373676 0.672627603340543\n",
      "Model_name =  temp/a62 Violations =  0.0\n",
      "Average_violations =  64569.25768413388 153.29511184489002\n",
      "MSE =  42.67214823665399 0.5526487165836335\n",
      "Model_name =  temp/a63 Violations =  0.0\n",
      "Average_violations =  64202.353133700984 145.62680458134747\n",
      "MSE =  51.01659602324827 0.8215536469257333\n",
      "Model_name =  temp/a64 Violations =  0.0\n",
      "Average_violations =  64781.92168158796 196.975780713167\n",
      "MSE =  44.68701035289696 0.6090516817629569\n",
      "Model_name =  temp/a65 Violations =  0.0\n",
      "Average_violations =  64468.93886398562 210.7882196124672\n",
      "MSE =  51.40044463755763 0.6901540475098789\n",
      "Model_name =  temp/a66 Violations =  0.0\n",
      "Average_violations =  64471.80294607967 201.48339078025555\n",
      "MSE =  49.21670175951253 0.7991493453923834\n",
      "Model_name =  temp/a67 Violations =  0.0\n",
      "Average_violations =  63927.813392866476 220.24649499444533\n",
      "MSE =  48.76799877597275 0.7339483161721863\n",
      "Model_name =  temp/a68 Violations =  0.0\n",
      "Average_violations =  64213.8417468149 266.9937251395353\n",
      "MSE =  49.810111319551055 0.7444221773099928\n",
      "Model_name =  temp/a69 Violations =  0.0\n",
      "Average_violations =  64365.72432049348 217.05822109632595\n",
      "MSE =  45.36054753251274 0.735915993014591\n",
      "Model_name =  temp/a70 Violations =  0.0\n",
      "Average_violations =  64306.11544848563 247.4446942457601\n",
      "MSE =  46.86369949608792 0.6446541386395243\n",
      "Model_name =  temp/a71 Violations =  0.0\n",
      "Average_violations =  63706.95479577173 194.8160644469498\n",
      "MSE =  49.98186906944957 0.7505931510047603\n",
      "Model_name =  temp/a72 Violations =  0.0\n",
      "Average_violations =  64540.31767721036 248.63482790754318\n",
      "MSE =  46.518187485803864 0.8371548230889378\n",
      "Model_name =  temp/a73 Violations =  0.0\n",
      "Average_violations =  64220.743137770005 176.82255777911175\n",
      "MSE =  45.38390939896964 0.7413062331321969\n",
      "Model_name =  temp/a74 Violations =  0.0\n",
      "Average_violations =  64446.49717822339 149.8216057684912\n",
      "MSE =  50.59153574858408 0.7340997696511953\n",
      "Model_name =  temp/a75 Violations =  0.0\n",
      "Average_violations =  63964.878576594274 301.02878735095453\n",
      "MSE =  43.35732306733427 0.6945655504491317\n",
      "Model_name =  temp/a76 Violations =  0.0\n",
      "Average_violations =  63872.907474253865 216.941058709454\n",
      "MSE =  42.16471181433132 0.6241392317387786\n",
      "Model_name =  temp/a77 Violations =  0.0\n",
      "Average_violations =  64036.946412127094 141.78472569349262\n",
      "MSE =  44.17828208619294 0.7575575072499726\n",
      "Model_name =  temp/a78 Violations =  0.0\n",
      "Average_violations =  64377.608534432955 207.13023787910052\n",
      "MSE =  47.34273402946944 0.6562411282073414\n",
      "Model_name =  temp/a79 Violations =  0.0\n",
      "Average_violations =  64267.131250280574 249.65933282111268\n",
      "MSE =  52.22912502405893 0.6833965179822291\n",
      "Model_name =  temp/a80 Violations =  0.0\n",
      "Average_violations =  64162.39262956851 231.71590341139816\n",
      "MSE =  51.98067617548277 0.6975201362468452\n",
      "Model_name =  temp/a81 Violations =  0.0\n",
      "Average_violations =  64754.99179826806 259.42370971015737\n",
      "MSE =  49.73592046886908 0.7274149773585978\n",
      "Model_name =  temp/a82 Violations =  0.0\n",
      "Average_violations =  64031.479519057415 165.92222152651394\n",
      "MSE =  47.563754018597734 0.6705151713207951\n",
      "Model_name =  temp/a83 Violations =  0.0\n",
      "Average_violations =  64542.71781489742 219.3331303890049\n",
      "MSE =  46.46452923497387 0.6696551522751918\n",
      "Model_name =  temp/a84 Violations =  0.0\n",
      "Average_violations =  64111.274101549425 191.8507564708706\n",
      "MSE =  54.3567044555446 0.9410427842606472\n",
      "Model_name =  temp/a85 Violations =  0.0\n",
      "Average_violations =  63639.46253875878 200.82222512061583\n",
      "MSE =  47.87120424865459 0.6546685882702106\n",
      "Model_name =  temp/a86 Violations =  0.0\n",
      "Average_violations =  64363.35434147279 245.3448659487497\n",
      "MSE =  45.77560045593116 0.7152526684064939\n",
      "Model_name =  temp/a87 Violations =  0.0\n",
      "Average_violations =  64262.52090262922 298.76757641570566\n",
      "MSE =  47.29662134994396 0.6230408015423717\n",
      "Model_name =  temp/a88 Violations =  0.0\n",
      "Average_violations =  63938.43366522204 173.3364721916896\n",
      "MSE =  46.149616848797386 0.6757096351453908\n",
      "Model_name =  temp/a89 Violations =  0.0\n",
      "Average_violations =  64255.91674408933 278.41751042047684\n",
      "MSE =  43.1003101611033 0.688772871768772\n",
      "Model_name =  temp/a90 Violations =  0.0\n",
      "Average_violations =  64309.988689781094 137.04467079361842\n",
      "MSE =  46.295352402350034 0.6482402117642873\n",
      "Model_name =  temp/a91 Violations =  0.0\n",
      "Average_violations =  64352.72175460233 170.52055485273195\n",
      "MSE =  49.042109962460124 0.7470875256201065\n",
      "Model_name =  temp/a92 Violations =  0.0\n",
      "Average_violations =  63892.09039430626 286.9041142273658\n",
      "MSE =  52.638399957748305 0.7993462028951727\n",
      "Model_name =  temp/a93 Violations =  0.0\n",
      "Average_violations =  63678.22855625792 169.3560532122961\n",
      "MSE =  44.32599823671684 0.6348238644119863\n",
      "Model_name =  temp/a94 Violations =  0.0\n",
      "Average_violations =  64659.30489618966 165.05659930948602\n",
      "MSE =  46.012085101665086 0.7102882913623945\n",
      "Model_name =  temp/a95 Violations =  0.0\n",
      "Average_violations =  64151.8810898137 146.7508141095757\n",
      "MSE =  46.0161469908926 0.7403846426129693\n",
      "Model_name =  temp/a96 Violations =  0.0\n",
      "Average_violations =  64187.60629815105 153.6640904511645\n",
      "MSE =  43.347635332922025 0.6426912844123516\n",
      "Model_name =  temp/a97 Violations =  0.0\n",
      "Average_violations =  64043.44580017645 181.57456019781483\n",
      "MSE =  46.746989336599036 0.6778851118070464\n",
      "Model_name =  temp/a98 Violations =  0.0\n",
      "Average_violations =  64404.2112584517 170.26179563808813\n",
      "MSE =  43.235236979022865 0.5996381080190287\n",
      "Model_name =  temp/a99 Violations =  0.0\n",
      "Average_violations =  64026.63695639389 216.61081756150276\n",
      "MSE =  42.56776240349106 0.6490960981803073\n"
     ]
    }
   ],
   "source": [
    "#the number of times to sample\n",
    "times = 4\n",
    "## the size of the test set\n",
    "\n",
    "\n",
    "violations = np.zeros(len(models))\n",
    "violation_mean = np.zeros((len(models), times))\n",
    "violation_mean2 = np.zeros((len(models), times))\n",
    "mean = np.zeros((len(models), times))\n",
    "\n",
    "fold = 0\n",
    "\n",
    "\n",
    "\n",
    "for t in range(times):\n",
    "    print(\"Times = \", t)\n",
    "    df_test = gen_data(SIZE = nb_test)\n",
    "    x_test = df_test[inputs].values\n",
    "    x_test_norm = normalize(df_test[inputs].values)\n",
    "    y_test = df_test[target].values\n",
    "    #bic_orig = get_bic(df_test,prior)\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        if type(models[idx]) is list:\n",
    "            keras.backend.clear_session()\n",
    "            model = load_model(model_name)\n",
    "        else:\n",
    "            model = models[idx]\n",
    "            \n",
    "        predicted = model.predict(x_test_norm)\n",
    "        test_df = pd.DataFrame(x_test, columns = inputs)\n",
    "        test_targets = pd.DataFrame(predicted,columns = target)\n",
    "        test_df = test_df.join(test_targets)\n",
    "       \n",
    "        \n",
    "    \n",
    "        mean[idx][t] = mean_squared_error(y_test, predicted) \n",
    "        test_df[test_df['cancer'] > 0.5] = 1\n",
    "        test_df[test_df['cancer'] <= 0.5] = 0\n",
    "        bic_pred = get_bic(test_df,prior)\n",
    "        \n",
    "        #bic_pred = get_bic(df_test.join(pd.DataFrame(model.predict(x_test), columns = ['target'])), prior)\n",
    "        \n",
    "        print(tetrad.getEdges())\n",
    "        print(bic_pred)\n",
    "        violation_mean[idx][t] = bic_pred\n",
    "        violation_mean2[idx][t] = bic_pred\n",
    "        #print(bic_orig - bic_pred)\n",
    "metric = []\n",
    "metric_err = []\n",
    "viol = []\n",
    "viol_err = []\n",
    "\n",
    "#normalize the violations for prettier graphing.\n",
    "#also violations are always positive, so just divide by max.\n",
    "\n",
    "#TMK \n",
    "#violation_mean = violation_mean / np.max(violation_mean)\n",
    "\n",
    "for i in range(len(violations)):\n",
    "    print(\"Model_name = \", model_names[i], \"Violations = \", violations[i])\n",
    "    print(\"Average_violations = \", np.mean(violation_mean[i]), np.std(violation_mean[i]))\n",
    "    print(\"MSE = \", np.mean(mean[i]), np.std(mean[i]))\n",
    "    #print(\"mean = \", mean[i])\n",
    "    metric.append(np.mean(mean[i]))\n",
    "    metric_err.append(np.std(mean[i]))\n",
    "    viol.append(np.mean(violation_mean[i]))\n",
    "    #viol.append(violations[i]/times)\n",
    "    viol_err.append(np.std(violation_mean[i]))\n",
    "   \n",
    "    \n",
    "MSE = []\n",
    "VIO = []\n",
    "VIO2 = []\n",
    "AUS = []\n",
    "for i, m in enumerate(models):\n",
    "    MSE.append(np.mean(mean[i]))\n",
    "    VIO.append(np.mean(violation_mean[i]))\n",
    "    VIO2.append(np.mean(violation_mean2[i]))\n",
    "    AUS.append(metrics_dicts[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20.74049842415203, 5.235394712056788)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(df['cancer']),np.max(perturbed_df['cancer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best by BIC =  5.383144733282942\n",
      "Best by AUC =  5.050848109479061\n",
      "Best by MET =  5.052532547089949\n",
      "Random =  5.3826957800316775\n",
      "-0.09893090523012207\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9909482395983314\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xd0lHXWwPHvzaQXShq9944QQEBAXLsuVhQbKCqvBXtb+64N2666FpC1wgrIqqy6KopKFUQIgqDSgqGHNCC9zMx9/5gBA4YwCZlMEu7nnJyZp98Zhrnz/KqoKsYYY8zRBAU6AGOMMXWDJQxjjDE+sYRhjDHGJ5YwjDHG+MQShjHGGJ9YwjDGGOMTSxjGGGN8YgnDGGOMTyxhGGOM8UlwoAOoTvHx8dq2bdtAh2GMMXVGcnJypqom+LJvvUoYbdu2ZeXKlYEOwxhj6gwR2errvlYkZYwxxieWMIwxxvjEEoYxxhifWMIwxhjjE78mDBFJFZG1IrJaRI5YGy0iA0TEJSIXl1k3TkQ2ef/G+TNOY4wxR1cTraRGqmrmkTaKiAN4BviyzLpY4FEgCVAgWUQ+UdW9/g7WGGNM+WpDkdQtwIdAepl1ZwDzVDXbmyTmAWcGIjhjjDEe/k4YCnwlIskiMuHwjSLSArgAmHLYphbA9jLLO7zr/kBEJojIShFZmZGRUU1hm9pqc3oeby75jZ927MPltumFjalJ/i6SGqqqu0QkEZgnIutVdVGZ7S8C96mqS0TKHnfIgle53w6qOhWYCpCUlGTfIPXYtqwCxkz9nsy8YgAahAdzYvs4hnSIY0jHeDolRnPY58gYU438mjBUdZf3MV1E5gADgbIJIwmY5f1PHg+cLSJOPHcUJ5fZryWwwJ+xmtotPbeIq95ajtPtZvb/DWb3/kKWpWTxXUomX/2yB4D46DBP8ugQx5AO8bSOiwxw1MbUL35LGCISBQSpaq73+enAY2X3UdV2ZfZ/B/ifqv7XW+n9lIg09m4+HbjfX7Ga2i2nqJSr31pBRm4x7103iBNaez4W5/X1lFJuzy44mDyWpmTxyZpdALRsHHEweQzuEEeTBuEBew3G1Af+vMNoAszx3j0EAzNUda6I3ACgqofXWxykqtki8jiwwrvqMVXN9mOsppYqKnVx/bsr2ZSey5vjBhxMFmW1io2kVWwklwxohaqSkpHH0pQsvtucyZc/72H2yh0AdEiIYmjHeIZ0iOPE9nE0igyt6ZdjTJ0mqvWn2D8pKUlt8MH6w+lyc9N7q5j36x5evLTvwTuKynC5lV9357A0JZPvNmexIjWbghIXItAmNpKEmDDio8OIiw4lLiqM+Jgw4qNCiYsOIz7a89ggPNjqRky9JSLJqprky771arTamuR2K7v2F5KSkU9Keh4pGXns3l/EFYNa86duTQIdXp2nqjw4Zx1f/bKHv/65e5WSBYAjSOjZoiE9WzRkwvAOlDjd/LRjH99tzmJjei5ZecVsTs/j+y3F7C0oLfccIQ7xJhNPUomLDiWhTJLpkBhNrxYNcQRZUjH1myWMoygscbElM++QxJCSkc9vmXkUlboP7tcgPJiIUAcTpifzzEW9ubh/ywBGXfc99+UG3l+5nVtP6cjVQ9sd/QAfhQYHkdQ2lqS2sX/Y5nS5yS4oITO3hKz8YrLySsjMKyYzr4SsvGKy8j3Lm9PzyMgrpsR56L//kA7xDOscz7COCVbhbuolSxh4fs2m5xZ7EkLm74lhS0Y+O/cVHtxPBFo1jqR9QhRDOsTRISGaDglRdEiMJi4qlIISFxOmr+Tu/6xhf2Ep155UfV90x5M3Fm/htQUpXD6oNXec1rnGrhvsCCIxJpzEmKNXjqsqecVOMvNKWLdzP4s3ZbBkUyZzf04DoHVsJMM6xTOsUzyDO8TTMCLE3+Eb43fHfR2G0+Wm/xNfs7/w9+KIyFDHwWTQPiHa8zwxirZxUYSHOCo8X7HTxe2zVvPFujRuOaUjd57W2cq/K+GjVTu4c/YazurZlFcu71eninlUlS2Z+SzemMGSzZksS8kiv8RFkECfVo0Y1jGeYZ0T6NuqESGO2jDIgjGVq8M47hMGwAvzNhIbFXowMTRtEH5MX/Iut/LAR2t5f+V2rjyxNY+N6klQHfriC5Rv1+/h+mnJDGoXy9vXDCAsuOLkXNuVutz8uG0fSzZlsHhzJmu278OtEB0WzIntYxnWKYGTOsXTPj7KflSYgLGEUQuoKk/PXc/rC7fw5z7N+fvoPoQG26/KI0nems0VbyynY2I0M68/kZjw+leEs7+glGVbMlm0KZMlmzLZll0AQPOG4ZzUKZ5Tujbh5C4JR72LNaY6WcKoRSYvSOGZues5uUsCk6/oT0SofRkcbkNaLqOnLCUuOoz/3DCY+OiwQIdUI7Zm5bPYmzyWpmSSU+QkMtTBKV0TObtXM0Z2SbTPi/E7Sxi1zMwftvHgnLX0a92YN68eYBWgZezYW8BFk5eiCh/eOIRWscdn6yKny833W7L5fN1uvlyXRlZ+CREhDkZ2TeDsXs04pWsikaHWRsVUP0sYtdDna3dz26wf6ZAQzbRrB/rUEqeqSpxu/rt6J5+v3U3Xpg0Y3jmepDaxta5ILCuvmNFTlpGZV8zsGwbTtWmDQIdUKzhdbn74zZM85q7bQ2ZeMeEhQZzcOZGze3uSR3SYJQ9TPSxh1FKLN2Xwf9OTSYgJ49/XDqr2X9NFpS7eX7GdqYu2sHNfIS0aRbAnpwinW4kKdTC4QxwjOicwvHMCbeKiqvXalZVX7OSyqd+zcU8u7103qNx+EcbTgGJFajafr93NF+vSyMgtJiw4iBGdPXcef+qWWOX6nqJSF9uzC9iaVcDW7AK2ZeV7HwvYsbeQ2KhQOjeNoXNiNJ2bxNC5aQydEqOJsmRVr1jCqMVWbdvLNW+vICw4iOnXDqJL05hjPmdesZN/f7+VNxb/RmZeMUltGnPzKR05uXMC+SUulqVksXBjOgs3ZrA929OvpG1cJMM7JzCicwInto+r0S+BYqeL8e+s4Pst2Uy9qr/1jPeRy60kb93rTR672ZNTTKgjiOGd4zm7VzNO7d6EBoclj/0FpWzNzmdrVgHbsgvYmvX787ScIsr+948JC6Z1XCRt4iJp2TiSzNxiNqbnsjn90E6qLRtHeBJIkxg6N/Ekk46J0VZZX0dZwqjlNu7J5ao3l1NU6ubtawbQr5wB9Xyxr6CEd5am8vZ3qewvLGVYp3huHtmRQe1iy22mqaqkZhWwaGMGCzdmsCwli8JSFyEOIalNLCO6JDC8UwLdmsX4rZmny63cMnMVn69N4++j+3CR9YivErdbWbVtL5+t3c0Xa9NIyykixCEM65RAVFjwwbuFfYcNdxIfHUbbuEhPYoiNoo33edu4KBpHhpT77+5yK9uzC9i4J9f7l8fGPbmkZORR6vJ8fwQJtImLolOZu5HOTaJpHx9d64pCzaEsYdQB27MLuPLN5aTnFDN1bH+GdUrw+diM3GLeWLKFfy/bSn6Ji9O6N+HmkR3p26pRpWIodrpYmbr3YAJZn5YLQGJMGMM6JTCiSwLDOsbTOKp6RnVVVR767zreW76NB8/uxvXD21fLeY93brfy4/Z9fLF2N1/9sgdFaRMb5U0EkbQ+kBhiI6v1TrLU5WZrVj4b9+SxIS2XTem5bEjLJTWr4OBsiMFBQrv4KLo0jaFr0xi6NG1A16YxtGgU4fe+SblFpQeT24G/XfuKaBUbebCYrVOTaDo1iTmu64QsYdQR6blFjHtrBZvTc3lpzAmc3atZhfvv3FfI1IUpzFqxnVKXm3N7N+emkR2qrbI4bX8RizZlsGhjBos3ZbK/sBQR6Nq0AbFRIUSGBhMdFkxkqIOoA4+hwUSGeR9DHZ7tYcFEhTp+fwwNJjQ4iH/M28g/v9nE/41oz/1ndauWmE3tU+x08VtmPhvSPF/SG9Ly2LAn52BxKEBUqINOTQ4kkRhvQmlAbBV+nBSWuNicnseGPbls2pPLhj25bEzLZdf+ooP7RIY66JQYTfNGEWzLLmBzeh7FZcYCa9Eogk7e4rUDd0kdq6m+xuVWsvKK2ZNTTHpuEXtyitmTU0R6bjHFThft46PomOhJXm1iIwmu4VEALGHUIfsLS7n2nRWs2raXJy/oxWUDW/9hny0ZeUxekMKcH3cCcGG/Ftx4ckfaxfuv4trlVn7asY9FGzNZtW0vecVO8oud5Jc4KSh2kV/iPKRc+2hCHEKpSxndvyXPXtzbejYfh/KKnd4E4vlbn5bDhrTcQ0YJTogJ8yQRb7FW16YxdEqMISLUQYnTzZbMvIOJ6MDdw7bsgoN1MaHBQXRIiKZLk2hvhb0nGR1+R1O2mG1Tet7B86Vk5B0yqOSB+ppOTaLpnOipt+mQGEVkaHCFiSDd+7gnp4jMvGIOn35eBOKiwghxCLvLJLYQh+eOrFNiDB0So+mUGE2nJtG0i4/y28gHljDqmMISFze+l8yCDRncd2ZXbjy5AwDr03J4dX4Kn/20ixBHEGMGtGLCiA60aBQR4Ig9XG6loMRJQYmL/OLfH/NLnOQXuygo+1jionFkCOOHtqvxX1Cm9lJVMnKLWe9NIhu8CWXjntyDdwAi0LRBOBm5xTi937yOA0VdZSvfm8Yc8y90l1vZmpXPpvQ8NpVJSlsy8ilx/R5P48hQ9hWUHDERJMaE0aRBGIkx4Z7HBuHedeEkNvDMwXJgPLH8YicpGXls2pPHpvQ8NqfnsTndkwgPnP9AHVHHxGg6Hkgkib8nr2NhCaMOKnW5ufs/a/h49S6uGNSaPTnFfP3rHqJCHVw5uA3XndSehJjjowe0MQe+uD13Ip4vzwPFRl2axvj1F3d5nC43W7MLDiaRtJwi4qNCSWgQTpMjJIJjVVTqKdrblJ7H5j25bPYmld8y8w8mTvAUp3Vr1oB/je1fpTt3Sxh1lNut/PXTn5m2bCsNI0K4Zmhbrh7S1qYSNcYcdKCxweZ0TwLZnJFHcambKVf1r9L5LGHUYarKitS9dG/e4LhuuWGMqRm1ZopWEUkFcgEX4Dw8KBE5D3gccANO4HZVXeLd5gLWenfdpqqj/BlrbSEiDGxnvZ6NMbVPTfyEHamqmUfY9g3wiaqqiPQGZgNdvdsKVbVvDcRnjDHGBwEt81DVvDKLUUD9KR8zxph6xt/tGxX4SkSSRWRCeTuIyAUish74DBhfZlO4iKwUke9F5Hw/x2mMMeYo/H2HMVRVd4lIIjBPRNar6qKyO6jqHGCOiAzHU59xqndTa++x7YFvRWStqqYcfgFvIpoA0Lr1Hzu9GWOMqR5+vcNQ1V3ex3RgDjCwgn0XAR1EJP6wY7cAC4ATjnDcVFVNUtWkhATfx2MyxhhTOX5LGCISJSIxB54DpwPrDtuno3h7mohIPyAUyBKRxiIS5l0fDwwFfvFXrMYYY47On0VSTfAUNR24zgxVnSsiNwCo6hTgImCsiJQChcCl3hZT3YDXRcSNJ6k9raqWMIwxJoCs454xxhzHKtNxz0aBM8aYusztpjR7S41cysaeMMaYushZwpYVk/nXz2+zVZy8N24VEuzfcecsYRhjTF1SlMP6ZX9n6qYP+DpUCA8WRicOpsRdShiWMIwxxuSmsWbxk/xr+zwWhocQHergulZ/4sohDxEbEVcjIVjCMMaYWkwzNrJy0WO8nvkDy8PDaBgezsT253HZoLtoEFo90zP7yhKGMcbUQrptOd8teZKpuev5MTyMuIgY7up6OZeccCORIZEBickShjHG1BZuN+6NXzB/6bNMde7ml7AwmkY15oFe13FBj6sIDw4PaHiWMIwxJtCcJbh+ep8vV7zIv4Ly2BwaSqvwBP7W92b+3GU0IY6QQEcIWMIwxpjAKcqhdOVb/G/167wZrmwND6FDeAue7n87Z7Q/m+Cg2vUVfcRoRKQj0ERVvzts/TBgV3kjxxpjjPFB7h6Kv3+F//4ygzejQ9kdE0y3qBa8kHQXp7T5E0FSO/tUV5S+XgQeKGd9oXfbn/0SkTHG1FfZv5H/3QvMTvmEaTGRZDaKpE/DjjyUdAfDWgzDO/ZerVVRwmirqj8dvlJVV4pIW79FZIwx9c2en9m3+Fne27mAGTHR5DSO4cT4Pjzd7xYGNh1Y6xPFARUljIqq4yOqOxBjjKl3tn3PnkXPMC17Ff+JiaGwUQNOaTaE606YSK+EXoGOrtIqShgrROR6Vf1X2ZUici2Q7N+wjDGmjlKFTfPYvuRZ3srfzMcx0bgbNuTsNqcxvs+NdGzcMdARVllFCeN2PPNZXMHvCSIJzyRHF/g7MGOMqVNcTvjlv2z87nnedKUzNyqS4IaNuLDDeVzd+3paxrQMdITH7IgJQ1X3AENEZCTQ07v6M1X9tkYiM8aYuqC0CFa/x5rlL/FGUD4LoiKJDGrMuK5juKrH1SRE1p+po31t5Ktl/owxxhTloCve4Pvk13kj3M0PMeE0dMRxU8+rubzbFTQMaxjoCKtdRf0wWgAfAUV4iqQEuEREngEuUNWdNROiMcbUIvlZuJe+zPx103gjKph1jcNICG3I3b2vZ3Tn0QEb56kmVHSH8QowWVXfKbtSRMYCrwHn+TEuY4ypXYrzcC17lS9Xv87UqFBSYqNoGZHIo31vZFSHUYQ6/DsXRW1QUcLorqp/qNxW1Wki8qAvJxeRVCAXcAHOw+eNFZHzgMcBN+AEblfVJd5t44CHvLs+oarv+nJNY4ypVs4S3Mnv8NXyvzMlAlIaR9ExuhXPnDCR09ueXuuG7/Cnil6po7yVIhJ0pG1HMFJVM4+w7RvgE1VVEekNzAa6ikgs8CieVlkKJIvIJ6q6txLXNcaYqnO7ca/7kK+XPM7kkGI2NwylQ1Rznku6g9PbnF5rh+/wp4oSxqci8i88v/rzAUQkCngB+Lw6Lq6qeWUWo/i9Uv0MYJ6qZnuvOw84E5hZHdc1xpgjUsW9eR7fzn+YyexjY1QobcNb8MyAuzij7Zk4girze7l+qShh3AtMAraKyFY8X+ZtgHcpf4yp8ijwlYgo8LqqTj18BxG5wHudROAc7+oWwPYyu+3wrjPGGL/R7SuZ/829TC7ZyfqwUNqENWVS0t2c1f7s4zpRHFBRP4xS4G4ReRjoiKeV1GZVLajE+Yeq6i4RSQTmich6VV102HXm4OkgOBxPfcap3mv9IaTyLiAiE4AJAK1bt65EaMYY46HpG1j49T28lreBX8NCaRWdwJNJd3J2x1HHVR3F0Rz1nVDVQmDtgWUROQ24V1VP8+HYXd7HdBGZAwwEFh1h30Ui0kFE4vHcUZxcZnNLYMERjpsKTAVISkqyfiLGGJ/p/p0s/voeXstK5uewUFpExfJ4v9s4t8vFlijKUVE/jFOAKUBz4L/AU8A0PL/+nzzaib31HUGqmut9fjrw2GH7dARSvJXe/fAMO5IFfAk8JSKNvbueDtxfyddmjDHl0oJsvvvmfibvXshPYSG0iGjomd2ux+WEBNWO2e1qo4pS6N/xFPUsA84CvgceVtWXfDx3EzxFTQeuM0NV54rIDQCqOgW4CBgrIqV45tm4VFUVyBaRx4EV3nM9dqAC3BhjqkqL81m28FFe2/oZa0KDaRYexaO9J3Ber/G1ZhrU2kw838/lbBBZpar9yiynqGqHGousCpKSknTlypWBDsMYU9u4nPz43bO8uPE9VoUE0YRgJnQfywX9Jh73iUJEkg/vI3ckFd1hNBKRCw897+/LqvpRVQM0xpgaocr21dN4IfkfzAtxkxAczAOdLuGiQfccFz2zq1tFCWMhh07DWnZZ8YwzZYwxtdL+zV8zddGDzAjKJyRYuKnFnxg3/EkiQ6MCHVqdVVGz2mtqMhBjjKkOpWlrmTXvTqaU7iQ3KIjzG/di4il/JzGmeaBDq/MqaiV1J7BfVd88bP0tgENVX/R3cMYY4yvdt51v5t3NC/vXsC0khMHRrbhrxLN0adIn0KHVGxUVSY0H+pWzfiqe1kuWMIwxgVe4l3XfPsJzO75iVXgoHSJiee3Ehzip/Vl4W2maalJRwlBVLSlnZbHYv4IxJtBKi9j13fO8tH46n0eEEhsRxcO9/48Le19rne78pMJ3VUSaeKdqPWSdf0MyxpgKuF3krnqHN1e+wPQwkIgwrm9/HuMH3Ud0aHSgo6vXKkoYzwGfichdwCrvuv7As8Dz/g7MGGMOoYpzw+d8sPARJocUkh3h4M+JA7l1+JM0jWoa6OiOCxW1kpomIhl4hvPo6V29DnhUVb+oieCMMQZAty1n0df38XdXGr+Fh5AU3YHXRjxNj/gegQ7tuFJhkZQ3MVhyMMYERsZG1s67j5dy1rE8Ipy2EYm8NPhhRrY51Sq0A6CiZrUvc+iQ4gpkAvMPTKNqjDF+kbOLDV8/yCt7FrMgMoLGUQ35S9+buaTHlTY4YABVdIdR3qBMscBzIvK+9cMwxlS7wr1smf84r239lC8jw4mJiuGW7uO4os/1RIVYD+1Aq6gO493y1ovIFGAp1g/DGFNdSovY/t3zTPl1Ov+LCCEsMpLrO13MuP630jCsYaCjM16VbqysqoVWdmiMqRZuF2krpjJ19avMCQNHZBhXtT2b8YPuJTY8NtDRmcNUKmGISDBwFZ4Z8YwxpmpUyVz3H978fhKzg0txhwkXtxjB9UMfITEyMdDRmSOoqNI7lz/Oo12IZ9Ta//NnUMaY+mt/yje8veghZpBDSUgQ5yUk8X/Dn6R5TItAh2aOoqI6jJiaDMQYU7/l7VzF9Pn3MK00jXwRzmrUnRtHTKJt446BDs34qLJFUh2Ay4AxqtrzaPsbY0xBVgozv76Dt/M3s9/h4E/Rbbl5xCQ6JfYOdGimko6aMESkGTAGT6LoDUzyPjfGmCMq3LedD+ffxxt7V5PlcHBSRDMmDn+CHi0GBzo0U0UV1WFcjycxtARmA9cBH6vq32ooNmNMHbQ//WdmLXyY93I3sNcRxIDQxrww+GFO6HBmoEMzx6iiO4xXgWXA5aq6EkBEDq8Er5CIpAK5gAtwHj7RuIhcAdznXcwDblTVNb4ca4ypXdK3f8/0JX9ldtF2CoKCGBYWz/ikO0nqcl6gQzPVpKKE0RwYDfzDO6T5bKAqffJHqmrmEbb9BoxQ1b0ichaeyZkG+XisMaYW2LrpC95e/jSfOLNwAWdEtOTaIQ/SpfXwQIdmqllFraQygcnAZBFpiaceI11EfgXmqOoDx3pxVV1aZvF7PMVfxpjaTpWf183krVUvM09zCQEujO7IuGGP0aqJVWbXVz61klLVHXjmwHheRLrgSR4+HQp85S3Kel1Vp1aw77UcOjJuZY41xtQAdbtZkTyZN9a9xbKgEqLdyvjYPlw5/AniG7cLdHjGz6oyNMgGwNeK76GquktEEoF5IrJeVRcdvpOIjMSTME6qwrETgAkArVu3ruzLMcb4wO0qZf7SZ3hz0wesdbiIA25PGMIlI54gJioh0OGZGuLXiW9VdZf3MV1E5gADgUO+9EWkN/AGcJaqZlXmWO/2qXjqPkhKSqpUpbwxpmKlxXl8tvhvvLVtLr85oKUID7c4k1HDHiE8zPr2Hm/8ljBEJAoIUtVc7/PT8czeV3af1sBHwFWqurEyxxpj/KegIJOPFjzEu2lLSHMIncXBMx0u5vQT7yU4ODTQ4ZkA8aXjngBXAO1V9THvl3xTVf3hKIc2AeZ4R7YNBmao6lwRuQFAVacAjwBxwGve/Q40ny332Kq8QGOM7/JzdzNz/l94NyuZfUFCv6BwHul+NSf1vwkJCgp0eCbARLXiUhwRmQy4gVNUtZuINAa+UtUBNRFgZSQlJenKleXN+2SMqUh+zk5mzr+Pd7J/ZH9QECdJFBP6TuSE3lcGOjTjZyKS7Gs/N1+KpAapaj8R+RHA22fC7kmNqQfy9+9g5rf38s6+NZ5E4WjAjQPuone3iwMdmqmFfEkYpSLiwDvUuYgk4LnjMMbUUXl7U5k5/z7e3beO/Y4ghgU34sYB99Cr6/mBDs3UYr4kjH8Cc4BEEXkSuBh4yK9RGWP8Ii87hRnf3se7Ob+S4whieGgsNwy8l16d/xzo0EwdcNSEoarviUgy8CdAgPNV9Ve/R2aMqTZ5mRuYMf9+3s3dQI4jiBFhcdww6C/07Hh2oEMzdUhFo9WWnVA3HZhZdpuqZvszMGPMsctN/4UZC+5nWt5mb6KI58bBD9Cj/RmBDs3UQRXdYSTjqbeQcrYp0N4vERljjllu2k+8t/ABpuX/Rq4jiJPDE7lhyEP0aPunQIdm6rCKBh+0gWGMqWNyd/3IewsfZFrhVk+iiGjKDUMepkebkwMdmqkHfOrpLSIX4hnnSYHFqvpfv0ZljPGd281v62Yxa/VkPnZlkx8UxMjI5tww5FG6tz7p6Mcb4yNfenq/BnTk9zqMG0TkNFW92a+RGWMq5Crcz+KlTzMz9TOWBivBqpwZ04GrBt1H91ZDAx2eqYd8ucMYAfRUb5dwEXkXWOvXqIwxR7Q/7SfmLH2SWXvXsTM4iERHEBObjeCiwQ8QH9Ms0OGZesyXhLEBaA1s9S63An7yW0TGmD9SZcPaGcxc8zqfObMoCgqif2hD7uh+Jaf0vY4Qhw2+YPzPl4QRB/wqIgcGGxwALBORTwBUdZS/gjPmeFdalMM3SycxM/ULVjlchKtyTkxHLht0N11aDQt0eOY440vCeMTvURhjDpG5Zy0ffPcE/9m3jnRHEC1EuLvZSM4f8gANo5sGOjxznPKlp/dCABFpUHZ/67hnTPVSt5u1P89ixpopfOnMxinCkJAGPNJ9LCf1vR6Hw6/znRlzVL60kpoAPA4U4hl0ULCOe8ZUm9KSAuZ+9yTvpX7Gz0EuotzKJdEdGHPivbSz1k6mFvHlJ8s9QA9VzfR3MMYcT3Lz0vlg0SP8e893pAdBOxUeaDqSUSc9RFRUYqDDM+YPfEkYKUCBvwMx5niRlrWefy96lA/2/Ux+kDCIUP7a5SpOGnAL4nAEOjxjjsiXhHE/sFRElgPFB1aq6q1+i8qYeujX7Ut4d+mTfFm4HQXOcDRkXL9b6N5zTKBDM8YnviSM14Fv8XTWs4mTjKkEVWXp+g95e9U/We7cS6TbzWWhzbhqyIM0azcy0OEZUym+JAyE877qAAAdiUlEQVSnqt5ZlZOLSCqQC7i850k6bPsVwH3exTzgRlVd4912JvAS4ADeUNWnqxKDMYFQ6irl8x+n8M4v09mshSQ6XdzRoAsXD/8bDZr0DnR4xlSJLwljvrel1KccWiTla7PakRVUmP8GjPDOE34WMBUY5J0S9lXgNGAHsEJEPlHVX3y8pjEBkVO8n/8sf54Zv31KOi46lTp5MmEQZ538OCENWwU6PGOOiS8J43Lv4/1l1lVLs1pVXVpm8Xugpff5QGCzqm4BEJFZwHmAJQxTK+3K2cG/lz3Jh7u/o0CUE0tcPN76TAYPexiJbBzo8IypFr503DuWeTEU+EpEFHhdVadWsO+1wBfe5y2A7WW27QAGHUMcxvjFz+lreHfZJL7a+zOCcmaJMK7rZXQdfAeERAQ6PGOqla/zYfQEugPhB9ap6jQfDh2qqrtEJBGYJyLrVXVROecfiSdhHBi8/0iz/JUX2wRgAkDr1q19CMmYY1PoLGTu+tm8v/Ztfi7JIsrt5ipnGFeccCNNT7gGrEe2qad86en9KHAynoTxOXAWsAQ4asJQ1V3ex3QRmYOnqOmQhCEivYE3gLNUNcu7egeeUXEPaAnsOsI1puKp+yApKancpGJMdfht3xZmJ7/Mxzvmk4uLDiUlPBDanD8PvI3obqMgKCjQIRrjV778FLoY6AP8qKrXiEgTPF/wFRKRKCBIVXO9z08HHjtsn9bAR8BVqrqxzKYVQCcRaQfsBMbwe12KMTWm1F3Kgt/m8v6PU1iev41gVU4rLOWS5sPpP+ReJKFToEM0psb4kjAKVdUtIk7vAITp+Fbh3QSYIyIHrjNDVeeKyA0AqjoFz0i4ccBr3v2cqpqkqk4RmQh8iadZ7Vuq+nNlX5wxVbUnfw8frvkXH6Z8TLq7iGZOJ7e6o7ig19XE97sGQqMCHaIxNc6XhLFSRBoB/wKS8fSX+KHiQ8DbwqlPOeunlHl+HXDdEY7/HE8RmDE1wq1ulu9axuxVrzA/ex1uVYYWFfNww54MG3EXjjZDQcqrXjPm+OBLK6mbvE+niMhcoIGq2ox7pt7YX7yfj3+Zyexfp7O1NIfGLhdji2F0xwtpdeJEiLH5J4yBChKGiLQB9qnqfu/ySOB8YKu3tVNJDcVojF+sy1zH+6te44vd31GMm75FxdwQ2pzTB9xMaPfzwRES6BCNqVUqusOYDVwA7BeRvsB/gEl4iple4whFScbUZqrK97uWMnnZk/yYv50It5vzCkq4pMXJdDnzDmjSPdAhGlNrVZQwIg40iwWuxFPx/HcRCQJW+z80Y6qPqrJ81/dM/v4JVuVto4nTyV9Kwzmv97VE9xsL4Q0CHaIxtV5FCaNs7d4peIcG8baY8mtQxlQXVeWHXd/z2vdPsipvK4lOJw86I7hw8EOE9hxtfSeMqYSKEsa3IjIb2A00xjPEOSLSDLD6C1Prrdi1nFeXPUFyXiqJTicPOMO56MQHCe01GoJsoiJjKquihHE7cCnQDDhJVUu965sCD/o7MGOqasXuH3ht2ROszP2NRKeT+0vDuWjwg4RZojDmmBwxYaiqArPKWf+jXyMypopWpq3gtaWPsyL3NxKcTv5SGsbFJ95PWO9LLVEYUw1slDRT5yWnreS1ZY/zQ84W4p0u7isN4eJBDxLee4wNBGhMNbL/TabOWpWWzGvLHmd5TgpxThf3loYweuA9hPe53BKFMX5QUce9b1T1TyLyjKred6T9jKlpP+5ZxatLH2d5zmbinC7uKQ1m9MD7iOhzhSUKY/yoov9dzURkBDDKO+PdIW1pVXWVXyMz5jBb96fy3ML7WLj3F2JdLu4uCeaSAfcS0fdKSxTG1ICK/pc9AvwFz1wU/zhsm+Lpm2GM3+WX5vP6d48xPfVzwtTN7UVBXDbgLiJPGGvDdxhTgypqJfUB8IGIPKyqj9dgTMYAntFjP13zBi+umUwmTs4rcnJ7r+uJHzQRgkMDHZ4xxx1fRqt9XERGAcO9qxao6v/8G5Y53q3duoBJSx5krTOH3sWl/LP1OfQa+VcIiwl0aMYct3yZonUSnqlV3/Ouuk1Ehqrq/X6NzByXMvZu4cWvb+OTglTinS6ebNSHcy98gSAbYtyYgPOlpvAcoK+qugFE5F3gR7xjSxlTHUqKc/n313fyevoySgXGhzZjwpkvENWkZ6BDM8Z4+dq0pBGQ7X3e0E+xmOOR28WiJU/xzObZbHPAyUFR3DP0b7TueGagIzPGHMaXhDEJ+FFE5uNpWjscu7swx0qV39bO4NmVz7PE4aStBDG5xwROSro50JEZY47Al0rvmSKyABiAJ2Hcp6pp/g7M1F+5qYt4fcFfeI8cwoOEe1qczmUnP0VIcFigQzPGVMCnIilV3Q18UtmTi0gqkAu4AKeqJh22vSvwNtAPeFBVn/f1WFP3uDM28PFXd/BicSp7g4K4oFEPbj31JeKirULbmLqgJrrHjlTVzCNsywZuxTNXeGWPNXXF/h0kf/0Az2Ut5+ewUPpGNuG1Ec/So9mAQEdmjKmEgI6noKrpQLqInBPIOIyf5GWwcf5feWnnPBZFhJEYEcOk/ndxTrcx2KyNxtQ9vvTDmK6qVx1t3REo8JWIKPC6qk6tRGw+HSsiE4AJAK1bt67E6Y3fFO5j5+JneHXzh/wvMpToyChu73Yll59wExHBEYGOzhhTRb7cYfQouyAiDqC/j+cfqqq7RCQRmCci61V1UXUe600kUwGSkpLUx3MbfyjJJ3vpS0z95R3ejwzFERXO1e3P49qBd9MwzFpjG1PXVTS8+f3AA0CEiOQcWI1nPm+f7hRUdZf3MV1E5uDpMe5TwjiWY00NcxaT/8NUpv34Ku9EOiiKCuOCFidzw+AHaRplFdrG1BcVDT44CZgkIpOqMgyIiEQBQaqa631+OvCYv481NcjlpHT1e8xe/hxTw91kR4dwakI/bhn6KO0btg90dMaYauZLkdQXIjL88JU+FC01AeZ4KzeDgRmqOldEbvAeP0VEmgIrgQaAW0RuB7oD8eUd6+NrMv7mduP++SM+X/Ikr4QUsTMqmAENu/DykL/SO7FPoKMzxviJLwnjnjLPw/EUDSVzlPkwVHUL8IdvD1WdUuZ5Gp75Ng6XU96xJsBU0Y1fsWThI7zEPjZEhtIlshWTBz/M0BYnWcsnY+o5X3p6/7nssoi0Ap71W0SmdkpdwppvHuLF0p2sjAinRWgiTw+8l7Pan0OQBAU6OmNMDahKP4wdgA0herxI/5Utc+/mn3nr+SYqktiwWO7vN5HRXcYQYrPdGXNc8aUfxst4+kQABAF9gTX+DMrUAqWFFC6YxKu/Tmd6gyjCoxtyU89rGNfrWiJDIgMdnTEmAHy5w1hZ5rkTmKmq3/kpHlMbbP6GFXPv4K/hJWxrGM1F7c7l1oH3EBseG+jIjDEB5EvCeB/oiOcuI0VVi/wbkgmYvHTy597HC7vn836DGFqGN+PN4c8wsNnAQEdmjKkFKuq4Fww8BYwHtuIpjmopIm/jGVm2tGZCNH7ndsOP0/hu4WP8rWE4aQ1iuLLLZdzS/3YrfjLGHFTRHcZzQAzQTlVzAUSkAfC89+82/4dn/C79V/Z/eivPF27mv3HRtItqwbThT9M3sW+gIzPG1DIVJYxzgc6qenB8JlXNEZEbgfVYwqjbSgth0XN8s+p1nohrxN6YGK7rOZ4b+t5ImMMmMjLG/FFFCUPLJosyK13eEWRNXZXyLdmf3c4kRx5zE2Pp0rADrw57iu5x3QMdmTFVVlpayo4dOygqsmrW8oSHh9OyZUtCQqreHL6ihPGLiIxV1WllV4rIlXjuMExdk5eOzr2fL377nEnx8eQ5GjCxz42M7zWekCDrU2Hqth07dhATE0Pbtm1t1IHDqCpZWVns2LGDdu3aVfk8FSWMm4GPRGQ8nqFAFM+83hHABVW+oql53krt9G8e5fGYUBYkxtMrrgePDX2Cjo07Bjo6Y6pFUVGRJYsjEBHi4uLIyMg4pvNUNFrtTmCQiJyCZ04MAb5Q1W+O6YqmZqX/in56G//du47nEuMpcYRwd79bubLblTiCHIGOzphqZcniyKrjvTnqIECq+q2qvqyq/7RkUYcU5cA3j7HzXyP4P9d2HkmIo0vT/nx03hzG9RhnycIYP3A4HPTt25c+ffrQr18/li5dCkBqaio9e/4+otIPP/zA8OHD6dKlC127duW6666joKAgUGH7LKBzehs/yM+C5ZMpXT6V/4S6eLFlMyQ4jIf638XoLqNtoEBj/CgiIoLVq1cD8OWXX3L//fezcOHCQ/bZs2cPo0ePZtasWQwePBhV5cMPPyQ3N5fIyNrd78kSRn2xfycse4W01dP4MNzBB83jyMTJ0OaDeWTwIzSPbh7oCI05ruTk5NC4ceM/rH/11VcZN24cgwcPBjxFRRdffHFNh1clljDquqwUdMkLLN/wEe9HRzK/WSxuhJNaDGZM1zEMazHMynXNcedvn/7ML7tyjr5jJXRv3oBH/9yjwn0KCwvp27cvRUVF7N69m2+//fYP+6xbt45x48ZVa2w1xRJGXZW2jpzFz/HJ9m95v0E0qU3iaBTagLGdL2J059G0imkV6AiNOe6ULZJatmwZY8eOZd26dQGOqvpYwqhrtq9g/aInmJX9E59HR1EY14jejbvyVI+xnN72dOulbQwc9U6gJgwePJjMzMw/NGXt0aMHycnJnHfeeQGKrOosYdQFqhRvnsdX3z3F+8W7WBMeRnjDhpzd9kwu6XEVPeIC/5/DGHOo9evX43K5iIuLO6QF1MSJExk4cCDnnHMOgwYNAuDf//43p556Kk2bNg1UuD7xa8IQkVQgF3ABTlVNOmx7V+BtoB+eEXCfL7PtTOAlwAG8oapP+zPWWsntZudP7/Gf5Jf5SPLY63DQJiaRe3tezaguo2kY1jDQERpjyjhQhwGe3tXvvvsuDsehTdibNGnCrFmzuPvuu0lPTycoKIjhw4dz4YUXBiLkSqmJO4yRqpp5hG3ZwK3A+WVXiogDeBU4Dc+UsCtE5BNV/cWvkdYSbmcJ3y17jvc3zmaRw4U4hJMbdObSAXdwYsth1jTWmFrK5XKVu75t27aH1GUMHjyYxYsX11RY1SagRVKqmg6ki8g5h20aCGxW1S0AIjILOA+o1wlDVfl6+Qu8+MvbbHNAXJBwfdPhjB7yAE0btAx0eMaY45y/E4YCX3lHt31dVaf6eFwLYHuZ5R3AoOoOrjZJ2bOaSV/fwnLnPjoiPNtxDKcOvIuQEKvENsbUDv5OGENVdZeIJALzRGS9qi7y4bjyOg6UO6S6iEwAJgC0bt266pEGSF5JHpMX3M+MXfOJcCt/aXwCl549heCw6ECHZowxh/BrwlDVXd7HdBGZg6eoyZeEsQMo25GgJbDrCNeYCkwFSEpKqjPzdLjVzafr3+eFFc+R7S7hQmcwt57yArHtRwY6NGOMKZffEoaIRAFBqprrfX468JiPh68AOolIO2AnMAa43D+R1ryfs37mqQX38VPeVnoXF/Nqy7PocdozEBIe6NCMMeaI/HmH0QSY4x2WIhiYoapzReQGAFWdIiJNgZVAA8AtIrcD3b1TwU4EvsTTrPYtVf3Zj7HWiOyibP654nk+2vIpjV0uHi+NYNQ5bxHUckCgQzPGmKPyW8LwtnDqU876KWWep+Epbirv+M+Bz/0VX01yup3M3jCbV5JfpNBZwFU5edzQ7SpiRj4EwVapbUx9ISJceeWVTJ8+HQCn00mzZs0YNGgQ//vf/9izZw/XXnst27dvp7S0lLZt2/L555+TmppKt27d6NKly8Fz3XnnnYwdOzZQL6Vc1tPbz1akrWDS90+waf8WTiws5H6No/3F06H5CYEOzRhTzaKioli3bh2FhYVEREQwb948WrRocXD7I488wmmnncZtt90GwE8//XRwW4cOHQ6OQ1VbWQ8wP0nLT+Pehfcy/svx5Gen8EJ6NlM7X0P76xdbsjCmHjvrrLP47LPPAJg5cyaXXXbZwW27d++mZcvfC1V69+5d4/EdC7vDqGYlrhKm/TKNqWtex+0q5sa9+7kmrBURV74Hzf5QQmeM8Ycv/gJpa6v3nE17wVlHH6FozJgxPPbYY5x77rn89NNPjB8//mCv7ptvvplLL72UV155hVNPPZVrrrmG5s09c9WkpKQcHFYE4OWXX2bYsGHV+xqOkSWMarR4x2Ke/uFptuVu409FLu7JzKTF0LvgpDsgODTQ4RljakDv3r1JTU1l5syZnH322YdsO+OMM9iyZQtz587liy++4IQTTjg4ZEhdKJKyhFFNZvw6g0k/TKKdhPP67nSGNOoM177v+VVijKlZPtwJ+NOoUaO4++67WbBgAVlZWYdsi42N5fLLL+fyyy/n3HPPZdGiRfTv3z9AkVaOJYxqMO3naTy38jlGFrt4Pu03QkfcByfdDo6QQIdmjAmA8ePH07BhQ3r16sWCBQsOrv/222858cQTiYyMJDc3l5SUlDo1QoUljGP05to3eXHVi5xeWMrTBUGETFgATXsGOixjTAC1bNnyYEuospKTk5k4cSLBwcG43W6uu+46BgwYQGpq6h/qMMaPH8+tt95ak2EflajWmdE0jiopKUlXrlxZY9ebsmYKr65+lbMLinmyJJzgsZ9C4zY1dn1jzO9+/fVXunXrFugwarXy3iMRST58rqIjsTuMKlBVXln9ClN/msqo/CIeczfEcc0n0KB5oEMzxhi/sYRRSarKi6te5K11b3FRXgGPBDUl6Or/QnRCoEMzxhi/soRRCarKcyufY/ov07k0N58HwtoQdMWHEBkb6NCMMcbvLGH4yK1uJi2fxKwNs7gyJ5d7o7oil8+G8AaBDs0YY2qEJQwfuNXN498/zgcbP+DqfTnc2fgEZMwMCI0MdGjGGFNjLGEchcvt4tGlj/Jxysdcv28/tyQMQS5510aZNcYcd2zwwQo43U4eXPIgH6d8zE1793FLs1OQS6dbsjDGlEtEuOqqqw4uO51OEhISOPfccwF45513SEhIoG/fvgf/1qxZc/B5bGws7dq1o2/fvpx66qmkpqYSERFB37596d69O2PHjqW0tBSABQsWHDwvwBdffEFSUhLdunWja9eu3H333dX++uwO4whK3aU8sPgB5qbO5dbsfVzfbhSM+icEOQIdmjGmljra8ObAwcEHyzowhtTVV1/Nueeey8UXXwxAamrqwTGmXC4Xp512GrNnz+aKK6445Ph169YxceJEPvvsM7p27YrT6WTq1KnV/vrsDqMcpa5S7l14D3NT53JX1l6u7zIGRr1sycIYc1QVDW9+LBwOBwMHDmTnzp1/2Pbss8/y4IMP0rVrVwCCg4O56aabquW6ZdkdxmFKXCXcteBOFuxYyF+ysrmi17Vw6t/AM9WsMaYOeOaHZ1ifvb5az9k1tiv3DbzvqPtVNLw5wPvvv8+SJUsOLi9btoyIiIijnreoqIjly5fz0ksv/WHbunXruOuuu3x8JVVndxhlFDmLuPXbW1iwYyEPZWZzRf/bLFkYYyqlouHNwVMktXr16oN/R0sWB8aYiouLo3Xr1gGddMnuMLwKnYXc+s1Elqf9wN8ysrhwyP0w5JZAh2WMqQJf7gT8qaLhzSvrQB3G7t27Ofnkk/nkk08YNWrUIfv06NGD5ORk+vTx7yRtfr3DEJFUEVkrIqtF5A+jAorHP0Vks4j8JCL9ymxzeY9bLSKf+DPOgtICbp53A8vTfuDxjCwuHPGYJQtjTJWNHz+eRx55hF69qm8+nGbNmvH0008zadKkP2y75557eOqpp9i4cSMAbrebf/zjH9V27QNqokhqpKr2PcJoiGcBnbx/E4DJZbYVeo/rq6qjyjm2WuSX5nPjV9eTvGcVkzKyOO/U52HAdf66nDHmOHCk4c3BU4dRtlnt0qVLfT7v+eefT0FBwSF1IuApBnvxxRe57LLL6NatGz179mT37t3H9BrK49fhzUUkFUhS1cwjbH8dWKCqM73LG4CTVXW3iOSpanRlrleV4c1L8jO45/0zOTs7jTPOegV6XFCp440xtYMNb350xzq8ub/vMBT4SkSSRWRCOdtbANvLLO/wrgMIF5GVIvK9iJx/pAuIyATvfiszMjIqHWBoeGNebNCXM/78hiULY4ypgL8rvYeq6i4RSQTmich6VV1UZnt5zY8O3PK09h7bHvhWRNaqasofdladCkwFzx1GpSN0BCMXv1npw4wx5njj1zsMVd3lfUwH5gADD9tlB9CqzHJLYNdhx24BFgAn+DNWY4wxFfNbwhCRKBGJOfAcOB1Yd9hunwBjva2lTgT2e+svGotImPfYeGAo8Iu/YjXG1A/1acrp6lYd740/i6SaAHPE0+ktGJihqnNF5AYAVZ0CfA6cDWwGCoBrvMd2A14XETeepPa0qlrCMMYcUXh4OFlZWcTFxSHW2fYQqkpWVhbh4eHHdB6/tpKqaVVpJWWMqR9KS0vZsWMHRUVFgQ6lVgoPD6dly5aEhIQcsr4yraSsp7cxpl4ICQmhXbt2gQ6jXrOxpIwxxvjEEoYxxhifWMIwxhjjk3pV6S0iGcDWQMdRTeKBcodUMfbeHIW9P0dm780ftVHVBF92rFcJoz4RkZW+tlw43th7UzF7f47M3ptjY0VSxhhjfGIJwxhjjE8sYdReUwMdQC1m703F7P05MntvjoHVYRhjjPGJ3WEYY4zxiSWMABORViIyX0R+FZGfReQ27/pYEZknIpu8j40DHWugiIhDRH4Ukf95l9uJyHLve/O+iIQGOsZAEZFGIvKBiKz3foYG22fndyJyh/f/1ToRmSki4fb5qTpLGIHnBO5S1W7AicDNItId+Avwjap2Ar7xLh+vbgN+LbP8DPCC973ZC1wbkKhqh5eAuaraFeiD532yzw4gIi2AW/FME90TcABjsM9PlVnCCDBV3a2qq7zPc/H8h28BnAe8693tXeCI09TWZyLSEjgHeMO7LMApwAfeXY7n96YBMBx4E0BVS1R1H/bZKSsYiBCRYCAS2I19fqrMEkYtIiJt8cwsuBxooqq7wZNUgMTARRZQLwL3Am7vchywT1Wd3uWy88Afb9oDGcDb3iK7N7yTldlnB1DVncDzwDY8iWI/kIx9fqrMEkYtISLRwIfA7aqaE+h4agMRORdIV9XksqvL2fV4beoXDPQDJqvqCUA+x2nxU3m8dTfnAe2A5kAUcFY5ux6vn59Ks4RRC4hICJ5k8Z6qfuRdvUdEmnm3NwPSAxVfAA0FRolIKjALT1HCi0AjbxEDlJkH/ji0A9ihqsu9yx/gSSD22fE4FfhNVTNUtRT4CBiCfX6qzBJGgHnL5N8EflXVf5TZ9Akwzvt8HPBxTccWaKp6v6q2VNW2eCorv1XVK4D5wMXe3Y7L9wZAVdOA7SLSxbvqT8Av2GfngG3AiSIS6f1/duD9sc9PFVnHvQATkZOAxcBafi+nfwBPPcZsoDWeD/5oVc0OSJC1gIicDNytqueKSHs8dxyxwI/AlapaHMj4AkVE+uJpEBAKbAGuwfND0D47gIj8DbgUT2vEH4Hr8NRZ2OenCixhGGOM8YkVSRljjPGJJQxjjDE+sYRhjDHGJ5YwjDHG+MQShjHGGJ9YwjDmGIiIisj0MsvBIpJRZmTdJiLyPxFZIyK/iMjn3vVtRaRQRFaX+RsbqNdhjC+Cj76LMaYC+UBPEYlQ1ULgNGBnme2PAfNU9SUAEeldZluKqvatuVCNOTZ2h2HMsfsCz4i6AJcBM8tsa4ZnCA8AVPWnGozLmGplCcOYYzcLGCMi4UBvPL30D3gVeNM7SdaDItK8zLYOhxVJDavJoI2pLCuSMuYYqepP3qHpLwM+P2zbl96hTM7EM1LqjyLS07vZiqRMnWJ3GMZUj0/wzL0w8/ANqpqtqjNU9SpgBZ5Jj4ypcyxhGFM93gIeU9W1ZVeKyCkiEul9HgN0wDMgoDF1jhVJGVMNVHUHnvm1D9cfeEVEnHh+oL2hqiu8RVgdRGR1mX3fUtV/+j1YY6rIRqs1xhjjEyuSMsYY4xNLGMYYY3xiCcMYY4xPLGEYY4zxiSUMY4wxPrGEYYwxxieWMIwxxvjEEoYxxhif/D+BoYuzdiOsSwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9909482395983314"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.polynomial.polynomial import polyfit  \n",
    "from scipy.stats import pearsonr\n",
    "from pylab import text\n",
    "\n",
    "def norm(a):\n",
    "    return (a - np.min(a)) / a.ptp()\n",
    "METRIC = norm(np.array(VIO)) + np.array(MSE)\n",
    "n_low = 10\n",
    "sorted_aus = [AUS for _,AUS in sorted(zip(VIO,AUS))]\n",
    "print(\"Best by BIC = \", np.mean(sorted_aus[:n_low]))\n",
    "\n",
    "sorted_aus = [AUS for _,AUS in sorted(zip(MSE,AUS))]\n",
    "print(\"Best by AUC = \", np.mean(sorted_aus[:n_low]))\n",
    "\n",
    "\n",
    "sorted_aus = [AUS for _,AUS in sorted(zip(METRIC,AUS))]\n",
    "print(\"Best by MET = \", np.mean(sorted_aus[:n_low]))\n",
    "print(\"Random = \", np.mean(AUS[:n_low]))\n",
    "print(pearsonr(VIO,AUS)[0])\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(VIO,AUS, 1)\n",
    "ax.plot(VIO,AUS, '.')\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(VIO,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "plt.plot(VIO, b + m * np.array(VIO), '-')\n",
    "ax.set_xlabel(\"BIC\")\n",
    "ax.set_ylabel(\"OoS MSE\")\n",
    "fig.savefig('Ex4VIOVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(pearsonr(METRIC,AUS)[0])\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(METRIC,AUS, 1)\n",
    "ax.plot(METRIC,AUS, '.')\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(METRIC,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "plt.plot(METRIC, b + m * np.array(METRIC), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"Combined\")\n",
    "ax.set_ylabel(\"OoS AUCROC\")\n",
    "fig.savefig('Ex4ProposedVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(MSE,AUS, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(MSE,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(MSE,AUS, '.')\n",
    "plt.plot(MSE, b + m * np.array(MSE), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"AUC\")\n",
    "ax.set_ylabel(\"OoS AUCROC\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "MSE = np.array(MSE)\n",
    "\n",
    "x = []\n",
    "y1 = []\n",
    "y2 = []\n",
    "y3 = []\n",
    "for split in range(10, len(AUS), 5):\n",
    "    #print(\"******\", split, \"*******\")\n",
    "    sorted_aus = [AUS for _,AUS in sorted(zip(VIO,AUS))]\n",
    "    sorted_mse = [MSE for _,MSE in sorted(zip(VIO,MSE))]\n",
    "\n",
    "    low = []\n",
    "    high = []\n",
    "    low = sorted_aus[:split]\n",
    "    high = sorted_aus[split:]\n",
    "\n",
    "    x.append(split)\n",
    "    \n",
    "    \n",
    "    #print(\"Low Violations = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High Violations = \", np.mean(high), \"for\", len(high))\n",
    "    y1.append(np.mean(low)) \n",
    "    sorted_aus_by_mse = [AUS for _,AUS in sorted(zip(MSE,AUS))]\n",
    "    low = sorted_aus_by_mse[:split]\n",
    "    high = sorted_aus_by_mse[split:]\n",
    "    #print(\"Low AUS by MSE = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High AUS by MSE = \", np.mean(high), \"for\", len(high))\n",
    "    y2.append(np.mean(low))\n",
    "    sorted_aus = [AUS for _,AUS in sorted(zip(METRIC,AUS))]\n",
    "    sorted_mse = [MSE for _,MSE in sorted(zip(METRIC,MSE))]\n",
    "\n",
    "    low = []\n",
    "    high = []\n",
    "    low = sorted_aus[:split]\n",
    "    high = sorted_aus[split:]\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"Low Metric = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High Metric = \", np.mean(high), \"for\", len(high))\n",
    "    y3.append(np.mean(low))\n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x,y1, '-', label = 'BIC')\n",
    "ax.plot(x,y2, '-', label = 'MSE')\n",
    "ax.plot(x,y3, '-', label = 'METRIC')\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"Out of Sample AUCROC\")\n",
    "plt.show()  \n",
    "pearsonr(METRIC,AUS)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5679502 ],\n",
       "       [ 7.250179  ],\n",
       "       [-0.81391793],\n",
       "       ...,\n",
       "       [ 3.4196062 ],\n",
       "       [ 9.027663  ],\n",
       "       [-1.712926  ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(perturbed_df[inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Instance of edu.cmu.tetrad.graph.EdgeListGraphSingleConnections: Graph Nodes:\n",
       "bmi;density;age;cancer;estrogen;genes;insomnia\n",
       "\n",
       "Graph Edges:\n",
       "1. age --> density\n",
       "2. age --> genes\n",
       "3. density --> cancer\n",
       "4. density --> genes\n",
       "5. estrogen --- bmi\n",
       "6. estrogen --> density\n",
       "7. estrogen --- insomnia\n",
       "\n",
       "Graph Attributes:\n",
       "BIC: -9748.799665\n",
       "\n",
       "Graph Node Attributes:\n",
       "BIC: [bmi: -1328.072399;density: -7698.697583;age: -1352.041438;cancer: -7769.691837;estrogen: -4572.470516;genes: -5385.427509;insomnia: -4946.981065]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_data(mean = 0, var = 0.1, SIZE = 20000):\n",
    "    # set bmi to these values real world mean and standard deviation for a certain country.\n",
    "    bmi = np.random.normal(20, var, SIZE)\n",
    "    estrogen =  2*bmi +  np.random.normal(20,var, SIZE)\n",
    "    age = np.random.normal(mean,var, SIZE)\n",
    "    genes = 0.1 * age + 0.1 * estrogen +  np.random.normal(mean,var, SIZE)\n",
    "    insomnia = estrogen +  np.random.normal(mean,var, SIZE)\n",
    "    density = estrogen + genes + np.random.normal(mean,var, SIZE)\n",
    "    cancer = 3 * density + np.random.normal(mean,var, SIZE)\n",
    "    return pd.DataFrame({'bmi' : bmi,'density' : density, 'age' : age, 'cancer' : cancer, 'estrogen': estrogen, 'genes':genes, 'insomnia': insomnia})\n",
    "\n",
    "def gen_data(mean = 0.4, var = 1.4, SIZE = 2000):\n",
    "    # set bmi to these values real world mean and standard deviation for a certain country.\n",
    "    bmi = np.random.normal(mean, var, SIZE)\n",
    "    estrogen =  2*bmi +  np.random.normal(mean,var, SIZE)\n",
    "    age = np.random.normal(mean,var, SIZE)\n",
    "    genes = 1.1 * age +  estrogen +  np.random.normal(mean,var, SIZE)\n",
    "    insomnia = estrogen +  np.random.normal(mean,var, SIZE)\n",
    "    density = estrogen + genes + np.random.normal(mean,var, SIZE)\n",
    "    cancer = density + np.random.normal(mean,var, SIZE)\n",
    "    return pd.DataFrame({'bmi' : bmi,'density' : density, 'age' : age, 'cancer' : cancer, 'estrogen': estrogen, 'genes':genes, 'insomnia': insomnia})\n",
    "\n",
    "def get_CG(df, tetrad):\n",
    "    tetrad.run(algoId = 'fges', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "df = gen_data()\n",
    "get_CG(perturbed_df, tetrad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
