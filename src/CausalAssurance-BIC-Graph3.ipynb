{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes \n",
    "### Required installing Oracle JAVA 8 to get javabridge installed\n",
    "### Then, I was able to install py-causal from https://bd2kccd.github.io/docs/py-causal/\n",
    "### GFCI is slower than RFCI, but more accurate (SPIRTES), GFCI and RFCI account for unobserved variables, FGES assumes no unobserved variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure Learning Performance Guarantees If the assumptions in the previous section hold, then in the large sample limit, the CBN structure output by GFCId will contain an edge of one of four kinds between Xand Y   if and only if Xand Yare not independent conditional on any subset of the other measured variables of less than or equal to a specified size. In addition, there is (1) an arc X->Y   if and only if Xdirectly or indirectly causes Y, and Y   does not directly or indirectly cause X; (2) an edge X <-->Y   if and only if X   is not a direct or indirect cause of Yand Y   is not a direct or indirect cause of X(which can only occur if there are latent confounders of Xand some other variable or Yand some other variable; (3) an edge Xo->Y   only if Yis not a direct or indirect cause of X, but Xmay or may not be an indirect cause of Y; (4) an edge X oâ€“o Y   indicates that Xand Y   are dependent no matter what subset of observed variables is conditioned on, but contains no orientation information (X   may be a direct or indirect cause of Y, and Ymay be an indirect cause of X, or there may be a latent common cause of Xand Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying some various ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512]] ['temp/f0', 'temp/f1', 'temp/f2', 'temp/f3', 'temp/f4', 'temp/f5', 'temp/f6', 'temp/f7', 'temp/f8', 'temp/f9', 'temp/f10', 'temp/f11', 'temp/f12', 'temp/f13', 'temp/f14', 'temp/f15', 'temp/f16', 'temp/f17', 'temp/f18', 'temp/f19']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from pycausal import search as s\n",
    "import configparser\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, \\\n",
    "                        Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "import keras.optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "import glob, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "# select your GPU Here\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" #Comment this line out if you want all GPUS (2 hehe)\n",
    "\n",
    "# python full-display web browser\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "def get_model(dense, dropouts, inputs):\n",
    "    # dense is an ordered list of the number of dense neurons like [1024, 2048, 1024]\n",
    "    # dropouts is an ordered list of the dropout masks like [0.2, 0.3, 0.4]\n",
    "    inputs = keras.Input(shape = (inputs,))\n",
    "\n",
    "    x = keras.layers.Dense(dense[0], activation = 'relu')(inputs)\n",
    "    x = keras.layers.Dropout(dropouts[0])(x, training=True)\n",
    "    for den, drop in zip(dense[1:], dropouts[1:]):\n",
    "        x = keras.layers.Dense(den, activation = 'relu')(x)\n",
    "        x = keras.layers.Dropout(drop)(x, training=True)\n",
    "    outputs = keras.layers.Dense(1, activation = 'linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def discrete_gauss(low, high, samples, std = 20):\n",
    "    x = np.arange(low, high)\n",
    "    xU, xL = x + 0.5, x - 0.5 \n",
    "    prob = ss.norm.cdf(xU, scale = std) - ss.norm.cdf(xL, scale = std)\n",
    "    prob = prob / prob.sum() #normalize the probabilities so their sum is 1\n",
    "    nums = np.random.choice(x, size = samples, p = prob)\n",
    "    return nums\n",
    "\n",
    "\n",
    "\n",
    "def bar_plot(x_ax, val1, val1std, val2, val2std):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ## the data\n",
    "    N = len(x_ax)\n",
    "\n",
    "    ## necessary variables\n",
    "    ind = np.arange(N)                # the x locations for the groups\n",
    "    width = 0.35                      # the width of the bars\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    ## the bars\n",
    "    rects1 = ax.bar(ind, val1, width,\n",
    "                    color='gray',\n",
    "                    yerr=val1std,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='blue'))\n",
    "\n",
    "    rects2 = ax.bar(ind+width, val2, width,\n",
    "                        color='blue',\n",
    "                        #yerr=val2std,\n",
    "                        error_kw=dict(elinewidth=2,ecolor='gray'))\n",
    "\n",
    "    # axes and labels\n",
    "    ax.set_xlim(-width,len(ind)+width)\n",
    "    #ax.set_ylim(0,45)\n",
    "    ax.set_ylabel('Percentage')\n",
    "    ax.set_title('')\n",
    "    plt.xticks(ind + width / 2, x_ax, rotation=75, size = 14)\n",
    "    ## add a legend\n",
    "    ax.legend( (rects1[0], rects2[0]), ('Accuracy', '% Violations') )\n",
    "    fig.savefig(\"violations.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 20000):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    a = np.random.normal(mean,var, SIZE)\n",
    "    b = np.random.normal(mean, var, SIZE)\n",
    "    c = a * b + np.random.normal(mean, var, SIZE)\n",
    "    d = -c - np.random.normal(mean, var, SIZE)\n",
    "    \n",
    "\n",
    "    #g = np.rint(g)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d})\n",
    "\n",
    "def get_CG(df, tetrad):\n",
    "    tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "    #tetrad.run(algoId = 'fges-mb', targetName = 'g', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "    #       structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "    #       completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "\n",
    "\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from collections import defaultdict\n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '5000M')\n",
    "tetrad = s.tetradrunner()\n",
    "\n",
    "\n",
    "verbosity = 1\n",
    "\n",
    "\n",
    "\n",
    "models = []\n",
    "model_names = []\n",
    "\n",
    "num_models =20\n",
    "model_layers = [2048, 2048, 512]\n",
    "for i in range(num_models):\n",
    "    models.append(model_layers)\n",
    "    model_names.append('temp/f' + str(i))\n",
    "\n",
    "print(models, model_names)\n",
    "\n",
    "from pycausal import prior as p\n",
    "def get_bic(df, prior):\n",
    "\n",
    "    tetrad.run(algoId = 'gfci', dfs = df,  scoreId = 'sem-bic-deterministic', dataType = 'continuous',\n",
    "               structurePrior = 1.0, samplePrior = 1, maxDegree = -1, maxPathLength = -1, priorKnowledge = prior,\n",
    "               completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True,\n",
    "               penaltyDiscount = 2\n",
    "               )\n",
    "    BIC = tetrad.getTetradGraph().getAllAttributes().toString()\n",
    "    BIC = float(BIC.split('=')[-1].split('}')[0])\n",
    "    return BIC #/ len(df)\n",
    "import itertools\n",
    "def get_pairs(lst):\n",
    "    a = set()\n",
    "    for i in itertools.permutations(lst,2):\n",
    "        a.add(i)\n",
    "    return a\n",
    "full_conx = get_pairs(['a', 'b', 'c', 'd'])\n",
    "forced_conx = set({('a','c'), ('b','c'),('c','d') })\n",
    "restricted_conx = full_conx.difference(forced_conx)   \n",
    "\n",
    "prior = p.knowledge(requiredirect =  list(map(list, forced_conx)),\n",
    "                       forbiddirect = list(map(list, restricted_conx))\n",
    "                       )\n",
    "\n",
    "\n",
    "\n",
    "inputs = ['a','b','d']\n",
    "target = ['c']\n",
    "\n",
    "\n",
    "df = gen_data()\n",
    "X = df[inputs].values\n",
    "y = df[target].values\n",
    "\n",
    "val_df = gen_data(SIZE = 2000)\n",
    "x_val = df[inputs].values\n",
    "y_val = df[target].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp/f0\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 4s 179us/step - loss: 0.6170 - mean_squared_error: 0.6170 - val_loss: 0.5281 - val_mean_squared_error: 0.5281\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.52813, saving model to temp/f0\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.5293 - mean_squared_error: 0.5293 - val_loss: 0.5284 - val_mean_squared_error: 0.5284\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.52813\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.5228 - mean_squared_error: 0.5228 - val_loss: 0.5247 - val_mean_squared_error: 0.5247\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52813 to 0.52470, saving model to temp/f0\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5212 - mean_squared_error: 0.5212 - val_loss: 0.5196 - val_mean_squared_error: 0.5196\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52470 to 0.51961, saving model to temp/f0\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.5168 - mean_squared_error: 0.5168 - val_loss: 0.5163 - val_mean_squared_error: 0.5163\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.51961 to 0.51633, saving model to temp/f0\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.5166 - mean_squared_error: 0.5166 - val_loss: 0.5183 - val_mean_squared_error: 0.5183\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51633\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.5153 - mean_squared_error: 0.5153 - val_loss: 0.5140 - val_mean_squared_error: 0.5140\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51633 to 0.51397, saving model to temp/f0\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5123 - mean_squared_error: 0.5123 - val_loss: 0.5095 - val_mean_squared_error: 0.5095\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.51397 to 0.50955, saving model to temp/f0\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.5114 - mean_squared_error: 0.5114 - val_loss: 0.5113 - val_mean_squared_error: 0.5113\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.50955\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5099 - mean_squared_error: 0.5099 - val_loss: 0.5123 - val_mean_squared_error: 0.5123\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.50955\n",
      "Epoch 00010: early stopping\n",
      "temp/f1\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.6191 - mean_squared_error: 0.6191 - val_loss: 0.5391 - val_mean_squared_error: 0.5391\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53907, saving model to temp/f1\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.5296 - mean_squared_error: 0.5296 - val_loss: 0.5245 - val_mean_squared_error: 0.5245\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53907 to 0.52452, saving model to temp/f1\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.5254 - mean_squared_error: 0.5254 - val_loss: 0.5233 - val_mean_squared_error: 0.5233\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52452 to 0.52334, saving model to temp/f1\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5208 - mean_squared_error: 0.5208 - val_loss: 0.5161 - val_mean_squared_error: 0.5161\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52334 to 0.51605, saving model to temp/f1\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5179 - mean_squared_error: 0.5179 - val_loss: 0.5165 - val_mean_squared_error: 0.5165\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51605\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.5180 - mean_squared_error: 0.5180 - val_loss: 0.5155 - val_mean_squared_error: 0.5155\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.51605 to 0.51547, saving model to temp/f1\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.5163 - mean_squared_error: 0.5163 - val_loss: 0.5123 - val_mean_squared_error: 0.5123\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51547 to 0.51234, saving model to temp/f1\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5140 - mean_squared_error: 0.5140 - val_loss: 0.5103 - val_mean_squared_error: 0.5103\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.51234 to 0.51033, saving model to temp/f1\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.5115 - mean_squared_error: 0.5115 - val_loss: 0.5114 - val_mean_squared_error: 0.5114\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51033\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.5085 - mean_squared_error: 0.5085 - val_loss: 0.5076 - val_mean_squared_error: 0.5076\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.51033 to 0.50758, saving model to temp/f1\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.5101 - mean_squared_error: 0.5101 - val_loss: 0.5071 - val_mean_squared_error: 0.5071\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.50758 to 0.50705, saving model to temp/f1\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.5093 - mean_squared_error: 0.5093 - val_loss: 0.5071 - val_mean_squared_error: 0.5071\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.50705\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5077 - mean_squared_error: 0.5077 - val_loss: 0.5034 - val_mean_squared_error: 0.5034\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.50705 to 0.50341, saving model to temp/f1\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5089 - mean_squared_error: 0.5089 - val_loss: 0.5056 - val_mean_squared_error: 0.5056\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.50341\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.5091 - mean_squared_error: 0.5091 - val_loss: 0.5071 - val_mean_squared_error: 0.5071\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.50341\n",
      "Epoch 00015: early stopping\n",
      "temp/f2\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 142us/step - loss: 0.6145 - mean_squared_error: 0.6145 - val_loss: 0.5318 - val_mean_squared_error: 0.5318\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53183, saving model to temp/f2\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.5276 - mean_squared_error: 0.5276 - val_loss: 0.5231 - val_mean_squared_error: 0.5231\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53183 to 0.52306, saving model to temp/f2\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5247 - mean_squared_error: 0.5247 - val_loss: 0.5188 - val_mean_squared_error: 0.5188\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52306 to 0.51878, saving model to temp/f2\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.5222 - mean_squared_error: 0.5222 - val_loss: 0.5194 - val_mean_squared_error: 0.5194\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51878\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5230 - mean_squared_error: 0.5230 - val_loss: 0.5157 - val_mean_squared_error: 0.5157\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.51878 to 0.51572, saving model to temp/f2\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.5162 - mean_squared_error: 0.5162 - val_loss: 0.5125 - val_mean_squared_error: 0.5125\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.51572 to 0.51254, saving model to temp/f2\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.5148 - mean_squared_error: 0.5148 - val_loss: 0.5125 - val_mean_squared_error: 0.5125\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51254 to 0.51251, saving model to temp/f2\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.5136 - mean_squared_error: 0.5136 - val_loss: 0.5115 - val_mean_squared_error: 0.5115\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.51251 to 0.51147, saving model to temp/f2\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5104 - mean_squared_error: 0.5104 - val_loss: 0.5106 - val_mean_squared_error: 0.5106\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.51147 to 0.51060, saving model to temp/f2\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.5105 - mean_squared_error: 0.5105 - val_loss: 0.5087 - val_mean_squared_error: 0.5087\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.51060 to 0.50874, saving model to temp/f2\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.5106 - mean_squared_error: 0.5106 - val_loss: 0.5072 - val_mean_squared_error: 0.5072\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.50874 to 0.50721, saving model to temp/f2\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5097 - mean_squared_error: 0.5097 - val_loss: 0.5041 - val_mean_squared_error: 0.5041\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.50721 to 0.50406, saving model to temp/f2\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.5099 - mean_squared_error: 0.5099 - val_loss: 0.5052 - val_mean_squared_error: 0.5052\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.50406\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 0.5075 - mean_squared_error: 0.5075 - val_loss: 0.5068 - val_mean_squared_error: 0.5068\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.50406\n",
      "Epoch 00014: early stopping\n",
      "temp/f3\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 0.6148 - mean_squared_error: 0.6148 - val_loss: 0.5349 - val_mean_squared_error: 0.5349\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53487, saving model to temp/f3\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 0.5308 - mean_squared_error: 0.5308 - val_loss: 0.5270 - val_mean_squared_error: 0.5270\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53487 to 0.52697, saving model to temp/f3\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.5222 - val_mean_squared_error: 0.5222\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52697 to 0.52217, saving model to temp/f3\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5218 - mean_squared_error: 0.5218 - val_loss: 0.5180 - val_mean_squared_error: 0.5180\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52217 to 0.51803, saving model to temp/f3\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.5220 - mean_squared_error: 0.5220 - val_loss: 0.5130 - val_mean_squared_error: 0.5130\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.51803 to 0.51302, saving model to temp/f3\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5165 - mean_squared_error: 0.5165 - val_loss: 0.5135 - val_mean_squared_error: 0.5135\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51302\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.5149 - mean_squared_error: 0.5149 - val_loss: 0.5128 - val_mean_squared_error: 0.5128\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51302 to 0.51276, saving model to temp/f3\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 0.5138 - mean_squared_error: 0.5138 - val_loss: 0.5122 - val_mean_squared_error: 0.5122\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.51276 to 0.51217, saving model to temp/f3\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.5120 - mean_squared_error: 0.5120 - val_loss: 0.5170 - val_mean_squared_error: 0.5170\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51217\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.5111 - mean_squared_error: 0.5111 - val_loss: 0.5082 - val_mean_squared_error: 0.5082\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.51217 to 0.50821, saving model to temp/f3\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5103 - mean_squared_error: 0.5103 - val_loss: 0.5080 - val_mean_squared_error: 0.5080\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.50821 to 0.50799, saving model to temp/f3\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5077 - mean_squared_error: 0.5077 - val_loss: 0.5056 - val_mean_squared_error: 0.5056\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.50799 to 0.50560, saving model to temp/f3\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 0.5104 - mean_squared_error: 0.5104 - val_loss: 0.5043 - val_mean_squared_error: 0.5043\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.50560 to 0.50433, saving model to temp/f3\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 0.5077 - mean_squared_error: 0.5077 - val_loss: 0.5056 - val_mean_squared_error: 0.5056\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.50433\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.5068 - mean_squared_error: 0.5068 - val_loss: 0.5095 - val_mean_squared_error: 0.5095\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.50433\n",
      "Epoch 00015: early stopping\n",
      "temp/f4\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 170us/step - loss: 0.6099 - mean_squared_error: 0.6099 - val_loss: 0.5323 - val_mean_squared_error: 0.5323\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53230, saving model to temp/f4\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5296 - mean_squared_error: 0.5296 - val_loss: 0.5226 - val_mean_squared_error: 0.5226\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53230 to 0.52262, saving model to temp/f4\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.5241 - mean_squared_error: 0.5241 - val_loss: 0.5201 - val_mean_squared_error: 0.5201\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52262 to 0.52015, saving model to temp/f4\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 0.5225 - mean_squared_error: 0.5225 - val_loss: 0.5253 - val_mean_squared_error: 0.5253\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.52015\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.5199 - mean_squared_error: 0.5199 - val_loss: 0.5140 - val_mean_squared_error: 0.5140\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.52015 to 0.51398, saving model to temp/f4\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.5182 - mean_squared_error: 0.5182 - val_loss: 0.5162 - val_mean_squared_error: 0.5162\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51398\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.5161 - mean_squared_error: 0.5161 - val_loss: 0.5122 - val_mean_squared_error: 0.5122\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51398 to 0.51216, saving model to temp/f4\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5128 - mean_squared_error: 0.5128 - val_loss: 0.5092 - val_mean_squared_error: 0.5092\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.51216 to 0.50923, saving model to temp/f4\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 160us/step - loss: 0.5120 - mean_squared_error: 0.5120 - val_loss: 0.5086 - val_mean_squared_error: 0.5086\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.50923 to 0.50862, saving model to temp/f4\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 0.5118 - mean_squared_error: 0.5118 - val_loss: 0.5098 - val_mean_squared_error: 0.5098\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.50862\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.5099 - mean_squared_error: 0.5099 - val_loss: 0.5087 - val_mean_squared_error: 0.5087\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.50862\n",
      "Epoch 00011: early stopping\n",
      "temp/f5\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 165us/step - loss: 0.6124 - mean_squared_error: 0.6124 - val_loss: 0.5293 - val_mean_squared_error: 0.5293\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.52927, saving model to temp/f5\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 156us/step - loss: 0.5294 - mean_squared_error: 0.5294 - val_loss: 0.5302 - val_mean_squared_error: 0.5302\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.52927\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.5254 - mean_squared_error: 0.5254 - val_loss: 0.5272 - val_mean_squared_error: 0.5272\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52927 to 0.52723, saving model to temp/f5\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.5238 - mean_squared_error: 0.5238 - val_loss: 0.5198 - val_mean_squared_error: 0.5198\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52723 to 0.51981, saving model to temp/f5\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.5196 - mean_squared_error: 0.5196 - val_loss: 0.5229 - val_mean_squared_error: 0.5229\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51981\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.5176 - mean_squared_error: 0.5176 - val_loss: 0.5147 - val_mean_squared_error: 0.5147\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.51981 to 0.51470, saving model to temp/f5\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.5163 - mean_squared_error: 0.5163 - val_loss: 0.5132 - val_mean_squared_error: 0.5132\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51470 to 0.51323, saving model to temp/f5\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.5119 - mean_squared_error: 0.5119 - val_loss: 0.5104 - val_mean_squared_error: 0.5104\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.51323 to 0.51042, saving model to temp/f5\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.5121 - mean_squared_error: 0.5121 - val_loss: 0.5080 - val_mean_squared_error: 0.5080\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.51042 to 0.50801, saving model to temp/f5\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 0.5111 - mean_squared_error: 0.5111 - val_loss: 0.5084 - val_mean_squared_error: 0.5084\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.50801\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 0.5098 - mean_squared_error: 0.5098 - val_loss: 0.5067 - val_mean_squared_error: 0.5067\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.50801 to 0.50672, saving model to temp/f5\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.5101 - mean_squared_error: 0.5101 - val_loss: 0.5093 - val_mean_squared_error: 0.5093\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.50672\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5087 - mean_squared_error: 0.5087 - val_loss: 0.5044 - val_mean_squared_error: 0.5044\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.50672 to 0.50436, saving model to temp/f5\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5078 - mean_squared_error: 0.5078 - val_loss: 0.5065 - val_mean_squared_error: 0.5065\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.50436\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 3s 160us/step - loss: 0.5060 - mean_squared_error: 0.5060 - val_loss: 0.5032 - val_mean_squared_error: 0.5032\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.50436 to 0.50324, saving model to temp/f5\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.5059 - mean_squared_error: 0.5059 - val_loss: 0.5026 - val_mean_squared_error: 0.5026\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.50324 to 0.50260, saving model to temp/f5\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5045 - mean_squared_error: 0.5045 - val_loss: 0.5051 - val_mean_squared_error: 0.5051\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.50260\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.5044 - mean_squared_error: 0.5044 - val_loss: 0.5034 - val_mean_squared_error: 0.5034\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.50260\n",
      "Epoch 00018: early stopping\n",
      "temp/f6\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 170us/step - loss: 0.6106 - mean_squared_error: 0.6106 - val_loss: 0.5335 - val_mean_squared_error: 0.5335\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53347, saving model to temp/f6\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 160us/step - loss: 0.5289 - mean_squared_error: 0.5289 - val_loss: 0.5246 - val_mean_squared_error: 0.5246\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53347 to 0.52458, saving model to temp/f6\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.5253 - mean_squared_error: 0.5253 - val_loss: 0.5193 - val_mean_squared_error: 0.5193\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52458 to 0.51926, saving model to temp/f6\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 0.5218 - mean_squared_error: 0.5218 - val_loss: 0.5202 - val_mean_squared_error: 0.5202\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51926\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 0.5175 - mean_squared_error: 0.5175 - val_loss: 0.5197 - val_mean_squared_error: 0.5197\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51926\n",
      "Epoch 00005: early stopping\n",
      "temp/f7\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 169us/step - loss: 0.6126 - mean_squared_error: 0.6126 - val_loss: 0.5343 - val_mean_squared_error: 0.5343\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53428, saving model to temp/f7\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5287 - mean_squared_error: 0.5287 - val_loss: 0.5244 - val_mean_squared_error: 0.5244\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53428 to 0.52436, saving model to temp/f7\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 161us/step - loss: 0.5241 - mean_squared_error: 0.5241 - val_loss: 0.5232 - val_mean_squared_error: 0.5232\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52436 to 0.52324, saving model to temp/f7\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5217 - mean_squared_error: 0.5217 - val_loss: 0.5196 - val_mean_squared_error: 0.5196\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52324 to 0.51956, saving model to temp/f7\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5200 - mean_squared_error: 0.5200 - val_loss: 0.5238 - val_mean_squared_error: 0.5238\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51956\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5168 - mean_squared_error: 0.5168 - val_loss: 0.5147 - val_mean_squared_error: 0.5147\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.51956 to 0.51467, saving model to temp/f7\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 160us/step - loss: 0.5149 - mean_squared_error: 0.5149 - val_loss: 0.5182 - val_mean_squared_error: 0.5182\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51467\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5129 - mean_squared_error: 0.5129 - val_loss: 0.5123 - val_mean_squared_error: 0.5123\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.51467 to 0.51226, saving model to temp/f7\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5116 - mean_squared_error: 0.5116 - val_loss: 0.5088 - val_mean_squared_error: 0.5088\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.51226 to 0.50876, saving model to temp/f7\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 0.5125 - mean_squared_error: 0.5125 - val_loss: 0.5100 - val_mean_squared_error: 0.5100\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.50876\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.5113 - mean_squared_error: 0.5113 - val_loss: 0.5089 - val_mean_squared_error: 0.5089\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.50876\n",
      "Epoch 00011: early stopping\n",
      "temp/f8\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 170us/step - loss: 0.6147 - mean_squared_error: 0.6147 - val_loss: 0.5334 - val_mean_squared_error: 0.5334\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53344, saving model to temp/f8\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5267 - mean_squared_error: 0.5267 - val_loss: 0.5239 - val_mean_squared_error: 0.5239\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53344 to 0.52390, saving model to temp/f8\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 160us/step - loss: 0.5237 - mean_squared_error: 0.5237 - val_loss: 0.5205 - val_mean_squared_error: 0.5205\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52390 to 0.52049, saving model to temp/f8\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 161us/step - loss: 0.5212 - mean_squared_error: 0.5212 - val_loss: 0.5183 - val_mean_squared_error: 0.5183\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52049 to 0.51828, saving model to temp/f8\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 160us/step - loss: 0.5169 - mean_squared_error: 0.5169 - val_loss: 0.5134 - val_mean_squared_error: 0.5134\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.51828 to 0.51336, saving model to temp/f8\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5163 - mean_squared_error: 0.5163 - val_loss: 0.5166 - val_mean_squared_error: 0.5166\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51336\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5148 - mean_squared_error: 0.5148 - val_loss: 0.5126 - val_mean_squared_error: 0.5126\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51336 to 0.51260, saving model to temp/f8\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 160us/step - loss: 0.5132 - mean_squared_error: 0.5132 - val_loss: 0.5140 - val_mean_squared_error: 0.5140\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51260\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 160us/step - loss: 0.5141 - mean_squared_error: 0.5141 - val_loss: 0.5110 - val_mean_squared_error: 0.5110\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.51260 to 0.51102, saving model to temp/f8\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 161us/step - loss: 0.5130 - mean_squared_error: 0.5130 - val_loss: 0.5140 - val_mean_squared_error: 0.5140\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51102\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5124 - mean_squared_error: 0.5124 - val_loss: 0.5112 - val_mean_squared_error: 0.5112\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51102\n",
      "Epoch 00011: early stopping\n",
      "temp/f9\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 173us/step - loss: 0.6146 - mean_squared_error: 0.6146 - val_loss: 0.5333 - val_mean_squared_error: 0.5333\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53331, saving model to temp/f9\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 160us/step - loss: 0.5303 - mean_squared_error: 0.5303 - val_loss: 0.5263 - val_mean_squared_error: 0.5263\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53331 to 0.52634, saving model to temp/f9\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5232 - mean_squared_error: 0.5232 - val_loss: 0.5201 - val_mean_squared_error: 0.5201\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52634 to 0.52011, saving model to temp/f9\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 160us/step - loss: 0.5209 - mean_squared_error: 0.5209 - val_loss: 0.5190 - val_mean_squared_error: 0.5190\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52011 to 0.51900, saving model to temp/f9\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.5178 - mean_squared_error: 0.5178 - val_loss: 0.5168 - val_mean_squared_error: 0.5168\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.51900 to 0.51679, saving model to temp/f9\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.5163 - mean_squared_error: 0.5163 - val_loss: 0.5172 - val_mean_squared_error: 0.5172\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51679\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5170 - mean_squared_error: 0.5170 - val_loss: 0.5122 - val_mean_squared_error: 0.5122\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51679 to 0.51215, saving model to temp/f9\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5128 - mean_squared_error: 0.5128 - val_loss: 0.5151 - val_mean_squared_error: 0.5151\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51215\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.5120 - mean_squared_error: 0.5120 - val_loss: 0.5122 - val_mean_squared_error: 0.5122\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51215\n",
      "Epoch 00009: early stopping\n",
      "temp/f10\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 161us/step - loss: 0.6188 - mean_squared_error: 0.6188 - val_loss: 0.5309 - val_mean_squared_error: 0.5309\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53093, saving model to temp/f10\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.5296 - mean_squared_error: 0.5296 - val_loss: 0.5217 - val_mean_squared_error: 0.5217\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53093 to 0.52172, saving model to temp/f10\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5240 - mean_squared_error: 0.5240 - val_loss: 0.5240 - val_mean_squared_error: 0.5240\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.52172\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5217 - mean_squared_error: 0.5217 - val_loss: 0.5176 - val_mean_squared_error: 0.5176\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52172 to 0.51758, saving model to temp/f10\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5186 - mean_squared_error: 0.5186 - val_loss: 0.5188 - val_mean_squared_error: 0.5188\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51758\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5169 - mean_squared_error: 0.5169 - val_loss: 0.5167 - val_mean_squared_error: 0.5167\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.51758 to 0.51666, saving model to temp/f10\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5165 - mean_squared_error: 0.5165 - val_loss: 0.5132 - val_mean_squared_error: 0.5132\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51666 to 0.51316, saving model to temp/f10\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 161us/step - loss: 0.5133 - mean_squared_error: 0.5133 - val_loss: 0.5104 - val_mean_squared_error: 0.5104\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.51316 to 0.51036, saving model to temp/f10\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 160us/step - loss: 0.5145 - mean_squared_error: 0.5145 - val_loss: 0.5125 - val_mean_squared_error: 0.5125\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51036\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 161us/step - loss: 0.5115 - mean_squared_error: 0.5115 - val_loss: 0.5079 - val_mean_squared_error: 0.5079\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.51036 to 0.50789, saving model to temp/f10\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5074 - mean_squared_error: 0.5074 - val_loss: 0.5101 - val_mean_squared_error: 0.5101\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.50789\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5108 - mean_squared_error: 0.5108 - val_loss: 0.5114 - val_mean_squared_error: 0.5114\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.50789\n",
      "Epoch 00012: early stopping\n",
      "temp/f11\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 173us/step - loss: 0.6110 - mean_squared_error: 0.6110 - val_loss: 0.5301 - val_mean_squared_error: 0.5301\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53014, saving model to temp/f11\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5283 - mean_squared_error: 0.5283 - val_loss: 0.5239 - val_mean_squared_error: 0.5239\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53014 to 0.52392, saving model to temp/f11\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5233 - mean_squared_error: 0.5233 - val_loss: 0.5244 - val_mean_squared_error: 0.5244\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.52392\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5217 - mean_squared_error: 0.5217 - val_loss: 0.5179 - val_mean_squared_error: 0.5179\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52392 to 0.51791, saving model to temp/f11\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 160us/step - loss: 0.5170 - mean_squared_error: 0.5170 - val_loss: 0.5170 - val_mean_squared_error: 0.5170\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.51791 to 0.51697, saving model to temp/f11\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5157 - mean_squared_error: 0.5157 - val_loss: 0.5142 - val_mean_squared_error: 0.5142\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.51697 to 0.51420, saving model to temp/f11\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5126 - mean_squared_error: 0.5126 - val_loss: 0.5170 - val_mean_squared_error: 0.5170\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51420\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5132 - mean_squared_error: 0.5132 - val_loss: 0.5081 - val_mean_squared_error: 0.5081\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.51420 to 0.50810, saving model to temp/f11\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5101 - mean_squared_error: 0.5101 - val_loss: 0.5068 - val_mean_squared_error: 0.5068\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.50810 to 0.50680, saving model to temp/f11\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 0.5108 - mean_squared_error: 0.5108 - val_loss: 0.5101 - val_mean_squared_error: 0.5101\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.50680\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 0.5105 - mean_squared_error: 0.5105 - val_loss: 0.5061 - val_mean_squared_error: 0.5061\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.50680 to 0.50613, saving model to temp/f11\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.5078 - mean_squared_error: 0.5078 - val_loss: 0.5054 - val_mean_squared_error: 0.5054\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.50613 to 0.50544, saving model to temp/f11\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5067 - mean_squared_error: 0.5067 - val_loss: 0.5075 - val_mean_squared_error: 0.5075\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.50544\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5083 - mean_squared_error: 0.5083 - val_loss: 0.5055 - val_mean_squared_error: 0.5055\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.50544\n",
      "Epoch 00014: early stopping\n",
      "temp/f12\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 172us/step - loss: 0.6156 - mean_squared_error: 0.6156 - val_loss: 0.5303 - val_mean_squared_error: 0.5303\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53034, saving model to temp/f12\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.5297 - mean_squared_error: 0.5297 - val_loss: 0.5275 - val_mean_squared_error: 0.5275\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53034 to 0.52750, saving model to temp/f12\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.5252 - mean_squared_error: 0.5252 - val_loss: 0.5263 - val_mean_squared_error: 0.5263\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52750 to 0.52631, saving model to temp/f12\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5206 - mean_squared_error: 0.5206 - val_loss: 0.5181 - val_mean_squared_error: 0.5181\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52631 to 0.51814, saving model to temp/f12\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 0.5191 - mean_squared_error: 0.5191 - val_loss: 0.5151 - val_mean_squared_error: 0.5151\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.51814 to 0.51514, saving model to temp/f12\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.5158 - mean_squared_error: 0.5158 - val_loss: 0.5142 - val_mean_squared_error: 0.5142\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.51514 to 0.51420, saving model to temp/f12\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5143 - mean_squared_error: 0.5143 - val_loss: 0.5127 - val_mean_squared_error: 0.5127\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51420 to 0.51273, saving model to temp/f12\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5139 - mean_squared_error: 0.5139 - val_loss: 0.5127 - val_mean_squared_error: 0.5127\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.51273 to 0.51271, saving model to temp/f12\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.5112 - mean_squared_error: 0.5112 - val_loss: 0.5106 - val_mean_squared_error: 0.5106\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.51271 to 0.51065, saving model to temp/f12\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5107 - mean_squared_error: 0.5107 - val_loss: 0.5106 - val_mean_squared_error: 0.5106\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.51065 to 0.51062, saving model to temp/f12\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5119 - mean_squared_error: 0.5119 - val_loss: 0.5060 - val_mean_squared_error: 0.5060\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.51062 to 0.50599, saving model to temp/f12\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5110 - mean_squared_error: 0.5110 - val_loss: 0.5102 - val_mean_squared_error: 0.5102\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.50599\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5079 - mean_squared_error: 0.5079 - val_loss: 0.5053 - val_mean_squared_error: 0.5053\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.50599 to 0.50532, saving model to temp/f12\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5080 - mean_squared_error: 0.5080 - val_loss: 0.5088 - val_mean_squared_error: 0.5088\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.50532\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5084 - mean_squared_error: 0.5084 - val_loss: 0.5034 - val_mean_squared_error: 0.5034\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.50532 to 0.50343, saving model to temp/f12\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5069 - mean_squared_error: 0.5069 - val_loss: 0.5119 - val_mean_squared_error: 0.5119\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.50343\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5055 - mean_squared_error: 0.5055 - val_loss: 0.5040 - val_mean_squared_error: 0.5040\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.50343\n",
      "Epoch 00017: early stopping\n",
      "temp/f13\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 171us/step - loss: 0.6130 - mean_squared_error: 0.6130 - val_loss: 0.5378 - val_mean_squared_error: 0.5378\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53780, saving model to temp/f13\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5284 - mean_squared_error: 0.5284 - val_loss: 0.5234 - val_mean_squared_error: 0.5234\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53780 to 0.52343, saving model to temp/f13\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5238 - mean_squared_error: 0.5238 - val_loss: 0.5275 - val_mean_squared_error: 0.5275\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.52343\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5229 - mean_squared_error: 0.5229 - val_loss: 0.5224 - val_mean_squared_error: 0.5224\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52343 to 0.52241, saving model to temp/f13\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.5180 - mean_squared_error: 0.5180 - val_loss: 0.5164 - val_mean_squared_error: 0.5164\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.52241 to 0.51638, saving model to temp/f13\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5168 - mean_squared_error: 0.5168 - val_loss: 0.5188 - val_mean_squared_error: 0.5188\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51638\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5162 - mean_squared_error: 0.5162 - val_loss: 0.5220 - val_mean_squared_error: 0.5220\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51638\n",
      "Epoch 00007: early stopping\n",
      "temp/f14\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 172us/step - loss: 0.6129 - mean_squared_error: 0.6129 - val_loss: 0.5355 - val_mean_squared_error: 0.5355\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53550, saving model to temp/f14\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5285 - mean_squared_error: 0.5285 - val_loss: 0.5237 - val_mean_squared_error: 0.5237\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53550 to 0.52369, saving model to temp/f14\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5250 - mean_squared_error: 0.5250 - val_loss: 0.5218 - val_mean_squared_error: 0.5218\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52369 to 0.52183, saving model to temp/f14\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 161us/step - loss: 0.5217 - mean_squared_error: 0.5217 - val_loss: 0.5199 - val_mean_squared_error: 0.5199\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52183 to 0.51987, saving model to temp/f14\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 0.5197 - mean_squared_error: 0.5197 - val_loss: 0.5162 - val_mean_squared_error: 0.5162\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.51987 to 0.51621, saving model to temp/f14\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.5177 - mean_squared_error: 0.5177 - val_loss: 0.5183 - val_mean_squared_error: 0.5183\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51621\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.5158 - mean_squared_error: 0.5158 - val_loss: 0.5122 - val_mean_squared_error: 0.5122\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51621 to 0.51215, saving model to temp/f14\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.5121 - mean_squared_error: 0.5121 - val_loss: 0.5134 - val_mean_squared_error: 0.5134\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51215\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 0.5111 - mean_squared_error: 0.5111 - val_loss: 0.5109 - val_mean_squared_error: 0.5109\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.51215 to 0.51092, saving model to temp/f14\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.5111 - mean_squared_error: 0.5111 - val_loss: 0.5121 - val_mean_squared_error: 0.5121\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51092\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.5100 - mean_squared_error: 0.5100 - val_loss: 0.5070 - val_mean_squared_error: 0.5070\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.51092 to 0.50699, saving model to temp/f14\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5092 - mean_squared_error: 0.5092 - val_loss: 0.5149 - val_mean_squared_error: 0.5149\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.50699\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5087 - mean_squared_error: 0.5087 - val_loss: 0.5053 - val_mean_squared_error: 0.5053\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.50699 to 0.50526, saving model to temp/f14\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5067 - mean_squared_error: 0.5067 - val_loss: 0.5088 - val_mean_squared_error: 0.5088\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.50526\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.5060 - mean_squared_error: 0.5060 - val_loss: 0.5036 - val_mean_squared_error: 0.5036\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.50526 to 0.50360, saving model to temp/f14\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 3s 142us/step - loss: 0.5075 - mean_squared_error: 0.5075 - val_loss: 0.5080 - val_mean_squared_error: 0.5080\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.50360\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5061 - mean_squared_error: 0.5061 - val_loss: 0.5025 - val_mean_squared_error: 0.5025\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.50360 to 0.50248, saving model to temp/f14\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.5059 - mean_squared_error: 0.5059 - val_loss: 0.5036 - val_mean_squared_error: 0.5036\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.50248\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5062 - mean_squared_error: 0.5062 - val_loss: 0.5050 - val_mean_squared_error: 0.5050\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.50248\n",
      "Epoch 00019: early stopping\n",
      "temp/f15\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 172us/step - loss: 0.6132 - mean_squared_error: 0.6132 - val_loss: 0.5304 - val_mean_squared_error: 0.5304\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53037, saving model to temp/f15\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5281 - mean_squared_error: 0.5281 - val_loss: 0.5278 - val_mean_squared_error: 0.5278\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53037 to 0.52777, saving model to temp/f15\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5239 - mean_squared_error: 0.5239 - val_loss: 0.5204 - val_mean_squared_error: 0.5204\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52777 to 0.52040, saving model to temp/f15\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 160us/step - loss: 0.5211 - mean_squared_error: 0.5211 - val_loss: 0.5275 - val_mean_squared_error: 0.5275\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.52040\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5190 - mean_squared_error: 0.5190 - val_loss: 0.5153 - val_mean_squared_error: 0.5153\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.52040 to 0.51533, saving model to temp/f15\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 0.5170 - mean_squared_error: 0.5170 - val_loss: 0.5150 - val_mean_squared_error: 0.5150\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.51533 to 0.51499, saving model to temp/f15\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 0.5167 - mean_squared_error: 0.5167 - val_loss: 0.5130 - val_mean_squared_error: 0.5130\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51499 to 0.51300, saving model to temp/f15\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.5136 - mean_squared_error: 0.5136 - val_loss: 0.5111 - val_mean_squared_error: 0.5111\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.51300 to 0.51105, saving model to temp/f15\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.5134 - mean_squared_error: 0.5134 - val_loss: 0.5148 - val_mean_squared_error: 0.5148\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51105\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 0.5115 - mean_squared_error: 0.5115 - val_loss: 0.5094 - val_mean_squared_error: 0.5094\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.51105 to 0.50940, saving model to temp/f15\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 0.5106 - mean_squared_error: 0.5106 - val_loss: 0.5068 - val_mean_squared_error: 0.5068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00011: val_loss improved from 0.50940 to 0.50679, saving model to temp/f15\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 0.5092 - mean_squared_error: 0.5092 - val_loss: 0.5091 - val_mean_squared_error: 0.5091\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.50679\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5075 - mean_squared_error: 0.5075 - val_loss: 0.5071 - val_mean_squared_error: 0.5071\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.50679\n",
      "Epoch 00013: early stopping\n",
      "temp/f16\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 0.6145 - mean_squared_error: 0.6145 - val_loss: 0.5308 - val_mean_squared_error: 0.5308\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53076, saving model to temp/f16\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.5296 - mean_squared_error: 0.5296 - val_loss: 0.5255 - val_mean_squared_error: 0.5255\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53076 to 0.52551, saving model to temp/f16\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.5224 - mean_squared_error: 0.5224 - val_loss: 0.5209 - val_mean_squared_error: 0.5209\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52551 to 0.52087, saving model to temp/f16\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.5211 - mean_squared_error: 0.5211 - val_loss: 0.5182 - val_mean_squared_error: 0.5182\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52087 to 0.51817, saving model to temp/f16\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 0.5196 - mean_squared_error: 0.5196 - val_loss: 0.5134 - val_mean_squared_error: 0.5134\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.51817 to 0.51341, saving model to temp/f16\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5184 - mean_squared_error: 0.5184 - val_loss: 0.5128 - val_mean_squared_error: 0.5128\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.51341 to 0.51277, saving model to temp/f16\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 160us/step - loss: 0.5140 - mean_squared_error: 0.5140 - val_loss: 0.5127 - val_mean_squared_error: 0.5127\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51277 to 0.51271, saving model to temp/f16\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.5116 - mean_squared_error: 0.5116 - val_loss: 0.5080 - val_mean_squared_error: 0.5080\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.51271 to 0.50804, saving model to temp/f16\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5115 - mean_squared_error: 0.5115 - val_loss: 0.5083 - val_mean_squared_error: 0.5083\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.50804\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5081 - mean_squared_error: 0.5081 - val_loss: 0.5169 - val_mean_squared_error: 0.5169\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.50804\n",
      "Epoch 00010: early stopping\n",
      "temp/f17\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 0.6189 - mean_squared_error: 0.6189 - val_loss: 0.5296 - val_mean_squared_error: 0.5296\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.52961, saving model to temp/f17\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 0.5278 - mean_squared_error: 0.5278 - val_loss: 0.5249 - val_mean_squared_error: 0.5249\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.52961 to 0.52488, saving model to temp/f17\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.5222 - val_mean_squared_error: 0.5222\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52488 to 0.52218, saving model to temp/f17\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.5223 - mean_squared_error: 0.5223 - val_loss: 0.5191 - val_mean_squared_error: 0.5191\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52218 to 0.51905, saving model to temp/f17\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5194 - mean_squared_error: 0.5194 - val_loss: 0.5162 - val_mean_squared_error: 0.5162\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.51905 to 0.51619, saving model to temp/f17\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.5164 - mean_squared_error: 0.5164 - val_loss: 0.5278 - val_mean_squared_error: 0.5278\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51619\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5138 - mean_squared_error: 0.5138 - val_loss: 0.5132 - val_mean_squared_error: 0.5132\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51619 to 0.51321, saving model to temp/f17\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5142 - mean_squared_error: 0.5142 - val_loss: 0.5088 - val_mean_squared_error: 0.5088\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.51321 to 0.50875, saving model to temp/f17\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5125 - mean_squared_error: 0.5125 - val_loss: 0.5143 - val_mean_squared_error: 0.5143\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.50875\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 0.5088 - mean_squared_error: 0.5088 - val_loss: 0.5076 - val_mean_squared_error: 0.5076\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.50875 to 0.50761, saving model to temp/f17\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5103 - mean_squared_error: 0.5103 - val_loss: 0.5082 - val_mean_squared_error: 0.5082\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.50761\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5103 - mean_squared_error: 0.5103 - val_loss: 0.5069 - val_mean_squared_error: 0.5069\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.50761 to 0.50690, saving model to temp/f17\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 0.5091 - mean_squared_error: 0.5091 - val_loss: 0.5067 - val_mean_squared_error: 0.5067\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.50690 to 0.50668, saving model to temp/f17\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 0.5083 - mean_squared_error: 0.5083 - val_loss: 0.5072 - val_mean_squared_error: 0.5072\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.50668\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 3s 160us/step - loss: 0.5104 - mean_squared_error: 0.5104 - val_loss: 0.5055 - val_mean_squared_error: 0.5055\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.50668 to 0.50554, saving model to temp/f17\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 3s 160us/step - loss: 0.5071 - mean_squared_error: 0.5071 - val_loss: 0.5065 - val_mean_squared_error: 0.5065\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.50554\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 3s 160us/step - loss: 0.5072 - mean_squared_error: 0.5072 - val_loss: 0.5024 - val_mean_squared_error: 0.5024\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.50554 to 0.50242, saving model to temp/f17\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5043 - mean_squared_error: 0.5043 - val_loss: 0.5055 - val_mean_squared_error: 0.5055\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.50242\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5054 - mean_squared_error: 0.5054 - val_loss: 0.5025 - val_mean_squared_error: 0.5025\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.50242\n",
      "Epoch 00019: early stopping\n",
      "temp/f18\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 169us/step - loss: 0.6176 - mean_squared_error: 0.6176 - val_loss: 0.5321 - val_mean_squared_error: 0.5321\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53210, saving model to temp/f18\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 0.5294 - mean_squared_error: 0.5294 - val_loss: 0.5256 - val_mean_squared_error: 0.5256\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53210 to 0.52564, saving model to temp/f18\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.5247 - mean_squared_error: 0.5247 - val_loss: 0.5209 - val_mean_squared_error: 0.5209\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52564 to 0.52087, saving model to temp/f18\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.5214 - mean_squared_error: 0.5214 - val_loss: 0.5187 - val_mean_squared_error: 0.5187\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52087 to 0.51873, saving model to temp/f18\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.5184 - mean_squared_error: 0.5184 - val_loss: 0.5146 - val_mean_squared_error: 0.5146\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.51873 to 0.51464, saving model to temp/f18\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.5176 - mean_squared_error: 0.5176 - val_loss: 0.5123 - val_mean_squared_error: 0.5123\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.51464 to 0.51226, saving model to temp/f18\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 0.5138 - mean_squared_error: 0.5138 - val_loss: 0.5147 - val_mean_squared_error: 0.5147\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51226\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5113 - mean_squared_error: 0.5113 - val_loss: 0.5116 - val_mean_squared_error: 0.5116\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.51226 to 0.51162, saving model to temp/f18\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5119 - mean_squared_error: 0.5119 - val_loss: 0.5088 - val_mean_squared_error: 0.5088\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.51162 to 0.50880, saving model to temp/f18\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5112 - mean_squared_error: 0.5112 - val_loss: 0.5151 - val_mean_squared_error: 0.5151\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.50880\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.5096 - mean_squared_error: 0.5096 - val_loss: 0.5054 - val_mean_squared_error: 0.5054\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.50880 to 0.50540, saving model to temp/f18\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5076 - mean_squared_error: 0.5076 - val_loss: 0.5069 - val_mean_squared_error: 0.5069\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.50540\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.5080 - mean_squared_error: 0.5080 - val_loss: 0.5062 - val_mean_squared_error: 0.5062\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.50540\n",
      "Epoch 00013: early stopping\n",
      "temp/f19\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 170us/step - loss: 0.6136 - mean_squared_error: 0.6136 - val_loss: 0.5303 - val_mean_squared_error: 0.5303\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53031, saving model to temp/f19\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 0.5312 - mean_squared_error: 0.5312 - val_loss: 0.5239 - val_mean_squared_error: 0.5239\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53031 to 0.52385, saving model to temp/f19\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 0.5238 - mean_squared_error: 0.5238 - val_loss: 0.5238 - val_mean_squared_error: 0.5238\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52385 to 0.52375, saving model to temp/f19\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.5212 - mean_squared_error: 0.5212 - val_loss: 0.5222 - val_mean_squared_error: 0.5222\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52375 to 0.52222, saving model to temp/f19\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 0.5188 - mean_squared_error: 0.5188 - val_loss: 0.5171 - val_mean_squared_error: 0.5171\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.52222 to 0.51713, saving model to temp/f19\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.5170 - mean_squared_error: 0.5170 - val_loss: 0.5157 - val_mean_squared_error: 0.5157\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.51713 to 0.51572, saving model to temp/f19\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.5157 - mean_squared_error: 0.5157 - val_loss: 0.5127 - val_mean_squared_error: 0.5127\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51572 to 0.51273, saving model to temp/f19\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5135 - mean_squared_error: 0.5135 - val_loss: 0.5130 - val_mean_squared_error: 0.5130\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51273\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5112 - mean_squared_error: 0.5112 - val_loss: 0.5130 - val_mean_squared_error: 0.5130\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51273\n",
      "Epoch 00009: early stopping\n"
     ]
    }
   ],
   "source": [
    "for idx, model_name in enumerate(model_names):\n",
    "    print(model_name)\n",
    "\n",
    "    if type(models[idx]) is list:\n",
    "        #clear session\n",
    "        keras.backend.clear_session() \n",
    "        #get model according to specification\n",
    "        model = get_model(models[idx], [0.2] * len(models), len(inputs))\n",
    "        callbacks = [ModelCheckpoint(model_name, verbose= verbosity, monitor='val_loss',save_best_only=True), \n",
    "                     EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose= verbosity, mode='auto')]\n",
    "        model.compile(optimizer = optimizers.SGD(lr = 0.001, momentum = 0.9, ), loss='mean_squared_error', metrics = ['mse'])\n",
    "        #print(len(X), len(y))\n",
    "        model.fit(X, y, epochs = 20, validation_data = (x_val, y_val), callbacks = callbacks, batch_size = 32, verbose = verbosity)\n",
    "    else:\n",
    "        models[idx].fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "1 1\n",
      "1 2\n",
      "1 3\n",
      "2 1\n",
      "2 2\n",
      "2 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n            setA = get_MB(get_CG(perturbed_df, tetrad), \\'g\\', pc)\\n            if setA != {\\'f\\'}:\\n                print(\"Error in SETA markov blanket\")\\n                #setA = {\\'f\\'}\\n            setC = get_MB(get_CG(test_df2, tetrad), \\'g\\', pc)\\n\\n            if setA != setC:\\n                causal_dicts[idx][str(m) + \\'_\\' + str(v)].append(1)\\n            else:\\n                causal_dicts[idx][str(m) + \\'_\\' + str(v)].append(0)\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_test = 2000\n",
    "metrics_dicts = []\n",
    "for m in models:\n",
    "    metrics_dicts.append(defaultdict(list))\n",
    "\n",
    "\n",
    "#means = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "#variances = [1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0]\n",
    "means = [0, 1, 2]\n",
    "variances = [1,2,3]\n",
    "\n",
    "\n",
    "# ok at this point we need to check the model on various variances and means\n",
    "for m in means:\n",
    "    for v in variances:\n",
    "        print(m,v)\n",
    "        #t0 = time.time()\n",
    "        perturbed_df = gen_data(mean =m, var = v, SIZE = nb_test)\n",
    "        y_test2 = perturbed_df[target]\n",
    "        x_test2 = perturbed_df[inputs]\n",
    "        #t1 = time.time()\n",
    "        #print(\"Time for gen_data = \", t1 - t0)\n",
    "        for idx, model_name in enumerate(model_names):\n",
    "            #t0 = time.time()\n",
    "            if type(models[idx]) is list:\n",
    "                keras.backend.clear_session()\n",
    "                model = load_model(model_name)\n",
    "            else:\n",
    "                model = models[idx]\n",
    "            #t1 = time.time()\n",
    "            #print(\"Time to load model = \", t1 - t0)\n",
    "            \n",
    "            y_pred2 = model.predict(x_test2)\n",
    "            metrics_dicts[idx][str(m) + '_' + str(v)].append(mean_squared_error(y_test2, y_pred2))\n",
    "\n",
    "            test_df2 = pd.DataFrame(x_test2, columns = inputs)\n",
    "            test_targets2 = pd.DataFrame(model.predict(x_test2), columns = target)\n",
    "            test_df2 = test_df2.join(test_targets2)\n",
    "'''\n",
    "            setA = get_MB(get_CG(perturbed_df, tetrad), 'g', pc)\n",
    "            if setA != {'f'}:\n",
    "                print(\"Error in SETA markov blanket\")\n",
    "                #setA = {'f'}\n",
    "            setC = get_MB(get_CG(test_df2, tetrad), 'g', pc)\n",
    "\n",
    "            if setA != setC:\n",
    "                causal_dicts[idx][str(m) + '_' + str(v)].append(1)\n",
    "            else:\n",
    "                causal_dicts[idx][str(m) + '_' + str(v)].append(0)\n",
    "\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times =  0\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3173.088345451235\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3182.7622265264636\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3264.8191558371614\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3225.6828902105913\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3076.346813071783\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3183.806955741884\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3270.308463284337\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3160.3063572396995\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3197.223536479797\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3231.4095946302077\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3219.093020765762\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3321.3805265396736\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3192.056874053581\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3207.6721258592893\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3225.081322336065\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3158.154971341197\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3190.000951566699\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3299.0330764105597\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3271.0033601609794\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3116.2688456108826\n",
      "Times =  1\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3323.289258067455\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3335.3077936325017\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3406.4064325244044\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3368.4682788512764\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3238.3480991233005\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3343.2295259986104\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3428.5854967754353\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3255.05881087125\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3375.3311338419744\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3368.629276363539\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3370.4944747917807\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3471.260124917849\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3343.346672959835\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3330.7151143882456\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3365.350604514479\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3303.370724300554\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3333.1093061316346\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3464.34928468755\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3432.9647053753597\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3314.7030562707196\n",
      "Times =  2\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2764.398296552342\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2729.107922873734\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2827.7411499240993\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2778.1525588062204\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2649.98954600526\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2750.3647010251298\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2912.3212620846825\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2717.0407521848724\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2784.352722333995\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2798.8465056673012\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2787.560036267962\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2898.539327463199\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2751.3360533069545\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2756.683206631455\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2753.214367869493\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2708.4793491529044\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2756.8395459910853\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2888.9697293025056\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2832.3827551398076\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2745.887681939055\n",
      "Times =  3\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2897.5323223181695\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2886.064417479803\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2977.0554926405657\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2910.166217964554\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2817.9963159185854\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2887.9382955339856\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2987.226869225983\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2844.1399055203865\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2928.474291476041\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2972.567043106524\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2898.1167906979144\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3036.0746070619616\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2906.6362617738155\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2860.6434960783117\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2902.5528211643473\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2844.0245551959338\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2895.3440629505885\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2992.020482862818\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2998.117185531339\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2844.840326521857\n",
      "Times =  4\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3009.285183904605\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3016.147845659167\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3057.7656123572974\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3047.28779646728\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2909.490668725084\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2979.482225134405\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3137.2294587013616\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2967.1665430436815\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3034.90743920523\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3050.9834291841294\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3040.6589397726675\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3125.803144078247\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3014.0753548227995\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3029.2690544773063\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3032.2356112525886\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2979.273691445579\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3003.7060481135823\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3114.0185669650077\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3090.7077110868195\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2979.0229513473523\n",
      "Times =  5\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2648.2934214616243\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2640.463802871607\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2715.9486530316294\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2720.3820418986693\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2554.864426912697\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2646.117087880757\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2748.4299927209427\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2608.411935632788\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2696.2141878867565\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2720.087527043638\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2681.3614346766894\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2811.904537690834\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2647.659691942582\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2650.4711911491695\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2690.0755866134914\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2630.0612921225174\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2641.2390725391137\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2760.5684128511966\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2754.6645112900187\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2640.2079158492506\n",
      "Times =  6\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2963.706649446437\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2943.486614043816\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3021.7354919259274\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3026.57075378268\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2856.849351335429\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2953.5513542127683\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3074.7453145914214\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2908.8841728268476\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2984.3638427629585\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3029.9298508515385\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2980.9038249228533\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3094.337455791884\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2944.901027969522\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2970.4746881265037\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2987.754193339974\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2949.161258101337\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2953.9846799704974\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3078.1722419558155\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3054.659909746977\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2939.0909834319223\n",
      "Times =  7\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3626.8681661993146\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3632.748696240861\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3682.8974474521465\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3675.8058905343087\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3517.028468234186\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3616.355976218379\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3719.275691515176\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3585.7648025561\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3633.310388545828\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3669.594350110573\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3634.065537326509\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3758.5383602862175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3632.465815941825\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3609.8426603412745\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3683.635404131259\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3580.6150687636073\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3624.7308103332903\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3710.0566278227307\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3729.8756082249133\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-3573.58523217396\n",
      "Times =  8\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2641.4162362703355\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2635.8918940109584\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2735.349663607839\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2633.365819317391\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2545.686503829628\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2649.5548119094683\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2748.436591521624\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2614.6258485031167\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2649.179034428474\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2692.7930820271836\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2640.2772651070454\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2765.932708948916\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2658.6889932171944\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2644.4899248160614\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2662.8583970786926\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2615.4443477496384\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2652.512020375153\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2749.978126861771\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2731.8864348971224\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2608.0472813494016\n",
      "Times =  9\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2693.8303221815877\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2704.6142148720364\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2770.4776253462783\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2728.55698292761\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2569.6499740100767\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2675.2082359437736\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2801.1117782868077\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2629.0991960192305\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2715.637843783065\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2737.8965154707785\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2720.266559059521\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2817.88658185125\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2718.4123168507185\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2659.825618971224\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2708.0292505896095\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2656.2057970681394\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2689.773583173691\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2793.33176305084\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2789.5064996853334\n",
      "['a --> c', 'b --> c', 'c --> d']\n",
      "-2671.5675584838827\n",
      "Model_name =  temp/f0 Violations =  0.0\n",
      "Average_violations =  -2974.1708201853107 305.6466368852391\n",
      "MSE =  0.5140439325301988 0.0166237133991286\n",
      "Model_name =  temp/f1 Violations =  0.0\n",
      "Average_violations =  -2970.659542821095 312.4912380066433\n",
      "MSE =  0.5110884729745236 0.015691994682249483\n",
      "Model_name =  temp/f2 Violations =  0.0\n",
      "Average_violations =  -3046.019672464735 302.96543417624224\n",
      "MSE =  0.5092416348337736 0.01627508625730568\n",
      "Model_name =  temp/f3 Violations =  0.0\n",
      "Average_violations =  -3011.443923076058 314.423644156391\n",
      "MSE =  0.5099694984528182 0.015601440082982402\n",
      "Model_name =  temp/f4 Violations =  0.0\n",
      "Average_violations =  -2873.625016716603 307.2970887387767\n",
      "MSE =  0.513726759268186 0.017112832261894643\n",
      "Model_name =  temp/f5 Violations =  0.0\n",
      "Average_violations =  -2968.560916959916 308.6406865142688\n",
      "MSE =  0.5098581686295676 0.014971205219897777\n",
      "Model_name =  temp/f6 Violations =  0.0\n",
      "Average_violations =  -3082.767091870777 300.6664997853062\n",
      "MSE =  0.5237290454289991 0.014339507931449858\n",
      "Model_name =  temp/f7 Violations =  0.0\n",
      "Average_violations =  -2929.0498324397972 306.0546611869729\n",
      "MSE =  0.5137659190675397 0.015162619086436223\n",
      "Model_name =  temp/f8 Violations =  0.0\n",
      "Average_violations =  -2999.899442074412 304.98495536298356\n",
      "MSE =  0.5162496694720884 0.015191846710215558\n",
      "Model_name =  temp/f9 Violations =  0.0\n",
      "Average_violations =  -3027.273717445542 302.7361395580378\n",
      "MSE =  0.5186663793450244 0.01552934714590793\n",
      "Model_name =  temp/f10 Violations =  0.0\n",
      "Average_violations =  -2997.2797883388707 308.85976377873936\n",
      "MSE =  0.5119745140272234 0.017382021934650518\n",
      "Model_name =  temp/f11 Violations =  0.0\n",
      "Average_violations =  -3110.165737463003 306.43782792227216\n",
      "MSE =  0.5101019068912349 0.014602125588740687\n",
      "Model_name =  temp/f12 Violations =  0.0\n",
      "Average_violations =  -2980.9579062838825 307.3878418830268\n",
      "MSE =  0.5091042559388244 0.015224559019200517\n",
      "Model_name =  temp/f13 Violations =  0.0\n",
      "Average_violations =  -2972.0087080838844 310.03246199042906\n",
      "MSE =  0.5204578869907978 0.015037416745191363\n",
      "Model_name =  temp/f14 Violations =  0.0\n",
      "Average_violations =  -3001.078755889 319.0675650657593\n",
      "MSE =  0.5078160267534972 0.015341307199836911\n",
      "Model_name =  temp/f15 Violations =  0.0\n",
      "Average_violations =  -2942.479105524141 305.99559709487727\n",
      "MSE =  0.5124274822359166 0.014560430763400063\n",
      "Model_name =  temp/f16 Violations =  0.0\n",
      "Average_violations =  -2974.1240081145334 307.9876348205665\n",
      "MSE =  0.5121144963367519 0.016258956957045483\n",
      "Model_name =  temp/f17 Violations =  0.0\n",
      "Average_violations =  -3085.049831277079 305.112999486365\n",
      "MSE =  0.5084994732276606 0.015129294376482198\n",
      "Model_name =  temp/f18 Violations =  0.0\n",
      "Average_violations =  -3068.576868113867 310.20773237209886\n",
      "MSE =  0.5105286365774468 0.013850263872370596\n",
      "Model_name =  temp/f19 Violations =  0.0\n",
      "Average_violations =  -2943.3221832978284 298.8604043766277\n",
      "MSE =  0.5125223930861785 0.015193122154860812\n",
      "[0.51404393 0.51108847 0.50924163 0.5099695  0.51372676 0.50985817\n",
      " 0.52372905 0.51376592 0.51624967 0.51866638 0.51197451 0.51010191\n",
      " 0.50910426 0.52045789 0.50781603 0.51242748 0.5121145  0.50849947\n",
      " 0.51052864 0.51252239] [0.01662371 0.01569199 0.01627509 0.01560144 0.01711283 0.01497121\n",
      " 0.01433951 0.01516262 0.01519185 0.01552935 0.01738202 0.01460213\n",
      " 0.01522456 0.01503742 0.01534131 0.01456043 0.01625896 0.01512929\n",
      " 0.01385026 0.01519312] [-2974.17082019 -2970.65954282 -3046.01967246 -3011.44392308\n",
      " -2873.62501672 -2968.56091696 -3082.76709187 -2929.04983244\n",
      " -2999.89944207 -3027.27371745 -2997.27978834 -3110.16573746\n",
      " -2980.95790628 -2972.00870808 -3001.07875589 -2942.47910552\n",
      " -2974.12400811 -3085.04983128 -3068.57686811 -2943.3221833 ] [305.64663689 312.49123801 302.96543418 314.42364416 307.29708874\n",
      " 308.64068651 300.66649979 306.05466119 304.98495536 302.73613956\n",
      " 308.85976378 306.43782792 307.38784188 310.03246199 319.06756507\n",
      " 305.99559709 307.98763482 305.11299949 310.20773237 298.86040438]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1332x756 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp/f0\n",
      "Area under surface (rectangular approx) =  64.13192838624221\n",
      "Violations =  0.0\n",
      "Average_violations =  -2974.1708201853107\n",
      "MSE =  0.5140439325301988\n",
      "temp/f1\n",
      "Area under surface (rectangular approx) =  59.733855078866725\n",
      "Violations =  0.0\n",
      "Average_violations =  -2970.659542821095\n",
      "MSE =  0.5110884729745236\n",
      "temp/f2\n",
      "Area under surface (rectangular approx) =  53.417624242027145\n",
      "Violations =  0.0\n",
      "Average_violations =  -3046.019672464735\n",
      "MSE =  0.5092416348337736\n",
      "temp/f3\n",
      "Area under surface (rectangular approx) =  52.76249888462506\n",
      "Violations =  0.0\n",
      "Average_violations =  -3011.443923076058\n",
      "MSE =  0.5099694984528182\n",
      "temp/f4\n",
      "Area under surface (rectangular approx) =  64.71908310620053\n",
      "Violations =  0.0\n",
      "Average_violations =  -2873.625016716603\n",
      "MSE =  0.513726759268186\n",
      "temp/f5\n",
      "Area under surface (rectangular approx) =  59.13133251960075\n",
      "Violations =  0.0\n",
      "Average_violations =  -2968.560916959916\n",
      "MSE =  0.5098581686295676\n",
      "temp/f6\n",
      "Area under surface (rectangular approx) =  61.10420433292059\n",
      "Violations =  0.0\n",
      "Average_violations =  -3082.767091870777\n",
      "MSE =  0.5237290454289991\n",
      "temp/f7\n",
      "Area under surface (rectangular approx) =  66.43203282921603\n",
      "Violations =  0.0\n",
      "Average_violations =  -2929.0498324397972\n",
      "MSE =  0.5137659190675397\n",
      "temp/f8\n",
      "Area under surface (rectangular approx) =  57.201618944735344\n",
      "Violations =  0.0\n",
      "Average_violations =  -2999.899442074412\n",
      "MSE =  0.5162496694720884\n",
      "temp/f9\n",
      "Area under surface (rectangular approx) =  61.258068613468524\n",
      "Violations =  0.0\n",
      "Average_violations =  -3027.273717445542\n",
      "MSE =  0.5186663793450244\n",
      "temp/f10\n",
      "Area under surface (rectangular approx) =  56.14730661947803\n",
      "Violations =  0.0\n",
      "Average_violations =  -2997.2797883388707\n",
      "MSE =  0.5119745140272234\n",
      "temp/f11\n",
      "Area under surface (rectangular approx) =  48.01386650019306\n",
      "Violations =  0.0\n",
      "Average_violations =  -3110.165737463003\n",
      "MSE =  0.5101019068912349\n",
      "temp/f12\n",
      "Area under surface (rectangular approx) =  56.379813890282755\n",
      "Violations =  0.0\n",
      "Average_violations =  -2980.9579062838825\n",
      "MSE =  0.5091042559388244\n",
      "temp/f13\n",
      "Area under surface (rectangular approx) =  60.68375204495372\n",
      "Violations =  0.0\n",
      "Average_violations =  -2972.0087080838844\n",
      "MSE =  0.5204578869907978\n",
      "temp/f14\n",
      "Area under surface (rectangular approx) =  54.06058526836374\n",
      "Violations =  0.0\n",
      "Average_violations =  -3001.078755889\n",
      "MSE =  0.5078160267534972\n",
      "temp/f15\n",
      "Area under surface (rectangular approx) =  56.280216402855885\n",
      "Violations =  0.0\n",
      "Average_violations =  -2942.479105524141\n",
      "MSE =  0.5124274822359166\n",
      "temp/f16\n",
      "Area under surface (rectangular approx) =  57.67164376546887\n",
      "Violations =  0.0\n",
      "Average_violations =  -2974.1240081145334\n",
      "MSE =  0.5121144963367519\n",
      "temp/f17\n",
      "Area under surface (rectangular approx) =  47.23263486667052\n",
      "Violations =  0.0\n",
      "Average_violations =  -3085.049831277079\n",
      "MSE =  0.5084994732276606\n",
      "temp/f18\n",
      "Area under surface (rectangular approx) =  53.01452778542884\n",
      "Violations =  0.0\n",
      "Average_violations =  -3068.576868113867\n",
      "MSE =  0.5105286365774468\n",
      "temp/f19\n",
      "Area under surface (rectangular approx) =  63.302828866067884\n",
      "Violations =  0.0\n",
      "Average_violations =  -2943.3221832978284\n",
      "MSE =  0.5125223930861785\n"
     ]
    }
   ],
   "source": [
    "#the number of times to sample\n",
    "times = 10\n",
    "## the size of the test set\n",
    "\n",
    "\n",
    "violations = np.zeros(len(models))\n",
    "violation_mean = np.zeros((len(models), times))\n",
    "violation_mean2 = np.zeros((len(models), times))\n",
    "mean = np.zeros((len(models), times))\n",
    "\n",
    "fold = 0\n",
    "\n",
    "\n",
    "\n",
    "for t in range(times):\n",
    "    print(\"Times = \", t)\n",
    "    df_test = gen_data(SIZE = nb_test)\n",
    "    x_test = df_test[inputs].values\n",
    "    y_test = df_test[target].values\n",
    "    #bic_orig = get_bic(df_test,prior)\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        if type(models[idx]) is list:\n",
    "            keras.backend.clear_session()\n",
    "            model = load_model(model_name)\n",
    "        else:\n",
    "            model = models[idx]\n",
    "        test_df = pd.DataFrame(x_test, columns = inputs)\n",
    "        test_targets = pd.DataFrame(model.predict(x_test), columns = target)\n",
    "        test_df = test_df.join(test_targets)\n",
    "       \n",
    "        \n",
    "    \n",
    "        mean[idx][t] = mean_squared_error(y_test, model.predict(x_test)) \n",
    "        \n",
    "        bic_pred = get_bic(test_df,prior)\n",
    "        \n",
    "        #bic_pred = get_bic(df_test.join(pd.DataFrame(model.predict(x_test), columns = ['target'])), prior)\n",
    "        \n",
    "        print(tetrad.getEdges())\n",
    "        print(bic_pred)\n",
    "        violation_mean[idx][t] = bic_pred\n",
    "        violation_mean2[idx][t] = bic_pred\n",
    "        #print(bic_orig - bic_pred)\n",
    "metric = []\n",
    "metric_err = []\n",
    "viol = []\n",
    "viol_err = []\n",
    "\n",
    "#normalize the violations for prettier graphing.\n",
    "#also violations are always positive, so just divide by max.\n",
    "\n",
    "#TMK\n",
    "#violation_mean = violation_mean / np.max(violation_mean)\n",
    "\n",
    "for i in range(len(violations)):\n",
    "    print(\"Model_name = \", model_names[i], \"Violations = \", violations[i])\n",
    "    print(\"Average_violations = \", np.mean(violation_mean[i]), np.std(violation_mean[i]))\n",
    "    print(\"MSE = \", np.mean(mean[i]), np.std(mean[i]))\n",
    "    #print(\"mean = \", mean[i])\n",
    "    metric.append(np.mean(mean[i]))\n",
    "    metric_err.append(np.std(mean[i]))\n",
    "    viol.append(np.mean(violation_mean[i]))\n",
    "    #viol.append(violations[i]/times)\n",
    "    viol_err.append(np.std(violation_mean[i]))\n",
    "print(np.array(metric), \n",
    "         np.array(metric_err), \n",
    "         np.array(viol), \n",
    "         np.array(viol_err))    \n",
    "\n",
    "bar_plot(model_names, \n",
    "         np.array(metric), \n",
    "         np.array(metric_err), \n",
    "         np.array(viol), \n",
    "         np.array(viol_err))\n",
    "\n",
    "\n",
    "def heat_plot(x,y,z, xlab = 'Mean', ylab = 'Variance', clim_low = 0, clim_high = 1):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    cax = ax.scatter(x, y, c=z, s=450, edgecolor='')\n",
    "    cax.set_clim(clim_low, clim_high)\n",
    "    ax.set_xlabel(xlab)\n",
    "    ax.set_ylabel(ylab)\n",
    "    plt.colorbar(cax)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "MSE = []\n",
    "VIO = []\n",
    "VIO2 = []\n",
    "AUS = []\n",
    "for i, m in enumerate(models):\n",
    "    print(model_names[i])\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "    \n",
    "    \n",
    "    rectangular_approx = 0\n",
    "    for k, v in metrics_dicts[i].items():\n",
    "        x.append(float(k.split('_')[0]))\n",
    "        y.append(float(k.split('_')[-1]))\n",
    "        z.append(np.mean(v))\n",
    "        rectangular_approx += np.mean(v)\n",
    "    print(\"Area under surface (rectangular approx) = \", rectangular_approx)\n",
    "    print(\"Violations = \", violations[i])\n",
    "    print(\"Average_violations = \", np.mean(violation_mean[i]))\n",
    "    print(\"MSE = \", np.mean(mean[i]))   \n",
    "    MSE.append(np.mean(mean[i]))\n",
    "    VIO.append(np.mean(violation_mean[i]))\n",
    "    VIO2.append(np.mean(violation_mean2[i]))\n",
    "    AUS.append(rectangular_approx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7195360899887455\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8106579784185599\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3Xd4VGXax/HvkwIJIZTQIYQktBA6RKSFIkWkqSBFIKCAbhF1VVRWV2VxWV3XXXXd8ipgYUIVRelNBEIRCQgISWhJIBXSCCE9M8/7xwSkpExCZibl/lwXF5nJnDn3Y8mdc86c+6e01gghhBClcbB3AUIIIaoGaRhCCCEsIg1DCCGERaRhCCGEsIg0DCGEEBaRhiGEEMIi0jCEEEJYRBqGEEIIi0jDEEIIYREnexdQkRo3bqy9vb3tXYYQQlQZR48eTdZaN7HktdWqYXh7exMaGmrvMoQQospQSl209LVySkoIIYRFpGEIIYSwiDQMIYQQFqlW1zCKkp+fT2xsLDk5OfYupVJycXHB09MTZ2dne5cihKjkqn3DiI2Nxd3dHW9vb5RS9i6nUtFak5KSQmxsLD4+PvYuRwhRyVX7U1I5OTk0atRImkURlFI0atRIjr6EEBap9g0DkGZRAvlnI4SwVI1oGEIIUW1dPAT7P7TJrqRh2ICjoyM9evSge/fu9OrVi4MHDwIQHR1Nly5dbr7up59+YtCgQXTs2BE/Pz/mzp1LVlaWvcoWQlRmuRmweT58PgqOfg55mVbfpVUveiulooEMwAgUaK0DCp9/FpgHFACbtdavWLptVeTq6srx48cB2L59O3/84x/Zu3fvba+5fPkykyZNYvXq1fTr1w+tNV9//TUZGRnUqVPHHmULISqr87tg4x8gPRbu/x088Ceo5Wb13driU1JDtdbJNx4opYYCDwPdtNa5Sqmmlm5bHVy7do2GDRve9fx//vMfZs2aRb9+/QDztYXHHnvM1uUJISqzrFTY/hqcWAWNO8KcHdC6j812b4+P1f4OeFdrnQugtb5iqx3/eeNpwuKvVeh7+resx1vjOpf4muzsbHr06EFOTg4JCQns3r37rtecOnWKWbNmVWhtQohqQmsI+w62zIfsNBj0svmPU22blmHtaxga2KGUOqqUerrwuQ5AoFLqsFJqr1LqvjJsexel1NNKqVClVGhSUlIFl18xbpySioiIYNu2bcycOROttb3LEkJUBRmJsGYGfDUL6rWCp/eYT0HZuFmA9Y8wBmit4wtPO+1USkUU7rMh0Be4D1irlPLVd/8EvWtbrfW+O3egtf4U+BQgICCgxJ/CpR0J2EK/fv1ITk7mzubWuXNnjh49ysMPP2ynyoQQlYrWcHyF+RRUQS4M/zP0mweO9rvf2qpHGFrr+MK/rwDrgT5ALPCNNvsJMAGNLdy2youIiMBoNNKoUaPbnp83bx5ffvklhw8fvvlccHAwiYmJti5RCGFvadFgeAS+ewaadobfHoCBf7BrswArHmEopdwAB611RuHXI4FFwHXgAWCPUqoDUAtItnDbKunGNQwwj+P48ssvcXR0vO01zZo1Y/Xq1cyfP58rV67g4ODAoEGDmDBhgj1KFkLYg8kIP30K3y8C5Qhj/gG9Z4ND5bgDwprtqhmwvvBOYidgpdZ6m1KqFvCZUuoUkAfM0lprpVRLYKnWenRx21qxVqsyGo1FPu/t7c2pU6duPu7Xrx8hISG2KksIUZlciYANz0LsT9BuBIz7EOp72ruq21itYWitI4HuRTyfB8wo4vl4YHRJ2wohRLVjzDffqb3vPahVFyYsga6ToBKO7an202qFEKLSiv8ZvpsHl09B5wnw0HtQ16J4bbuQhiGEELaWnw173oGDH4NbU5i6EvzG2LuqUknDEEIIW4o+YL5WkXoBes2CEYvAtYG9q7KINAwhhLCFnGuwayGELoOG3jBzA/gOtndVZSINQwghrO3sDtj0B8hIMN98N/Q1mwwLrGiV48O91ZxSiqCgoJuPCwoKaNKkCWPHjgXMk2rHjh1L9+7d8ff3Z/To0YB5/Lmrqys9evS4+Wf58uV2WYMQohwyU+Drp2DlJKjtDnN2woOLq2SzADnCsAk3NzdOnTpFdnY2rq6u7Ny5k1atWt38/ptvvsmIESN4/vnnATh58uTN77Vt2/bmaHQhRBWhNZz+Bra8AjlXYfACCHzRLvOfKpIcYdjIQw89xObNmwFYtWoVjz/++M3vJSQk4On56w063bp1s3l9QogKci0BVk+DdbOhQWv4zT4Y+scq3yygph1hbF0Aib9U7Hs27woPvVvqy6ZOncqiRYsYO3YsJ0+eZPbs2Tfv6n7mmWeYMmUK//73vxk+fDhPPvkkLVu2BODChQs3x4oAfPzxxwQGBlbsGoQQ905rOLYcdrwBxjwY+RdzuJGd5z9VpOqzkkquW7duREdHs2rVqpvXKG548MEHiYyMZNu2bWzdupWePXveHBkip6SEqAJSI2Hj8xC1D7wDYdxH0KitvauqcDWrYVhwJGBN48ePZ/78+ezZs4eUlJTbvufh4cG0adOYNm0aY8eOZd++ffTu3dtOlQohLGIywo//g91/AUdnGPuh+d6KSjIssKLVrIZhZ7Nnz6Z+/fp07dqVPXv23Hx+9+7d9O3blzp16pCRkcGFCxfw8vKyX6FCiNJdDoMN8yDuKHQYBWP+CfVblb5dFSYNw4Y8PT1vfhLqVkePHmXevHk4OTlhMpmYO3cu9913H9HR0Xddw5g9ezbPPfecLcsWQtyqIA/2/xP2vQ8u9WDiMugysVIOC6xoqjpFhQYEBOjQ0NDbngsPD6dTp052qqhqkH9GQlgo7qh5WOCVMPNE2VHvgttd+W9VilLqqNY6wJLXyhGGEEKUJi8LflgMP/4X6jaHx9dAx1H2rsrmpGEIIURJovaZhwWmRUPvJ2HEn8Glvr2rsgtpGEIIUZScdNj5Jhz9Ahr6wKxN4FOz74GShiGEEHc6sxU2vQDXL0P/Z2HIa1Crjr2rKlJCejan464x3L+Z1fclDUMIIW7ITIatr8KpddC0M0xdAa0q5/1Qp+LSWRISyeaTCdSp5chPrw/HxdnRqvuUhiGEEFrDL+tg6yuQmwFDX4cBfwCnWvau7DYmk2Z3xBWWhERyOCoVt1qOzOznzZMDvK3eLECGD9pEaePNv/jiC5o0aXLbGPMTJ07c/NrDwwMfHx969OjB8OHDbxt77u/vz8yZM8nPzwdgz549N98XYOvWrQQEBNCpUyf8/PyYP3++bRcvRGWXHgerpsI3c8HDF34bAoNfqVTNIjvPSPCPFxn+z73MXR5KTGoWr4/uxKHXhvHmOH9ae9jmdJkcYdhAaePNgZvDB291Y4bUE088wdixY3nssccAc07GjRlTRqORESNGsHbtWqZPn37b9qdOnWLevHls3rwZPz8/CgoK+PTTT624UiGqEJMJjn0BO94EbYQH34H7fwMO1v9N3VJXMnIwHLpI8I8XScvKp5tnff71eE8e6tIcZ0fb/74vDcNGbow3f+yxx26ON78xrfZeODo60qdPH+Li4u763nvvvcfrr7+On58fAE5OTvz+97+/530KUeWlXIANz8HF/eAz2Dws0MPH3lXddCYxg6UhkXx3PJ58k4nhnZoxd6APfXw8UHa8o7xGNYy//fQ3IlIjKvQ9/Tz8eLXPq6W+rqTx5gBr1qxh//79Nx8fOnQIV1fXUt83JyeHw4cP89FHH931vVOnTvHSSy9ZuBIhagBjgfnmux8Wg2NtGP8x9AyqFGM9tNaEnEtmSUgkIeeScXF2YMp9rXlygDe+TerauzyghjUMeyppvDkUfUqqJDdmTJ07d47HHntMQpeEKE3iKfOwwPifoeMYGPMPqNfC3lWRW2Dku+PxLAuJ4szlDJq41+blBzsyrY8XDd0qz3UUqGENw5IjAWsqabx5Wd24hpGQkMCQIUPYsGED48ePv+01nTt35ujRo3Tv3v2e9iVElVaQax4UuP+f4NIAHvscOj9q96OK1Mw8Vvx4kS8PXST5ei5+zd15f1J3xnVvQW0ny6+jxGbEcib1DMPaDLNitWY1qmHYW3Hjze9FixYtePfdd3nnnXfuahgvv/wyEyZMYODAgXTo0AGTycSHH37Iiy++WCH7FqLSizliPqpIioBuU2HUO1DHw64lXUi6zmf7o/j6WCw5+SYGd2jCU4G+DGjXyOLrE1prjicdxxBm4PtL31PXuS4DPQdS29G6MbDSMGyouPHmcPc1jP/+97/079/fovd95JFHWLhw4V0X0bt168aHH37I448/TlZWFkopxowZU/4FCFFV5GWaQ41+/B/UawXT10H7EXYrR2vN4ahUloZEsiv8CrUcHXi0ZyvmBPrQoZm7xe+Tb8pnZ/RODGEGTqWcol6tejzZ+Uke93vc6s0CZLy5QP4ZiWomco/5E1BXL8J9c2HYW+bcCjvIN5rY8ksCS0IiORV3DQ+3Wszo24agvm1o4m75D/j03HTWnV3HqohVXM66jHc9b2Z0msG4tuOo43xv92BUmvHmSqloIAMwAgU3ilJKPQvMAwqAzVrrV4rYdhTwEeAILNVa2zdfVQhRuWVfhR1/gp8N4NEWntgC3gPsUkp6dj6rf7rEFwejSUjPwbeJG399tCsTerUq0x3ZF69dJDgsmO8ufEd2QTb3N7+fN/q+QaBnIA6qet6HMVRrnXzjgVJqKPAw0E1rnauUanrnBkopR+A/wAggFjiilNqgtQ6zQb1CiKomYjNsehEyk8wjPYYsAOfSP5Ze0WJSs/jsQBRrj8SQmWekn28jFj/ahSEdmuLgYPn1iSOJRzCEGdgbuxcnBydG+4wmyD+Ijh4drbyCktnjGsbvgHe11rkAWusrRbymD3Beax0JoJRajbnJlKthaK3terNLZVadTkmKGuj6FfP8p9ProVlXmLYaWva0eRlHL6axbH8k204l4qAU47q3ZM5AH7q0sjw3I8+Yx9aorQSHBxORGoGHiwe/6f4bpnScQmPXypHqZ+2GoYEdSikNfKK1/hToAAQqpRYDOcB8rfWRO7ZrBcTc8jgWuL88Bbi4uJCSkkKjRpZ/AqGm0FqTkpKCi4uLvUsRomy0hpNrYNsC8wXuB96AAc+Do7PNSjCaNNtPJ7I0JJJjl65Sz8WJpwe15Yn+3jSvb/n/U2k5aaw9s5bVZ1aTnJ1Muwbt+HP/PzPGd4xNLmSXhbUbxgCtdXzhaaedSqmIwn02BPoC9wFrlVK++vZfdYv6yV7kr8JKqaeBpwG8vLzu+r6npyexsbEkJSXd20qqKRcXFzw9Pe1dhhCWuxpjzqo4vxM8+8DD/4YmtjtVcz23gK9CY/jsQBQxqdl4edRh4Th/JgW0xq225T9SL1y9gCHMwKbITeQacxnQagCLOy2mX8t+lfaXW6s2DK11fOHfV5RS6zGfaooFvilsED8ppUxAY+DWn+ixQOtbHnsC8cXs41PgUzB/SurO7zs7O+PjU3lmxAghyslkgtBlsGuh+QjjoffMn4Ky0bDAhPRsvjgYzcrDl8jIKaB3m4a8ProTI/yb41iG6xOH4g+xPGw5B+IPUNuxNuPajmNGpxm0bdDWyiu4d1ZrGEopN8BBa51R+PVIYBFwHXgA2KOU6gDUApLv2PwI0F4p5QPEAVOBadaqVQhRySWfN+dqXzoIvkPNwwIbtrHJrk/FpbM0JJJNJxMwac1DXVowJ9CHXl4NLX6PnIIcNkduJjg8mPNXz9PYtTHzesxjcsfJNHSx/H3szZpHGM2A9YWHVk7ASq31NqVULeAzpdQpIA+YpbXWSqmWmD8+O1prXaCUmgdsx/yx2s+01qetWKsQojIyFsChj+GHd8DZBR7+L/SYZvWxHjeCipbuj+THyNuDisqSPZGcnczqiNWsPbOWtNw0/Dz8WDxwMaO8R1HLsXLNibJEtb9xTwhRRSWcNI/1SDgBncbB6PfBvblVd5mdZ+TrY7F8tj+KyORMWtR34ckB3kzt40U9F8svqJ9JPcPysOVsjdpKgamAwZ6Dmdl5JgHNAird9YlKc+OeEEKUWX4O7HsP9n8IdRrB5OXg/7BVd3lnUFHXVvX5aGoPRndtYXFQkUmbCIkNwRBm4HDiYVydXJnYfiIz/GfQpp5tTp9ZmzQMIUTlcemw+agi+Sx0nwYPLrbqsMAziRks2x/Jtz+bg4qG+TXjqcCyBRVl5Wex4cIGVoSvIPpaNM3qNOOF3i8wsf1E6te2/D6MqkAahhDC/nKvw/eL4KdPob4nzPga2g23yq5uBBUt3R/FvrNJuDg7MPk+T2YP8ClTUNHlzMusiljFV2e/4lreNbo06sJ7g95jeJvhODvY7n4QW5KGIYSwr/Pfw8Y/QHoM9HkKhr0JtS2f4GqpooKK5o/swPT725QpqOh08mmWhy1nR/QOTJgY5jWMIP8gejTpUemuT1Q0aRhCCPvISjUPCzy+Ahq1h9nbwKtvhe8mLTOP4DuCiv7+WDfG92hpcVCR0WTkh5gfMIQZOHblGG7Objze6XGm+U3D073m3PgqDUMIYXth38Hm+ZCVAoEvwaBXzB+brUCRSddZdkdQ0dxAHwa2a2zxkUBmfibrz60nODyYuOtxtKrbipcDXmZC+wnUrVU5crZtSRqGEMJ2Mi7DlvkQvgGadzNfq2hRcXn0twYVfR9xBWeH8gUVxV2PY2X4Sr459w3X86/Ts2lPXgp4iaGth+LkUHN/bNbclQshbEdrOL4Str8G+dnmUKP+z1bYsMAbQUVLQ6L4JS6dhnWceXZoO4L6eZcpqOj4leMsD1vO95e+R6EY6T2SoE5BdG3StULqrOqkYQghrCvtImz6A1zYDV79YPzH0Lh9hbx1UUFFix/twsRenhYHFRWYCth1cReGMAMnk0/iXsudWZ1nMc1vGs3drHujYFUjDUMIYR0mExxZArv+bB7lMfp9CJgDDveeFFdUUNFfHunC0I6WBxVdy7vG12e/ZmXEShIzE/Fy9+K1+1/j4bYP33PsaXUlDUMIUfGSzpqHBcb8aL6fYuwH0ODu+IGyOnYpjaUh9xZUFHMthuDwYNafX092QTZ9mvfh9ftfZ5DnILvEnlYl0jCEEBXHmA8HPoK9f4NabvDoJ9Btyj0NCzSaNDtOJ7LkjqCiWf3b0KK+ZTGsWmtCL4diCDOwJ2YPjg6ON2NP/Tz8yl1bTSMNQwhRMeKPm8d6JP4C/o/A6L9D3ablfrvM3ALW3hJU1NrDtcxBRfnGfLZFb8MQZiA8NZwGtRvwVLenmNpxKk3qNCl3bTWVNAwhxL3JzzYfURz4F7g1hinB5umy5VRUUNFrD3ViZGfLg4qu5lzlq7NfsSpiFUnZSfjW9+XNfm8yznccLk4SSVxe0jCEEOV38ZD5qCLlPPScASP/Aq7lCwSqiKCiyPRIgsOC2XhhIznGHPq37M+iAYvo37K/XJ+oANIwhBBll5th/vTTkSXmi9lB30LboWV+G5NJ88OZKywJKX9QkdaaQwmHMIQZ2B+3n1oOtW7GnrZr2K7MNYniScMQQpTNuZ3mYYHX4qDv72Ho61C7bGMysvOMfPNzLMv2RxGZZA4qem20H1Pu86K+q2U38+Uac9kSuQVDuIFzaedo5NKIZ3o8w+SOk/Fwsd5I9JpMGoYQwjJZqbDtj3ByNTTuCHN2QOs+ZXqLpIxcDIeiMdxDUFFydjJrz6xlzZk1pOak0qFhB94e8DajfUZXydjTqkQahhCiZFpD2Lew5WXITjMPChw0H5wsH7lx9nIGS0PuLajobNpZDGEGNkduJt+Uz2DPwQT5B9GneZ9qP1a8spCGIYQoXkYibH4JIjZBix4QtB6aWzZXSWvN/vPJLAkpf1CRSZvYH7cfQ5iBHxN+xMXRhQntJzC903R86vvcy8pEOUjDEELcTWv4ORi2vw7GXBixCPo+A46l/8jILTCy4Xg8y/ZHEZH4a1DRtPvb4GFhUFF2QTYbL2zEEGYg+lo0TV2b8nyv55nUYVK1iz2tSqRhCCFulxplHhYYuQfaDIBx/4LGpX/aKC0zjxWHzUFFSRnlCyq6knXlZuxpem46nRt15t3AdxnpPbLaxp5WJdIwhBBmJiMc/gR2vw3KEcb8E3o/WeqwwMik63x2IIp1R81BRYM6NOGfk8sWVBSWEoYhzMC26G0YTUYe8HqAmf4z6dm0p1yfqESkYQgh4EqE+Qa82CPQfqR5WGD94qNHfw0qiuL7iMs4OzjwSM+WzBnoS8fmlgUVGU1G9sTuwRBm4Ojlo9RxqsPUjlOZ1mkard1bV9TKRAWShiFETVaQBwc+hH1/h1p1YcIS6Dqp2GGBFRFUlJmfybfnv2VF+ApiMmJo4daC+QHzmdB+Au61LE/FE7YnDUOImirumHkE+eVT0GUijPob1C16IN+1nMKgogPRxKfn4NvYHFQ0oacnrrUsuz6RcD2BlREr+frs12TkZ9C9SXee7/U8w7yG1ejY06pE/i0JUdPkZcGed+DQv6FuM5i6CvxGF/nSmNQsPj8QzZojl24GFb1dxqCiE0knMIQZ2HVxFwAj2owgyD+Ibk0qLstb2IY0DCFqkuj95qOK1EjoNcv8cVnXBne97NilNJaFRLH1VAIOSjG2WwvmBvpaHFRUYCrg+0vfszxsOSeTTuLu7E6QfxDT/KbRom6Lil6VsBFpGELUBDnXYNdbEPoZNPSGmRvAd/BtL7kRVLR0fxRHL6bh7uLEU4N8eaK/t8VBRRl5GXxz7htWhq8kPjOe1u6tWdBnAY+0ewQ3ZzcrLEzYkjQMIaq7s9th0wuQkQD95pmHBdb6dRJsUUFFb43zZ3IZgopirsWwImIF68+tJ6sgi4BmAbza51UGew7G0cGyaxyi8rNqw1BKRQMZgBEo0FoHKKUWAk8BSYUve01rvcWSba1ZqxDVTmYKbFsAv6yFJp1g8nLw/PV/o4T0bL48eJGVhy9yLaeAXl4NyhRUpLXm2JVjLD+9nB9ifsBROTLKZxRB/kH4N/K35sqEndjiCGOo1jr5juc+0Fq/X85thRAl0RpOfQ1bXzGfihq8AAJfAifzWI5Tceks2x/FxhPxN4OKZg/0oXcby4KK8o35bL+4HUOYgbCUMOrXrs/crnOZ6jeVpnXKH8kqKj85JSVEdXIt3jws8MwWaNkLHv43NOtsDioKv8zSkCgORaaUK6goPTfdHHsavoor2VfwrufNG33fYFzbcbg6WXaNQ1Rt1m4YGtihlNLAJ1rrTwufn6eUmgmEAi9prdPKsO1tlFJPA08DeHl5VfgChKgStIZjX8KON8CYDyMXQ9/fkWOEbw5fYtn+SC4UBhX98SE/pvaxPKgoKj2KFeEr+O78d+QYc+jboi9v9X+Lga0GSuxpDaO01tZ7c6Vaaq3jlVJNgZ3As8AZIBlzQ3gbaKG1nm3JtlrrfSXtLyAgQIeGhlb4OoSo1FIjYcNzEB0C3oEw/l8kObfC8ONFgn+8SGpmHl1a1eOpQF+Lg4q01hxOPIwhzMC+2H04Ozgz1ncsM/xn0KFhBxssStiKUuqopdeIrXqEobWOL/z7ilJqPdDn1h/6SqklwCZLtwVKbBhC1CgmI/z4P9j9F3B0hnEfcbbVBJbujuLbn3ffDCqaG+jD/RYGFeUZ89gStQVDmIGzaWfxcPHgd91/x+SOk2ns2tgGixKVmdUahlLKDXDQWmcUfj0SWKSUaqG1Tih82aPAKUu3tVatQlQ5l8PMwwLjjqI7jOKnzn/iv0ez2ftVCC7ODkwK8GTOQMuDilKyU1h7di1rItaQkpNCuwbtWNR/EaN9R1Pb0fJkPVG9WfMIoxmwvvC3GidgpdZ6m1LKoJTqgfmUVDTwGzCfggKWaq1HF7etFWsVomooyIOQf0DIP9Au9Tjc8z0WRvoRcfISjeuWPajoXNo5gsOD2XRhE3mmPAJbBRLkH0TfFn1lrLi4i1WvYdiaXMMQ1VrsUfjuGUgK50yTUTyTNoXz12vTsZk7cwN9LA4qMmkTB+IOYAgzcCjhEC6OLoxrO44Z/jPwre9rg4WIyqTSXMMQQlSAvCz4YTH6x/9yzakRrxpfZltMTwZ1aMKbA30IbG9ZUFFOQQ4bIzcSHBZMZHokTVyb8FzP55jUYRINXO6eJyXEnaRhCFGJ6ci95H4zD5frl1hhHMY/8qYzomc7tpchqCgpK+lm7OnV3Kt08ujEXwf+lVHeo3B2lNhTYTlpGEJUQvmZacStfRnvi1+RYGrGYseF+AeOZnu/NjR1d7HoPSJSIzCEGdgStQWjyciQ1kMI8g8ioFmAXJ8Q5SINQ4hK5FpOPge3GOh98m1a6zRW13oUPWQBH9/X3qKgIpM2sTdmL4ZwA0cSj+Dq5MrkDpOZ3mk6XvXkxlZxb6RhCFEJxKRmsWbPMfxPLGa0OshFJ28ihyxhcv9hFgUVZeVn3Yw9vZRxieZuzXmp90tM6DCBerXq2WAFoiYotmEopZ4C9mitzynz8etnwETMH4V9Qmt9zDYlClF9/XwpjaX7IqkVvo43nJZTT+VwufdLtHloAW2cSv9obGJmIivDV7Lu3Doy8jLo1rgbz/Z8luFthkvsqahwJf0X9TzwReHXjwPdAB+gJ/AREGjVyoSopowmzc6wRJaGRBF38TzvunzOYOdj5LXojdOj/6FZ006lvscvSb9gCDOw4+IONJrhXsMJ8g+iR9MeNliBqKlKahgFWuv8wq/HAsu11inALqXUe9YvTYjqJTO3gK9CY/jsQDQxqdd5xj2E59yCcXbQMOxdavV5GkoIGyowFbD70m4MYQaOJx2nrnNdpneazrRO02hVt5UNVyJqqpIahkkp1QJIA4YBi2/5nswyFsJCiek5fHEw+mZQ0ZiWmaz3/JRGyUfAZzCM+wg8fIrd/nredXPsacRK4q7H0apuK16971Uebf+oxJ4KmyqpYbyJefy4I7BBa30aQCk1GIi0QW1CVGmn49NZGvJrUNHozk1Y0GA3nsc/AMfaMP7f0HMGFPMR19iMWFaEr2D9+fVk5mfSq2kvXg54mSGth0jsqbCLYhuG1nqTUqoN4H5HXkUoMMXqlQlRBZlMmj1nr7Bk369BRUH92vCbDlk03zMfzh+HjmNgzD+gXou7tteU9c4DAAAgAElEQVRa8/OVnzGEGdgdsxsHHBjpPZKZ/jPp3LizHVYkxK9K+pTUhFu+BvOwwGTguNY6w/qlCVF15OQb+eZY3N1BRb2aUf/Ih7DmA3BtCJO+AP9H7jqqyDflszN6J8vDlnM65TT1atXjyc5P8rjf4zRza2afRQlxh5JOSY0r4jkPoJtSao7WereVahKiyki+nsvyQ7cHFX00tYc5qCg+FL58HJLPQPfH4cG/Qh2P27ZPz01n3dl1rIpYxeWsy3jX8+ZP9/+JcW3HUcfZsuhUIWylpFNSTxb1fOFpqrXA/dYqSojK7tzlDJaGRLH+eBx5BSaGd2rK3EBfc1BRfhbseA0O/x/UawXT10H7EbdtH50eTXB4MBsubCC7IJv7m9/PG33fINAzUGJPRaVV5jt7tNYXlVIysUzUOFprDpxPYUlIJHvPJlHbyYFJvT2ZPdCHtjeCii78ABufg6uX4L6nYPhbUNv95vZHEo9gCDOwN3YvTg5OjPYZTZB/EB09OtpxZUJYpswNQynlB+RaoRYhKqXcAiMbTySwNCSSiMQMGtetzUsjOjC97y1BRdlpsONP8HMweLSFJ7dCm/6AOfZ0a9RWgsODiUiNoGHthvym+2+Y0nGKxJ6KKqWki94bMV/ovpUH0AKYYc2ihKgMrmblseLwJb48GM2VjFw6NnPnvce68fCdQUXhG2HzS5CZDANfgMGvgrMrqTmprD2zljVn1pCcnUzb+m1Z2G8hY3zH4OJk2cRZISqTko4w3r/jsQZSMTeNGcAhaxUlhD1FJWfy2f4o1h2NJTvfSGD7xrw/qfvdQUXXr8CWlyHsW2jeFaathZY9uHD1AoYwA5siN5FrzGVAqwEs7rSYfi37yVhxUaWVdNF7742vCzO4pwGTgSjga+uXJoTtaK05Ep3GkpBIdoVfxtnBgYd7tGRuYBFBRVrDidWwbQHkZ8EDb6D7P8fBy0cw7PwtB+IPUNuxtjn2tNMM2jZoa59FCVHBSjol1QGYinnwYAqwBnMG+FAb1SaE1eUbTWw9lcjSkEhOxqbTsI4z84a2I6i4oKKrMbDpD3B+F7S+n5wx/2DztbMYNk3mQvoFGrs2Zl6PeUzuOJmGLg1tvyAhrKikU1IRQAgwTmt9HkAp9YJNqhLCyq7l5LPmpxg+PxBFfHoOvo3d+MsjXZjYy7PooCKTCUKXwa6FoDXJI95itasza/c8Q1puGn4efiweuJhR3qOo5Vj6WHIhqqKSGsZEzEcYPyiltgGrATkBK6q0mNQsvjgYzZojMVzPLaCvrweLHu7CA35Niw8qSj4HG56FS4c449Of5a07sjUymAJTAYM9BzOz80yJPRU1QknXMNYD65VSbsAjwAtAM6XU/4D1WusdNqpRiHt2POYqS0Ii2fpLAg5KMbZbC+YM9KWrZ/3iNzLmw8GPMe15l5C6dTF0DeTw9Yu4JqQwsf1EZvjPoE29NrZbhBB2Vup9GFrrTGAFsEIp5QFMAhYA0jBEpWYOKrrM0pBIQi+m4e7ixFOBvszq703LBqVM6E84QdZ3z7AhM4oVbbyI1rk0NWXzQu8XmNh+IvVrl9BohKimynTjntY6Ffik8I8QlVJmbgHrjsby2YEoLqZk4dnQlTfH+jP5vtbUrV3Kf/L5OSTuXsiqM6tZ516Xa64edPFoz9/8gxjhPQJnBxlyIGouCf0V1UZieg5fHopm5eFLpGfn09OrAa+O8mOkfzOcHEufz3T61GqW//gOO5yMmOq7M6zVYIK6zaFHkx5yfUIIpGGIauB0fDrLQqLYUBhUNKpLc+YM9KV3m9I/1mo0GfkhciuGw+9xrCANNyd4vNUQpvVdgKe7pw2qF6LqkIYhqqQbQUVLQ6I4eCGFOoVBRU/298GrUeljwTPzM1l/bj3BJ5cQl5tKq/wCXm7QhQmj/k3dupI/IURRpGGIKiUn38j6n+NYGmIOKmperzCoqI8X9V1Lv74Qdz2OleEr+ebs11wvyKRnTg4vaXeGjvo/nLwH2GAFQlRdVm0YSqloIAMwAgVa6wCl1ELgKSCp8GWvaa23FLHtKOAjzJniS7XW71qzVlG5JV/PxVAYVJSSmUfnlvX4cEoPxnRrgXMp1ye01pxIOsHysOV8f+l7lIaROfkEpabQtc88GPQyOMswQCFKY4sjjKFa6+Q7nvtAa33ncMOblFKOwH+AEUAscEQptUFrHWbFOkUldO5yBsv2R/HNz78GFc0Z6EtfX49SL0Tnm/LZdXEXhjADvyT/grtzXWY5NGJa1HGaN+kMT6yGFt1stBIhqr7KekqqD3Beax0JoJRaDTwMSMOoAW4EFS3dH8meM8UEFZXgWt41vj77NSsjVpKYmYiXuxevtRzBw8fWUScvB4a+Af2eBcfK+p+/EJWTtf+P0cAOpZQGPtFaf1r4/Dyl1EwgFHhJa512x3atgJhbHscikbDVXl6BiQ0n4ksOKirBpWuXCA4P5tvz35JdkE2f5n14vfNTDPppBQ4nl4FXPxj/MTRub4PVCFH9WLthDNBaxyulmgI7lVIRwP+AtzE3k7eBfwCz79iuqHMNd4Y5mV+o1NPA0wBeXl4VVbewoeKCisZ3b4mLcxGDAG+htSb0ciiGMAN7Yvbg6OBojj31m4bf+RD45g+gFIx+HwLmgIPkZQtRXlZtGFrr+MK/ryil1gN9tNb7bnxfKbUE2FTEprFA61seewLxxezjU+BTgICAgCKbiqicopMz+exAFF+F/hpU9PdJ3Rl0Z1BREfKN+WyL3oYhzEB4ajgNajfgqW5PMbXjVJpkpsJ3z0LMYWg3HMZ+CA1al/h+QojSWa1hFA4tdNBaZxR+PRJYpJRqobVOKHzZo8CpIjY/ArRXSvkAcZin5k6zVq3Cdm4EFS0NiWTnLUFFcwJ98Gter9Tt03LS+OrsV6yOWE1SdhK+9X15s9+bjPMdh4tyhAMfwt73oJYbPPoJdJtiPsIQQtwzax5hNMM87fbGflZqrbcppQyFCX4aiAZ+A6CUaon547OjtdYFSql5wHbMH6v9TGt92oq1CisrMJrYciqRZSGRnIhNp0FpQUV3iLwaiSHcwMYLG8k15tK/ZX8WDVhE/5b9cVAOEH8cvpsHl3+Bzo/CQ+9B3aY2WJkQNYfSuvqcxQkICNChoaH2LkPc4kZQ0RcHo4m7mo1vYzdmD/QpPqjoFlprDiUcwhBmYH/cfmo51GJs27HM6DSD9g0LL1znZ8Oed+Hgx+DWGMb8EzqNtcHKhKgelFJHtdYBlrxWPlcorCI2LYvPD/waVHS/jwd/Ht+55KCiQrnGXDZHbsYQZuD81fM0cmnE73v8nskdJtPItdGvL4w+YA42Sr0APYNg5NvgKrGoQliLNAxRoW4EFW07lQjA2G4tmFtaUFGh5Oxk1p5Zy5oza0jNSaVDww68PeBtRvuMvj32NOcafP9nOLIUGnhB0LfQVqLmhbA2aRjint0IKlq2P5Ij0eagorkDfSwLKgLOpp3FEGZgc+Rm8k35DPYcTJB/EH2a97n701LndsLGP8C1OOj7e3jgT+YL3EIIq5OGIcotK6+Ar0LLF1Rk0ib2x+1nedhyDiccxsXRhQntJzC903R86vsUsbNU2PZHOLkamvjBnJ3Q+j4rrUwIURRpGKLMLl/L4YuD5Qsqyi7IZuOFjRjCDERfi6apa1Oe7/U8kzpMKjr2VGs4vR62vAw5V2HQKzBoPjjVttLqhBDFkYYhLHYjqGjjyXiMJs2DnZszN9CyoKLLmZdZfWY1X539ivTcdPwb+fNO4Ds82OZBnB2LGUt+LQE2vwRnNkOLHjDzO2jepYJXJYSwlDQMUSKTSbP3bBJLQiJvBhVNv78NswdYFlR0OuU0hjAD26O2Y9RGHvB6gCD/IHo17VX83dxaw88G2P4nMObCiEXQ9xkZFiiEncn/gaJIN4KKlu2P4vyV6zSv58KCh/x43IKgIqPJyJ7YPRjCDBy9fJQ6TnWY6jeVaZ2m0dq9lBEdqVGw8TmI2gdtBpiHBTZqW4ErE0KUlzQMcZvigopGd21BLaeSr09k5mfy7flvWRG+gpiMGFq4tWB+wHwmtJ+Aey33kndsMsLhT2D326AcYewH0OsJGRYoRCUiDUMAcP5KBktDfg0qGubXlLmBlgUVxV+PN8eenvuGjPwMujfpzvO9nmeY1zCcHCz4T+xKuHmsR1wotH/Q3Czqt6qglQkhKoo0jBpMa83BCyksCfk1qOix3p7MsTCo6ETSCQxhBnZd3AXAiDYjCPIPolsTC1PsCvJ+HRZY2x0mLIWuj8mwQCEqKWkYNVBegYmNJ+JZuj+K8IRrNK5bixdHdGCGBUFFBaYCdl0yx56eTDqJu7M7Qf5BTPObRou6LSwvIu6oeQT5ldPQZaJ5WKBb43tcmRDCmqRh1CB3BhV1aFaX9yZ2Y3yP0oOKMvIy+ObcN6wIX0FCZgKt3VuzoM8CHmn3CG7OZbjTOi8L9vwVDv0H6jaDqavAb/Q9rkwIYQvSMGqAewkqirkWw4qIFaw/t56sgix6N+vNgj4LGOw5GEeHkpvMXaJCzJ+ASo2E3k+YPy7rUvqMKSFE5SANo5rSWhN6MY0l+8xBRU4Oiod7tGKuBUFFWmuOXTnG8tPL+SHmBxyVI6N8RhHkH4R/I/+yF5OTDjvfgqOfQ0MfmLURfAaVc2VCCHuRhlHNFBhNbD2VyNI7g4r6tqFpvZKDivKN+Wy/uB1DmIGwlDDq167P3K5zmeo3laZ1yhlGdHa7eVjg9UToNw+Gvg61Sr/hTwhR+UjDqCYycvJZcySGzw+Yg4p8Grvx9iNdeMyCoKL03HS+OvsVq8JXcSX7Ct71vHmj7xuMazsOV6fSp80WKTMZti2AX76Cpv4wJRg8e5fvvYQQlYI0jCou7mo2n++PYvUtQUULx3dmmAVBRVHpUQSHBbPhwgZyjDn0bdGXt/q/xcBWA82xp+WhNZz6Gra+Ys6tGPJHGPgiOJX86SshROUnDaOKOlEYVLT1lqCiOQN96ObZoMTttNYcTjyMIczAvth9ODs4M9Z3LDP8Z9ChYYd7Kyo9Dja/CGe3QaveMP7f0Kwc1zyEEJWSNIwqxGjS7Aq/zNKQwqCi2k7MGejDExYEFeUZ89gcuZng8GDOpp3Fw8WD33X/HZM7Tqax6z3e/2AywbEvYeebYMyHkYuh7++grJ+iEkJUatIwqoCsvALWHY3ls/1RRKdk0aqBK2+M9WeKBUFFKdkprD27ltURq0nNSaVdg3Ys6r+I0b6jqe1YAZkSKRdg4/MQHQLegTD+X+Dhe+/vK4SodKRhVGKXr+Xw5cFoVhQGFfVo3YD/POjHg51LDyo6l3aO4PBgNl3YRJ4pj8BWgQT5B9G3Rd9S772wiMkIP/4Xdi8GR2cY9y/oNVPGeghRjUnDqITC4q+xdH8kG0/cGlTkQ+82HiVuZ9ImDsQdwBBm4FDCIVwcXXi43cPM8J+Bb/0K/K3/8mnzsMD4Y9DhIRj7T6jXsuLeXwhRKUnDqCRMJs3ec0ksDYnkwPmyBRXdiD0NDg8mKj2KJq5NeK7nc0zqMIkGLiVfBC+TglwI+Yf5j0sDeOwz6DxBjiqEqCGkYdhZTr6Rb3+OY+mdQUX3eVG/TslBRUlZSayKWMVXZ7/iau5VOnl04q8D/8oo71HFx56WV2yo+agiKRy6ToZR74Jbo4rdhxCiUpOGYSfJ13MJ/vEihkPmoCL/FvX4YEp3xnRtWWpQUXhKOIYwA1ujt2I0GRnSeghB/kEENAuomOsTt8rLNF+n+PG/5tNO09ZChwcrdh9CiCpBGoaNnb+SwbL9UXx97NegojmBPvTzbVTiD3uTNrE3Zi+GcANHEo/g6uTK5A6Tmd5pOl71vKxTbORe87DAtGgImAPDF4JLyXOohBDVlzQMG7gRVLQ0JJIfbgkqmj3Ah3ZNSw4qysrPuhl7einjEs3dmvNi7xeZ2GEi9WpZ6Yd39lXY+QYcW27+iOwTm8F7oHX2JYSoMqRhWFFegYlNJ+NZGhJF2C1BRdPv96JR3ZLvgUjMTGRl+ErWnVtHRl4G3Rp349mezzKszTCcHSr4+sStIjbDphch8woMeN482sO5nPOkhBDVijQMK7ialcfKn8xBRZevlS2o6JekX1getpydF3ei0Qz3Gk6QfxA9mvawbtHXk8zzn05/A007w+OroFUv6+5TCFGlWLVhKKWigQzACBRorQNu+d584O9AE611chHbGoFfCh9e0lqPt2atFSE6OZPPD0Sx9pagovceKz2oqMBUwO5LuzGEGTiedJy6znWZ3mk60zpNo1XdVtYtWms4uRa2vWq+wD30T+YjCxkWKIS4gy2OMIbe2RCUUq2BEcClErbL1lpb+dfqe3cjqGhpSCQ7wn4NKpoz0IdOLUq+xnAj9nRl+EriM+NpVbcVr973Ko+2f7RssafllR4Lm16AczvA8z7zsMCmftbfrxCiSrLXKakPgFeA7+y0/3t2M6hofxQnYq7SoI4zzwxpx8x+pQcVxWbEsiJ8BevPryczP5NeTXvxyn2vMKT1kLLHnpaHyQRHP4OdC0EbzfdU9HlahgUKIUpk7YahgR1KKQ18orX+VCk1HojTWp8o5Z4BF6VUKFAAvKu1/tbKtVqkvEFFWmt+vvIzhjADu2N244ADI71HMtN/Jp0bd7bdApLPmz8qe/EA+A6BcR9BQ2/b7V8IUWVZu2EM0FrHK6WaAjuVUhHA68BIC7b1KtzWF9itlPpFa33hzhcppZ4Gngbw8rLS/QiYg4q+OBDF6p9iyMgtoI+FQUX5pnx2RO/AEGbgdMpp6tWqx5Odn2Sq31SauzW3Wr13MRbAoX/DnnfAsbb59FPPGTLWQwhhMaW1ts2OlFqI+eL3s0BW4dOeQDzQR2udWMK2XwCbtNbrStpHQECADg0NrZB6bzgRc5Wl+6PY8ksCAGO6tmBuYOlBRem56aw7u46VESu5kmWOPZ3RaQbj2o6jjrONM60Tf4HvnoGEE+A3Fka/D/Va2LYGIUSlpJQ6eusHkkpitSMMpZQb4KC1zij8eiSwSGvd9JbXRAMBRVwUbwhkaa1zlVKNgQHAe9aq9U43goqWhUTxU3RqmYKKotOjCQ43x55mF2Rzf/P7ebPvmwR6BpY/9rS8CnJh399h/wfg2hAmfQn+D8tRhRCiXKx5SqoZsL7wOoUTsFJrva24FyulAoDfaq3nAp2AT5RSJsAB8zWMMCvWChQfVDQ5wBN3l+JvltNacyTxCMvDlrMvdh9ODk6M9hlNkH8QHT06WrvsosX8ZB4WmHwGuj8OD/4V6pQ8Hl0IIUpis1NStlDeU1JFBRU9FehbalBRnjGPrVFbMYQZOJN2hoa1GzLFbwpTOk6599jT8sq9Drv/Aof/D+p7wtgPof1w+9QihKj0KsUpqaoiIyefoe/vITvfyIP+zXlqUOlBRak5qaw9s5Y1Z9aQnJ1M2/ptWdhvIWN8x+DiVPJHaq3qwm5zXOrVS3DfUzD8Lajtbr96hBDVSo1vGO4uzvzlkS70btOQNo1KvlnufNp5c+xp5CZyjbkMaDWAxZ0W069lv4ofK14W2Wmw/U9wPBgatYMnt0Kb/varRwhRLdX4hgEwoZdnsd/TWnMw/iCGMAMH4g9Q27E2Y33HEuQfRNsGbW1YZTHCN8LmlyAzGQa+AIMXgLMdj3KEENWWNIxi5BTksClyE8FhwVxIv0Bj18bM6zGPSR0n4eFSCS4eZ1yGrS9D2HfQvKs52KhlpZ+kIoSowqRh3CE5O5nVEatZe2Ytablp+Hn4sXjgYkZ5j6KWYyUYyKc1nFgN2xZAfjYMexP6PwcVHckqhBB3kIZR6EzqGZaHLWdr1FYKTAUM9hzMzM4zrRN7Wl5XL8HGP8CF76H1/ea7tZt0sHdVQogaosY3jOt513n+h+f5KfEnXJ1cmdh+IjP8Z9CmXht7l/YrkwmOLIVdC82PH/o73DcXHGx8I6AQokar8Q3DzdkN91ruvND7BSa2n0j92vXtXdLtks+Zb8CL+RHaPmC+r6JhJWpmQogao8Y3DKUUHw790N5l3M2YDwf/BXv+Zo5IfeR/5ju2K8vpMSFEjVPjG0allHDCfFSReNI8++mhv4N7M3tXJYSo4aRhVCb5ObD3b3DgI6jTCCYbwL/SJ9MKIWoIaRiVxcVDsOFZSDkHPWbAg38xT5gVQohKQhqGveVmwK4/w5ElUN8LZnwD7YbZuyohhLiLNAx7Or/LfF9Feizc/1t44A2oXdfeVQkhRJGkYdhDVipsfw1OrILGHWD2NvDqa++qhBCiRNIwbO30t7BlvnnCbOB8GPSyDAsUQlQJ0jBsJSPRPFU2YhO06G6+VtGim72rEkIIi0nDsDat4fgK8ymo/BwYvhD6PQuO8o9eCFG1yE8ta0qLNifgRe4Br/4w/mNo3M7eVQkhRLlIw7AGkxF+WgLf/xmUA4x+HwLmyLBAIUSVJg2joiWdMY/1iP0J2o2AsR9Ag9b2rkoIIe6ZNIyKYsyHAx/C3veglhs8+il0myzDAoUQ1YY0jIoQ/7P5qOLyKej8qHlYYN0m9q5KCCEqlDSMe5GfDXvegYMfg1tTmLICOo21d1VCCGEV0jDKK/qAeVhg6gXoGQQj/wKuDexdlRBCWI00jLLKuWaOSg1dBg3awMzvwHeInYsSQgjrk4ZRFmd3wKYX4Foc9H0GHnjdfIFbCCFqAGkYlshMge1/hJNroIkfzNkJre+zd1VCCGFT0jBKojWc/ga2vAI5V2HwqxD4EjjVtndlQghhc1a99VgpFa2U+kUpdVwpFXrH9+YrpbRSqnEx285SSp0r/DPLmnUW6VoCrJ4G62abb7x7ei8MfU2ahRCixrLFEcZQrXXyrU8opVoDI4BLRW2glPIA3gICAA0cVUpt0FqnWbtYtIZjy2HHG2DMhRFvQ9/fy7BAIUSNZ6+fgh8ArwDfFfP9B4GdWutUAKXUTmAUsMqqVaVGwcbnIGoftBkI4/8FjdpadZdCCFFVWLthaGCHUkoDn2itP1VKjQfitNYnVPFjM1oBMbc8ji18zjpMRjj8f/D92+DgZJ7/1OsJGRYohBC3sHbDGKC1jldKNQV2KqUigNeBkaVsV1Qn0UW+UKmngacBvLy8yl5hdhoEPwZxodD+QXOzqG+93iSEEFWVVX+F1lrHF/59BVgPDAZ8gBNKqWjAEzimlGp+x6axwK0jXj2B+GL28anWOkBrHdCkSTnmN7k0AA8fmLAUpq2RZiGEEMWw2hGGUsoNcNBaZxR+PRJYpLVuestrooGAOy+KA9uBvyqlGhY+Hgn80UqFwsSlVnlrIYSoTqx5SqoZsL7wOoUTsFJrva24FyulAoDfaq3naq1TlVJvA0cKv73oxgVwIYQQ9qG0LvLSQJUUEBCgQ0NDS3+hEEIIAJRSR7XWAZa8Vj4GJIQQwiLSMIQQQlhEGoYQQgiLSMMQQghhEWkYQgghLCINQwghhEWq1cdqlVJJwMVybt4YuPMGwupO1lz91bT1gqy5rNporS0ak1GtGsa9UEqFWvpZ5OpC1lz91bT1gqzZmuSUlBBCCItIwxBCCGERaRi/+tTeBdiBrLn6q2nrBVmz1cg1DCGEEBaRIwwhhBAWqZENQyn1mVLqilLq1C3PeSildiqlzhX+3bCk96hKilnvJKXUaaWUqXC0fLVSzJr/rpSKUEqdVEqtV0o1sGeNFa2YNb9duN7jSqkdSqmW9qyxohW15lu+N18ppZVSje1Rm7UU8+95oVIqrvDf83Gl1Ghr7LtGNgzgC2DUHc8tAL7XWrcHvi98XF18wd3rPQVMAPbZvBrb+IK717wT6KK17gacxVqhXPbzBXev+e9a625a6x7AJuBNm1dlXV9w95pRSrUGRgCXbF2QDXxBEWsGPtBa9yj8s8UaO66RDUNrvQ+4M5DpYeDLwq+/BB6xaVFWVNR6tdbhWuszdirJ6opZ8w6tdUHhwx8xR/9WG8Ws+dotD92AanXRspj/lwE+AF6hmq0XSlyz1dXIhlGMZlrrBIDCv5uW8npRtc0Gttq7CFtQSi1WSsUA06l+Rxh3UUqNB+K01ifsXYuNzSs8/fiZtU6pS8MQNY5S6nWgAFhh71psQWv9uta6Neb1zrN3PdaklKoDvE4NaIx3+B/QFugBJAD/sMZOpGH86rJSqgVA4d9X7FyPsAKl1CxgLDBd17zPlK8EJtq7CCtrC/gAJ5RS0ZhPOx5TSjW3a1VWprW+rLU2aq1NwBKgjzX2Iw3jVxuAWYVfzwK+s2MtwgqUUqOAV4HxWusse9djC0qp9rc8HA9E2KsWW9Ba/6K1bqq19tZaewOxQC+tdaKdS7OqG7/sFnoU84daKn4/Ne+XLFBKrQKGYJ7weBl4C/gWWAt4Yf5kxSSttV0uLFW0YtabCnwMNAGuAse11g/aq8aKVsya/wjUBlIKX/aj1vq3dinQCopZ82igI2DCPMn5t1rrOHvVWNGKWrPWetkt348GArTW1WZ6bTH/nodgPh2lgWjgNzeuyVbovmtiwxBCCFF2ckpKCCGERaRhCCGEsIg0DCGEEBaRhiGEEMIi0jCEEEJYRBqGEPegcBqq4ZbHTkqpJKXUpsLHzZRSm5RSJ5RSYUqpLYXPeyulsm+ZLnpcKTXTXusQwhJO9i5AiCouE+iilHLVWmdjnpB6630Oi4CdWuuPAJT6//buWBeCMIri+DmdxgPoxLayjZpCJ16AwpPo1BpCx5aoFUSlli3IqnVqTyA5ipliTBQfMyZZ/r9qMneL2535brLf9bhRe6lvkQXmAicMoLtbSdv1866ky0ZtSdW/jSVJSWYD9gX0isAAuruStGN7QdJY0kOjdirp3Pa97f3WAqNRayS1PmTTwHcxkgI6SjKzvazqdHHTqt3ZXlG18GZL0qPt1brMSApzhRMG0I9rSYf6PI6SJCV5S3KRZE/SVNLG0M0BfSAwgH5MJGwaIycAAABtSURBVB0keW6+tL1Z72iQ7UVV12//xbWh+AcYSQE9SPIq6eiL0pqkE9vvqj7QzpJM6xHWyPZT47eTJMe/3izwQ9xWCwAowkgKAFCEwAAAFCEwAABFCAwAQBECAwBQhMAAABQhMAAARQgMAECRD4+Ttv8vR6zPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8106579784185599"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.polynomial.polynomial import polyfit  \n",
    "from scipy.stats import pearsonr\n",
    "from pylab import text\n",
    "\n",
    "\n",
    "print(pearsonr(VIO,AUS)[0])\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(VIO,AUS, 1)\n",
    "ax.plot(VIO,AUS, '.')\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(VIO,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "plt.plot(VIO, b + m * np.array(VIO), '-')\n",
    "ax.set_xlabel(\"Proposed metric\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "fig.savefig('Ex4VIOVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "METRIC = -(VIO/np.max(VIO)) + np.array(MSE)\n",
    "print(pearsonr(METRIC,AUS)[0])\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(METRIC,AUS, 1)\n",
    "ax.plot(METRIC,AUS, '.')\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(METRIC,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "plt.plot(METRIC, b + m * np.array(METRIC), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"Proposed metric\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "fig.savefig('Ex4ProposedVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(MSE,AUS, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(MSE,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(MSE,AUS, '.')\n",
    "plt.plot(MSE, b + m * np.array(MSE), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "MSE = np.array(MSE)\n",
    "\n",
    "x = []\n",
    "y1 = []\n",
    "y2 = []\n",
    "y3 = []\n",
    "for split in range(10, len(AUS), 5):\n",
    "    #print(\"******\", split, \"*******\")\n",
    "    sorted_aus = [AUS for _,AUS in sorted(zip(VIO,AUS))]\n",
    "    sorted_mse = [MSE for _,MSE in sorted(zip(VIO,MSE))]\n",
    "\n",
    "    low = []\n",
    "    high = []\n",
    "    low = sorted_aus[:split]\n",
    "    high = sorted_aus[split:]\n",
    "\n",
    "    x.append(split)\n",
    "    \n",
    "    \n",
    "    #print(\"Low Violations = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High Violations = \", np.mean(high), \"for\", len(high))\n",
    "    y1.append(np.mean(low)) \n",
    "    sorted_aus_by_mse = [AUS for _,AUS in sorted(zip(MSE,AUS))]\n",
    "    low = sorted_aus_by_mse[:split]\n",
    "    high = sorted_aus_by_mse[split:]\n",
    "    #print(\"Low AUS by MSE = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High AUS by MSE = \", np.mean(high), \"for\", len(high))\n",
    "    y2.append(np.mean(low))\n",
    "    sorted_aus = [AUS for _,AUS in sorted(zip(METRIC,AUS))]\n",
    "    sorted_mse = [MSE for _,MSE in sorted(zip(METRIC,MSE))]\n",
    "\n",
    "    low = []\n",
    "    high = []\n",
    "    low = sorted_aus[:split]\n",
    "    high = sorted_aus[split:]\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"Low Metric = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High Metric = \", np.mean(high), \"for\", len(high))\n",
    "    y3.append(np.mean(low))\n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x,y1, '-', label = 'BIC')\n",
    "ax.plot(x,y2, '-', label = 'MSE')\n",
    "ax.plot(x,y3, '-', label = 'METRIC')\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "plt.show()  \n",
    "pearsonr(METRIC,AUS)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
