{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes \n",
    "### Required installing Oracle JAVA 8 to get javabridge installed\n",
    "### Then, I was able to install py-causal from https://bd2kccd.github.io/docs/py-causal/\n",
    "### GFCI is slower than RFCI, but more accurate (SPIRTES), GFCI and RFCI account for unobserved variables, FGES assumes no unobserved variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure Learning Performance Guarantees If the assumptions in the previous section hold, then in the large sample limit, the CBN structure output by GFCId will contain an edge of one of four kinds between Xand Y   if and only if Xand Yare not independent conditional on any subset of the other measured variables of less than or equal to a specified size. In addition, there is (1) an arc X->Y   if and only if Xdirectly or indirectly causes Y, and Y   does not directly or indirectly cause X; (2) an edge X <-->Y   if and only if X   is not a direct or indirect cause of Yand Y   is not a direct or indirect cause of X(which can only occur if there are latent confounders of Xand some other variable or Yand some other variable; (3) an edge Xo->Y   only if Yis not a direct or indirect cause of X, but Xmay or may not be an indirect cause of Y; (4) an edge X oâ€“o Y   indicates that Xand Y   are dependent no matter what subset of observed variables is conditioned on, but contains no orientation information (X   may be a direct or indirect cause of Y, and Ymay be an indirect cause of X, or there may be a latent common cause of Xand Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying some various ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import configparser\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, \\\n",
    "                        Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "import keras.optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "import glob, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "# select your GPU Here\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #Comment this line out if you want all GPUS (2 hehe)\n",
    "\n",
    "# python full-display web browser\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "def get_model(dense, dropouts, inputs):\n",
    "    # dense is an ordered list of the number of dense neurons like [1024, 2048, 1024]\n",
    "    # dropouts is an ordered list of the dropout masks like [0.2, 0.3, 0.4]\n",
    "    inputs = keras.Input(shape = (inputs,))\n",
    "\n",
    "    x = keras.layers.Dense(dense[0], activation = 'relu')(inputs)\n",
    "    x = keras.layers.Dropout(dropouts[0])(x, training=True)\n",
    "    for den, drop in zip(dense[1:], dropouts[1:]):\n",
    "        x = keras.layers.Dense(den, activation = 'relu')(x)\n",
    "        x = keras.layers.Dropout(drop)(x, training=True)\n",
    "    outputs = keras.layers.Dense(1, activation = 'linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512]] ['temp/f0', 'temp/f1', 'temp/f2', 'temp/f3', 'temp/f4', 'temp/f5', 'temp/f6', 'temp/f7', 'temp/f8', 'temp/f9', 'temp/f10', 'temp/f11', 'temp/f12', 'temp/f13', 'temp/f14', 'temp/f15', 'temp/f16', 'temp/f17', 'temp/f18', 'temp/f19', 'temp/f20', 'temp/f21', 'temp/f22', 'temp/f23', 'temp/f24', 'temp/f25', 'temp/f26', 'temp/f27', 'temp/f28', 'temp/f29', 'temp/f30', 'temp/f31', 'temp/f32', 'temp/f33', 'temp/f34', 'temp/f35', 'temp/f36', 'temp/f37', 'temp/f38', 'temp/f39', 'temp/f40', 'temp/f41', 'temp/f42', 'temp/f43', 'temp/f44', 'temp/f45', 'temp/f46', 'temp/f47', 'temp/f48', 'temp/f49']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from pycausal import search as s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def discrete_gauss(low, high, samples, std = 20):\n",
    "    x = np.arange(low, high)\n",
    "    xU, xL = x + 0.5, x - 0.5 \n",
    "    prob = ss.norm.cdf(xU, scale = std) - ss.norm.cdf(xL, scale = std)\n",
    "    prob = prob / prob.sum() #normalize the probabilities so their sum is 1\n",
    "    nums = np.random.choice(x, size = samples, p = prob)\n",
    "    return nums\n",
    "\n",
    "\n",
    "\n",
    "def bar_plot(x_ax, val1, val1std, val2, val2std):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ## the data\n",
    "    N = len(x_ax)\n",
    "\n",
    "    ## necessary variables\n",
    "    ind = np.arange(N)                # the x locations for the groups\n",
    "    width = 0.35                      # the width of the bars\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    ## the bars\n",
    "    rects1 = ax.bar(ind, val1, width,\n",
    "                    color='gray',\n",
    "                    yerr=val1std,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='blue'))\n",
    "\n",
    "    rects2 = ax.bar(ind+width, val2, width,\n",
    "                        color='blue',\n",
    "                        #yerr=val2std,\n",
    "                        error_kw=dict(elinewidth=2,ecolor='gray'))\n",
    "\n",
    "    # axes and labels\n",
    "    ax.set_xlim(-width,len(ind)+width)\n",
    "    #ax.set_ylim(0,45)\n",
    "    ax.set_ylabel('Percentage')\n",
    "    ax.set_title('')\n",
    "    plt.xticks(ind + width / 2, x_ax, rotation=75, size = 14)\n",
    "    ## add a legend\n",
    "    ax.legend( (rects1[0], rects2[0]), ('Accuracy', '% Violations') )\n",
    "    fig.savefig(\"violations.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 5000):\n",
    "    a = np.random.gumbel(mean, var, SIZE)\n",
    "    b = np.random.gumbel(mean, var, SIZE)\n",
    "    c = np.random.gumbel(mean, var, SIZE)\n",
    "    d = np.random.gumbel(mean, var, SIZE)\n",
    "    e = np.random.gumbel(mean, var, SIZE)\n",
    "    f= a + b + c + d + e + np.random.gumbel(mean, var, SIZE)\n",
    "    g = f + np.random.gumbel(mean,var, SIZE)\n",
    "    g = np.rint(g)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 5000):\n",
    "    a = np.random.gumbel(mean, var, SIZE)\n",
    "    b = np.random.gumbel(mean, var, SIZE)\n",
    "    c = np.random.gumbel(mean, var, SIZE)\n",
    "    d = np.random.gumbel(mean, var, SIZE)\n",
    "\n",
    "    f= a + b + c + d + np.random.gumbel(mean, var, SIZE)\n",
    "    g = f + np.random.gumbel(mean,var, SIZE)\n",
    "    \n",
    "    \n",
    "    g = np.rint(g)\n",
    "    e = g + np.random.gumbel(mean,var,SIZE)\n",
    "    \n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 400000):\n",
    "    f = np.random.normal(mean, var, SIZE)\n",
    "    a = f + np.random.normal(mean, var, SIZE)\n",
    "    b = f + np.random.normal(mean, var, SIZE)\n",
    "    c = f + np.random.normal(mean, var, SIZE)\n",
    "    d = f + np.random.normal(mean, var, SIZE)\n",
    "    e = f + np.random.normal(mean, var, SIZE)\n",
    "    g = a + b + c + d  + e + np.random.normal(mean, var, SIZE)\n",
    "\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 20000):\n",
    "    a = np.random.normal(mean, var, SIZE)\n",
    "    b = np.random.normal(mean, var, SIZE)\n",
    "    c = np.random.normal(mean, var, SIZE)\n",
    "    d = np.random.normal(mean, var, SIZE)\n",
    "    e = np.random.normal(mean, var, SIZE)\n",
    "    f= a + b + c + d + e + np.random.normal(mean, var, SIZE)\n",
    "    g = f + np.random.normal(mean,var, SIZE)\n",
    "    #g = np.rint(g)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 40000):\n",
    "    a = np.random.normal(mean, var, SIZE)\n",
    "    b = np.random.normal(mean, var, SIZE)\n",
    "    c = np.random.normal(mean, var, SIZE)\n",
    "    g = a + b + c + np.random.normal(mean,var, SIZE)\n",
    "    d = g + np.random.normal(mean, var, SIZE)\n",
    "    e = g + np.random.normal(mean, var, SIZE)\n",
    "    f = g + np.random.normal(mean, var, SIZE)\n",
    "    \n",
    "    #g = np.rint(g)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def get_CG(df, tetrad):\n",
    "    tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "    #tetrad.run(algoId = 'fges-mb', targetName = 'g', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "    #       structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "    #       completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "def get_MB(graph, var, pc):\n",
    "    parents = set()\n",
    "    children = set()\n",
    "    for i in pc.extractTetradGraphEdges(graph):\n",
    "        if i[-1] == var and i[3:5] == '->':\n",
    "            parents.add(i[0])\n",
    "        if i[0] == var and i[3:5] == '->':\n",
    "            children.add(i[-1])\n",
    "    return parents, children\n",
    "\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from collections import defaultdict\n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '5000M')\n",
    "tetrad = s.tetradrunner()\n",
    "\n",
    "\n",
    "verbosity = 1\n",
    "\n",
    "\n",
    "\n",
    "models = []\n",
    "model_names = []\n",
    "\n",
    "num_models =50\n",
    "model_layers = [1024, 512]\n",
    "for i in range(num_models):\n",
    "    models.append(model_layers)\n",
    "    model_names.append('temp/f' + str(i))\n",
    "\n",
    "print(models, model_names)\n",
    "\n",
    "\n",
    "df = gen_data()\n",
    "X = df[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "y = df['g'].values\n",
    "\n",
    "val_df = gen_data(SIZE = 2000)\n",
    "x_val = df[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "y_val = df['g'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp/f0\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 137us/step - loss: 0.2953 - mean_squared_error: 0.2953 - val_loss: 0.2681 - val_mean_squared_error: 0.2681\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26806, saving model to temp/f0\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 0.2685 - mean_squared_error: 0.2685 - val_loss: 0.2655 - val_mean_squared_error: 0.2655\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26806 to 0.26548, saving model to temp/f0\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 0.2657 - mean_squared_error: 0.2657 - val_loss: 0.2627 - val_mean_squared_error: 0.2627\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26548 to 0.26270, saving model to temp/f0\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2630 - mean_squared_error: 0.2630 - val_loss: 0.2691 - val_mean_squared_error: 0.2691\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26270\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 0.2635 - mean_squared_error: 0.2635 - val_loss: 0.2635 - val_mean_squared_error: 0.2635\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26270\n",
      "Epoch 00005: early stopping\n",
      "temp/f1\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2976 - mean_squared_error: 0.2976 - val_loss: 0.2689 - val_mean_squared_error: 0.2689\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26886, saving model to temp/f1\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 118us/step - loss: 0.2674 - mean_squared_error: 0.2674 - val_loss: 0.2644 - val_mean_squared_error: 0.2644\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26886 to 0.26436, saving model to temp/f1\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 0.2645 - mean_squared_error: 0.2645 - val_loss: 0.2623 - val_mean_squared_error: 0.2623\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26436 to 0.26233, saving model to temp/f1\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 118us/step - loss: 0.2642 - mean_squared_error: 0.2642 - val_loss: 0.2646 - val_mean_squared_error: 0.2646\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26233\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 0.2629 - mean_squared_error: 0.2629 - val_loss: 0.2608 - val_mean_squared_error: 0.2608\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.26233 to 0.26079, saving model to temp/f1\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 0.2621 - mean_squared_error: 0.2621 - val_loss: 0.2608 - val_mean_squared_error: 0.2608\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.26079\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 0.2618 - mean_squared_error: 0.2618 - val_loss: 0.2596 - val_mean_squared_error: 0.2596\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.26079 to 0.25964, saving model to temp/f1\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 0.2619 - mean_squared_error: 0.2619 - val_loss: 0.2613 - val_mean_squared_error: 0.2613\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.25964\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 0.2607 - mean_squared_error: 0.2607 - val_loss: 0.2589 - val_mean_squared_error: 0.2589\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.25964 to 0.25891, saving model to temp/f1\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 0.2599 - mean_squared_error: 0.2599 - val_loss: 0.2590 - val_mean_squared_error: 0.2590\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.25891\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 0.2602 - mean_squared_error: 0.2602 - val_loss: 0.2579 - val_mean_squared_error: 0.2579\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.25891 to 0.25789, saving model to temp/f1\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 0.2599 - mean_squared_error: 0.2599 - val_loss: 0.2595 - val_mean_squared_error: 0.2595\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.25789\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2594 - mean_squared_error: 0.2594 - val_loss: 0.2604 - val_mean_squared_error: 0.2604\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.25789\n",
      "Epoch 00013: early stopping\n",
      "temp/f2\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 126us/step - loss: 0.2995 - mean_squared_error: 0.2995 - val_loss: 0.2672 - val_mean_squared_error: 0.2672\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26723, saving model to temp/f2\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 0.2674 - mean_squared_error: 0.2674 - val_loss: 0.2711 - val_mean_squared_error: 0.2711\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.26723\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2653 - mean_squared_error: 0.2653 - val_loss: 0.2622 - val_mean_squared_error: 0.2622\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26723 to 0.26223, saving model to temp/f2\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 0.2656 - mean_squared_error: 0.2656 - val_loss: 0.2640 - val_mean_squared_error: 0.2640\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26223\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 0.2629 - mean_squared_error: 0.2629 - val_loss: 0.2607 - val_mean_squared_error: 0.2607\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.26223 to 0.26068, saving model to temp/f2\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2606 - mean_squared_error: 0.2606 - val_loss: 0.2610 - val_mean_squared_error: 0.2610\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.26068\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 0.2614 - mean_squared_error: 0.2614 - val_loss: 0.2612 - val_mean_squared_error: 0.2612\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.26068\n",
      "Epoch 00007: early stopping\n",
      "temp/f3\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2989 - mean_squared_error: 0.2989 - val_loss: 0.2688 - val_mean_squared_error: 0.2688\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26879, saving model to temp/f3\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 0.2663 - mean_squared_error: 0.2663 - val_loss: 0.2646 - val_mean_squared_error: 0.2646\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26879 to 0.26455, saving model to temp/f3\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 0.2657 - mean_squared_error: 0.2657 - val_loss: 0.2637 - val_mean_squared_error: 0.2637\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26455 to 0.26374, saving model to temp/f3\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2640 - mean_squared_error: 0.2640 - val_loss: 0.2623 - val_mean_squared_error: 0.2623\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26374 to 0.26225, saving model to temp/f3\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 0.2621 - mean_squared_error: 0.2621 - val_loss: 0.2656 - val_mean_squared_error: 0.2656\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26225\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: 0.2620 - mean_squared_error: 0.2620 - val_loss: 0.2618 - val_mean_squared_error: 0.2618\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26225 to 0.26177, saving model to temp/f3\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 0.2610 - mean_squared_error: 0.2610 - val_loss: 0.2591 - val_mean_squared_error: 0.2591\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.26177 to 0.25909, saving model to temp/f3\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2616 - mean_squared_error: 0.2616 - val_loss: 0.2586 - val_mean_squared_error: 0.2586\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.25909 to 0.25862, saving model to temp/f3\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2603 - mean_squared_error: 0.2603 - val_loss: 0.2611 - val_mean_squared_error: 0.2611\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.25862\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: 0.2609 - mean_squared_error: 0.2609 - val_loss: 0.2614 - val_mean_squared_error: 0.2614\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.25862\n",
      "Epoch 00010: early stopping\n",
      "temp/f4\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.3028 - mean_squared_error: 0.3028 - val_loss: 0.2675 - val_mean_squared_error: 0.2675\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26752, saving model to temp/f4\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 118us/step - loss: 0.2693 - mean_squared_error: 0.2693 - val_loss: 0.2675 - val_mean_squared_error: 0.2675\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26752 to 0.26746, saving model to temp/f4\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 0.2654 - mean_squared_error: 0.2654 - val_loss: 0.2641 - val_mean_squared_error: 0.2641\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26746 to 0.26406, saving model to temp/f4\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 0.2637 - mean_squared_error: 0.2637 - val_loss: 0.2618 - val_mean_squared_error: 0.2618\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26406 to 0.26183, saving model to temp/f4\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 5s 116us/step - loss: 0.2636 - mean_squared_error: 0.2636 - val_loss: 0.2609 - val_mean_squared_error: 0.2609\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.26183 to 0.26092, saving model to temp/f4\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 0.2629 - mean_squared_error: 0.2629 - val_loss: 0.2714 - val_mean_squared_error: 0.2714\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.26092\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 0.2613 - mean_squared_error: 0.2613 - val_loss: 0.2649 - val_mean_squared_error: 0.2649\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.26092\n",
      "Epoch 00007: early stopping\n",
      "temp/f5\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 0.2954 - mean_squared_error: 0.2954 - val_loss: 0.2676 - val_mean_squared_error: 0.2676\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26761, saving model to temp/f5\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2664 - mean_squared_error: 0.2664 - val_loss: 0.2647 - val_mean_squared_error: 0.2647\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26761 to 0.26470, saving model to temp/f5\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2655 - mean_squared_error: 0.2655 - val_loss: 0.2714 - val_mean_squared_error: 0.2714\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26470\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2639 - mean_squared_error: 0.2639 - val_loss: 0.2712 - val_mean_squared_error: 0.2712\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26470\n",
      "Epoch 00004: early stopping\n",
      "temp/f6\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 0.2971 - mean_squared_error: 0.2971 - val_loss: 0.2695 - val_mean_squared_error: 0.2695\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26955, saving model to temp/f6\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 0.2685 - mean_squared_error: 0.2685 - val_loss: 0.2691 - val_mean_squared_error: 0.2691\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26955 to 0.26906, saving model to temp/f6\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 4s 96us/step - loss: 0.2648 - mean_squared_error: 0.2648 - val_loss: 0.2634 - val_mean_squared_error: 0.2634\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26906 to 0.26337, saving model to temp/f6\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 4s 95us/step - loss: 0.2640 - mean_squared_error: 0.2640 - val_loss: 0.2648 - val_mean_squared_error: 0.2648\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26337\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 4s 94us/step - loss: 0.2630 - mean_squared_error: 0.2630 - val_loss: 0.2595 - val_mean_squared_error: 0.2595\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.26337 to 0.25949, saving model to temp/f6\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 4s 99us/step - loss: 0.2625 - mean_squared_error: 0.2625 - val_loss: 0.2604 - val_mean_squared_error: 0.2604\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.25949\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 4s 97us/step - loss: 0.2616 - mean_squared_error: 0.2616 - val_loss: 0.2608 - val_mean_squared_error: 0.2608\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.25949\n",
      "Epoch 00007: early stopping\n",
      "temp/f7\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2941 - mean_squared_error: 0.2941 - val_loss: 0.2675 - val_mean_squared_error: 0.2675\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26748, saving model to temp/f7\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 4s 99us/step - loss: 0.2665 - mean_squared_error: 0.2665 - val_loss: 0.2683 - val_mean_squared_error: 0.2683\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.26748\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 4s 100us/step - loss: 0.2647 - mean_squared_error: 0.2647 - val_loss: 0.2638 - val_mean_squared_error: 0.2638\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26748 to 0.26375, saving model to temp/f7\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 4s 94us/step - loss: 0.2625 - mean_squared_error: 0.2625 - val_loss: 0.2623 - val_mean_squared_error: 0.2623\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26375 to 0.26228, saving model to temp/f7\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 4s 99us/step - loss: 0.2634 - mean_squared_error: 0.2634 - val_loss: 0.2630 - val_mean_squared_error: 0.2630\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26228\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 4s 100us/step - loss: 0.2612 - mean_squared_error: 0.2612 - val_loss: 0.2670 - val_mean_squared_error: 0.2670\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.26228\n",
      "Epoch 00006: early stopping\n",
      "temp/f8\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 0.3003 - mean_squared_error: 0.3003 - val_loss: 0.2684 - val_mean_squared_error: 0.2684\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26838, saving model to temp/f8\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2677 - mean_squared_error: 0.2677 - val_loss: 0.2652 - val_mean_squared_error: 0.2652\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26838 to 0.26517, saving model to temp/f8\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2640 - mean_squared_error: 0.2640 - val_loss: 0.2628 - val_mean_squared_error: 0.2628\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26517 to 0.26277, saving model to temp/f8\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2645 - mean_squared_error: 0.2645 - val_loss: 0.2666 - val_mean_squared_error: 0.2666\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26277\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2626 - mean_squared_error: 0.2626 - val_loss: 0.2631 - val_mean_squared_error: 0.2631\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26277\n",
      "Epoch 00005: early stopping\n",
      "temp/f9\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 0.3034 - mean_squared_error: 0.3034 - val_loss: 0.2715 - val_mean_squared_error: 0.2715\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27152, saving model to temp/f9\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2686 - mean_squared_error: 0.2686 - val_loss: 0.2701 - val_mean_squared_error: 0.2701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_loss improved from 0.27152 to 0.27005, saving model to temp/f9\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 0.2651 - mean_squared_error: 0.2651 - val_loss: 0.2627 - val_mean_squared_error: 0.2627\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.27005 to 0.26269, saving model to temp/f9\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2653 - mean_squared_error: 0.2653 - val_loss: 0.2613 - val_mean_squared_error: 0.2613\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26269 to 0.26127, saving model to temp/f9\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 0.2632 - mean_squared_error: 0.2632 - val_loss: 0.2621 - val_mean_squared_error: 0.2621\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26127\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 0.2614 - mean_squared_error: 0.2614 - val_loss: 0.2630 - val_mean_squared_error: 0.2630\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.26127\n",
      "Epoch 00006: early stopping\n",
      "temp/f10\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 0.2948 - mean_squared_error: 0.2948 - val_loss: 0.2707 - val_mean_squared_error: 0.2707\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27071, saving model to temp/f10\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 0.2679 - mean_squared_error: 0.2679 - val_loss: 0.2647 - val_mean_squared_error: 0.2647\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.27071 to 0.26470, saving model to temp/f10\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2654 - mean_squared_error: 0.2654 - val_loss: 0.2622 - val_mean_squared_error: 0.2622\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26470 to 0.26220, saving model to temp/f10\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2655 - mean_squared_error: 0.2655 - val_loss: 0.2599 - val_mean_squared_error: 0.2599\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26220 to 0.25991, saving model to temp/f10\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 0.2632 - mean_squared_error: 0.2632 - val_loss: 0.2625 - val_mean_squared_error: 0.2625\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.25991\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 0.2619 - mean_squared_error: 0.2619 - val_loss: 0.2597 - val_mean_squared_error: 0.2597\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.25991 to 0.25973, saving model to temp/f10\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2609 - mean_squared_error: 0.2609 - val_loss: 0.2635 - val_mean_squared_error: 0.2635\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.25973\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 0.2602 - mean_squared_error: 0.2602 - val_loss: 0.2595 - val_mean_squared_error: 0.2595\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.25973 to 0.25955, saving model to temp/f10\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2602 - mean_squared_error: 0.2602 - val_loss: 0.2593 - val_mean_squared_error: 0.2593\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.25955 to 0.25928, saving model to temp/f10\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 0.2596 - mean_squared_error: 0.2596 - val_loss: 0.2582 - val_mean_squared_error: 0.2582\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.25928 to 0.25822, saving model to temp/f10\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 0.2602 - mean_squared_error: 0.2602 - val_loss: 0.2612 - val_mean_squared_error: 0.2612\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.25822\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 0.2595 - mean_squared_error: 0.2595 - val_loss: 0.2596 - val_mean_squared_error: 0.2596\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.25822\n",
      "Epoch 00012: early stopping\n",
      "temp/f11\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 4s 100us/step - loss: 0.2984 - mean_squared_error: 0.2984 - val_loss: 0.2698 - val_mean_squared_error: 0.2698\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26985, saving model to temp/f11\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 4s 98us/step - loss: 0.2679 - mean_squared_error: 0.2679 - val_loss: 0.2645 - val_mean_squared_error: 0.2645\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26985 to 0.26446, saving model to temp/f11\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 0.2658 - mean_squared_error: 0.2658 - val_loss: 0.2650 - val_mean_squared_error: 0.2650\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26446\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 0.2649 - mean_squared_error: 0.2649 - val_loss: 0.2624 - val_mean_squared_error: 0.2624\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26446 to 0.26238, saving model to temp/f11\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 4s 102us/step - loss: 0.2628 - mean_squared_error: 0.2628 - val_loss: 0.2599 - val_mean_squared_error: 0.2599\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.26238 to 0.25993, saving model to temp/f11\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 4s 99us/step - loss: 0.2636 - mean_squared_error: 0.2636 - val_loss: 0.2592 - val_mean_squared_error: 0.2592\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.25993 to 0.25920, saving model to temp/f11\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 4s 100us/step - loss: 0.2624 - mean_squared_error: 0.2624 - val_loss: 0.2632 - val_mean_squared_error: 0.2632\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.25920\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 0.2614 - mean_squared_error: 0.2614 - val_loss: 0.2626 - val_mean_squared_error: 0.2626\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.25920\n",
      "Epoch 00008: early stopping\n",
      "temp/f12\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.3009 - mean_squared_error: 0.3009 - val_loss: 0.2712 - val_mean_squared_error: 0.2712\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27116, saving model to temp/f12\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2671 - mean_squared_error: 0.2671 - val_loss: 0.2645 - val_mean_squared_error: 0.2645\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.27116 to 0.26447, saving model to temp/f12\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 4s 100us/step - loss: 0.2652 - mean_squared_error: 0.2652 - val_loss: 0.2617 - val_mean_squared_error: 0.2617\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26447 to 0.26173, saving model to temp/f12\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 4s 100us/step - loss: 0.2634 - mean_squared_error: 0.2634 - val_loss: 0.2625 - val_mean_squared_error: 0.2625\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26173\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 0.2639 - mean_squared_error: 0.2639 - val_loss: 0.2673 - val_mean_squared_error: 0.2673\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26173\n",
      "Epoch 00005: early stopping\n",
      "temp/f13\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 0.2998 - mean_squared_error: 0.2998 - val_loss: 0.2684 - val_mean_squared_error: 0.2684\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26838, saving model to temp/f13\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2676 - mean_squared_error: 0.2676 - val_loss: 0.2634 - val_mean_squared_error: 0.2634\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26838 to 0.26344, saving model to temp/f13\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2657 - mean_squared_error: 0.2657 - val_loss: 0.2620 - val_mean_squared_error: 0.2620\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26344 to 0.26200, saving model to temp/f13\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 0.2645 - mean_squared_error: 0.2645 - val_loss: 0.2623 - val_mean_squared_error: 0.2623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26200\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2618 - mean_squared_error: 0.2618 - val_loss: 0.2614 - val_mean_squared_error: 0.2614\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.26200 to 0.26138, saving model to temp/f13\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 0.2627 - mean_squared_error: 0.2627 - val_loss: 0.2605 - val_mean_squared_error: 0.2605\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26138 to 0.26045, saving model to temp/f13\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 0.2611 - mean_squared_error: 0.2611 - val_loss: 0.2620 - val_mean_squared_error: 0.2620\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.26045\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 0.2616 - mean_squared_error: 0.2616 - val_loss: 0.2603 - val_mean_squared_error: 0.2603\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.26045 to 0.26027, saving model to temp/f13\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 4s 102us/step - loss: 0.2605 - mean_squared_error: 0.2605 - val_loss: 0.2607 - val_mean_squared_error: 0.2607\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.26027\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2609 - mean_squared_error: 0.2609 - val_loss: 0.2618 - val_mean_squared_error: 0.2618\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.26027\n",
      "Epoch 00010: early stopping\n",
      "temp/f14\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 0.2976 - mean_squared_error: 0.2976 - val_loss: 0.2668 - val_mean_squared_error: 0.2668\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26677, saving model to temp/f14\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2684 - mean_squared_error: 0.2684 - val_loss: 0.2646 - val_mean_squared_error: 0.2646\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26677 to 0.26457, saving model to temp/f14\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2644 - mean_squared_error: 0.2644 - val_loss: 0.2658 - val_mean_squared_error: 0.2658\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26457\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2634 - mean_squared_error: 0.2634 - val_loss: 0.2622 - val_mean_squared_error: 0.2622\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26457 to 0.26215, saving model to temp/f14\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 4s 99us/step - loss: 0.2626 - mean_squared_error: 0.2626 - val_loss: 0.2623 - val_mean_squared_error: 0.2623\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26215\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 4s 99us/step - loss: 0.2609 - mean_squared_error: 0.2609 - val_loss: 0.2607 - val_mean_squared_error: 0.2607\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26215 to 0.26070, saving model to temp/f14\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 4s 99us/step - loss: 0.2615 - mean_squared_error: 0.2615 - val_loss: 0.2597 - val_mean_squared_error: 0.2597\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.26070 to 0.25971, saving model to temp/f14\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 4s 95us/step - loss: 0.2607 - mean_squared_error: 0.2607 - val_loss: 0.2593 - val_mean_squared_error: 0.2593\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.25971 to 0.25933, saving model to temp/f14\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 4s 96us/step - loss: 0.2608 - mean_squared_error: 0.2608 - val_loss: 0.2595 - val_mean_squared_error: 0.2595\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.25933\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 4s 98us/step - loss: 0.2600 - mean_squared_error: 0.2600 - val_loss: 0.2598 - val_mean_squared_error: 0.2598\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.25933\n",
      "Epoch 00010: early stopping\n",
      "temp/f15\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 0.2977 - mean_squared_error: 0.2977 - val_loss: 0.2677 - val_mean_squared_error: 0.2677\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26767, saving model to temp/f15\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 4s 100us/step - loss: 0.2663 - mean_squared_error: 0.2663 - val_loss: 0.2664 - val_mean_squared_error: 0.2664\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26767 to 0.26642, saving model to temp/f15\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2649 - mean_squared_error: 0.2649 - val_loss: 0.2610 - val_mean_squared_error: 0.2610\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26642 to 0.26095, saving model to temp/f15\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 0.2637 - mean_squared_error: 0.2637 - val_loss: 0.2619 - val_mean_squared_error: 0.2619\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26095\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 0.2631 - mean_squared_error: 0.2631 - val_loss: 0.2643 - val_mean_squared_error: 0.2643\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26095\n",
      "Epoch 00005: early stopping\n",
      "temp/f16\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 0.2997 - mean_squared_error: 0.2997 - val_loss: 0.2663 - val_mean_squared_error: 0.2663\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26627, saving model to temp/f16\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2670 - mean_squared_error: 0.2670 - val_loss: 0.2675 - val_mean_squared_error: 0.2675\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.26627\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2641 - mean_squared_error: 0.2641 - val_loss: 0.2626 - val_mean_squared_error: 0.2626\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26627 to 0.26258, saving model to temp/f16\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2646 - mean_squared_error: 0.2646 - val_loss: 0.2604 - val_mean_squared_error: 0.2604\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26258 to 0.26036, saving model to temp/f16\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 4s 102us/step - loss: 0.2630 - mean_squared_error: 0.2630 - val_loss: 0.2637 - val_mean_squared_error: 0.2637\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26036\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 4s 102us/step - loss: 0.2620 - mean_squared_error: 0.2620 - val_loss: 0.2633 - val_mean_squared_error: 0.2633\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.26036\n",
      "Epoch 00006: early stopping\n",
      "temp/f17\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 0.2966 - mean_squared_error: 0.2966 - val_loss: 0.2703 - val_mean_squared_error: 0.2703\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27025, saving model to temp/f17\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2670 - mean_squared_error: 0.2670 - val_loss: 0.2622 - val_mean_squared_error: 0.2622\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.27025 to 0.26225, saving model to temp/f17\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 0.2639 - mean_squared_error: 0.2639 - val_loss: 0.2616 - val_mean_squared_error: 0.2616\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26225 to 0.26163, saving model to temp/f17\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 4s 100us/step - loss: 0.2632 - mean_squared_error: 0.2632 - val_loss: 0.2615 - val_mean_squared_error: 0.2615\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26163 to 0.26149, saving model to temp/f17\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 0.2635 - mean_squared_error: 0.2635 - val_loss: 0.2615 - val_mean_squared_error: 0.2615\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.26149 to 0.26149, saving model to temp/f17\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2623 - mean_squared_error: 0.2623 - val_loss: 0.2605 - val_mean_squared_error: 0.2605\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26149 to 0.26050, saving model to temp/f17\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 4s 104us/step - loss: 0.2614 - mean_squared_error: 0.2614 - val_loss: 0.2591 - val_mean_squared_error: 0.2591\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.26050 to 0.25907, saving model to temp/f17\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 4s 100us/step - loss: 0.2615 - mean_squared_error: 0.2615 - val_loss: 0.2591 - val_mean_squared_error: 0.2591\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.25907 to 0.25905, saving model to temp/f17\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 4s 102us/step - loss: 0.2604 - mean_squared_error: 0.2604 - val_loss: 0.2586 - val_mean_squared_error: 0.2586\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.25905 to 0.25861, saving model to temp/f17\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2600 - mean_squared_error: 0.2600 - val_loss: 0.2582 - val_mean_squared_error: 0.2582\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.25861 to 0.25818, saving model to temp/f17\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 4s 102us/step - loss: 0.2605 - mean_squared_error: 0.2605 - val_loss: 0.2587 - val_mean_squared_error: 0.2587\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.25818\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2594 - mean_squared_error: 0.2594 - val_loss: 0.2590 - val_mean_squared_error: 0.2590\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.25818\n",
      "Epoch 00012: early stopping\n",
      "temp/f18\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 0.2960 - mean_squared_error: 0.2960 - val_loss: 0.2664 - val_mean_squared_error: 0.2664\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26638, saving model to temp/f18\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2690 - mean_squared_error: 0.2690 - val_loss: 0.2638 - val_mean_squared_error: 0.2638\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26638 to 0.26379, saving model to temp/f18\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2651 - mean_squared_error: 0.2651 - val_loss: 0.2716 - val_mean_squared_error: 0.2716\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26379\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 0.2638 - mean_squared_error: 0.2638 - val_loss: 0.2604 - val_mean_squared_error: 0.2604\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26379 to 0.26043, saving model to temp/f18\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 0.2627 - mean_squared_error: 0.2627 - val_loss: 0.2620 - val_mean_squared_error: 0.2620\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26043\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 0.2623 - mean_squared_error: 0.2623 - val_loss: 0.2601 - val_mean_squared_error: 0.2601\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26043 to 0.26013, saving model to temp/f18\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 4s 102us/step - loss: 0.2611 - mean_squared_error: 0.2611 - val_loss: 0.2601 - val_mean_squared_error: 0.2601\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.26013 to 0.26010, saving model to temp/f18\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 0.2600 - mean_squared_error: 0.2600 - val_loss: 0.2592 - val_mean_squared_error: 0.2592\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.26010 to 0.25916, saving model to temp/f18\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 4s 102us/step - loss: 0.2610 - mean_squared_error: 0.2610 - val_loss: 0.2600 - val_mean_squared_error: 0.2600\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.25916\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 4s 98us/step - loss: 0.2605 - mean_squared_error: 0.2605 - val_loss: 0.2608 - val_mean_squared_error: 0.2608\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.25916\n",
      "Epoch 00010: early stopping\n",
      "temp/f19\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 0.3056 - mean_squared_error: 0.3056 - val_loss: 0.2704 - val_mean_squared_error: 0.2704\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27044, saving model to temp/f19\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2676 - mean_squared_error: 0.2676 - val_loss: 0.2623 - val_mean_squared_error: 0.2623\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.27044 to 0.26231, saving model to temp/f19\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 0.2652 - mean_squared_error: 0.2652 - val_loss: 0.2663 - val_mean_squared_error: 0.2663\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26231\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 0.2639 - mean_squared_error: 0.2639 - val_loss: 0.2614 - val_mean_squared_error: 0.2614\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26231 to 0.26138, saving model to temp/f19\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 0.2634 - mean_squared_error: 0.2634 - val_loss: 0.2611 - val_mean_squared_error: 0.2611\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.26138 to 0.26106, saving model to temp/f19\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 4s 102us/step - loss: 0.2628 - mean_squared_error: 0.2628 - val_loss: 0.2591 - val_mean_squared_error: 0.2591\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26106 to 0.25913, saving model to temp/f19\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 0.2620 - mean_squared_error: 0.2620 - val_loss: 0.2611 - val_mean_squared_error: 0.2611\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.25913\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 4s 102us/step - loss: 0.2622 - mean_squared_error: 0.2622 - val_loss: 0.2594 - val_mean_squared_error: 0.2594\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.25913\n",
      "Epoch 00008: early stopping\n",
      "temp/f20\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 0.2992 - mean_squared_error: 0.2992 - val_loss: 0.2695 - val_mean_squared_error: 0.2695\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26946, saving model to temp/f20\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2663 - mean_squared_error: 0.2663 - val_loss: 0.2639 - val_mean_squared_error: 0.2639\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26946 to 0.26392, saving model to temp/f20\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2642 - mean_squared_error: 0.2642 - val_loss: 0.2624 - val_mean_squared_error: 0.2624\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26392 to 0.26244, saving model to temp/f20\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2631 - mean_squared_error: 0.2631 - val_loss: 0.2620 - val_mean_squared_error: 0.2620\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26244 to 0.26201, saving model to temp/f20\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 4s 102us/step - loss: 0.2620 - mean_squared_error: 0.2620 - val_loss: 0.2648 - val_mean_squared_error: 0.2648\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26201\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 4s 99us/step - loss: 0.2623 - mean_squared_error: 0.2623 - val_loss: 0.2620 - val_mean_squared_error: 0.2620\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.26201\n",
      "Epoch 00006: early stopping\n",
      "temp/f21\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 4s 102us/step - loss: 0.2961 - mean_squared_error: 0.2961 - val_loss: 0.2693 - val_mean_squared_error: 0.2693\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26928, saving model to temp/f21\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 4s 97us/step - loss: 0.2667 - mean_squared_error: 0.2667 - val_loss: 0.2641 - val_mean_squared_error: 0.2641\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26928 to 0.26411, saving model to temp/f21\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 4s 96us/step - loss: 0.2651 - mean_squared_error: 0.2651 - val_loss: 0.2632 - val_mean_squared_error: 0.2632\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26411 to 0.26322, saving model to temp/f21\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 4s 102us/step - loss: 0.2637 - mean_squared_error: 0.2637 - val_loss: 0.2621 - val_mean_squared_error: 0.2621\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26322 to 0.26214, saving model to temp/f21\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 4s 102us/step - loss: 0.2628 - mean_squared_error: 0.2628 - val_loss: 0.2625 - val_mean_squared_error: 0.2625\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26214\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 4s 102us/step - loss: 0.2624 - mean_squared_error: 0.2624 - val_loss: 0.2606 - val_mean_squared_error: 0.2606\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26214 to 0.26063, saving model to temp/f21\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 0.2613 - mean_squared_error: 0.2613 - val_loss: 0.2624 - val_mean_squared_error: 0.2624\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.26063\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 4s 102us/step - loss: 0.2610 - mean_squared_error: 0.2610 - val_loss: 0.2584 - val_mean_squared_error: 0.2584\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.26063 to 0.25843, saving model to temp/f21\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 0.2614 - mean_squared_error: 0.2614 - val_loss: 0.2590 - val_mean_squared_error: 0.2590\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.25843\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 4s 94us/step - loss: 0.2607 - mean_squared_error: 0.2607 - val_loss: 0.2599 - val_mean_squared_error: 0.2599\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.25843\n",
      "Epoch 00010: early stopping\n",
      "temp/f22\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 0.3022 - mean_squared_error: 0.3022 - val_loss: 0.2714 - val_mean_squared_error: 0.2714\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27138, saving model to temp/f22\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2690 - mean_squared_error: 0.2690 - val_loss: 0.2666 - val_mean_squared_error: 0.2666\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.27138 to 0.26658, saving model to temp/f22\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: 0.2658 - mean_squared_error: 0.2658 - val_loss: 0.2612 - val_mean_squared_error: 0.2612\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26658 to 0.26120, saving model to temp/f22\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 0.2643 - mean_squared_error: 0.2643 - val_loss: 0.2637 - val_mean_squared_error: 0.2637\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26120\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 0.2627 - mean_squared_error: 0.2627 - val_loss: 0.2615 - val_mean_squared_error: 0.2615\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26120\n",
      "Epoch 00005: early stopping\n",
      "temp/f23\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 0.2987 - mean_squared_error: 0.2987 - val_loss: 0.2681 - val_mean_squared_error: 0.2681\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26807, saving model to temp/f23\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 116us/step - loss: 0.2679 - mean_squared_error: 0.2679 - val_loss: 0.2649 - val_mean_squared_error: 0.2649\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26807 to 0.26489, saving model to temp/f23\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 5s 118us/step - loss: 0.2658 - mean_squared_error: 0.2658 - val_loss: 0.2642 - val_mean_squared_error: 0.2642\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26489 to 0.26421, saving model to temp/f23\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 0.2647 - mean_squared_error: 0.2647 - val_loss: 0.2630 - val_mean_squared_error: 0.2630\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26421 to 0.26304, saving model to temp/f23\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 0.2627 - mean_squared_error: 0.2627 - val_loss: 0.2676 - val_mean_squared_error: 0.2676\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26304\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 0.2614 - mean_squared_error: 0.2614 - val_loss: 0.2605 - val_mean_squared_error: 0.2605\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26304 to 0.26052, saving model to temp/f23\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 0.2614 - mean_squared_error: 0.2614 - val_loss: 0.2634 - val_mean_squared_error: 0.2634\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.26052\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 0.2618 - mean_squared_error: 0.2618 - val_loss: 0.2607 - val_mean_squared_error: 0.2607\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.26052\n",
      "Epoch 00008: early stopping\n",
      "temp/f24\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2970 - mean_squared_error: 0.2970 - val_loss: 0.2705 - val_mean_squared_error: 0.2705\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27052, saving model to temp/f24\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 0.2667 - mean_squared_error: 0.2667 - val_loss: 0.2653 - val_mean_squared_error: 0.2653\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.27052 to 0.26529, saving model to temp/f24\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 5s 118us/step - loss: 0.2657 - mean_squared_error: 0.2657 - val_loss: 0.2644 - val_mean_squared_error: 0.2644\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26529 to 0.26444, saving model to temp/f24\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 0.2641 - mean_squared_error: 0.2641 - val_loss: 0.2614 - val_mean_squared_error: 0.2614\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26444 to 0.26142, saving model to temp/f24\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 0.2630 - mean_squared_error: 0.2630 - val_loss: 0.2617 - val_mean_squared_error: 0.2617\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26142\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 5s 118us/step - loss: 0.2633 - mean_squared_error: 0.2633 - val_loss: 0.2606 - val_mean_squared_error: 0.2606\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26142 to 0.26061, saving model to temp/f24\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 5s 116us/step - loss: 0.2625 - mean_squared_error: 0.2625 - val_loss: 0.2583 - val_mean_squared_error: 0.2583\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.26061 to 0.25831, saving model to temp/f24\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 0.2624 - mean_squared_error: 0.2624 - val_loss: 0.2619 - val_mean_squared_error: 0.2619\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.25831\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 0.2607 - mean_squared_error: 0.2607 - val_loss: 0.2619 - val_mean_squared_error: 0.2619\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.25831\n",
      "Epoch 00009: early stopping\n",
      "temp/f25\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 0.2951 - mean_squared_error: 0.2951 - val_loss: 0.2696 - val_mean_squared_error: 0.2696\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26956, saving model to temp/f25\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 0.2678 - mean_squared_error: 0.2678 - val_loss: 0.2657 - val_mean_squared_error: 0.2657\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26956 to 0.26568, saving model to temp/f25\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 0.2657 - mean_squared_error: 0.2657 - val_loss: 0.2645 - val_mean_squared_error: 0.2645\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26568 to 0.26449, saving model to temp/f25\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 0.2642 - mean_squared_error: 0.2642 - val_loss: 0.2620 - val_mean_squared_error: 0.2620\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26449 to 0.26197, saving model to temp/f25\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 5s 121us/step - loss: 0.2631 - mean_squared_error: 0.2631 - val_loss: 0.2630 - val_mean_squared_error: 0.2630\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26197\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: 0.2617 - mean_squared_error: 0.2617 - val_loss: 0.2599 - val_mean_squared_error: 0.2599\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26197 to 0.25986, saving model to temp/f25\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 0.2621 - mean_squared_error: 0.2621 - val_loss: 0.2623 - val_mean_squared_error: 0.2623\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.25986\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: 0.2617 - mean_squared_error: 0.2617 - val_loss: 0.2592 - val_mean_squared_error: 0.2592\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.25986 to 0.25918, saving model to temp/f25\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2605 - mean_squared_error: 0.2605 - val_loss: 0.2625 - val_mean_squared_error: 0.2625\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.25918\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2605 - mean_squared_error: 0.2605 - val_loss: 0.2591 - val_mean_squared_error: 0.2591\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.25918 to 0.25911, saving model to temp/f25\n",
      "Epoch 00010: early stopping\n",
      "temp/f26\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 128us/step - loss: 0.2985 - mean_squared_error: 0.2985 - val_loss: 0.2691 - val_mean_squared_error: 0.2691\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26914, saving model to temp/f26\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 0.2671 - mean_squared_error: 0.2671 - val_loss: 0.2665 - val_mean_squared_error: 0.2665\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26914 to 0.26652, saving model to temp/f26\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2649 - mean_squared_error: 0.2649 - val_loss: 0.2629 - val_mean_squared_error: 0.2629\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26652 to 0.26290, saving model to temp/f26\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 125us/step - loss: 0.2632 - mean_squared_error: 0.2632 - val_loss: 0.2662 - val_mean_squared_error: 0.2662\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26290\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 5s 125us/step - loss: 0.2631 - mean_squared_error: 0.2631 - val_loss: 0.2615 - val_mean_squared_error: 0.2615\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.26290 to 0.26150, saving model to temp/f26\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 5s 125us/step - loss: 0.2625 - mean_squared_error: 0.2625 - val_loss: 0.2598 - val_mean_squared_error: 0.2598\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26150 to 0.25984, saving model to temp/f26\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2623 - mean_squared_error: 0.2623 - val_loss: 0.2601 - val_mean_squared_error: 0.2601\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.25984\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2602 - mean_squared_error: 0.2602 - val_loss: 0.2620 - val_mean_squared_error: 0.2620\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.25984\n",
      "Epoch 00008: early stopping\n",
      "temp/f27\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 127us/step - loss: 0.2974 - mean_squared_error: 0.2974 - val_loss: 0.2718 - val_mean_squared_error: 0.2718\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27180, saving model to temp/f27\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2678 - mean_squared_error: 0.2678 - val_loss: 0.2664 - val_mean_squared_error: 0.2664\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.27180 to 0.26639, saving model to temp/f27\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 0.2661 - mean_squared_error: 0.2661 - val_loss: 0.2721 - val_mean_squared_error: 0.2721\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26639\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2643 - mean_squared_error: 0.2643 - val_loss: 0.2613 - val_mean_squared_error: 0.2613\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26639 to 0.26127, saving model to temp/f27\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2633 - mean_squared_error: 0.2633 - val_loss: 0.2621 - val_mean_squared_error: 0.2621\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26127\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2630 - mean_squared_error: 0.2630 - val_loss: 0.2638 - val_mean_squared_error: 0.2638\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.26127\n",
      "Epoch 00006: early stopping\n",
      "temp/f28\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 126us/step - loss: 0.2960 - mean_squared_error: 0.2960 - val_loss: 0.2706 - val_mean_squared_error: 0.2706\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27060, saving model to temp/f28\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 0.2678 - mean_squared_error: 0.2678 - val_loss: 0.2649 - val_mean_squared_error: 0.2649\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.27060 to 0.26491, saving model to temp/f28\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 0.2660 - mean_squared_error: 0.2660 - val_loss: 0.2640 - val_mean_squared_error: 0.2640\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26491 to 0.26398, saving model to temp/f28\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 0.2641 - mean_squared_error: 0.2641 - val_loss: 0.2630 - val_mean_squared_error: 0.2630\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26398 to 0.26305, saving model to temp/f28\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 0.2639 - mean_squared_error: 0.2639 - val_loss: 0.2612 - val_mean_squared_error: 0.2612\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.26305 to 0.26121, saving model to temp/f28\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 0.2619 - mean_squared_error: 0.2619 - val_loss: 0.2603 - val_mean_squared_error: 0.2603\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26121 to 0.26035, saving model to temp/f28\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2624 - mean_squared_error: 0.2624 - val_loss: 0.2620 - val_mean_squared_error: 0.2620\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.26035\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2612 - mean_squared_error: 0.2612 - val_loss: 0.2651 - val_mean_squared_error: 0.2651\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.26035\n",
      "Epoch 00008: early stopping\n",
      "temp/f29\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 125us/step - loss: 0.2972 - mean_squared_error: 0.2972 - val_loss: 0.2682 - val_mean_squared_error: 0.2682\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26823, saving model to temp/f29\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 0.2683 - mean_squared_error: 0.2683 - val_loss: 0.2639 - val_mean_squared_error: 0.2639\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26823 to 0.26386, saving model to temp/f29\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 0.2651 - mean_squared_error: 0.2651 - val_loss: 0.2667 - val_mean_squared_error: 0.2667\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26386\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 0.2637 - mean_squared_error: 0.2637 - val_loss: 0.2648 - val_mean_squared_error: 0.2648\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26386\n",
      "Epoch 00004: early stopping\n",
      "temp/f30\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 126us/step - loss: 0.2999 - mean_squared_error: 0.2999 - val_loss: 0.2673 - val_mean_squared_error: 0.2673\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26726, saving model to temp/f30\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 5s 122us/step - loss: 0.2672 - mean_squared_error: 0.2672 - val_loss: 0.2642 - val_mean_squared_error: 0.2642\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26726 to 0.26418, saving model to temp/f30\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2670 - mean_squared_error: 0.2670 - val_loss: 0.2618 - val_mean_squared_error: 0.2618\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26418 to 0.26177, saving model to temp/f30\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2643 - mean_squared_error: 0.2643 - val_loss: 0.2626 - val_mean_squared_error: 0.2626\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26177\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2622 - mean_squared_error: 0.2622 - val_loss: 0.2645 - val_mean_squared_error: 0.2645\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26177\n",
      "Epoch 00005: early stopping\n",
      "temp/f31\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 128us/step - loss: 0.2956 - mean_squared_error: 0.2956 - val_loss: 0.2740 - val_mean_squared_error: 0.2740\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27402, saving model to temp/f31\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2679 - mean_squared_error: 0.2679 - val_loss: 0.2645 - val_mean_squared_error: 0.2645\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.27402 to 0.26448, saving model to temp/f31\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2651 - mean_squared_error: 0.2651 - val_loss: 0.2630 - val_mean_squared_error: 0.2630\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26448 to 0.26301, saving model to temp/f31\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: 0.2634 - mean_squared_error: 0.2634 - val_loss: 0.2640 - val_mean_squared_error: 0.2640\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26301\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2624 - mean_squared_error: 0.2624 - val_loss: 0.2650 - val_mean_squared_error: 0.2650\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26301\n",
      "Epoch 00005: early stopping\n",
      "temp/f32\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 127us/step - loss: 0.2978 - mean_squared_error: 0.2978 - val_loss: 0.2671 - val_mean_squared_error: 0.2671\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26712, saving model to temp/f32\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 0.2683 - mean_squared_error: 0.2683 - val_loss: 0.2636 - val_mean_squared_error: 0.2636\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26712 to 0.26357, saving model to temp/f32\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 0.2652 - mean_squared_error: 0.2652 - val_loss: 0.2672 - val_mean_squared_error: 0.2672\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26357\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 0.2640 - mean_squared_error: 0.2640 - val_loss: 0.2666 - val_mean_squared_error: 0.2666\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26357\n",
      "Epoch 00004: early stopping\n",
      "temp/f33\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 127us/step - loss: 0.2972 - mean_squared_error: 0.2972 - val_loss: 0.2677 - val_mean_squared_error: 0.2677\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26770, saving model to temp/f33\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2676 - mean_squared_error: 0.2676 - val_loss: 0.2673 - val_mean_squared_error: 0.2673\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26770 to 0.26732, saving model to temp/f33\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 0.2653 - mean_squared_error: 0.2653 - val_loss: 0.2661 - val_mean_squared_error: 0.2661\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26732 to 0.26611, saving model to temp/f33\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 0.2638 - mean_squared_error: 0.2638 - val_loss: 0.2612 - val_mean_squared_error: 0.2612\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26611 to 0.26116, saving model to temp/f33\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2643 - mean_squared_error: 0.2643 - val_loss: 0.2600 - val_mean_squared_error: 0.2600\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.26116 to 0.25997, saving model to temp/f33\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2617 - mean_squared_error: 0.2617 - val_loss: 0.2614 - val_mean_squared_error: 0.2614\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.25997\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2617 - mean_squared_error: 0.2617 - val_loss: 0.2596 - val_mean_squared_error: 0.2596\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.25997 to 0.25962, saving model to temp/f33\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 5s 125us/step - loss: 0.2608 - mean_squared_error: 0.2608 - val_loss: 0.2594 - val_mean_squared_error: 0.2594\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.25962 to 0.25937, saving model to temp/f33\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2607 - mean_squared_error: 0.2607 - val_loss: 0.2599 - val_mean_squared_error: 0.2599\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.25937\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2612 - mean_squared_error: 0.2612 - val_loss: 0.2605 - val_mean_squared_error: 0.2605\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.25937\n",
      "Epoch 00010: early stopping\n",
      "temp/f34\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 126us/step - loss: 0.2991 - mean_squared_error: 0.2991 - val_loss: 0.2682 - val_mean_squared_error: 0.2682\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26817, saving model to temp/f34\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2684 - mean_squared_error: 0.2684 - val_loss: 0.2654 - val_mean_squared_error: 0.2654\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26817 to 0.26539, saving model to temp/f34\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 0.2661 - mean_squared_error: 0.2661 - val_loss: 0.2635 - val_mean_squared_error: 0.2635\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26539 to 0.26347, saving model to temp/f34\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2645 - mean_squared_error: 0.2645 - val_loss: 0.2640 - val_mean_squared_error: 0.2640\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26347\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2629 - mean_squared_error: 0.2629 - val_loss: 0.2649 - val_mean_squared_error: 0.2649\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26347\n",
      "Epoch 00005: early stopping\n",
      "temp/f35\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 0.2959 - mean_squared_error: 0.2959 - val_loss: 0.2678 - val_mean_squared_error: 0.2678\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26778, saving model to temp/f35\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2684 - mean_squared_error: 0.2684 - val_loss: 0.2654 - val_mean_squared_error: 0.2654\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26778 to 0.26543, saving model to temp/f35\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2648 - mean_squared_error: 0.2648 - val_loss: 0.2641 - val_mean_squared_error: 0.2641\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26543 to 0.26409, saving model to temp/f35\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2644 - mean_squared_error: 0.2644 - val_loss: 0.2658 - val_mean_squared_error: 0.2658\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26409\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2637 - mean_squared_error: 0.2637 - val_loss: 0.2618 - val_mean_squared_error: 0.2618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: val_loss improved from 0.26409 to 0.26180, saving model to temp/f35\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 5s 125us/step - loss: 0.2619 - mean_squared_error: 0.2619 - val_loss: 0.2605 - val_mean_squared_error: 0.2605\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26180 to 0.26050, saving model to temp/f35\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 5s 125us/step - loss: 0.2619 - mean_squared_error: 0.2619 - val_loss: 0.2617 - val_mean_squared_error: 0.2617\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.26050\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: 0.2611 - mean_squared_error: 0.2611 - val_loss: 0.2593 - val_mean_squared_error: 0.2593\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.26050 to 0.25929, saving model to temp/f35\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 0.2612 - mean_squared_error: 0.2612 - val_loss: 0.2602 - val_mean_squared_error: 0.2602\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.25929\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2597 - mean_squared_error: 0.2597 - val_loss: 0.2589 - val_mean_squared_error: 0.2589\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.25929 to 0.25892, saving model to temp/f35\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2600 - mean_squared_error: 0.2600 - val_loss: 0.2589 - val_mean_squared_error: 0.2589\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.25892 to 0.25889, saving model to temp/f35\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2613 - mean_squared_error: 0.2613 - val_loss: 0.2602 - val_mean_squared_error: 0.2602\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.25889\n",
      "Epoch 00012: early stopping\n",
      "temp/f36\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 128us/step - loss: 0.2954 - mean_squared_error: 0.2954 - val_loss: 0.2656 - val_mean_squared_error: 0.2656\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26558, saving model to temp/f36\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2675 - mean_squared_error: 0.2675 - val_loss: 0.2667 - val_mean_squared_error: 0.2667\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.26558\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 5s 125us/step - loss: 0.2656 - mean_squared_error: 0.2656 - val_loss: 0.2655 - val_mean_squared_error: 0.2655\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26558 to 0.26553, saving model to temp/f36\n",
      "Epoch 00003: early stopping\n",
      "temp/f37\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 129us/step - loss: 0.2987 - mean_squared_error: 0.2987 - val_loss: 0.2747 - val_mean_squared_error: 0.2747\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27470, saving model to temp/f37\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2681 - mean_squared_error: 0.2681 - val_loss: 0.2654 - val_mean_squared_error: 0.2654\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.27470 to 0.26542, saving model to temp/f37\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2656 - mean_squared_error: 0.2656 - val_loss: 0.2640 - val_mean_squared_error: 0.2640\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26542 to 0.26404, saving model to temp/f37\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 125us/step - loss: 0.2642 - mean_squared_error: 0.2642 - val_loss: 0.2646 - val_mean_squared_error: 0.2646\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26404\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2623 - mean_squared_error: 0.2623 - val_loss: 0.2625 - val_mean_squared_error: 0.2625\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.26404 to 0.26249, saving model to temp/f37\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2629 - mean_squared_error: 0.2629 - val_loss: 0.2599 - val_mean_squared_error: 0.2599\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26249 to 0.25987, saving model to temp/f37\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2611 - mean_squared_error: 0.2611 - val_loss: 0.2621 - val_mean_squared_error: 0.2621\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.25987\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: 0.2608 - mean_squared_error: 0.2608 - val_loss: 0.2618 - val_mean_squared_error: 0.2618\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.25987\n",
      "Epoch 00008: early stopping\n",
      "temp/f38\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 0.2973 - mean_squared_error: 0.2973 - val_loss: 0.2692 - val_mean_squared_error: 0.2692\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26918, saving model to temp/f38\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 118us/step - loss: 0.2690 - mean_squared_error: 0.2690 - val_loss: 0.2631 - val_mean_squared_error: 0.2631\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26918 to 0.26310, saving model to temp/f38\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 0.2654 - mean_squared_error: 0.2654 - val_loss: 0.2652 - val_mean_squared_error: 0.2652\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26310\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2648 - mean_squared_error: 0.2648 - val_loss: 0.2711 - val_mean_squared_error: 0.2711\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26310\n",
      "Epoch 00004: early stopping\n",
      "temp/f39\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 126us/step - loss: 0.2965 - mean_squared_error: 0.2965 - val_loss: 0.2734 - val_mean_squared_error: 0.2734\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27337, saving model to temp/f39\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 0.2687 - mean_squared_error: 0.2687 - val_loss: 0.2670 - val_mean_squared_error: 0.2670\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.27337 to 0.26701, saving model to temp/f39\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 0.2650 - mean_squared_error: 0.2650 - val_loss: 0.2628 - val_mean_squared_error: 0.2628\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26701 to 0.26278, saving model to temp/f39\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 0.2632 - mean_squared_error: 0.2632 - val_loss: 0.2619 - val_mean_squared_error: 0.2619\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26278 to 0.26188, saving model to temp/f39\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 0.2622 - mean_squared_error: 0.2622 - val_loss: 0.2634 - val_mean_squared_error: 0.2634\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26188\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 0.2630 - mean_squared_error: 0.2630 - val_loss: 0.2592 - val_mean_squared_error: 0.2592\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26188 to 0.25921, saving model to temp/f39\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: 0.2627 - mean_squared_error: 0.2627 - val_loss: 0.2625 - val_mean_squared_error: 0.2625\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.25921\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2609 - mean_squared_error: 0.2609 - val_loss: 0.2621 - val_mean_squared_error: 0.2621\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.25921\n",
      "Epoch 00008: early stopping\n",
      "temp/f40\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 127us/step - loss: 0.2980 - mean_squared_error: 0.2980 - val_loss: 0.2716 - val_mean_squared_error: 0.2716\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27157, saving model to temp/f40\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2661 - mean_squared_error: 0.2661 - val_loss: 0.2661 - val_mean_squared_error: 0.2661\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.27157 to 0.26605, saving model to temp/f40\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 5s 122us/step - loss: 0.2661 - mean_squared_error: 0.2661 - val_loss: 0.2639 - val_mean_squared_error: 0.2639\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26605 to 0.26391, saving model to temp/f40\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2636 - mean_squared_error: 0.2636 - val_loss: 0.2664 - val_mean_squared_error: 0.2664\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26391\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2632 - mean_squared_error: 0.2632 - val_loss: 0.2621 - val_mean_squared_error: 0.2621\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.26391 to 0.26211, saving model to temp/f40\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 5s 126us/step - loss: 0.2627 - mean_squared_error: 0.2627 - val_loss: 0.2592 - val_mean_squared_error: 0.2592\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26211 to 0.25923, saving model to temp/f40\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2619 - mean_squared_error: 0.2619 - val_loss: 0.2619 - val_mean_squared_error: 0.2619\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.25923\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2613 - mean_squared_error: 0.2613 - val_loss: 0.2597 - val_mean_squared_error: 0.2597\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.25923\n",
      "Epoch 00008: early stopping\n",
      "temp/f41\n",
      "Train on 40000 samples, validate on 40000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 5s 128us/step - loss: 0.3007 - mean_squared_error: 0.3007 - val_loss: 0.2685 - val_mean_squared_error: 0.2685\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26852, saving model to temp/f41\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2679 - mean_squared_error: 0.2679 - val_loss: 0.2653 - val_mean_squared_error: 0.2653\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26852 to 0.26532, saving model to temp/f41\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 5s 125us/step - loss: 0.2658 - mean_squared_error: 0.2658 - val_loss: 0.2629 - val_mean_squared_error: 0.2629\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26532 to 0.26292, saving model to temp/f41\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2646 - mean_squared_error: 0.2646 - val_loss: 0.2618 - val_mean_squared_error: 0.2618\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26292 to 0.26181, saving model to temp/f41\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 0.2629 - mean_squared_error: 0.2629 - val_loss: 0.2617 - val_mean_squared_error: 0.2617\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.26181 to 0.26173, saving model to temp/f41\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 5s 125us/step - loss: 0.2621 - mean_squared_error: 0.2621 - val_loss: 0.2617 - val_mean_squared_error: 0.2617\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26173 to 0.26167, saving model to temp/f41\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 0.2608 - mean_squared_error: 0.2608 - val_loss: 0.2598 - val_mean_squared_error: 0.2598\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.26167 to 0.25983, saving model to temp/f41\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2615 - mean_squared_error: 0.2615 - val_loss: 0.2602 - val_mean_squared_error: 0.2602\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.25983\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 0.2611 - mean_squared_error: 0.2611 - val_loss: 0.2595 - val_mean_squared_error: 0.2595\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.25983 to 0.25949, saving model to temp/f41\n",
      "Epoch 10/20\n",
      " 8384/40000 [=====>........................] - ETA: 2s - loss: 0.2592 - mean_squared_error: 0.2592"
     ]
    }
   ],
   "source": [
    "for idx, model_name in enumerate(model_names):\n",
    "    print(model_name)\n",
    "\n",
    "    if type(models[idx]) is list:\n",
    "        #clear session\n",
    "        keras.backend.clear_session() \n",
    "        #get model according to specification\n",
    "        model = get_model(models[idx], [0.2] * len(models), 6)\n",
    "        callbacks = [ModelCheckpoint(model_name, verbose= verbosity, monitor='val_loss',save_best_only=True), \n",
    "                     EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose= verbosity, mode='auto')]\n",
    "        model.compile(optimizer = optimizers.SGD(lr = 0.001, momentum = 0.9, ), loss='mean_squared_error', metrics = ['mse'])\n",
    "        #print(len(X), len(y))\n",
    "        model.fit(X, y, epochs = 20, validation_data = (x_val, y_val), callbacks = callbacks, batch_size = 32, verbose = verbosity)\n",
    "    else:\n",
    "        models[idx].fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_test = 2000\n",
    "metrics_dicts = []\n",
    "for m in models:\n",
    "    metrics_dicts.append(defaultdict(list))\n",
    "\n",
    "\n",
    "#means = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "#variances = [1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0]\n",
    "means = [0, 1, 2]\n",
    "variances = [1,2,3]\n",
    "\n",
    "\n",
    "# ok at this point we need to check the model on various variances and means\n",
    "for m in means:\n",
    "    for v in variances:\n",
    "        print(m,v)\n",
    "        #t0 = time.time()\n",
    "        perturbed_df = gen_data(mean =m, var = v, SIZE = nb_test)\n",
    "        y_test2 = perturbed_df['g']\n",
    "        x_test2 = perturbed_df[['a', 'b', 'c', 'd', 'e', 'f']]\n",
    "        #t1 = time.time()\n",
    "        #print(\"Time for gen_data = \", t1 - t0)\n",
    "        for idx, model_name in enumerate(model_names):\n",
    "            #t0 = time.time()\n",
    "            if type(models[idx]) is list:\n",
    "                keras.backend.clear_session()\n",
    "                model = load_model(model_name)\n",
    "            else:\n",
    "                model = models[idx]\n",
    "            #t1 = time.time()\n",
    "            #print(\"Time to load model = \", t1 - t0)\n",
    "            \n",
    "            y_pred2 = model.predict(x_test2)\n",
    "            metrics_dicts[idx][str(m) + '_' + str(v)].append(mean_squared_error(y_test2, y_pred2))\n",
    "\n",
    "            test_df2 = pd.DataFrame(x_test2, columns = ['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "            test_targets2 = pd.DataFrame(model.predict(x_test2), columns = ['g'])\n",
    "            test_df2 = test_df2.join(test_targets2)\n",
    "'''\n",
    "            setA = get_MB(get_CG(perturbed_df, tetrad), 'g', pc)\n",
    "            if setA != {'f'}:\n",
    "                print(\"Error in SETA markov blanket\")\n",
    "                #setA = {'f'}\n",
    "            setC = get_MB(get_CG(test_df2, tetrad), 'g', pc)\n",
    "\n",
    "            if setA != setC:\n",
    "                causal_dicts[idx][str(m) + '_' + str(v)].append(1)\n",
    "            else:\n",
    "                causal_dicts[idx][str(m) + '_' + str(v)].append(0)\n",
    "\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the number of times to sample \n",
    "times = 1\n",
    "\n",
    "## the size of the test set\n",
    "\n",
    "\n",
    "violations = np.zeros(len(models))\n",
    "violation_mean = np.zeros((len(models), times))\n",
    "violation_mean2 = np.zeros((len(models), times))\n",
    "mean = np.zeros((len(models), times))\n",
    "\n",
    "fold = 0\n",
    "\n",
    "#metrics_dicts = []\n",
    "causal_dicts = []\n",
    "for m in models:\n",
    "#    metrics_dicts.append(defaultdict(list))\n",
    "    causal_dicts.append(defaultdict(list))\n",
    "from pycausal import prior as p\n",
    "def get_bic(df):\n",
    "    prior = p.knowledge(requiredirect =  [['a', 'g'], ['a', 'g'], ['a', 'g'], ['g', 'd'],['g', 'e'],['g', 'f']])\n",
    "    tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "               structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, priorKnowledge = prior,\n",
    "               completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "    BIC = tetrad.getTetradGraph().getAllAttributes().toString()\n",
    "    BIC = float(BIC.split('=')[-1].split('}')[0])\n",
    "    return BIC / len(df)\n",
    "\n",
    "for t in range(times):\n",
    "    print(\"Times = \", t)\n",
    "    df_test = gen_data(SIZE = nb_test)\n",
    "    x_test = df_test[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "    y_test = df_test['g'].values\n",
    "    bic_orig = get_bic(df_test)\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        if type(models[idx]) is list:\n",
    "            keras.backend.clear_session()\n",
    "            model = load_model(model_name)\n",
    "        else:\n",
    "            model = models[idx]\n",
    "        test_df = pd.DataFrame(x_test, columns = ['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "        test_targets = pd.DataFrame(model.predict(x_test), columns = ['g'])\n",
    "        test_df = test_df.join(test_targets)\n",
    "        mean[idx][t] = mean_squared_error(y_test, model.predict(x_test))  \n",
    "        bic_pred = get_bic(test_df)\n",
    "        print(bic_orig, bic_pred)\n",
    "        violation_mean[idx][t] =  bic_pred\n",
    "        violation_mean2[idx][t] = bic_pred\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric = []\n",
    "metric_err = []\n",
    "viol = []\n",
    "viol_err = []\n",
    "\n",
    "#normalize the violations for prettier graphing.\n",
    "#also violations are always positive, so just divide by max.\n",
    "\n",
    "#TMK\n",
    "#violation_mean = violation_mean / np.max(violation_mean)\n",
    "\n",
    "for i in range(len(violations)):\n",
    "    print(\"Model_name = \", model_names[i], \"Violations = \", violations[i])\n",
    "    print(\"Average_violations = \", np.mean(violation_mean[i]), np.std(violation_mean[i]))\n",
    "    print(\"MSE = \", np.mean(mean[i]), np.std(mean[i]))\n",
    "    #print(\"mean = \", mean[i])\n",
    "    metric.append(np.mean(mean[i]))\n",
    "    metric_err.append(np.std(mean[i]))\n",
    "    viol.append(np.mean(violation_mean[i]))\n",
    "    #viol.append(violations[i]/times)\n",
    "    viol_err.append(np.std(violation_mean[i]))\n",
    "print(np.array(metric), \n",
    "         np.array(metric_err), \n",
    "         np.array(viol), \n",
    "         np.array(viol_err))    \n",
    "\n",
    "bar_plot(model_names, \n",
    "         np.array(metric), \n",
    "         np.array(metric_err), \n",
    "         np.array(viol), \n",
    "         np.array(viol_err))\n",
    "\n",
    "\n",
    "def heat_plot(x,y,z, xlab = 'Mean', ylab = 'Variance', clim_low = 0, clim_high = 1):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    cax = ax.scatter(x, y, c=z, s=450, edgecolor='')\n",
    "    cax.set_clim(clim_low, clim_high)\n",
    "    ax.set_xlabel(xlab)\n",
    "    ax.set_ylabel(ylab)\n",
    "    plt.colorbar(cax)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "MSE = []\n",
    "VIO = []\n",
    "VIO2 = []\n",
    "AUS = []\n",
    "for i, m in enumerate(models):\n",
    "    print(model_names[i])\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "    \n",
    "    \n",
    "    rectangular_approx = 0\n",
    "    for k, v in metrics_dicts[i].items():\n",
    "        x.append(float(k.split('_')[0]))\n",
    "        y.append(float(k.split('_')[-1]))\n",
    "        z.append(np.mean(v))\n",
    "        rectangular_approx += np.mean(v)\n",
    "    print(\"Area under surface (rectangular approx) = \", rectangular_approx)\n",
    "    print(\"Violations = \", violations[i])\n",
    "    print(\"Average_violations = \", np.mean(violation_mean[i]))\n",
    "    print(\"MSE = \", np.mean(mean[i]))   \n",
    "    MSE.append(np.mean(mean[i]))\n",
    "    VIO.append(np.mean(violation_mean[i]))\n",
    "    VIO2.append(np.mean(violation_mean2[i]))\n",
    "    #VIO.append(violations[i]/times)\n",
    "    AUS.append(rectangular_approx)\n",
    "    \n",
    "    #heat_plot(x,y,z, clim_low = 0, clim_high = 10)\n",
    "    \n",
    "#heat_plot(MSE,VIO,AUS, xlab = 'MSE', ylab='Violations', clim_low = np.min(AUS), clim_high = np.max(AUS))\n",
    "    \n",
    "#VIO = np.abs(VIO)\n",
    "#VIO2 = np.abs(VIO2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from numpy.polynomial.polynomial import polyfit  \n",
    "from scipy.stats import pearsonr\n",
    "from pylab import text\n",
    "\n",
    "\n",
    "print(pearsonr(VIO,AUS)[0])\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(VIO,AUS, 1)\n",
    "ax.plot(VIO,AUS, '.')\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(VIO,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "plt.plot(VIO, b + m * np.array(VIO), '-')\n",
    "ax.set_xlabel(\"Proposed metric\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "fig.savefig('Ex4VIOVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "METRIC = (VIO/np.max(VIO)) + np.array(MSE)\n",
    "print(pearsonr(METRIC,AUS)[0])\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(METRIC,AUS, 1)\n",
    "ax.plot(METRIC,AUS, '.')\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(METRIC,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "plt.plot(METRIC, b + m * np.array(METRIC), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"Proposed metric\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "fig.savefig('Ex4ProposedVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(MSE,AUS, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(MSE,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(MSE,AUS, '.')\n",
    "plt.plot(MSE, b + m * np.array(MSE), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(VIO2,AUS, 1)\n",
    "print(b,m)\n",
    "ax.plot(VIO2,AUS, '.')\n",
    "plt.plot(VIO2, b + m * np.array(VIO2), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"Violations2\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(VIO,MSE, 1)\n",
    "print(b,m)\n",
    "ax.plot(VIO,MSE, '.')\n",
    "plt.plot(VIO, b + m * np.array(VIO), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"Violations\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(METRIC,AUS, 1)\n",
    "print(b,m)\n",
    "ax.plot(METRIC,AUS, '.')\n",
    "plt.plot(METRIC, b + m * np.array(METRIC), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"MSExVIO\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "plt.show()\n",
    "\n",
    "MSE = np.array(MSE)\n",
    "METRIC = VIO/np.max(VIO)+ MSE\n",
    "x = []\n",
    "y1 = []\n",
    "y2 = []\n",
    "y3 = []\n",
    "for split in range(10, len(AUS), 5):\n",
    "    #print(\"******\", split, \"*******\")\n",
    "    sorted_aus = [AUS for _,AUS in sorted(zip(VIO,AUS))]\n",
    "    sorted_mse = [MSE for _,MSE in sorted(zip(VIO,MSE))]\n",
    "\n",
    "    low = []\n",
    "    high = []\n",
    "    low = sorted_aus[:split]\n",
    "    high = sorted_aus[split:]\n",
    "\n",
    "    x.append(split)\n",
    "    \n",
    "    \n",
    "    #print(\"Low Violations = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High Violations = \", np.mean(high), \"for\", len(high))\n",
    "    y1.append(np.mean(low))\n",
    "    sorted_aus_by_mse = [AUS for _,AUS in sorted(zip(MSE,AUS))]\n",
    "    low = sorted_aus_by_mse[:split]\n",
    "    high = sorted_aus_by_mse[split:]\n",
    "    #print(\"Low AUS by MSE = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High AUS by MSE = \", np.mean(high), \"for\", len(high))\n",
    "    y2.append(np.mean(low))\n",
    "    sorted_aus = [AUS for _,AUS in sorted(zip(METRIC,AUS))]\n",
    "    sorted_mse = [MSE for _,MSE in sorted(zip(METRIC,MSE))]\n",
    "\n",
    "    low = []\n",
    "    high = []\n",
    "    low = sorted_aus[:split]\n",
    "    high = sorted_aus[split:]\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"Low Metric = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High Metric = \", np.mean(high), \"for\", len(high))\n",
    "    y3.append(np.mean(low))\n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x,y1, '-', label = 'Violations')\n",
    "ax.plot(x,y2, '-', label = 'MSE')\n",
    "ax.plot(x,y3, '-', label = 'METRIC')\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "plt.show()  \n",
    "pearsonr(METRIC,AUS)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
