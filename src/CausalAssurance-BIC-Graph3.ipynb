{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes \n",
    "### Required installing Oracle JAVA 8 to get javabridge installed\n",
    "### Then, I was able to install py-causal from https://bd2kccd.github.io/docs/py-causal/\n",
    "### GFCI is slower than RFCI, but more accurate (SPIRTES), GFCI and RFCI account for unobserved variables, FGES assumes no unobserved variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure Learning Performance Guarantees If the assumptions in the previous section hold, then in the large sample limit, the CBN structure output by GFCId will contain an edge of one of four kinds between Xand Y   if and only if Xand Yare not independent conditional on any subset of the other measured variables of less than or equal to a specified size. In addition, there is (1) an arc X->Y   if and only if Xdirectly or indirectly causes Y, and Y   does not directly or indirectly cause X; (2) an edge X <-->Y   if and only if X   is not a direct or indirect cause of Yand Y   is not a direct or indirect cause of X(which can only occur if there are latent confounders of Xand some other variable or Yand some other variable; (3) an edge Xo->Y   only if Yis not a direct or indirect cause of X, but Xmay or may not be an indirect cause of Y; (4) an edge X oâ€“o Y   indicates that Xand Y   are dependent no matter what subset of observed variables is conditioned on, but contains no orientation information (X   may be a direct or indirect cause of Y, and Ymay be an indirect cause of X, or there may be a latent common cause of Xand Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying some various ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import configparser\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, \\\n",
    "                        Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "import keras.optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "import glob, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "# select your GPU Here\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #Comment this line out if you want all GPUS (2 hehe)\n",
    "\n",
    "# python full-display web browser\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "def get_model(dense, dropouts, inputs):\n",
    "    # dense is an ordered list of the number of dense neurons like [1024, 2048, 1024]\n",
    "    # dropouts is an ordered list of the dropout masks like [0.2, 0.3, 0.4]\n",
    "    inputs = keras.Input(shape = (inputs,))\n",
    "\n",
    "    x = keras.layers.Dense(dense[0], activation = 'relu')(inputs)\n",
    "    x = keras.layers.Dropout(dropouts[0])(x, training=True)\n",
    "    for den, drop in zip(dense[1:], dropouts[1:]):\n",
    "        x = keras.layers.Dense(den, activation = 'relu')(x)\n",
    "        x = keras.layers.Dropout(drop)(x, training=True)\n",
    "    outputs = keras.layers.Dense(1, activation = 'linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512]] ['temp/f0', 'temp/f1', 'temp/f2', 'temp/f3', 'temp/f4', 'temp/f5', 'temp/f6', 'temp/f7', 'temp/f8', 'temp/f9', 'temp/f10', 'temp/f11', 'temp/f12', 'temp/f13', 'temp/f14', 'temp/f15', 'temp/f16', 'temp/f17', 'temp/f18', 'temp/f19', 'temp/f20', 'temp/f21', 'temp/f22', 'temp/f23', 'temp/f24', 'temp/f25', 'temp/f26', 'temp/f27', 'temp/f28', 'temp/f29', 'temp/f30', 'temp/f31', 'temp/f32', 'temp/f33', 'temp/f34', 'temp/f35', 'temp/f36', 'temp/f37', 'temp/f38', 'temp/f39', 'temp/f40', 'temp/f41', 'temp/f42', 'temp/f43', 'temp/f44', 'temp/f45', 'temp/f46', 'temp/f47', 'temp/f48', 'temp/f49', 'temp/f50', 'temp/f51', 'temp/f52', 'temp/f53', 'temp/f54', 'temp/f55', 'temp/f56', 'temp/f57', 'temp/f58', 'temp/f59', 'temp/f60', 'temp/f61', 'temp/f62', 'temp/f63', 'temp/f64', 'temp/f65', 'temp/f66', 'temp/f67', 'temp/f68', 'temp/f69', 'temp/f70', 'temp/f71', 'temp/f72', 'temp/f73', 'temp/f74', 'temp/f75', 'temp/f76', 'temp/f77', 'temp/f78', 'temp/f79', 'temp/f80', 'temp/f81', 'temp/f82', 'temp/f83', 'temp/f84', 'temp/f85', 'temp/f86', 'temp/f87', 'temp/f88', 'temp/f89', 'temp/f90', 'temp/f91', 'temp/f92', 'temp/f93', 'temp/f94', 'temp/f95', 'temp/f96', 'temp/f97', 'temp/f98', 'temp/f99', 'temp/f100', 'temp/f101', 'temp/f102', 'temp/f103', 'temp/f104', 'temp/f105', 'temp/f106', 'temp/f107', 'temp/f108', 'temp/f109', 'temp/f110', 'temp/f111', 'temp/f112', 'temp/f113', 'temp/f114', 'temp/f115', 'temp/f116', 'temp/f117', 'temp/f118', 'temp/f119', 'temp/f120', 'temp/f121', 'temp/f122', 'temp/f123', 'temp/f124', 'temp/f125', 'temp/f126', 'temp/f127', 'temp/f128', 'temp/f129', 'temp/f130', 'temp/f131', 'temp/f132', 'temp/f133', 'temp/f134', 'temp/f135', 'temp/f136', 'temp/f137', 'temp/f138', 'temp/f139', 'temp/f140', 'temp/f141', 'temp/f142', 'temp/f143', 'temp/f144', 'temp/f145', 'temp/f146', 'temp/f147', 'temp/f148', 'temp/f149', 'temp/f150', 'temp/f151', 'temp/f152', 'temp/f153', 'temp/f154', 'temp/f155', 'temp/f156', 'temp/f157', 'temp/f158', 'temp/f159', 'temp/f160', 'temp/f161', 'temp/f162', 'temp/f163', 'temp/f164', 'temp/f165', 'temp/f166', 'temp/f167', 'temp/f168', 'temp/f169', 'temp/f170', 'temp/f171', 'temp/f172', 'temp/f173', 'temp/f174', 'temp/f175', 'temp/f176', 'temp/f177', 'temp/f178', 'temp/f179', 'temp/f180', 'temp/f181', 'temp/f182', 'temp/f183', 'temp/f184', 'temp/f185', 'temp/f186', 'temp/f187', 'temp/f188', 'temp/f189', 'temp/f190', 'temp/f191', 'temp/f192', 'temp/f193', 'temp/f194', 'temp/f195', 'temp/f196', 'temp/f197', 'temp/f198', 'temp/f199', 'temp/f200', 'temp/f201', 'temp/f202', 'temp/f203', 'temp/f204', 'temp/f205', 'temp/f206', 'temp/f207', 'temp/f208', 'temp/f209', 'temp/f210', 'temp/f211', 'temp/f212', 'temp/f213', 'temp/f214', 'temp/f215', 'temp/f216', 'temp/f217', 'temp/f218', 'temp/f219', 'temp/f220', 'temp/f221', 'temp/f222', 'temp/f223', 'temp/f224', 'temp/f225', 'temp/f226', 'temp/f227', 'temp/f228', 'temp/f229', 'temp/f230', 'temp/f231', 'temp/f232', 'temp/f233', 'temp/f234', 'temp/f235', 'temp/f236', 'temp/f237', 'temp/f238', 'temp/f239', 'temp/f240', 'temp/f241', 'temp/f242', 'temp/f243', 'temp/f244', 'temp/f245', 'temp/f246', 'temp/f247', 'temp/f248', 'temp/f249', 'temp/f250', 'temp/f251', 'temp/f252', 'temp/f253', 'temp/f254', 'temp/f255', 'temp/f256', 'temp/f257', 'temp/f258', 'temp/f259', 'temp/f260', 'temp/f261', 'temp/f262', 'temp/f263', 'temp/f264', 'temp/f265', 'temp/f266', 'temp/f267', 'temp/f268', 'temp/f269', 'temp/f270', 'temp/f271', 'temp/f272', 'temp/f273', 'temp/f274', 'temp/f275', 'temp/f276', 'temp/f277', 'temp/f278', 'temp/f279', 'temp/f280', 'temp/f281', 'temp/f282', 'temp/f283', 'temp/f284', 'temp/f285', 'temp/f286', 'temp/f287', 'temp/f288', 'temp/f289', 'temp/f290', 'temp/f291', 'temp/f292', 'temp/f293', 'temp/f294', 'temp/f295', 'temp/f296', 'temp/f297', 'temp/f298', 'temp/f299']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from pycausal import search as s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def discrete_gauss(low, high, samples, std = 20):\n",
    "    x = np.arange(low, high)\n",
    "    xU, xL = x + 0.5, x - 0.5 \n",
    "    prob = ss.norm.cdf(xU, scale = std) - ss.norm.cdf(xL, scale = std)\n",
    "    prob = prob / prob.sum() #normalize the probabilities so their sum is 1\n",
    "    nums = np.random.choice(x, size = samples, p = prob)\n",
    "    return nums\n",
    "\n",
    "\n",
    "\n",
    "def bar_plot(x_ax, val1, val1std, val2, val2std):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ## the data\n",
    "    N = len(x_ax)\n",
    "\n",
    "    ## necessary variables\n",
    "    ind = np.arange(N)                # the x locations for the groups\n",
    "    width = 0.35                      # the width of the bars\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    ## the bars\n",
    "    rects1 = ax.bar(ind, val1, width,\n",
    "                    color='gray',\n",
    "                    yerr=val1std,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='blue'))\n",
    "\n",
    "    rects2 = ax.bar(ind+width, val2, width,\n",
    "                        color='blue',\n",
    "                        #yerr=val2std,\n",
    "                        error_kw=dict(elinewidth=2,ecolor='gray'))\n",
    "\n",
    "    # axes and labels\n",
    "    ax.set_xlim(-width,len(ind)+width)\n",
    "    #ax.set_ylim(0,45)\n",
    "    ax.set_ylabel('Percentage')\n",
    "    ax.set_title('')\n",
    "    plt.xticks(ind + width / 2, x_ax, rotation=75, size = 14)\n",
    "    ## add a legend\n",
    "    ax.legend( (rects1[0], rects2[0]), ('Accuracy', '% Violations') )\n",
    "    fig.savefig(\"violations.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 5000):\n",
    "    a = np.random.gumbel(mean, var, SIZE)\n",
    "    b = np.random.gumbel(mean, var, SIZE)\n",
    "    c = np.random.gumbel(mean, var, SIZE)\n",
    "    d = np.random.gumbel(mean, var, SIZE)\n",
    "    e = np.random.gumbel(mean, var, SIZE)\n",
    "    f= a + b + c + d + e + np.random.gumbel(mean, var, SIZE)\n",
    "    g = f + np.random.gumbel(mean,var, SIZE)\n",
    "    g = np.rint(g)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 5000):\n",
    "    a = np.random.gumbel(mean, var, SIZE)\n",
    "    b = np.random.gumbel(mean, var, SIZE)\n",
    "    c = np.random.gumbel(mean, var, SIZE)\n",
    "    d = np.random.gumbel(mean, var, SIZE)\n",
    "\n",
    "    f= a + b + c + d + np.random.gumbel(mean, var, SIZE)\n",
    "    g = f + np.random.gumbel(mean,var, SIZE)\n",
    "    \n",
    "    \n",
    "    g = np.rint(g)\n",
    "    e = g + np.random.gumbel(mean,var,SIZE)\n",
    "    \n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 400000):\n",
    "    f = np.random.normal(mean, var, SIZE)\n",
    "    a = f + np.random.normal(mean, var, SIZE)\n",
    "    b = f + np.random.normal(mean, var, SIZE)\n",
    "    c = f + np.random.normal(mean, var, SIZE)\n",
    "    d = f + np.random.normal(mean, var, SIZE)\n",
    "    e = f + np.random.normal(mean, var, SIZE)\n",
    "    g = a + b + c + d  + e + np.random.normal(mean, var, SIZE)\n",
    "\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 20000):\n",
    "    a = np.random.normal(mean, var, SIZE)\n",
    "    b = np.random.normal(mean, var, SIZE)\n",
    "    c = np.random.normal(mean, var, SIZE)\n",
    "    d = np.random.normal(mean, var, SIZE)\n",
    "    e = np.random.normal(mean, var, SIZE)\n",
    "    f= a + b + c + d + e + np.random.normal(mean, var, SIZE)\n",
    "    g = f + np.random.normal(mean,var, SIZE)\n",
    "    #g = np.rint(g)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 20000):\n",
    "    a = np.random.normal(mean, var, SIZE)\n",
    "    b = np.random.normal(mean, var, SIZE)\n",
    "    c = np.random.normal(mean, var, SIZE)\n",
    "    d = np.random.normal(mean, var, SIZE)\n",
    "    g = a + b + c + d +np.random.normal(mean,var, SIZE)\n",
    "    e = g + np.random.normal(mean, var, SIZE)\n",
    "    f = g + np.random.normal(mean, var, SIZE)\n",
    "    \n",
    "    #g = np.rint(g)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def get_CG(df, tetrad):\n",
    "    tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "    #tetrad.run(algoId = 'fges-mb', targetName = 'g', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "    #       structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "    #       completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "def get_MB(graph, var, pc):\n",
    "    parents = set()\n",
    "    children = set()\n",
    "    for i in pc.extractTetradGraphEdges(graph):\n",
    "        if i[-1] == var and i[3:5] == '->':\n",
    "            parents.add(i[0])\n",
    "        if i[0] == var and i[3:5] == '->':\n",
    "            children.add(i[-1])\n",
    "    return parents, children\n",
    "\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from collections import defaultdict\n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '5000M')\n",
    "tetrad = s.tetradrunner()\n",
    "\n",
    "\n",
    "verbosity = 1\n",
    "\n",
    "\n",
    "\n",
    "models = []\n",
    "model_names = []\n",
    "\n",
    "num_models =300\n",
    "model_layers = [2048, 2048, 512]\n",
    "for i in range(num_models):\n",
    "    models.append(model_layers)\n",
    "    model_names.append('temp/f' + str(i))\n",
    "\n",
    "print(models, model_names)\n",
    "\n",
    "\n",
    "df = gen_data()\n",
    "X = df[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "y = df['g'].values\n",
    "\n",
    "val_df = gen_data(SIZE = 2000)\n",
    "x_val = df[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "y_val = df['g'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp/f0\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 4s 182us/step - loss: 0.4559 - mean_squared_error: 0.4559 - val_loss: 0.3636 - val_mean_squared_error: 0.3636\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36364, saving model to temp/f0\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3640 - mean_squared_error: 0.3640 - val_loss: 0.3597 - val_mean_squared_error: 0.3597\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36364 to 0.35966, saving model to temp/f0\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3620 - mean_squared_error: 0.3620 - val_loss: 0.3604 - val_mean_squared_error: 0.3604\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35966\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3588 - mean_squared_error: 0.3588 - val_loss: 0.3547 - val_mean_squared_error: 0.3547\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35966 to 0.35468, saving model to temp/f0\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3580 - mean_squared_error: 0.3580 - val_loss: 0.3690 - val_mean_squared_error: 0.3690\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35468\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3566 - mean_squared_error: 0.3566 - val_loss: 0.3542 - val_mean_squared_error: 0.3542\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35468 to 0.35424, saving model to temp/f0\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3579 - val_mean_squared_error: 0.3579\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35424\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3565 - mean_squared_error: 0.3565 - val_loss: 0.3547 - val_mean_squared_error: 0.3547\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35424\n",
      "Epoch 00008: early stopping\n",
      "temp/f1\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4515 - mean_squared_error: 0.4515 - val_loss: 0.3700 - val_mean_squared_error: 0.3700\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37003, saving model to temp/f1\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3664 - mean_squared_error: 0.3664 - val_loss: 0.3616 - val_mean_squared_error: 0.3616\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37003 to 0.36161, saving model to temp/f1\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3607 - mean_squared_error: 0.3607 - val_loss: 0.3681 - val_mean_squared_error: 0.3681\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36161\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3588 - mean_squared_error: 0.3588 - val_loss: 0.3552 - val_mean_squared_error: 0.3552\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36161 to 0.35525, saving model to temp/f1\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3542 - val_mean_squared_error: 0.3542\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35525 to 0.35416, saving model to temp/f1\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3571 - val_mean_squared_error: 0.3571\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35416\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3533 - val_mean_squared_error: 0.3533\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35416 to 0.35328, saving model to temp/f1\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3582 - mean_squared_error: 0.3582 - val_loss: 0.3521 - val_mean_squared_error: 0.3521\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35328 to 0.35210, saving model to temp/f1\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3554 - mean_squared_error: 0.3554 - val_loss: 0.3499 - val_mean_squared_error: 0.3499\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35210 to 0.34989, saving model to temp/f1\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3539 - val_mean_squared_error: 0.3539\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.34989\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3532 - mean_squared_error: 0.3532 - val_loss: 0.3523 - val_mean_squared_error: 0.3523\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.34989\n",
      "Epoch 00011: early stopping\n",
      "temp/f2\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4530 - mean_squared_error: 0.4530 - val_loss: 0.3634 - val_mean_squared_error: 0.3634\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36343, saving model to temp/f2\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3648 - mean_squared_error: 0.3648 - val_loss: 0.3657 - val_mean_squared_error: 0.3657\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36343\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3609 - mean_squared_error: 0.3609 - val_loss: 0.3610 - val_mean_squared_error: 0.3610\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36343 to 0.36097, saving model to temp/f2\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3602 - mean_squared_error: 0.3602 - val_loss: 0.3568 - val_mean_squared_error: 0.3568\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36097 to 0.35685, saving model to temp/f2\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3598 - val_mean_squared_error: 0.3598\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35685\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3564 - val_mean_squared_error: 0.3564\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35685 to 0.35641, saving model to temp/f2\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3567 - mean_squared_error: 0.3567 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35641 to 0.35502, saving model to temp/f2\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3575 - mean_squared_error: 0.3575 - val_loss: 0.3563 - val_mean_squared_error: 0.3563\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35502\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3565 - mean_squared_error: 0.3565 - val_loss: 0.3528 - val_mean_squared_error: 0.3528\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35502 to 0.35281, saving model to temp/f2\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3539 - mean_squared_error: 0.3539 - val_loss: 0.3510 - val_mean_squared_error: 0.3510\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.35281 to 0.35102, saving model to temp/f2\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3555 - mean_squared_error: 0.3555 - val_loss: 0.3507 - val_mean_squared_error: 0.3507\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.35102 to 0.35073, saving model to temp/f2\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3517 - mean_squared_error: 0.3517 - val_loss: 0.3521 - val_mean_squared_error: 0.3521\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35073\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3534 - mean_squared_error: 0.3534 - val_loss: 0.3565 - val_mean_squared_error: 0.3565\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.35073\n",
      "Epoch 00013: early stopping\n",
      "temp/f3\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4546 - mean_squared_error: 0.4546 - val_loss: 0.3669 - val_mean_squared_error: 0.3669\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36692, saving model to temp/f3\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3643 - mean_squared_error: 0.3643 - val_loss: 0.3655 - val_mean_squared_error: 0.3655\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36692 to 0.36547, saving model to temp/f3\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3618 - mean_squared_error: 0.3618 - val_loss: 0.3635 - val_mean_squared_error: 0.3635\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36547 to 0.36352, saving model to temp/f3\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3602 - mean_squared_error: 0.3602 - val_loss: 0.3567 - val_mean_squared_error: 0.3567\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36352 to 0.35671, saving model to temp/f3\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3595 - mean_squared_error: 0.3595 - val_loss: 0.3551 - val_mean_squared_error: 0.3551\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35671 to 0.35509, saving model to temp/f3\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3568 - mean_squared_error: 0.3568 - val_loss: 0.3541 - val_mean_squared_error: 0.3541\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35509 to 0.35410, saving model to temp/f3\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3569 - mean_squared_error: 0.3569 - val_loss: 0.3557 - val_mean_squared_error: 0.3557\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35410\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.3552 - mean_squared_error: 0.3552 - val_loss: 0.3515 - val_mean_squared_error: 0.3515\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35410 to 0.35147, saving model to temp/f3\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3538 - mean_squared_error: 0.3538 - val_loss: 0.3517 - val_mean_squared_error: 0.3517\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35147\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3562 - mean_squared_error: 0.3562 - val_loss: 0.3535 - val_mean_squared_error: 0.3535\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35147\n",
      "Epoch 00010: early stopping\n",
      "temp/f4\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 0.4533 - mean_squared_error: 0.4533 - val_loss: 0.3686 - val_mean_squared_error: 0.3686\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36856, saving model to temp/f4\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3647 - mean_squared_error: 0.3647 - val_loss: 0.3755 - val_mean_squared_error: 0.3755\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36856\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3607 - mean_squared_error: 0.3607 - val_loss: 0.3596 - val_mean_squared_error: 0.3596\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36856 to 0.35959, saving model to temp/f4\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3618 - mean_squared_error: 0.3618 - val_loss: 0.3572 - val_mean_squared_error: 0.3572\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35959 to 0.35722, saving model to temp/f4\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3597 - mean_squared_error: 0.3597 - val_loss: 0.3577 - val_mean_squared_error: 0.3577\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35722\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3597 - mean_squared_error: 0.3597 - val_loss: 0.3638 - val_mean_squared_error: 0.3638\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35722\n",
      "Epoch 00006: early stopping\n",
      "temp/f5\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4477 - mean_squared_error: 0.4477 - val_loss: 0.3734 - val_mean_squared_error: 0.3734\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37344, saving model to temp/f5\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3659 - mean_squared_error: 0.3659 - val_loss: 0.3600 - val_mean_squared_error: 0.3600\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37344 to 0.36001, saving model to temp/f5\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3630 - mean_squared_error: 0.3630 - val_loss: 0.3627 - val_mean_squared_error: 0.3627\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36001\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3586 - mean_squared_error: 0.3586 - val_loss: 0.3670 - val_mean_squared_error: 0.3670\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36001\n",
      "Epoch 00004: early stopping\n",
      "temp/f6\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4575 - mean_squared_error: 0.4575 - val_loss: 0.3682 - val_mean_squared_error: 0.3682\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36820, saving model to temp/f6\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3658 - mean_squared_error: 0.3658 - val_loss: 0.3663 - val_mean_squared_error: 0.3663\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36820 to 0.36633, saving model to temp/f6\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3643 - mean_squared_error: 0.3643 - val_loss: 0.3647 - val_mean_squared_error: 0.3647\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36633 to 0.36472, saving model to temp/f6\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3589 - val_mean_squared_error: 0.3589\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36472 to 0.35888, saving model to temp/f6\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3581 - mean_squared_error: 0.3581 - val_loss: 0.3526 - val_mean_squared_error: 0.3526\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35888 to 0.35259, saving model to temp/f6\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3580 - mean_squared_error: 0.3580 - val_loss: 0.3543 - val_mean_squared_error: 0.3543\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35259\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3580 - mean_squared_error: 0.3580 - val_loss: 0.3588 - val_mean_squared_error: 0.3588\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35259\n",
      "Epoch 00007: early stopping\n",
      "temp/f7\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4457 - mean_squared_error: 0.4457 - val_loss: 0.3681 - val_mean_squared_error: 0.3681\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36812, saving model to temp/f7\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3661 - mean_squared_error: 0.3661 - val_loss: 0.3567 - val_mean_squared_error: 0.3567\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36812 to 0.35668, saving model to temp/f7\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3656 - mean_squared_error: 0.3656 - val_loss: 0.3585 - val_mean_squared_error: 0.3585\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35668\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3604 - mean_squared_error: 0.3604 - val_loss: 0.3620 - val_mean_squared_error: 0.3620\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35668\n",
      "Epoch 00004: early stopping\n",
      "temp/f8\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 0.4568 - mean_squared_error: 0.4568 - val_loss: 0.3671 - val_mean_squared_error: 0.3671\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36714, saving model to temp/f8\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3660 - mean_squared_error: 0.3660 - val_loss: 0.3584 - val_mean_squared_error: 0.3584\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36714 to 0.35840, saving model to temp/f8\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3600 - mean_squared_error: 0.3600 - val_loss: 0.3585 - val_mean_squared_error: 0.3585\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35840\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3628 - mean_squared_error: 0.3628 - val_loss: 0.3595 - val_mean_squared_error: 0.3595\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35840\n",
      "Epoch 00004: early stopping\n",
      "temp/f9\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 0.4500 - mean_squared_error: 0.4500 - val_loss: 0.3646 - val_mean_squared_error: 0.3646\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36455, saving model to temp/f9\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3651 - mean_squared_error: 0.3651 - val_loss: 0.3615 - val_mean_squared_error: 0.3615\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36455 to 0.36147, saving model to temp/f9\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3623 - mean_squared_error: 0.3623 - val_loss: 0.3743 - val_mean_squared_error: 0.3743\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36147\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3615 - mean_squared_error: 0.3615 - val_loss: 0.3577 - val_mean_squared_error: 0.3577\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36147 to 0.35766, saving model to temp/f9\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3592 - mean_squared_error: 0.3592 - val_loss: 0.3551 - val_mean_squared_error: 0.3551\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35766 to 0.35515, saving model to temp/f9\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3629 - mean_squared_error: 0.3629 - val_loss: 0.3555 - val_mean_squared_error: 0.3555\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35515\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3569 - mean_squared_error: 0.3569 - val_loss: 0.3621 - val_mean_squared_error: 0.3621\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35515\n",
      "Epoch 00007: early stopping\n",
      "temp/f10\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 0.4507 - mean_squared_error: 0.4507 - val_loss: 0.3710 - val_mean_squared_error: 0.3710\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37105, saving model to temp/f10\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3644 - mean_squared_error: 0.3644 - val_loss: 0.3623 - val_mean_squared_error: 0.3623\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37105 to 0.36234, saving model to temp/f10\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3600 - mean_squared_error: 0.3600 - val_loss: 0.3708 - val_mean_squared_error: 0.3708\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36234\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3619 - mean_squared_error: 0.3619 - val_loss: 0.3631 - val_mean_squared_error: 0.3631\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36234\n",
      "Epoch 00004: early stopping\n",
      "temp/f11\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4456 - mean_squared_error: 0.4456 - val_loss: 0.3632 - val_mean_squared_error: 0.3632\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36324, saving model to temp/f11\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3637 - mean_squared_error: 0.3637 - val_loss: 0.3603 - val_mean_squared_error: 0.3603\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36324 to 0.36028, saving model to temp/f11\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3632 - mean_squared_error: 0.3632 - val_loss: 0.3629 - val_mean_squared_error: 0.3629\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36028\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3599 - mean_squared_error: 0.3599 - val_loss: 0.3571 - val_mean_squared_error: 0.3571\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36028 to 0.35707, saving model to temp/f11\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3626 - mean_squared_error: 0.3626 - val_loss: 0.3551 - val_mean_squared_error: 0.3551\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35707 to 0.35509, saving model to temp/f11\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3710 - val_mean_squared_error: 0.3710\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35509\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3564 - mean_squared_error: 0.3564 - val_loss: 0.3540 - val_mean_squared_error: 0.3540\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35509 to 0.35404, saving model to temp/f11\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3571 - mean_squared_error: 0.3571 - val_loss: 0.3548 - val_mean_squared_error: 0.3548\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35404\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3548 - mean_squared_error: 0.3548 - val_loss: 0.3587 - val_mean_squared_error: 0.3587\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35404\n",
      "Epoch 00009: early stopping\n",
      "temp/f12\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4484 - mean_squared_error: 0.4484 - val_loss: 0.3616 - val_mean_squared_error: 0.3616\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36157, saving model to temp/f12\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3653 - mean_squared_error: 0.3653 - val_loss: 0.3677 - val_mean_squared_error: 0.3677\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36157\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3618 - mean_squared_error: 0.3618 - val_loss: 0.3764 - val_mean_squared_error: 0.3764\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36157\n",
      "Epoch 00003: early stopping\n",
      "temp/f13\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4567 - mean_squared_error: 0.4567 - val_loss: 0.3636 - val_mean_squared_error: 0.3636\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36361, saving model to temp/f13\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3645 - mean_squared_error: 0.3645 - val_loss: 0.3727 - val_mean_squared_error: 0.3727\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36361\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3633 - mean_squared_error: 0.3633 - val_loss: 0.3585 - val_mean_squared_error: 0.3585\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36361 to 0.35846, saving model to temp/f13\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3608 - mean_squared_error: 0.3608 - val_loss: 0.3587 - val_mean_squared_error: 0.3587\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35846\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3579 - mean_squared_error: 0.3579 - val_loss: 0.3591 - val_mean_squared_error: 0.3591\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35846\n",
      "Epoch 00005: early stopping\n",
      "temp/f14\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4524 - mean_squared_error: 0.4524 - val_loss: 0.3610 - val_mean_squared_error: 0.3610\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36097, saving model to temp/f14\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3666 - mean_squared_error: 0.3666 - val_loss: 0.3655 - val_mean_squared_error: 0.3655\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36097\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3634 - mean_squared_error: 0.3634 - val_loss: 0.3564 - val_mean_squared_error: 0.3564\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36097 to 0.35644, saving model to temp/f14\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3613 - mean_squared_error: 0.3613 - val_loss: 0.3564 - val_mean_squared_error: 0.3564\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35644\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3595 - mean_squared_error: 0.3595 - val_loss: 0.3521 - val_mean_squared_error: 0.3521\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35644 to 0.35211, saving model to temp/f14\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3575 - mean_squared_error: 0.3575 - val_loss: 0.3633 - val_mean_squared_error: 0.3633\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35211\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3566 - mean_squared_error: 0.3566 - val_loss: 0.3568 - val_mean_squared_error: 0.3568\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35211\n",
      "Epoch 00007: early stopping\n",
      "temp/f15\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4596 - mean_squared_error: 0.4596 - val_loss: 0.3636 - val_mean_squared_error: 0.3636\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36361, saving model to temp/f15\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3664 - mean_squared_error: 0.3664 - val_loss: 0.3619 - val_mean_squared_error: 0.3619\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36361 to 0.36191, saving model to temp/f15\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3629 - mean_squared_error: 0.3629 - val_loss: 0.3738 - val_mean_squared_error: 0.3738\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36191\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3621 - mean_squared_error: 0.3621 - val_loss: 0.3682 - val_mean_squared_error: 0.3682\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36191\n",
      "Epoch 00004: early stopping\n",
      "temp/f16\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4514 - mean_squared_error: 0.4514 - val_loss: 0.3638 - val_mean_squared_error: 0.3638\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36380, saving model to temp/f16\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3674 - mean_squared_error: 0.3674 - val_loss: 0.3618 - val_mean_squared_error: 0.3618\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36380 to 0.36182, saving model to temp/f16\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3635 - mean_squared_error: 0.3635 - val_loss: 0.3563 - val_mean_squared_error: 0.3563\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36182 to 0.35632, saving model to temp/f16\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3590 - mean_squared_error: 0.3590 - val_loss: 0.3665 - val_mean_squared_error: 0.3665\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35632\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3599 - mean_squared_error: 0.3599 - val_loss: 0.3537 - val_mean_squared_error: 0.3537\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35632 to 0.35368, saving model to temp/f16\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3580 - mean_squared_error: 0.3580 - val_loss: 0.3526 - val_mean_squared_error: 0.3526\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35368 to 0.35260, saving model to temp/f16\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3602 - mean_squared_error: 0.3602 - val_loss: 0.3520 - val_mean_squared_error: 0.3520\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35260 to 0.35196, saving model to temp/f16\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3569 - mean_squared_error: 0.3569 - val_loss: 0.3555 - val_mean_squared_error: 0.3555\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35196\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3571 - mean_squared_error: 0.3571 - val_loss: 0.3524 - val_mean_squared_error: 0.3524\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35196\n",
      "Epoch 00009: early stopping\n",
      "temp/f17\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4518 - mean_squared_error: 0.4518 - val_loss: 0.3683 - val_mean_squared_error: 0.3683\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36832, saving model to temp/f17\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3658 - mean_squared_error: 0.3658 - val_loss: 0.3605 - val_mean_squared_error: 0.3605\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36832 to 0.36055, saving model to temp/f17\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3602 - mean_squared_error: 0.3602 - val_loss: 0.3560 - val_mean_squared_error: 0.3560\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36055 to 0.35597, saving model to temp/f17\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3624 - mean_squared_error: 0.3624 - val_loss: 0.3568 - val_mean_squared_error: 0.3568\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35597\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3611 - mean_squared_error: 0.3611 - val_loss: 0.3539 - val_mean_squared_error: 0.3539\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35597 to 0.35386, saving model to temp/f17\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3575 - mean_squared_error: 0.3575 - val_loss: 0.3616 - val_mean_squared_error: 0.3616\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35386\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3546 - val_mean_squared_error: 0.3546\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35386\n",
      "Epoch 00007: early stopping\n",
      "temp/f18\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4519 - mean_squared_error: 0.4519 - val_loss: 0.3659 - val_mean_squared_error: 0.3659\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36586, saving model to temp/f18\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3678 - mean_squared_error: 0.3678 - val_loss: 0.3598 - val_mean_squared_error: 0.3598\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36586 to 0.35982, saving model to temp/f18\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3629 - mean_squared_error: 0.3629 - val_loss: 0.3614 - val_mean_squared_error: 0.3614\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35982\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3584 - val_mean_squared_error: 0.3584\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35982 to 0.35841, saving model to temp/f18\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3591 - mean_squared_error: 0.3591 - val_loss: 0.3689 - val_mean_squared_error: 0.3689\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35841\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3583 - mean_squared_error: 0.3583 - val_loss: 0.3652 - val_mean_squared_error: 0.3652\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35841\n",
      "Epoch 00006: early stopping\n",
      "temp/f19\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4442 - mean_squared_error: 0.4442 - val_loss: 0.3677 - val_mean_squared_error: 0.3677\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36771, saving model to temp/f19\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3655 - mean_squared_error: 0.3655 - val_loss: 0.3732 - val_mean_squared_error: 0.3732\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36771\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3624 - mean_squared_error: 0.3624 - val_loss: 0.3573 - val_mean_squared_error: 0.3573\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36771 to 0.35727, saving model to temp/f19\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3588 - mean_squared_error: 0.3588 - val_loss: 0.3543 - val_mean_squared_error: 0.3543\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35727 to 0.35431, saving model to temp/f19\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3609 - mean_squared_error: 0.3609 - val_loss: 0.3534 - val_mean_squared_error: 0.3534\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35431 to 0.35344, saving model to temp/f19\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3569 - val_mean_squared_error: 0.3569\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35344\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3556 - mean_squared_error: 0.3556 - val_loss: 0.3531 - val_mean_squared_error: 0.3531\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35344 to 0.35314, saving model to temp/f19\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3556 - mean_squared_error: 0.3556 - val_loss: 0.3594 - val_mean_squared_error: 0.3594\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35314\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3568 - mean_squared_error: 0.3568 - val_loss: 0.3527 - val_mean_squared_error: 0.3527\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35314 to 0.35274, saving model to temp/f19\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3535 - mean_squared_error: 0.3535 - val_loss: 0.3485 - val_mean_squared_error: 0.3485\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.35274 to 0.34853, saving model to temp/f19\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3534 - mean_squared_error: 0.3534 - val_loss: 0.3517 - val_mean_squared_error: 0.3517\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.34853\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3528 - mean_squared_error: 0.3528 - val_loss: 0.3629 - val_mean_squared_error: 0.3629\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.34853\n",
      "Epoch 00012: early stopping\n",
      "temp/f20\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4607 - mean_squared_error: 0.4607 - val_loss: 0.3652 - val_mean_squared_error: 0.3652\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36517, saving model to temp/f20\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3674 - mean_squared_error: 0.3674 - val_loss: 0.3579 - val_mean_squared_error: 0.3579\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36517 to 0.35789, saving model to temp/f20\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3632 - mean_squared_error: 0.3632 - val_loss: 0.3651 - val_mean_squared_error: 0.3651\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35789\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3583 - mean_squared_error: 0.3583 - val_loss: 0.3597 - val_mean_squared_error: 0.3597\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35789\n",
      "Epoch 00004: early stopping\n",
      "temp/f21\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4506 - mean_squared_error: 0.4506 - val_loss: 0.3655 - val_mean_squared_error: 0.3655\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36549, saving model to temp/f21\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3675 - mean_squared_error: 0.3675 - val_loss: 0.3748 - val_mean_squared_error: 0.3748\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36549\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3636 - mean_squared_error: 0.3636 - val_loss: 0.3610 - val_mean_squared_error: 0.3610\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36549 to 0.36100, saving model to temp/f21\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3632 - mean_squared_error: 0.3632 - val_loss: 0.3551 - val_mean_squared_error: 0.3551\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36100 to 0.35509, saving model to temp/f21\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3547 - val_mean_squared_error: 0.3547\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35509 to 0.35467, saving model to temp/f21\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3641 - val_mean_squared_error: 0.3641\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35467\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3578 - mean_squared_error: 0.3578 - val_loss: 0.3526 - val_mean_squared_error: 0.3526\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35467 to 0.35258, saving model to temp/f21\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3554 - mean_squared_error: 0.3554 - val_loss: 0.3538 - val_mean_squared_error: 0.3538\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35258\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3551 - mean_squared_error: 0.3551 - val_loss: 0.3618 - val_mean_squared_error: 0.3618\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35258\n",
      "Epoch 00009: early stopping\n",
      "temp/f22\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4590 - mean_squared_error: 0.4590 - val_loss: 0.3659 - val_mean_squared_error: 0.3659\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36588, saving model to temp/f22\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3626 - mean_squared_error: 0.3626 - val_loss: 0.3584 - val_mean_squared_error: 0.3584\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36588 to 0.35836, saving model to temp/f22\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3623 - mean_squared_error: 0.3623 - val_loss: 0.3557 - val_mean_squared_error: 0.3557\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35836 to 0.35575, saving model to temp/f22\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3582 - mean_squared_error: 0.3582 - val_loss: 0.3612 - val_mean_squared_error: 0.3612\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35575\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3612 - mean_squared_error: 0.3612 - val_loss: 0.3544 - val_mean_squared_error: 0.3544\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35575 to 0.35442, saving model to temp/f22\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3552 - val_mean_squared_error: 0.3552\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35442\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3580 - mean_squared_error: 0.3580 - val_loss: 0.3531 - val_mean_squared_error: 0.3531\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35442 to 0.35311, saving model to temp/f22\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3563 - mean_squared_error: 0.3563 - val_loss: 0.3596 - val_mean_squared_error: 0.3596\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35311\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3557 - mean_squared_error: 0.3557 - val_loss: 0.3525 - val_mean_squared_error: 0.3525\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35311 to 0.35246, saving model to temp/f22\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3549 - mean_squared_error: 0.3549 - val_loss: 0.3540 - val_mean_squared_error: 0.3540\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35246\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3551 - mean_squared_error: 0.3551 - val_loss: 0.3522 - val_mean_squared_error: 0.3522\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.35246 to 0.35222, saving model to temp/f22\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3536 - mean_squared_error: 0.3536 - val_loss: 0.3521 - val_mean_squared_error: 0.3521\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.35222 to 0.35208, saving model to temp/f22\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3531 - mean_squared_error: 0.3531 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00013: val_loss did not improve from 0.35208\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3540 - mean_squared_error: 0.3540 - val_loss: 0.3534 - val_mean_squared_error: 0.3534\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.35208\n",
      "Epoch 00014: early stopping\n",
      "temp/f23\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4471 - mean_squared_error: 0.4471 - val_loss: 0.3663 - val_mean_squared_error: 0.3663\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36626, saving model to temp/f23\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3626 - mean_squared_error: 0.3626 - val_loss: 0.3643 - val_mean_squared_error: 0.3643\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36626 to 0.36434, saving model to temp/f23\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3618 - mean_squared_error: 0.3618 - val_loss: 0.3692 - val_mean_squared_error: 0.3692\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36434\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3612 - mean_squared_error: 0.3612 - val_loss: 0.3555 - val_mean_squared_error: 0.3555\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36434 to 0.35549, saving model to temp/f23\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3576 - val_mean_squared_error: 0.3576\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35549\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3594 - mean_squared_error: 0.3594 - val_loss: 0.3535 - val_mean_squared_error: 0.3535\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35549 to 0.35350, saving model to temp/f23\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3536 - val_mean_squared_error: 0.3536\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35350\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3569 - mean_squared_error: 0.3569 - val_loss: 0.3543 - val_mean_squared_error: 0.3543\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35350\n",
      "Epoch 00008: early stopping\n",
      "temp/f24\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 0.4574 - mean_squared_error: 0.4574 - val_loss: 0.3734 - val_mean_squared_error: 0.3734\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37335, saving model to temp/f24\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3654 - mean_squared_error: 0.3654 - val_loss: 0.3614 - val_mean_squared_error: 0.3614\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37335 to 0.36141, saving model to temp/f24\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3639 - mean_squared_error: 0.3639 - val_loss: 0.3553 - val_mean_squared_error: 0.3553\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36141 to 0.35528, saving model to temp/f24\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3600 - mean_squared_error: 0.3600 - val_loss: 0.3577 - val_mean_squared_error: 0.3577\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35528\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3582 - mean_squared_error: 0.3582 - val_loss: 0.3585 - val_mean_squared_error: 0.3585\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35528\n",
      "Epoch 00005: early stopping\n",
      "temp/f25\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4527 - mean_squared_error: 0.4527 - val_loss: 0.3642 - val_mean_squared_error: 0.3642\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36418, saving model to temp/f25\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3666 - mean_squared_error: 0.3666 - val_loss: 0.3708 - val_mean_squared_error: 0.3708\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36418\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3634 - mean_squared_error: 0.3634 - val_loss: 0.3540 - val_mean_squared_error: 0.3540\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36418 to 0.35403, saving model to temp/f25\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3615 - mean_squared_error: 0.3615 - val_loss: 0.3591 - val_mean_squared_error: 0.3591\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35403\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3623 - mean_squared_error: 0.3623 - val_loss: 0.3567 - val_mean_squared_error: 0.3567\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35403\n",
      "Epoch 00005: early stopping\n",
      "temp/f26\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4524 - mean_squared_error: 0.4524 - val_loss: 0.3790 - val_mean_squared_error: 0.3790\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37903, saving model to temp/f26\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3656 - mean_squared_error: 0.3656 - val_loss: 0.3651 - val_mean_squared_error: 0.3651\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37903 to 0.36507, saving model to temp/f26\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3628 - mean_squared_error: 0.3628 - val_loss: 0.3681 - val_mean_squared_error: 0.3681\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36507\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3589 - mean_squared_error: 0.3589 - val_loss: 0.3581 - val_mean_squared_error: 0.3581\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36507 to 0.35812, saving model to temp/f26\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3556 - val_mean_squared_error: 0.3556\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35812 to 0.35563, saving model to temp/f26\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3591 - mean_squared_error: 0.3591 - val_loss: 0.3556 - val_mean_squared_error: 0.3556\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35563 to 0.35561, saving model to temp/f26\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3517 - val_mean_squared_error: 0.3517\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35561 to 0.35167, saving model to temp/f26\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3559 - mean_squared_error: 0.3559 - val_loss: 0.3566 - val_mean_squared_error: 0.3566\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35167\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3519 - mean_squared_error: 0.3519 - val_loss: 0.3555 - val_mean_squared_error: 0.3555\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35167\n",
      "Epoch 00009: early stopping\n",
      "temp/f27\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4630 - mean_squared_error: 0.4630 - val_loss: 0.3636 - val_mean_squared_error: 0.3636\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36360, saving model to temp/f27\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3677 - mean_squared_error: 0.3677 - val_loss: 0.3604 - val_mean_squared_error: 0.3604\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36360 to 0.36041, saving model to temp/f27\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3638 - mean_squared_error: 0.3638 - val_loss: 0.3561 - val_mean_squared_error: 0.3561\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36041 to 0.35612, saving model to temp/f27\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3610 - mean_squared_error: 0.3610 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35612\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3593 - mean_squared_error: 0.3593 - val_loss: 0.3600 - val_mean_squared_error: 0.3600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35612\n",
      "Epoch 00005: early stopping\n",
      "temp/f28\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4501 - mean_squared_error: 0.4501 - val_loss: 0.3635 - val_mean_squared_error: 0.3635\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36355, saving model to temp/f28\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3650 - mean_squared_error: 0.3650 - val_loss: 0.3586 - val_mean_squared_error: 0.3586\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36355 to 0.35860, saving model to temp/f28\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3614 - mean_squared_error: 0.3614 - val_loss: 0.3560 - val_mean_squared_error: 0.3560\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35860 to 0.35603, saving model to temp/f28\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3603 - mean_squared_error: 0.3603 - val_loss: 0.3616 - val_mean_squared_error: 0.3616\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35603\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3568 - mean_squared_error: 0.3568 - val_loss: 0.3561 - val_mean_squared_error: 0.3561\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35603\n",
      "Epoch 00005: early stopping\n",
      "temp/f29\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 0.4492 - mean_squared_error: 0.4492 - val_loss: 0.3648 - val_mean_squared_error: 0.3648\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36484, saving model to temp/f29\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3643 - mean_squared_error: 0.3643 - val_loss: 0.3646 - val_mean_squared_error: 0.3646\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36484 to 0.36456, saving model to temp/f29\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3596 - mean_squared_error: 0.3596 - val_loss: 0.3575 - val_mean_squared_error: 0.3575\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36456 to 0.35746, saving model to temp/f29\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3562 - val_mean_squared_error: 0.3562\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35746 to 0.35624, saving model to temp/f29\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3598 - mean_squared_error: 0.3598 - val_loss: 0.3610 - val_mean_squared_error: 0.3610\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35624\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3584 - mean_squared_error: 0.3584 - val_loss: 0.3575 - val_mean_squared_error: 0.3575\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35624\n",
      "Epoch 00006: early stopping\n",
      "temp/f30\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4503 - mean_squared_error: 0.4503 - val_loss: 0.3614 - val_mean_squared_error: 0.3614\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36142, saving model to temp/f30\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3658 - mean_squared_error: 0.3658 - val_loss: 0.3582 - val_mean_squared_error: 0.3582\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36142 to 0.35815, saving model to temp/f30\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3609 - mean_squared_error: 0.3609 - val_loss: 0.3735 - val_mean_squared_error: 0.3735\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35815\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3617 - mean_squared_error: 0.3617 - val_loss: 0.3547 - val_mean_squared_error: 0.3547\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35815 to 0.35465, saving model to temp/f30\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3563 - mean_squared_error: 0.3563 - val_loss: 0.3653 - val_mean_squared_error: 0.3653\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35465\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3530 - val_mean_squared_error: 0.3530\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35465 to 0.35304, saving model to temp/f30\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3558 - mean_squared_error: 0.3558 - val_loss: 0.3572 - val_mean_squared_error: 0.3572\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35304\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3541 - mean_squared_error: 0.3541 - val_loss: 0.3594 - val_mean_squared_error: 0.3594\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35304\n",
      "Epoch 00008: early stopping\n",
      "temp/f31\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4470 - mean_squared_error: 0.4470 - val_loss: 0.3664 - val_mean_squared_error: 0.3664\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36642, saving model to temp/f31\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3635 - mean_squared_error: 0.3635 - val_loss: 0.3632 - val_mean_squared_error: 0.3632\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36642 to 0.36325, saving model to temp/f31\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3629 - mean_squared_error: 0.3629 - val_loss: 0.3682 - val_mean_squared_error: 0.3682\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36325\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3601 - mean_squared_error: 0.3601 - val_loss: 0.3615 - val_mean_squared_error: 0.3615\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36325 to 0.36147, saving model to temp/f31\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3543 - val_mean_squared_error: 0.3543\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.36147 to 0.35425, saving model to temp/f31\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3557 - mean_squared_error: 0.3557 - val_loss: 0.3648 - val_mean_squared_error: 0.3648\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35425\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3570 - mean_squared_error: 0.3570 - val_loss: 0.3581 - val_mean_squared_error: 0.3581\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35425\n",
      "Epoch 00007: early stopping\n",
      "temp/f32\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4527 - mean_squared_error: 0.4527 - val_loss: 0.3811 - val_mean_squared_error: 0.3811\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.38113, saving model to temp/f32\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3667 - mean_squared_error: 0.3667 - val_loss: 0.3595 - val_mean_squared_error: 0.3595\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.38113 to 0.35948, saving model to temp/f32\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3621 - mean_squared_error: 0.3621 - val_loss: 0.3563 - val_mean_squared_error: 0.3563\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35948 to 0.35628, saving model to temp/f32\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3619 - mean_squared_error: 0.3619 - val_loss: 0.3567 - val_mean_squared_error: 0.3567\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35628\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3584 - mean_squared_error: 0.3584 - val_loss: 0.3574 - val_mean_squared_error: 0.3574\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35628\n",
      "Epoch 00005: early stopping\n",
      "temp/f33\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4536 - mean_squared_error: 0.4536 - val_loss: 0.3620 - val_mean_squared_error: 0.3620\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36196, saving model to temp/f33\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3665 - mean_squared_error: 0.3665 - val_loss: 0.3677 - val_mean_squared_error: 0.3677\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36196\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3627 - mean_squared_error: 0.3627 - val_loss: 0.3583 - val_mean_squared_error: 0.3583\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36196 to 0.35834, saving model to temp/f33\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3600 - mean_squared_error: 0.3600 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35834 to 0.35503, saving model to temp/f33\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3603 - mean_squared_error: 0.3603 - val_loss: 0.3618 - val_mean_squared_error: 0.3618\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35503\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3581 - mean_squared_error: 0.3581 - val_loss: 0.3618 - val_mean_squared_error: 0.3618\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35503\n",
      "Epoch 00006: early stopping\n",
      "temp/f34\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 0.4499 - mean_squared_error: 0.4499 - val_loss: 0.3685 - val_mean_squared_error: 0.3685\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36849, saving model to temp/f34\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3651 - mean_squared_error: 0.3651 - val_loss: 0.3618 - val_mean_squared_error: 0.3618\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36849 to 0.36182, saving model to temp/f34\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3635 - mean_squared_error: 0.3635 - val_loss: 0.3615 - val_mean_squared_error: 0.3615\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36182 to 0.36150, saving model to temp/f34\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3604 - mean_squared_error: 0.3604 - val_loss: 0.3595 - val_mean_squared_error: 0.3595\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36150 to 0.35954, saving model to temp/f34\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3606 - mean_squared_error: 0.3606 - val_loss: 0.3586 - val_mean_squared_error: 0.3586\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35954 to 0.35857, saving model to temp/f34\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3596 - mean_squared_error: 0.3596 - val_loss: 0.3562 - val_mean_squared_error: 0.3562\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35857 to 0.35623, saving model to temp/f34\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3578 - mean_squared_error: 0.3578 - val_loss: 0.3544 - val_mean_squared_error: 0.3544\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35623 to 0.35441, saving model to temp/f34\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3563 - mean_squared_error: 0.3563 - val_loss: 0.3546 - val_mean_squared_error: 0.3546\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35441\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3540 - mean_squared_error: 0.3540 - val_loss: 0.3509 - val_mean_squared_error: 0.3509\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35441 to 0.35092, saving model to temp/f34\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3550 - mean_squared_error: 0.3550 - val_loss: 0.3515 - val_mean_squared_error: 0.3515\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35092\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3514 - val_mean_squared_error: 0.3514\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35092\n",
      "Epoch 00011: early stopping\n",
      "temp/f35\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4544 - mean_squared_error: 0.4544 - val_loss: 0.3657 - val_mean_squared_error: 0.3657\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36565, saving model to temp/f35\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3659 - mean_squared_error: 0.3659 - val_loss: 0.3589 - val_mean_squared_error: 0.3589\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36565 to 0.35890, saving model to temp/f35\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3627 - mean_squared_error: 0.3627 - val_loss: 0.3566 - val_mean_squared_error: 0.3566\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35890 to 0.35656, saving model to temp/f35\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3604 - mean_squared_error: 0.3604 - val_loss: 0.3577 - val_mean_squared_error: 0.3577\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35656\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3596 - mean_squared_error: 0.3596 - val_loss: 0.3557 - val_mean_squared_error: 0.3557\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35656 to 0.35570, saving model to temp/f35\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3572 - mean_squared_error: 0.3572 - val_loss: 0.3656 - val_mean_squared_error: 0.3656\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35570\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3567 - mean_squared_error: 0.3567 - val_loss: 0.3568 - val_mean_squared_error: 0.3568\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35570\n",
      "Epoch 00007: early stopping\n",
      "temp/f36\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4545 - mean_squared_error: 0.4545 - val_loss: 0.3629 - val_mean_squared_error: 0.3629\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36289, saving model to temp/f36\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3666 - mean_squared_error: 0.3666 - val_loss: 0.3634 - val_mean_squared_error: 0.3634\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36289\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3668 - mean_squared_error: 0.3668 - val_loss: 0.3674 - val_mean_squared_error: 0.3674\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36289\n",
      "Epoch 00003: early stopping\n",
      "temp/f37\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4561 - mean_squared_error: 0.4561 - val_loss: 0.3628 - val_mean_squared_error: 0.3628\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36283, saving model to temp/f37\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3664 - mean_squared_error: 0.3664 - val_loss: 0.3631 - val_mean_squared_error: 0.3631\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36283\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3673 - mean_squared_error: 0.3673 - val_loss: 0.3592 - val_mean_squared_error: 0.3592\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36283 to 0.35916, saving model to temp/f37\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3591 - val_mean_squared_error: 0.3591\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35916 to 0.35911, saving model to temp/f37\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3633 - mean_squared_error: 0.3633 - val_loss: 0.3542 - val_mean_squared_error: 0.3542\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35911 to 0.35420, saving model to temp/f37\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3585 - mean_squared_error: 0.3585 - val_loss: 0.3536 - val_mean_squared_error: 0.3536\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35420 to 0.35360, saving model to temp/f37\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3569 - mean_squared_error: 0.3569 - val_loss: 0.3526 - val_mean_squared_error: 0.3526\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35360 to 0.35260, saving model to temp/f37\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3567 - mean_squared_error: 0.3567 - val_loss: 0.3547 - val_mean_squared_error: 0.3547\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35260\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3577 - mean_squared_error: 0.3577 - val_loss: 0.3553 - val_mean_squared_error: 0.3553\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35260\n",
      "Epoch 00009: early stopping\n",
      "temp/f38\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 0.4572 - mean_squared_error: 0.4572 - val_loss: 0.3644 - val_mean_squared_error: 0.3644\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36439, saving model to temp/f38\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3644 - mean_squared_error: 0.3644 - val_loss: 0.3655 - val_mean_squared_error: 0.3655\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36439\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3650 - mean_squared_error: 0.3650 - val_loss: 0.3661 - val_mean_squared_error: 0.3661\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36439\n",
      "Epoch 00003: early stopping\n",
      "temp/f39\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4516 - mean_squared_error: 0.4516 - val_loss: 0.3698 - val_mean_squared_error: 0.3698\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36983, saving model to temp/f39\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3672 - mean_squared_error: 0.3672 - val_loss: 0.3646 - val_mean_squared_error: 0.3646\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36983 to 0.36456, saving model to temp/f39\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3629 - mean_squared_error: 0.3629 - val_loss: 0.3571 - val_mean_squared_error: 0.3571\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36456 to 0.35713, saving model to temp/f39\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3618 - mean_squared_error: 0.3618 - val_loss: 0.3602 - val_mean_squared_error: 0.3602\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35713\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3564 - mean_squared_error: 0.3564 - val_loss: 0.3585 - val_mean_squared_error: 0.3585\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35713\n",
      "Epoch 00005: early stopping\n",
      "temp/f40\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4523 - mean_squared_error: 0.4523 - val_loss: 0.3668 - val_mean_squared_error: 0.3668\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36682, saving model to temp/f40\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3662 - mean_squared_error: 0.3662 - val_loss: 0.3667 - val_mean_squared_error: 0.3667\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36682 to 0.36669, saving model to temp/f40\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3600 - mean_squared_error: 0.3600 - val_loss: 0.3672 - val_mean_squared_error: 0.3672\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36669\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3606 - mean_squared_error: 0.3606 - val_loss: 0.3600 - val_mean_squared_error: 0.3600\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36669 to 0.36004, saving model to temp/f40\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3577 - mean_squared_error: 0.3577 - val_loss: 0.3541 - val_mean_squared_error: 0.3541\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.36004 to 0.35409, saving model to temp/f40\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3571 - mean_squared_error: 0.3571 - val_loss: 0.3618 - val_mean_squared_error: 0.3618\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35409\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3582 - mean_squared_error: 0.3582 - val_loss: 0.3587 - val_mean_squared_error: 0.3587\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35409\n",
      "Epoch 00007: early stopping\n",
      "temp/f41\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4596 - mean_squared_error: 0.4596 - val_loss: 0.3683 - val_mean_squared_error: 0.3683\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36833, saving model to temp/f41\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3657 - mean_squared_error: 0.3657 - val_loss: 0.3579 - val_mean_squared_error: 0.3579\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36833 to 0.35790, saving model to temp/f41\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3619 - mean_squared_error: 0.3619 - val_loss: 0.3570 - val_mean_squared_error: 0.3570\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35790 to 0.35702, saving model to temp/f41\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3555 - val_mean_squared_error: 0.3555\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35702 to 0.35548, saving model to temp/f41\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3612 - val_mean_squared_error: 0.3612\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35548\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3596 - mean_squared_error: 0.3596 - val_loss: 0.3603 - val_mean_squared_error: 0.3603\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35548\n",
      "Epoch 00006: early stopping\n",
      "temp/f42\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4519 - mean_squared_error: 0.4519 - val_loss: 0.3695 - val_mean_squared_error: 0.3695\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36954, saving model to temp/f42\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3634 - mean_squared_error: 0.3634 - val_loss: 0.3594 - val_mean_squared_error: 0.3594\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36954 to 0.35938, saving model to temp/f42\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3604 - mean_squared_error: 0.3604 - val_loss: 0.3600 - val_mean_squared_error: 0.3600\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35938\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3590 - mean_squared_error: 0.3590 - val_loss: 0.3559 - val_mean_squared_error: 0.3559\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35938 to 0.35594, saving model to temp/f42\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3613 - mean_squared_error: 0.3613 - val_loss: 0.3547 - val_mean_squared_error: 0.3547\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35594 to 0.35465, saving model to temp/f42\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3581 - mean_squared_error: 0.3581 - val_loss: 0.3552 - val_mean_squared_error: 0.3552\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35465\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3538 - mean_squared_error: 0.3538 - val_loss: 0.3571 - val_mean_squared_error: 0.3571\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35465\n",
      "Epoch 00007: early stopping\n",
      "temp/f43\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 0.4498 - mean_squared_error: 0.4498 - val_loss: 0.3642 - val_mean_squared_error: 0.3642\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36420, saving model to temp/f43\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3646 - mean_squared_error: 0.3646 - val_loss: 0.3591 - val_mean_squared_error: 0.3591\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36420 to 0.35912, saving model to temp/f43\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3678 - val_mean_squared_error: 0.3678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35912\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3606 - mean_squared_error: 0.3606 - val_loss: 0.3557 - val_mean_squared_error: 0.3557\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35912 to 0.35566, saving model to temp/f43\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3584 - mean_squared_error: 0.3584 - val_loss: 0.3531 - val_mean_squared_error: 0.3531\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35566 to 0.35314, saving model to temp/f43\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3558 - mean_squared_error: 0.3558 - val_loss: 0.3576 - val_mean_squared_error: 0.3576\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35314\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3570 - mean_squared_error: 0.3570 - val_loss: 0.3623 - val_mean_squared_error: 0.3623\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35314\n",
      "Epoch 00007: early stopping\n",
      "temp/f44\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4584 - mean_squared_error: 0.4584 - val_loss: 0.3652 - val_mean_squared_error: 0.3652\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36521, saving model to temp/f44\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3661 - mean_squared_error: 0.3661 - val_loss: 0.3569 - val_mean_squared_error: 0.3569\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36521 to 0.35688, saving model to temp/f44\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3635 - mean_squared_error: 0.3635 - val_loss: 0.3645 - val_mean_squared_error: 0.3645\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35688\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3607 - mean_squared_error: 0.3607 - val_loss: 0.3591 - val_mean_squared_error: 0.3591\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35688\n",
      "Epoch 00004: early stopping\n",
      "temp/f45\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4505 - mean_squared_error: 0.4505 - val_loss: 0.3629 - val_mean_squared_error: 0.3629\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36292, saving model to temp/f45\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3638 - mean_squared_error: 0.3638 - val_loss: 0.3760 - val_mean_squared_error: 0.3760\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36292\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3637 - mean_squared_error: 0.3637 - val_loss: 0.3673 - val_mean_squared_error: 0.3673\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36292\n",
      "Epoch 00003: early stopping\n",
      "temp/f46\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 0.4684 - mean_squared_error: 0.4684 - val_loss: 0.3718 - val_mean_squared_error: 0.3718\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37185, saving model to temp/f46\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3665 - mean_squared_error: 0.3665 - val_loss: 0.3604 - val_mean_squared_error: 0.3604\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37185 to 0.36042, saving model to temp/f46\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3579 - mean_squared_error: 0.3579 - val_loss: 0.3569 - val_mean_squared_error: 0.3569\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36042 to 0.35689, saving model to temp/f46\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3598 - mean_squared_error: 0.3598 - val_loss: 0.3576 - val_mean_squared_error: 0.3576\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35689\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3596 - mean_squared_error: 0.3596 - val_loss: 0.3508 - val_mean_squared_error: 0.3508\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35689 to 0.35080, saving model to temp/f46\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3579 - mean_squared_error: 0.3579 - val_loss: 0.3559 - val_mean_squared_error: 0.3559\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35080\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3588 - mean_squared_error: 0.3588 - val_loss: 0.3547 - val_mean_squared_error: 0.3547\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35080\n",
      "Epoch 00007: early stopping\n",
      "temp/f47\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4528 - mean_squared_error: 0.4528 - val_loss: 0.3635 - val_mean_squared_error: 0.3635\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36348, saving model to temp/f47\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3658 - mean_squared_error: 0.3658 - val_loss: 0.3632 - val_mean_squared_error: 0.3632\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36348 to 0.36325, saving model to temp/f47\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3624 - mean_squared_error: 0.3624 - val_loss: 0.3601 - val_mean_squared_error: 0.3601\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36325 to 0.36011, saving model to temp/f47\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3599 - mean_squared_error: 0.3599 - val_loss: 0.3569 - val_mean_squared_error: 0.3569\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36011 to 0.35687, saving model to temp/f47\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3591 - mean_squared_error: 0.3591 - val_loss: 0.3538 - val_mean_squared_error: 0.3538\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35687 to 0.35379, saving model to temp/f47\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3568 - mean_squared_error: 0.3568 - val_loss: 0.3565 - val_mean_squared_error: 0.3565\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35379\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3598 - mean_squared_error: 0.3598 - val_loss: 0.3691 - val_mean_squared_error: 0.3691\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35379\n",
      "Epoch 00007: early stopping\n",
      "temp/f48\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4566 - mean_squared_error: 0.4566 - val_loss: 0.3805 - val_mean_squared_error: 0.3805\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.38054, saving model to temp/f48\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3667 - mean_squared_error: 0.3667 - val_loss: 0.3608 - val_mean_squared_error: 0.3608\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.38054 to 0.36080, saving model to temp/f48\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3629 - mean_squared_error: 0.3629 - val_loss: 0.3664 - val_mean_squared_error: 0.3664\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36080\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3611 - mean_squared_error: 0.3611 - val_loss: 0.3626 - val_mean_squared_error: 0.3626\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36080\n",
      "Epoch 00004: early stopping\n",
      "temp/f49\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4647 - mean_squared_error: 0.4647 - val_loss: 0.3803 - val_mean_squared_error: 0.3803\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.38030, saving model to temp/f49\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3643 - mean_squared_error: 0.3643 - val_loss: 0.3583 - val_mean_squared_error: 0.3583\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.38030 to 0.35829, saving model to temp/f49\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3618 - mean_squared_error: 0.3618 - val_loss: 0.3613 - val_mean_squared_error: 0.3613\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35829\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3613 - mean_squared_error: 0.3613 - val_loss: 0.3627 - val_mean_squared_error: 0.3627\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35829\n",
      "Epoch 00004: early stopping\n",
      "temp/f50\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4691 - mean_squared_error: 0.4691 - val_loss: 0.3633 - val_mean_squared_error: 0.3633\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36326, saving model to temp/f50\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3668 - mean_squared_error: 0.3668 - val_loss: 0.3583 - val_mean_squared_error: 0.3583\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36326 to 0.35831, saving model to temp/f50\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3626 - mean_squared_error: 0.3626 - val_loss: 0.3572 - val_mean_squared_error: 0.3572\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35831 to 0.35718, saving model to temp/f50\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3629 - mean_squared_error: 0.3629 - val_loss: 0.3554 - val_mean_squared_error: 0.3554\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35718 to 0.35537, saving model to temp/f50\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3606 - mean_squared_error: 0.3606 - val_loss: 0.3596 - val_mean_squared_error: 0.3596\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35537\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3593 - mean_squared_error: 0.3593 - val_loss: 0.3579 - val_mean_squared_error: 0.3579\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35537\n",
      "Epoch 00006: early stopping\n",
      "temp/f51\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4547 - mean_squared_error: 0.4547 - val_loss: 0.3628 - val_mean_squared_error: 0.3628\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36278, saving model to temp/f51\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3648 - mean_squared_error: 0.3648 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36278 to 0.35802, saving model to temp/f51\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3625 - mean_squared_error: 0.3625 - val_loss: 0.3716 - val_mean_squared_error: 0.3716\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35802\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3618 - mean_squared_error: 0.3618 - val_loss: 0.3556 - val_mean_squared_error: 0.3556\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35802 to 0.35560, saving model to temp/f51\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3599 - mean_squared_error: 0.3599 - val_loss: 0.3554 - val_mean_squared_error: 0.3554\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35560 to 0.35541, saving model to temp/f51\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3568 - mean_squared_error: 0.3568 - val_loss: 0.3612 - val_mean_squared_error: 0.3612\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35541\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3565 - mean_squared_error: 0.3565 - val_loss: 0.3549 - val_mean_squared_error: 0.3549\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35541 to 0.35488, saving model to temp/f51\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3581 - mean_squared_error: 0.3581 - val_loss: 0.3539 - val_mean_squared_error: 0.3539\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35488 to 0.35394, saving model to temp/f51\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3560 - mean_squared_error: 0.3560 - val_loss: 0.3547 - val_mean_squared_error: 0.3547\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35394\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3534 - mean_squared_error: 0.3534 - val_loss: 0.3548 - val_mean_squared_error: 0.3548\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35394\n",
      "Epoch 00010: early stopping\n",
      "temp/f52\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4557 - mean_squared_error: 0.4557 - val_loss: 0.3745 - val_mean_squared_error: 0.3745\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37447, saving model to temp/f52\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3665 - mean_squared_error: 0.3665 - val_loss: 0.3577 - val_mean_squared_error: 0.3577\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37447 to 0.35765, saving model to temp/f52\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3628 - mean_squared_error: 0.3628 - val_loss: 0.3588 - val_mean_squared_error: 0.3588\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35765\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3625 - mean_squared_error: 0.3625 - val_loss: 0.3605 - val_mean_squared_error: 0.3605\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35765\n",
      "Epoch 00004: early stopping\n",
      "temp/f53\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4558 - mean_squared_error: 0.4558 - val_loss: 0.3620 - val_mean_squared_error: 0.3620\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36200, saving model to temp/f53\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3691 - mean_squared_error: 0.3691 - val_loss: 0.3615 - val_mean_squared_error: 0.3615\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36200 to 0.36153, saving model to temp/f53\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3632 - mean_squared_error: 0.3632 - val_loss: 0.3592 - val_mean_squared_error: 0.3592\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36153 to 0.35920, saving model to temp/f53\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3620 - mean_squared_error: 0.3620 - val_loss: 0.3578 - val_mean_squared_error: 0.3578\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35920 to 0.35783, saving model to temp/f53\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3618 - mean_squared_error: 0.3618 - val_loss: 0.3590 - val_mean_squared_error: 0.3590\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35783\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3560 - mean_squared_error: 0.3560 - val_loss: 0.3565 - val_mean_squared_error: 0.3565\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35783 to 0.35648, saving model to temp/f53\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3565 - mean_squared_error: 0.3565 - val_loss: 0.3516 - val_mean_squared_error: 0.3516\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35648 to 0.35157, saving model to temp/f53\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3704 - val_mean_squared_error: 0.3704\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35157\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3536 - mean_squared_error: 0.3536 - val_loss: 0.3513 - val_mean_squared_error: 0.3513\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35157 to 0.35126, saving model to temp/f53\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3541 - mean_squared_error: 0.3541 - val_loss: 0.3506 - val_mean_squared_error: 0.3506\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.35126 to 0.35063, saving model to temp/f53\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3543 - mean_squared_error: 0.3543 - val_loss: 0.3519 - val_mean_squared_error: 0.3519\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35063\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3541 - mean_squared_error: 0.3541 - val_loss: 0.3697 - val_mean_squared_error: 0.3697\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35063\n",
      "Epoch 00012: early stopping\n",
      "temp/f54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 0.4618 - mean_squared_error: 0.4618 - val_loss: 0.3681 - val_mean_squared_error: 0.3681\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36807, saving model to temp/f54\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3665 - mean_squared_error: 0.3665 - val_loss: 0.3634 - val_mean_squared_error: 0.3634\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36807 to 0.36344, saving model to temp/f54\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3635 - mean_squared_error: 0.3635 - val_loss: 0.3569 - val_mean_squared_error: 0.3569\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36344 to 0.35690, saving model to temp/f54\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3586 - mean_squared_error: 0.3586 - val_loss: 0.3555 - val_mean_squared_error: 0.3555\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35690 to 0.35554, saving model to temp/f54\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3606 - mean_squared_error: 0.3606 - val_loss: 0.3630 - val_mean_squared_error: 0.3630\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35554\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3585 - mean_squared_error: 0.3585 - val_loss: 0.3579 - val_mean_squared_error: 0.3579\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35554\n",
      "Epoch 00006: early stopping\n",
      "temp/f55\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 0.4543 - mean_squared_error: 0.4543 - val_loss: 0.3667 - val_mean_squared_error: 0.3667\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36673, saving model to temp/f55\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3636 - mean_squared_error: 0.3636 - val_loss: 0.3625 - val_mean_squared_error: 0.3625\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36673 to 0.36250, saving model to temp/f55\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.3631 - mean_squared_error: 0.3631 - val_loss: 0.3655 - val_mean_squared_error: 0.3655\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36250\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3597 - mean_squared_error: 0.3597 - val_loss: 0.3577 - val_mean_squared_error: 0.3577\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36250 to 0.35774, saving model to temp/f55\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3564 - val_mean_squared_error: 0.3564\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35774 to 0.35639, saving model to temp/f55\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.3582 - mean_squared_error: 0.3582 - val_loss: 0.3552 - val_mean_squared_error: 0.3552\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35639 to 0.35522, saving model to temp/f55\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3567 - mean_squared_error: 0.3567 - val_loss: 0.3509 - val_mean_squared_error: 0.3509\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35522 to 0.35087, saving model to temp/f55\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 126us/step - loss: 0.3540 - mean_squared_error: 0.3540 - val_loss: 0.3578 - val_mean_squared_error: 0.3578\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35087\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3556 - mean_squared_error: 0.3556 - val_loss: 0.3659 - val_mean_squared_error: 0.3659\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35087\n",
      "Epoch 00009: early stopping\n",
      "temp/f56\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 142us/step - loss: 0.4483 - mean_squared_error: 0.4483 - val_loss: 0.3654 - val_mean_squared_error: 0.3654\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36537, saving model to temp/f56\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 125us/step - loss: 0.3691 - mean_squared_error: 0.3691 - val_loss: 0.3584 - val_mean_squared_error: 0.3584\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36537 to 0.35839, saving model to temp/f56\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3651 - mean_squared_error: 0.3651 - val_loss: 0.3579 - val_mean_squared_error: 0.3579\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35839 to 0.35791, saving model to temp/f56\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3627 - mean_squared_error: 0.3627 - val_loss: 0.3634 - val_mean_squared_error: 0.3634\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35791\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3598 - mean_squared_error: 0.3598 - val_loss: 0.3569 - val_mean_squared_error: 0.3569\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35791 to 0.35688, saving model to temp/f56\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3555 - mean_squared_error: 0.3555 - val_loss: 0.3569 - val_mean_squared_error: 0.3569\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35688\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3593 - val_mean_squared_error: 0.3593\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35688\n",
      "Epoch 00007: early stopping\n",
      "temp/f57\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.4530 - mean_squared_error: 0.4530 - val_loss: 0.3625 - val_mean_squared_error: 0.3625\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36246, saving model to temp/f57\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3672 - mean_squared_error: 0.3672 - val_loss: 0.3576 - val_mean_squared_error: 0.3576\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36246 to 0.35755, saving model to temp/f57\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3615 - mean_squared_error: 0.3615 - val_loss: 0.3585 - val_mean_squared_error: 0.3585\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35755\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3617 - mean_squared_error: 0.3617 - val_loss: 0.3553 - val_mean_squared_error: 0.3553\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35755 to 0.35531, saving model to temp/f57\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3556 - val_mean_squared_error: 0.3556\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35531\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 0.3575 - mean_squared_error: 0.3575 - val_loss: 0.3602 - val_mean_squared_error: 0.3602\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35531\n",
      "Epoch 00006: early stopping\n",
      "temp/f58\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4635 - mean_squared_error: 0.4635 - val_loss: 0.3711 - val_mean_squared_error: 0.3711\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37106, saving model to temp/f58\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3649 - mean_squared_error: 0.3649 - val_loss: 0.3736 - val_mean_squared_error: 0.3736\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.37106\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3613 - mean_squared_error: 0.3613 - val_loss: 0.3586 - val_mean_squared_error: 0.3586\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.37106 to 0.35856, saving model to temp/f58\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3623 - mean_squared_error: 0.3623 - val_loss: 0.3585 - val_mean_squared_error: 0.3585\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35856 to 0.35849, saving model to temp/f58\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3588 - mean_squared_error: 0.3588 - val_loss: 0.3663 - val_mean_squared_error: 0.3663\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35849\n",
      "Epoch 00005: early stopping\n",
      "temp/f59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 0.4529 - mean_squared_error: 0.4529 - val_loss: 0.3658 - val_mean_squared_error: 0.3658\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36577, saving model to temp/f59\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3638 - mean_squared_error: 0.3638 - val_loss: 0.3598 - val_mean_squared_error: 0.3598\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36577 to 0.35975, saving model to temp/f59\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3613 - mean_squared_error: 0.3613 - val_loss: 0.3567 - val_mean_squared_error: 0.3567\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35975 to 0.35669, saving model to temp/f59\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3621 - mean_squared_error: 0.3621 - val_loss: 0.3581 - val_mean_squared_error: 0.3581\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35669\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3601 - mean_squared_error: 0.3601 - val_loss: 0.3537 - val_mean_squared_error: 0.3537\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35669 to 0.35368, saving model to temp/f59\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3574 - val_mean_squared_error: 0.3574\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35368\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3554 - val_mean_squared_error: 0.3554\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35368\n",
      "Epoch 00007: early stopping\n",
      "temp/f60\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 0.4626 - mean_squared_error: 0.4626 - val_loss: 0.3624 - val_mean_squared_error: 0.3624\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36235, saving model to temp/f60\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3665 - mean_squared_error: 0.3665 - val_loss: 0.3614 - val_mean_squared_error: 0.3614\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36235 to 0.36143, saving model to temp/f60\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.3635 - mean_squared_error: 0.3635 - val_loss: 0.3619 - val_mean_squared_error: 0.3619\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36143\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3592 - mean_squared_error: 0.3592 - val_loss: 0.3593 - val_mean_squared_error: 0.3593\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36143 to 0.35927, saving model to temp/f60\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3610 - mean_squared_error: 0.3610 - val_loss: 0.3576 - val_mean_squared_error: 0.3576\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35927 to 0.35757, saving model to temp/f60\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3575 - mean_squared_error: 0.3575 - val_loss: 0.3591 - val_mean_squared_error: 0.3591\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35757\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3577 - mean_squared_error: 0.3577 - val_loss: 0.3572 - val_mean_squared_error: 0.3572\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35757 to 0.35723, saving model to temp/f60\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.3583 - mean_squared_error: 0.3583 - val_loss: 0.3547 - val_mean_squared_error: 0.3547\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35723 to 0.35468, saving model to temp/f60\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3565 - mean_squared_error: 0.3565 - val_loss: 0.3506 - val_mean_squared_error: 0.3506\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35468 to 0.35056, saving model to temp/f60\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 0.3555 - mean_squared_error: 0.3555 - val_loss: 0.3632 - val_mean_squared_error: 0.3632\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35056\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 2s 125us/step - loss: 0.3554 - mean_squared_error: 0.3554 - val_loss: 0.3515 - val_mean_squared_error: 0.3515\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35056\n",
      "Epoch 00011: early stopping\n",
      "temp/f61\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 0.4477 - mean_squared_error: 0.4477 - val_loss: 0.3738 - val_mean_squared_error: 0.3738\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37381, saving model to temp/f61\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3669 - mean_squared_error: 0.3669 - val_loss: 0.3595 - val_mean_squared_error: 0.3595\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37381 to 0.35949, saving model to temp/f61\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.3614 - mean_squared_error: 0.3614 - val_loss: 0.3591 - val_mean_squared_error: 0.3591\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35949 to 0.35910, saving model to temp/f61\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.3596 - mean_squared_error: 0.3596 - val_loss: 0.3605 - val_mean_squared_error: 0.3605\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35910\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3594 - mean_squared_error: 0.3594 - val_loss: 0.3573 - val_mean_squared_error: 0.3573\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35910 to 0.35731, saving model to temp/f61\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.3580 - mean_squared_error: 0.3580 - val_loss: 0.3555 - val_mean_squared_error: 0.3555\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35731 to 0.35551, saving model to temp/f61\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3545 - val_mean_squared_error: 0.3545\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35551 to 0.35454, saving model to temp/f61\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3516 - val_mean_squared_error: 0.3516\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35454 to 0.35157, saving model to temp/f61\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3542 - mean_squared_error: 0.3542 - val_loss: 0.3518 - val_mean_squared_error: 0.3518\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35157\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3549 - mean_squared_error: 0.3549 - val_loss: 0.3565 - val_mean_squared_error: 0.3565\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35157\n",
      "Epoch 00010: early stopping\n",
      "temp/f62\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4498 - mean_squared_error: 0.4498 - val_loss: 0.3766 - val_mean_squared_error: 0.3766\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37657, saving model to temp/f62\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3665 - mean_squared_error: 0.3665 - val_loss: 0.3675 - val_mean_squared_error: 0.3675\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37657 to 0.36750, saving model to temp/f62\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3603 - mean_squared_error: 0.3603 - val_loss: 0.3583 - val_mean_squared_error: 0.3583\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36750 to 0.35826, saving model to temp/f62\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3593 - val_mean_squared_error: 0.3593\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35826\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3590 - mean_squared_error: 0.3590 - val_loss: 0.3555 - val_mean_squared_error: 0.3555\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35826 to 0.35554, saving model to temp/f62\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3558 - mean_squared_error: 0.3558 - val_loss: 0.3594 - val_mean_squared_error: 0.3594\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35554\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3546 - mean_squared_error: 0.3546 - val_loss: 0.3554 - val_mean_squared_error: 0.3554\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35554 to 0.35538, saving model to temp/f62\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3543 - mean_squared_error: 0.3543 - val_loss: 0.3649 - val_mean_squared_error: 0.3649\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35538\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3583 - mean_squared_error: 0.3583 - val_loss: 0.3595 - val_mean_squared_error: 0.3595\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35538\n",
      "Epoch 00009: early stopping\n",
      "temp/f63\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 0.4606 - mean_squared_error: 0.4606 - val_loss: 0.3634 - val_mean_squared_error: 0.3634\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36342, saving model to temp/f63\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 0.3654 - mean_squared_error: 0.3654 - val_loss: 0.3739 - val_mean_squared_error: 0.3739\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36342\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3621 - mean_squared_error: 0.3621 - val_loss: 0.3582 - val_mean_squared_error: 0.3582\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36342 to 0.35819, saving model to temp/f63\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3588 - mean_squared_error: 0.3588 - val_loss: 0.3578 - val_mean_squared_error: 0.3578\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35819 to 0.35779, saving model to temp/f63\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3590 - mean_squared_error: 0.3590 - val_loss: 0.3551 - val_mean_squared_error: 0.3551\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35779 to 0.35509, saving model to temp/f63\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3597 - mean_squared_error: 0.3597 - val_loss: 0.3548 - val_mean_squared_error: 0.3548\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35509 to 0.35481, saving model to temp/f63\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3551 - mean_squared_error: 0.3551 - val_loss: 0.3573 - val_mean_squared_error: 0.3573\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35481\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3572 - mean_squared_error: 0.3572 - val_loss: 0.3544 - val_mean_squared_error: 0.3544\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35481 to 0.35445, saving model to temp/f63\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3546 - mean_squared_error: 0.3546 - val_loss: 0.3497 - val_mean_squared_error: 0.3497\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35445 to 0.34974, saving model to temp/f63\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3539 - mean_squared_error: 0.3539 - val_loss: 0.3553 - val_mean_squared_error: 0.3553\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.34974\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.3527 - mean_squared_error: 0.3527 - val_loss: 0.3545 - val_mean_squared_error: 0.3545\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.34974\n",
      "Epoch 00011: early stopping\n",
      "temp/f64\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4505 - mean_squared_error: 0.4505 - val_loss: 0.3619 - val_mean_squared_error: 0.3619\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36192, saving model to temp/f64\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3662 - mean_squared_error: 0.3662 - val_loss: 0.3618 - val_mean_squared_error: 0.3618\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36192 to 0.36180, saving model to temp/f64\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3618 - mean_squared_error: 0.3618 - val_loss: 0.3591 - val_mean_squared_error: 0.3591\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36180 to 0.35909, saving model to temp/f64\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3610 - mean_squared_error: 0.3610 - val_loss: 0.3586 - val_mean_squared_error: 0.3586\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35909 to 0.35860, saving model to temp/f64\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3597 - mean_squared_error: 0.3597 - val_loss: 0.3549 - val_mean_squared_error: 0.3549\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35860 to 0.35486, saving model to temp/f64\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3580 - mean_squared_error: 0.3580 - val_loss: 0.3514 - val_mean_squared_error: 0.3514\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35486 to 0.35144, saving model to temp/f64\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3589 - mean_squared_error: 0.3589 - val_loss: 0.3589 - val_mean_squared_error: 0.3589\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35144\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3570 - mean_squared_error: 0.3570 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35144\n",
      "Epoch 00008: early stopping\n",
      "temp/f65\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4565 - mean_squared_error: 0.4565 - val_loss: 0.3704 - val_mean_squared_error: 0.3704\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37043, saving model to temp/f65\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3677 - mean_squared_error: 0.3677 - val_loss: 0.3659 - val_mean_squared_error: 0.3659\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37043 to 0.36591, saving model to temp/f65\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3602 - mean_squared_error: 0.3602 - val_loss: 0.3578 - val_mean_squared_error: 0.3578\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36591 to 0.35785, saving model to temp/f65\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3580 - mean_squared_error: 0.3580 - val_loss: 0.3552 - val_mean_squared_error: 0.3552\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35785 to 0.35516, saving model to temp/f65\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3538 - val_mean_squared_error: 0.3538\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35516 to 0.35382, saving model to temp/f65\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3555 - mean_squared_error: 0.3555 - val_loss: 0.3517 - val_mean_squared_error: 0.3517\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35382 to 0.35173, saving model to temp/f65\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3567 - mean_squared_error: 0.3567 - val_loss: 0.3563 - val_mean_squared_error: 0.3563\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35173\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3567 - mean_squared_error: 0.3567 - val_loss: 0.3591 - val_mean_squared_error: 0.3591\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35173\n",
      "Epoch 00008: early stopping\n",
      "temp/f66\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4546 - mean_squared_error: 0.4546 - val_loss: 0.3687 - val_mean_squared_error: 0.3687\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36867, saving model to temp/f66\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3655 - mean_squared_error: 0.3655 - val_loss: 0.3586 - val_mean_squared_error: 0.3586\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36867 to 0.35855, saving model to temp/f66\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3654 - mean_squared_error: 0.3654 - val_loss: 0.3650 - val_mean_squared_error: 0.3650\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35855\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3617 - mean_squared_error: 0.3617 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35855 to 0.35804, saving model to temp/f66\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3592 - mean_squared_error: 0.3592 - val_loss: 0.3540 - val_mean_squared_error: 0.3540\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35804 to 0.35401, saving model to temp/f66\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3590 - mean_squared_error: 0.3590 - val_loss: 0.3586 - val_mean_squared_error: 0.3586\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35401\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3577 - mean_squared_error: 0.3577 - val_loss: 0.3524 - val_mean_squared_error: 0.3524\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35401 to 0.35244, saving model to temp/f66\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3527 - val_mean_squared_error: 0.3527\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35244\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3540 - mean_squared_error: 0.3540 - val_loss: 0.3611 - val_mean_squared_error: 0.3611\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35244\n",
      "Epoch 00009: early stopping\n",
      "temp/f67\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4582 - mean_squared_error: 0.4582 - val_loss: 0.3654 - val_mean_squared_error: 0.3654\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36544, saving model to temp/f67\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3655 - mean_squared_error: 0.3655 - val_loss: 0.3603 - val_mean_squared_error: 0.3603\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36544 to 0.36034, saving model to temp/f67\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3645 - mean_squared_error: 0.3645 - val_loss: 0.3596 - val_mean_squared_error: 0.3596\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36034 to 0.35956, saving model to temp/f67\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3600 - mean_squared_error: 0.3600 - val_loss: 0.3535 - val_mean_squared_error: 0.3535\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35956 to 0.35347, saving model to temp/f67\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3585 - mean_squared_error: 0.3585 - val_loss: 0.3569 - val_mean_squared_error: 0.3569\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35347\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3531 - val_mean_squared_error: 0.3531\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35347 to 0.35306, saving model to temp/f67\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3572 - mean_squared_error: 0.3572 - val_loss: 0.3611 - val_mean_squared_error: 0.3611\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35306\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3568 - mean_squared_error: 0.3568 - val_loss: 0.3588 - val_mean_squared_error: 0.3588\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35306\n",
      "Epoch 00008: early stopping\n",
      "temp/f68\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4573 - mean_squared_error: 0.4573 - val_loss: 0.3683 - val_mean_squared_error: 0.3683\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36826, saving model to temp/f68\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3620 - mean_squared_error: 0.3620 - val_loss: 0.3594 - val_mean_squared_error: 0.3594\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36826 to 0.35941, saving model to temp/f68\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3631 - mean_squared_error: 0.3631 - val_loss: 0.3574 - val_mean_squared_error: 0.3574\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35941 to 0.35742, saving model to temp/f68\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3629 - mean_squared_error: 0.3629 - val_loss: 0.3714 - val_mean_squared_error: 0.3714\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35742\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3519 - val_mean_squared_error: 0.3519\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35742 to 0.35194, saving model to temp/f68\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3566 - mean_squared_error: 0.3566 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35194\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3582 - val_mean_squared_error: 0.3582\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35194\n",
      "Epoch 00007: early stopping\n",
      "temp/f69\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4500 - mean_squared_error: 0.4500 - val_loss: 0.3626 - val_mean_squared_error: 0.3626\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36264, saving model to temp/f69\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3681 - mean_squared_error: 0.3681 - val_loss: 0.3584 - val_mean_squared_error: 0.3584\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36264 to 0.35836, saving model to temp/f69\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3615 - mean_squared_error: 0.3615 - val_loss: 0.3647 - val_mean_squared_error: 0.3647\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35836\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3603 - mean_squared_error: 0.3603 - val_loss: 0.3654 - val_mean_squared_error: 0.3654\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35836\n",
      "Epoch 00004: early stopping\n",
      "temp/f70\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4508 - mean_squared_error: 0.4508 - val_loss: 0.3632 - val_mean_squared_error: 0.3632\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36319, saving model to temp/f70\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3636 - mean_squared_error: 0.3636 - val_loss: 0.3608 - val_mean_squared_error: 0.3608\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36319 to 0.36077, saving model to temp/f70\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3629 - mean_squared_error: 0.3629 - val_loss: 0.3586 - val_mean_squared_error: 0.3586\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36077 to 0.35858, saving model to temp/f70\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3615 - mean_squared_error: 0.3615 - val_loss: 0.3610 - val_mean_squared_error: 0.3610\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35858\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3591 - mean_squared_error: 0.3591 - val_loss: 0.3613 - val_mean_squared_error: 0.3613\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35858\n",
      "Epoch 00005: early stopping\n",
      "temp/f71\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4543 - mean_squared_error: 0.4543 - val_loss: 0.3714 - val_mean_squared_error: 0.3714\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37137, saving model to temp/f71\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3655 - mean_squared_error: 0.3655 - val_loss: 0.3615 - val_mean_squared_error: 0.3615\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37137 to 0.36148, saving model to temp/f71\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3620 - mean_squared_error: 0.3620 - val_loss: 0.3573 - val_mean_squared_error: 0.3573\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36148 to 0.35731, saving model to temp/f71\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3598 - mean_squared_error: 0.3598 - val_loss: 0.3541 - val_mean_squared_error: 0.3541\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35731 to 0.35414, saving model to temp/f71\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3594 - mean_squared_error: 0.3594 - val_loss: 0.3554 - val_mean_squared_error: 0.3554\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35414\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3577 - mean_squared_error: 0.3577 - val_loss: 0.3538 - val_mean_squared_error: 0.3538\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35414 to 0.35380, saving model to temp/f71\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35380\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3570 - mean_squared_error: 0.3570 - val_loss: 0.3541 - val_mean_squared_error: 0.3541\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35380\n",
      "Epoch 00008: early stopping\n",
      "temp/f72\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4526 - mean_squared_error: 0.4526 - val_loss: 0.3831 - val_mean_squared_error: 0.3831\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.38308, saving model to temp/f72\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3684 - mean_squared_error: 0.3684 - val_loss: 0.3648 - val_mean_squared_error: 0.3648\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.38308 to 0.36479, saving model to temp/f72\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3627 - mean_squared_error: 0.3627 - val_loss: 0.3562 - val_mean_squared_error: 0.3562\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36479 to 0.35623, saving model to temp/f72\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3621 - mean_squared_error: 0.3621 - val_loss: 0.3567 - val_mean_squared_error: 0.3567\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35623\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3561 - val_mean_squared_error: 0.3561\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35623 to 0.35612, saving model to temp/f72\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3594 - mean_squared_error: 0.3594 - val_loss: 0.3518 - val_mean_squared_error: 0.3518\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35612 to 0.35184, saving model to temp/f72\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3572 - mean_squared_error: 0.3572 - val_loss: 0.3512 - val_mean_squared_error: 0.3512\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35184 to 0.35120, saving model to temp/f72\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3544 - mean_squared_error: 0.3544 - val_loss: 0.3578 - val_mean_squared_error: 0.3578\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35120\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3543 - mean_squared_error: 0.3543 - val_loss: 0.3545 - val_mean_squared_error: 0.3545\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35120\n",
      "Epoch 00009: early stopping\n",
      "temp/f73\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4526 - mean_squared_error: 0.4526 - val_loss: 0.3637 - val_mean_squared_error: 0.3637\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36365, saving model to temp/f73\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3684 - mean_squared_error: 0.3684 - val_loss: 0.3597 - val_mean_squared_error: 0.3597\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36365 to 0.35971, saving model to temp/f73\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3638 - mean_squared_error: 0.3638 - val_loss: 0.3571 - val_mean_squared_error: 0.3571\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35971 to 0.35709, saving model to temp/f73\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3590 - mean_squared_error: 0.3590 - val_loss: 0.3530 - val_mean_squared_error: 0.3530\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35709 to 0.35296, saving model to temp/f73\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3593 - mean_squared_error: 0.3593 - val_loss: 0.3606 - val_mean_squared_error: 0.3606\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35296\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3567 - mean_squared_error: 0.3567 - val_loss: 0.3553 - val_mean_squared_error: 0.3553\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35296\n",
      "Epoch 00006: early stopping\n",
      "temp/f74\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4514 - mean_squared_error: 0.4514 - val_loss: 0.3714 - val_mean_squared_error: 0.3714\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37139, saving model to temp/f74\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3656 - mean_squared_error: 0.3656 - val_loss: 0.3632 - val_mean_squared_error: 0.3632\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37139 to 0.36323, saving model to temp/f74\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3612 - mean_squared_error: 0.3612 - val_loss: 0.3673 - val_mean_squared_error: 0.3673\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36323\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3621 - mean_squared_error: 0.3621 - val_loss: 0.3583 - val_mean_squared_error: 0.3583\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36323 to 0.35829, saving model to temp/f74\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3557 - val_mean_squared_error: 0.3557\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35829 to 0.35570, saving model to temp/f74\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3529 - val_mean_squared_error: 0.3529\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35570 to 0.35285, saving model to temp/f74\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3558 - mean_squared_error: 0.3558 - val_loss: 0.3530 - val_mean_squared_error: 0.3530\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35285\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3579 - mean_squared_error: 0.3579 - val_loss: 0.3548 - val_mean_squared_error: 0.3548\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35285\n",
      "Epoch 00008: early stopping\n",
      "temp/f75\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4511 - mean_squared_error: 0.4511 - val_loss: 0.3634 - val_mean_squared_error: 0.3634\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36338, saving model to temp/f75\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3662 - mean_squared_error: 0.3662 - val_loss: 0.3606 - val_mean_squared_error: 0.3606\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36338 to 0.36060, saving model to temp/f75\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3618 - mean_squared_error: 0.3618 - val_loss: 0.3791 - val_mean_squared_error: 0.3791\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36060\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3596 - mean_squared_error: 0.3596 - val_loss: 0.3560 - val_mean_squared_error: 0.3560\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36060 to 0.35605, saving model to temp/f75\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3528 - val_mean_squared_error: 0.3528\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35605 to 0.35282, saving model to temp/f75\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3568 - mean_squared_error: 0.3568 - val_loss: 0.3554 - val_mean_squared_error: 0.3554\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35282\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3588 - mean_squared_error: 0.3588 - val_loss: 0.3608 - val_mean_squared_error: 0.3608\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35282\n",
      "Epoch 00007: early stopping\n",
      "temp/f76\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 0.4525 - mean_squared_error: 0.4525 - val_loss: 0.3626 - val_mean_squared_error: 0.3626\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36261, saving model to temp/f76\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3640 - mean_squared_error: 0.3640 - val_loss: 0.3657 - val_mean_squared_error: 0.3657\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36261\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3643 - mean_squared_error: 0.3643 - val_loss: 0.3656 - val_mean_squared_error: 0.3656\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36261\n",
      "Epoch 00003: early stopping\n",
      "temp/f77\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 0.4431 - mean_squared_error: 0.4431 - val_loss: 0.3655 - val_mean_squared_error: 0.3655\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36545, saving model to temp/f77\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3649 - mean_squared_error: 0.3649 - val_loss: 0.3650 - val_mean_squared_error: 0.3650\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36545 to 0.36498, saving model to temp/f77\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3628 - mean_squared_error: 0.3628 - val_loss: 0.3659 - val_mean_squared_error: 0.3659\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36498\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3606 - mean_squared_error: 0.3606 - val_loss: 0.3595 - val_mean_squared_error: 0.3595\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36498 to 0.35954, saving model to temp/f77\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3611 - mean_squared_error: 0.3611 - val_loss: 0.3553 - val_mean_squared_error: 0.3553\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35954 to 0.35534, saving model to temp/f77\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3566 - mean_squared_error: 0.3566 - val_loss: 0.3648 - val_mean_squared_error: 0.3648\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35534\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3557 - mean_squared_error: 0.3557 - val_loss: 0.3554 - val_mean_squared_error: 0.3554\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35534\n",
      "Epoch 00007: early stopping\n",
      "temp/f78\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4472 - mean_squared_error: 0.4472 - val_loss: 0.3671 - val_mean_squared_error: 0.3671\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36708, saving model to temp/f78\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3677 - mean_squared_error: 0.3677 - val_loss: 0.3574 - val_mean_squared_error: 0.3574\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36708 to 0.35742, saving model to temp/f78\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3644 - mean_squared_error: 0.3644 - val_loss: 0.3629 - val_mean_squared_error: 0.3629\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35742\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3604 - mean_squared_error: 0.3604 - val_loss: 0.3554 - val_mean_squared_error: 0.3554\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35742 to 0.35538, saving model to temp/f78\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3563 - mean_squared_error: 0.3563 - val_loss: 0.3694 - val_mean_squared_error: 0.3694\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35538\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3592 - mean_squared_error: 0.3592 - val_loss: 0.3561 - val_mean_squared_error: 0.3561\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35538\n",
      "Epoch 00006: early stopping\n",
      "temp/f79\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4662 - mean_squared_error: 0.4662 - val_loss: 0.3666 - val_mean_squared_error: 0.3666\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36665, saving model to temp/f79\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3670 - mean_squared_error: 0.3670 - val_loss: 0.3619 - val_mean_squared_error: 0.3619\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36665 to 0.36190, saving model to temp/f79\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3617 - mean_squared_error: 0.3617 - val_loss: 0.3646 - val_mean_squared_error: 0.3646\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36190\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3602 - mean_squared_error: 0.3602 - val_loss: 0.3571 - val_mean_squared_error: 0.3571\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36190 to 0.35709, saving model to temp/f79\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3557 - val_mean_squared_error: 0.3557\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35709 to 0.35575, saving model to temp/f79\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3562 - mean_squared_error: 0.3562 - val_loss: 0.3542 - val_mean_squared_error: 0.3542\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35575 to 0.35423, saving model to temp/f79\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3568 - mean_squared_error: 0.3568 - val_loss: 0.3530 - val_mean_squared_error: 0.3530\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35423 to 0.35302, saving model to temp/f79\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3549 - mean_squared_error: 0.3549 - val_loss: 0.3528 - val_mean_squared_error: 0.3528\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35302 to 0.35283, saving model to temp/f79\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3541 - mean_squared_error: 0.3541 - val_loss: 0.3574 - val_mean_squared_error: 0.3574\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35283\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3544 - mean_squared_error: 0.3544 - val_loss: 0.3505 - val_mean_squared_error: 0.3505\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.35283 to 0.35049, saving model to temp/f79\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3525 - mean_squared_error: 0.3525 - val_loss: 0.3514 - val_mean_squared_error: 0.3514\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35049\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3534 - mean_squared_error: 0.3534 - val_loss: 0.3548 - val_mean_squared_error: 0.3548\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35049\n",
      "Epoch 00012: early stopping\n",
      "temp/f80\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4539 - mean_squared_error: 0.4539 - val_loss: 0.3660 - val_mean_squared_error: 0.3660\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36597, saving model to temp/f80\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3657 - mean_squared_error: 0.3657 - val_loss: 0.3578 - val_mean_squared_error: 0.3578\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36597 to 0.35781, saving model to temp/f80\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3633 - mean_squared_error: 0.3633 - val_loss: 0.3591 - val_mean_squared_error: 0.3591\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35781\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3617 - mean_squared_error: 0.3617 - val_loss: 0.3595 - val_mean_squared_error: 0.3595\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35781\n",
      "Epoch 00004: early stopping\n",
      "temp/f81\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4559 - mean_squared_error: 0.4559 - val_loss: 0.3654 - val_mean_squared_error: 0.3654\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36541, saving model to temp/f81\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3640 - mean_squared_error: 0.3640 - val_loss: 0.3679 - val_mean_squared_error: 0.3679\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36541\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3627 - mean_squared_error: 0.3627 - val_loss: 0.3568 - val_mean_squared_error: 0.3568\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36541 to 0.35685, saving model to temp/f81\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3610 - mean_squared_error: 0.3610 - val_loss: 0.3577 - val_mean_squared_error: 0.3577\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35685\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3559 - mean_squared_error: 0.3559 - val_loss: 0.3570 - val_mean_squared_error: 0.3570\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35685\n",
      "Epoch 00005: early stopping\n",
      "temp/f82\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4555 - mean_squared_error: 0.4555 - val_loss: 0.3622 - val_mean_squared_error: 0.3622\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36218, saving model to temp/f82\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3659 - mean_squared_error: 0.3659 - val_loss: 0.3581 - val_mean_squared_error: 0.3581\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36218 to 0.35813, saving model to temp/f82\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3626 - mean_squared_error: 0.3626 - val_loss: 0.3599 - val_mean_squared_error: 0.3599\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35813\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3614 - mean_squared_error: 0.3614 - val_loss: 0.3582 - val_mean_squared_error: 0.3582\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35813\n",
      "Epoch 00004: early stopping\n",
      "temp/f83\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4502 - mean_squared_error: 0.4502 - val_loss: 0.3681 - val_mean_squared_error: 0.3681\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36806, saving model to temp/f83\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3672 - mean_squared_error: 0.3672 - val_loss: 0.3639 - val_mean_squared_error: 0.3639\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36806 to 0.36390, saving model to temp/f83\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3645 - mean_squared_error: 0.3645 - val_loss: 0.3618 - val_mean_squared_error: 0.3618\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36390 to 0.36185, saving model to temp/f83\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3612 - mean_squared_error: 0.3612 - val_loss: 0.3612 - val_mean_squared_error: 0.3612\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36185 to 0.36124, saving model to temp/f83\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3605 - mean_squared_error: 0.3605 - val_loss: 0.3529 - val_mean_squared_error: 0.3529\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.36124 to 0.35291, saving model to temp/f83\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3547 - val_mean_squared_error: 0.3547\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35291\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3568 - mean_squared_error: 0.3568 - val_loss: 0.3525 - val_mean_squared_error: 0.3525\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35291 to 0.35252, saving model to temp/f83\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3575 - mean_squared_error: 0.3575 - val_loss: 0.3516 - val_mean_squared_error: 0.3516\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35252 to 0.35159, saving model to temp/f83\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3539 - mean_squared_error: 0.3539 - val_loss: 0.3530 - val_mean_squared_error: 0.3530\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35159\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3546 - mean_squared_error: 0.3546 - val_loss: 0.3504 - val_mean_squared_error: 0.3504\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.35159 to 0.35042, saving model to temp/f83\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3650 - val_mean_squared_error: 0.3650\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35042\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3535 - mean_squared_error: 0.3535 - val_loss: 0.3535 - val_mean_squared_error: 0.3535\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35042\n",
      "Epoch 00012: early stopping\n",
      "temp/f84\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 0.4524 - mean_squared_error: 0.4524 - val_loss: 0.3678 - val_mean_squared_error: 0.3678\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36785, saving model to temp/f84\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3651 - mean_squared_error: 0.3651 - val_loss: 0.3669 - val_mean_squared_error: 0.3669\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36785 to 0.36691, saving model to temp/f84\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3638 - mean_squared_error: 0.3638 - val_loss: 0.3585 - val_mean_squared_error: 0.3585\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36691 to 0.35853, saving model to temp/f84\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3600 - mean_squared_error: 0.3600 - val_loss: 0.3593 - val_mean_squared_error: 0.3593\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35853\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3598 - mean_squared_error: 0.3598 - val_loss: 0.3589 - val_mean_squared_error: 0.3589\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35853\n",
      "Epoch 00005: early stopping\n",
      "temp/f85\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4624 - mean_squared_error: 0.4624 - val_loss: 0.3646 - val_mean_squared_error: 0.3646\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36464, saving model to temp/f85\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3672 - mean_squared_error: 0.3672 - val_loss: 0.3598 - val_mean_squared_error: 0.3598\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36464 to 0.35981, saving model to temp/f85\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3629 - mean_squared_error: 0.3629 - val_loss: 0.3578 - val_mean_squared_error: 0.3578\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35981 to 0.35784, saving model to temp/f85\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3618 - mean_squared_error: 0.3618 - val_loss: 0.3549 - val_mean_squared_error: 0.3549\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35784 to 0.35488, saving model to temp/f85\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3566 - mean_squared_error: 0.3566 - val_loss: 0.3525 - val_mean_squared_error: 0.3525\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35488 to 0.35249, saving model to temp/f85\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3571 - mean_squared_error: 0.3571 - val_loss: 0.3527 - val_mean_squared_error: 0.3527\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35249\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3566 - mean_squared_error: 0.3566 - val_loss: 0.3560 - val_mean_squared_error: 0.3560\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35249\n",
      "Epoch 00007: early stopping\n",
      "temp/f86\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4597 - mean_squared_error: 0.4597 - val_loss: 0.3662 - val_mean_squared_error: 0.3662\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36622, saving model to temp/f86\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3673 - mean_squared_error: 0.3673 - val_loss: 0.3700 - val_mean_squared_error: 0.3700\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36622\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3633 - mean_squared_error: 0.3633 - val_loss: 0.3589 - val_mean_squared_error: 0.3589\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36622 to 0.35892, saving model to temp/f86\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3620 - mean_squared_error: 0.3620 - val_loss: 0.3561 - val_mean_squared_error: 0.3561\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35892 to 0.35607, saving model to temp/f86\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3591 - mean_squared_error: 0.3591 - val_loss: 0.3548 - val_mean_squared_error: 0.3548\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35607 to 0.35476, saving model to temp/f86\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3573 - val_mean_squared_error: 0.3573\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35476\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3582 - mean_squared_error: 0.3582 - val_loss: 0.3619 - val_mean_squared_error: 0.3619\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35476\n",
      "Epoch 00007: early stopping\n",
      "temp/f87\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4485 - mean_squared_error: 0.4485 - val_loss: 0.3657 - val_mean_squared_error: 0.3657\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36572, saving model to temp/f87\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3685 - mean_squared_error: 0.3685 - val_loss: 0.3643 - val_mean_squared_error: 0.3643\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36572 to 0.36431, saving model to temp/f87\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3619 - mean_squared_error: 0.3619 - val_loss: 0.3563 - val_mean_squared_error: 0.3563\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36431 to 0.35628, saving model to temp/f87\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3636 - mean_squared_error: 0.3636 - val_loss: 0.3670 - val_mean_squared_error: 0.3670\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35628\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3599 - mean_squared_error: 0.3599 - val_loss: 0.3555 - val_mean_squared_error: 0.3555\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35628 to 0.35555, saving model to temp/f87\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3600 - mean_squared_error: 0.3600 - val_loss: 0.3556 - val_mean_squared_error: 0.3556\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35555\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3563 - mean_squared_error: 0.3563 - val_loss: 0.3527 - val_mean_squared_error: 0.3527\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35555 to 0.35269, saving model to temp/f87\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3529 - val_mean_squared_error: 0.3529\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35269\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3539 - mean_squared_error: 0.3539 - val_loss: 0.3495 - val_mean_squared_error: 0.3495\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35269 to 0.34948, saving model to temp/f87\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3557 - mean_squared_error: 0.3557 - val_loss: 0.3503 - val_mean_squared_error: 0.3503\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.34948\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3558 - mean_squared_error: 0.3558 - val_loss: 0.3566 - val_mean_squared_error: 0.3566\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.34948\n",
      "Epoch 00011: early stopping\n",
      "temp/f88\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4574 - mean_squared_error: 0.4574 - val_loss: 0.3631 - val_mean_squared_error: 0.3631\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36305, saving model to temp/f88\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3662 - mean_squared_error: 0.3662 - val_loss: 0.3567 - val_mean_squared_error: 0.3567\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36305 to 0.35670, saving model to temp/f88\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3603 - mean_squared_error: 0.3603 - val_loss: 0.3559 - val_mean_squared_error: 0.3559\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35670 to 0.35585, saving model to temp/f88\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3600 - mean_squared_error: 0.3600 - val_loss: 0.3665 - val_mean_squared_error: 0.3665\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35585\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3595 - mean_squared_error: 0.3595 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35585\n",
      "Epoch 00005: early stopping\n",
      "temp/f89\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4500 - mean_squared_error: 0.4500 - val_loss: 0.3726 - val_mean_squared_error: 0.3726\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37264, saving model to temp/f89\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3644 - mean_squared_error: 0.3644 - val_loss: 0.3573 - val_mean_squared_error: 0.3573\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37264 to 0.35734, saving model to temp/f89\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3602 - mean_squared_error: 0.3602 - val_loss: 0.3569 - val_mean_squared_error: 0.3569\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35734 to 0.35686, saving model to temp/f89\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3603 - mean_squared_error: 0.3603 - val_loss: 0.3563 - val_mean_squared_error: 0.3563\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35686 to 0.35629, saving model to temp/f89\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3627 - val_mean_squared_error: 0.3627\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35629\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3590 - mean_squared_error: 0.3590 - val_loss: 0.3533 - val_mean_squared_error: 0.3533\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35629 to 0.35326, saving model to temp/f89\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3554 - mean_squared_error: 0.3554 - val_loss: 0.3543 - val_mean_squared_error: 0.3543\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35326\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3555 - mean_squared_error: 0.3555 - val_loss: 0.3544 - val_mean_squared_error: 0.3544\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35326\n",
      "Epoch 00008: early stopping\n",
      "temp/f90\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 147us/step - loss: 0.4555 - mean_squared_error: 0.4555 - val_loss: 0.3670 - val_mean_squared_error: 0.3670\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36697, saving model to temp/f90\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3662 - mean_squared_error: 0.3662 - val_loss: 0.3640 - val_mean_squared_error: 0.3640\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36697 to 0.36401, saving model to temp/f90\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3629 - mean_squared_error: 0.3629 - val_loss: 0.3611 - val_mean_squared_error: 0.3611\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36401 to 0.36112, saving model to temp/f90\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3607 - mean_squared_error: 0.3607 - val_loss: 0.3599 - val_mean_squared_error: 0.3599\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36112 to 0.35993, saving model to temp/f90\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3592 - mean_squared_error: 0.3592 - val_loss: 0.3576 - val_mean_squared_error: 0.3576\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35993 to 0.35762, saving model to temp/f90\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3536 - val_mean_squared_error: 0.3536\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35762 to 0.35356, saving model to temp/f90\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3582 - mean_squared_error: 0.3582 - val_loss: 0.3536 - val_mean_squared_error: 0.3536\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35356\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3564 - mean_squared_error: 0.3564 - val_loss: 0.3526 - val_mean_squared_error: 0.3526\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35356 to 0.35259, saving model to temp/f90\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3550 - mean_squared_error: 0.3550 - val_loss: 0.3514 - val_mean_squared_error: 0.3514\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35259 to 0.35136, saving model to temp/f90\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3543 - val_mean_squared_error: 0.3543\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35136\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3548 - mean_squared_error: 0.3548 - val_loss: 0.3558 - val_mean_squared_error: 0.3558\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35136\n",
      "Epoch 00011: early stopping\n",
      "temp/f91\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4612 - mean_squared_error: 0.4612 - val_loss: 0.3696 - val_mean_squared_error: 0.3696\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36958, saving model to temp/f91\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3657 - mean_squared_error: 0.3657 - val_loss: 0.3615 - val_mean_squared_error: 0.3615\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36958 to 0.36154, saving model to temp/f91\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3612 - mean_squared_error: 0.3612 - val_loss: 0.3583 - val_mean_squared_error: 0.3583\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36154 to 0.35826, saving model to temp/f91\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3609 - mean_squared_error: 0.3609 - val_loss: 0.3629 - val_mean_squared_error: 0.3629\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35826\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3612 - mean_squared_error: 0.3612 - val_loss: 0.3559 - val_mean_squared_error: 0.3559\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35826 to 0.35593, saving model to temp/f91\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3529 - val_mean_squared_error: 0.3529\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35593 to 0.35293, saving model to temp/f91\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3570 - mean_squared_error: 0.3570 - val_loss: 0.3514 - val_mean_squared_error: 0.3514\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35293 to 0.35135, saving model to temp/f91\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3575 - mean_squared_error: 0.3575 - val_loss: 0.3554 - val_mean_squared_error: 0.3554\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35135\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3562 - mean_squared_error: 0.3562 - val_loss: 0.3509 - val_mean_squared_error: 0.3509\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35135 to 0.35092, saving model to temp/f91\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3527 - mean_squared_error: 0.3527 - val_loss: 0.3533 - val_mean_squared_error: 0.3533\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35092\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3558 - mean_squared_error: 0.3558 - val_loss: 0.3561 - val_mean_squared_error: 0.3561\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35092\n",
      "Epoch 00011: early stopping\n",
      "temp/f92\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4561 - mean_squared_error: 0.4561 - val_loss: 0.3624 - val_mean_squared_error: 0.3624\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36244, saving model to temp/f92\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3654 - mean_squared_error: 0.3654 - val_loss: 0.3689 - val_mean_squared_error: 0.3689\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36244\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3622 - mean_squared_error: 0.3622 - val_loss: 0.3611 - val_mean_squared_error: 0.3611\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36244 to 0.36109, saving model to temp/f92\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3623 - mean_squared_error: 0.3623 - val_loss: 0.3539 - val_mean_squared_error: 0.3539\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36109 to 0.35391, saving model to temp/f92\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3598 - mean_squared_error: 0.3598 - val_loss: 0.3542 - val_mean_squared_error: 0.3542\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35391\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3579 - mean_squared_error: 0.3579 - val_loss: 0.3548 - val_mean_squared_error: 0.3548\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35391\n",
      "Epoch 00006: early stopping\n",
      "temp/f93\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4532 - mean_squared_error: 0.4532 - val_loss: 0.3795 - val_mean_squared_error: 0.3795\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37953, saving model to temp/f93\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3634 - mean_squared_error: 0.3634 - val_loss: 0.3656 - val_mean_squared_error: 0.3656\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37953 to 0.36556, saving model to temp/f93\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3613 - mean_squared_error: 0.3613 - val_loss: 0.3559 - val_mean_squared_error: 0.3559\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36556 to 0.35591, saving model to temp/f93\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3615 - val_mean_squared_error: 0.3615\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35591\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3603 - mean_squared_error: 0.3603 - val_loss: 0.3540 - val_mean_squared_error: 0.3540\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35591 to 0.35404, saving model to temp/f93\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3583 - mean_squared_error: 0.3583 - val_loss: 0.3584 - val_mean_squared_error: 0.3584\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35404\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3580 - mean_squared_error: 0.3580 - val_loss: 0.3566 - val_mean_squared_error: 0.3566\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35404\n",
      "Epoch 00007: early stopping\n",
      "temp/f94\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4545 - mean_squared_error: 0.4545 - val_loss: 0.3691 - val_mean_squared_error: 0.3691\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36908, saving model to temp/f94\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3663 - mean_squared_error: 0.3663 - val_loss: 0.3594 - val_mean_squared_error: 0.3594\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36908 to 0.35941, saving model to temp/f94\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3633 - mean_squared_error: 0.3633 - val_loss: 0.3647 - val_mean_squared_error: 0.3647\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35941\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3604 - mean_squared_error: 0.3604 - val_loss: 0.3559 - val_mean_squared_error: 0.3559\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35941 to 0.35586, saving model to temp/f94\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3611 - mean_squared_error: 0.3611 - val_loss: 0.3537 - val_mean_squared_error: 0.3537\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35586 to 0.35366, saving model to temp/f94\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3578 - mean_squared_error: 0.3578 - val_loss: 0.3549 - val_mean_squared_error: 0.3549\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35366\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3578 - mean_squared_error: 0.3578 - val_loss: 0.3530 - val_mean_squared_error: 0.3530\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35366 to 0.35300, saving model to temp/f94\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3561 - mean_squared_error: 0.3561 - val_loss: 0.3530 - val_mean_squared_error: 0.3530\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35300\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3561 - mean_squared_error: 0.3561 - val_loss: 0.3491 - val_mean_squared_error: 0.3491\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35300 to 0.34906, saving model to temp/f94\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3565 - mean_squared_error: 0.3565 - val_loss: 0.3619 - val_mean_squared_error: 0.3619\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.34906\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3551 - mean_squared_error: 0.3551 - val_loss: 0.3532 - val_mean_squared_error: 0.3532\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.34906\n",
      "Epoch 00011: early stopping\n",
      "temp/f95\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4643 - mean_squared_error: 0.4643 - val_loss: 0.3884 - val_mean_squared_error: 0.3884\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.38842, saving model to temp/f95\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3664 - mean_squared_error: 0.3664 - val_loss: 0.3600 - val_mean_squared_error: 0.3600\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.38842 to 0.35998, saving model to temp/f95\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3573 - val_mean_squared_error: 0.3573\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35998 to 0.35734, saving model to temp/f95\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3619 - mean_squared_error: 0.3619 - val_loss: 0.3562 - val_mean_squared_error: 0.3562\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35734 to 0.35622, saving model to temp/f95\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3570 - val_mean_squared_error: 0.3570\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35622\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3548 - val_mean_squared_error: 0.3548\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35622 to 0.35479, saving model to temp/f95\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3575 - mean_squared_error: 0.3575 - val_loss: 0.3533 - val_mean_squared_error: 0.3533\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35479 to 0.35329, saving model to temp/f95\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3549 - mean_squared_error: 0.3549 - val_loss: 0.3581 - val_mean_squared_error: 0.3581\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35329\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3567 - mean_squared_error: 0.3567 - val_loss: 0.3516 - val_mean_squared_error: 0.3516\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35329 to 0.35155, saving model to temp/f95\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3560 - mean_squared_error: 0.3560 - val_loss: 0.3506 - val_mean_squared_error: 0.3506\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.35155 to 0.35056, saving model to temp/f95\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3562 - mean_squared_error: 0.3562 - val_loss: 0.3517 - val_mean_squared_error: 0.3517\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35056\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3539 - mean_squared_error: 0.3539 - val_loss: 0.3510 - val_mean_squared_error: 0.3510\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35056\n",
      "Epoch 00012: early stopping\n",
      "temp/f96\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4620 - mean_squared_error: 0.4620 - val_loss: 0.3738 - val_mean_squared_error: 0.3738\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37377, saving model to temp/f96\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3677 - mean_squared_error: 0.3677 - val_loss: 0.3584 - val_mean_squared_error: 0.3584\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37377 to 0.35844, saving model to temp/f96\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3649 - mean_squared_error: 0.3649 - val_loss: 0.3648 - val_mean_squared_error: 0.3648\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35844\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3612 - mean_squared_error: 0.3612 - val_loss: 0.3570 - val_mean_squared_error: 0.3570\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35844 to 0.35701, saving model to temp/f96\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3607 - mean_squared_error: 0.3607 - val_loss: 0.3526 - val_mean_squared_error: 0.3526\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35701 to 0.35260, saving model to temp/f96\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3568 - mean_squared_error: 0.3568 - val_loss: 0.3588 - val_mean_squared_error: 0.3588\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35260\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3581 - mean_squared_error: 0.3581 - val_loss: 0.3576 - val_mean_squared_error: 0.3576\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35260\n",
      "Epoch 00007: early stopping\n",
      "temp/f97\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 0.4518 - mean_squared_error: 0.4518 - val_loss: 0.3713 - val_mean_squared_error: 0.3713\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37130, saving model to temp/f97\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3656 - mean_squared_error: 0.3656 - val_loss: 0.3569 - val_mean_squared_error: 0.3569\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37130 to 0.35691, saving model to temp/f97\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3628 - mean_squared_error: 0.3628 - val_loss: 0.3567 - val_mean_squared_error: 0.3567\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35691 to 0.35668, saving model to temp/f97\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3610 - mean_squared_error: 0.3610 - val_loss: 0.3647 - val_mean_squared_error: 0.3647\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35668\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3593 - mean_squared_error: 0.3593 - val_loss: 0.3545 - val_mean_squared_error: 0.3545\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35668 to 0.35447, saving model to temp/f97\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3577 - mean_squared_error: 0.3577 - val_loss: 0.3558 - val_mean_squared_error: 0.3558\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35447\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3549 - mean_squared_error: 0.3549 - val_loss: 0.3593 - val_mean_squared_error: 0.3593\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35447\n",
      "Epoch 00007: early stopping\n",
      "temp/f98\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 0.4443 - mean_squared_error: 0.4443 - val_loss: 0.3618 - val_mean_squared_error: 0.3618\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36178, saving model to temp/f98\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3653 - mean_squared_error: 0.3653 - val_loss: 0.3638 - val_mean_squared_error: 0.3638\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36178\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3565 - val_mean_squared_error: 0.3565\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36178 to 0.35649, saving model to temp/f98\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 126us/step - loss: 0.3602 - mean_squared_error: 0.3602 - val_loss: 0.3548 - val_mean_squared_error: 0.3548\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35649 to 0.35478, saving model to temp/f98\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3603 - mean_squared_error: 0.3603 - val_loss: 0.3572 - val_mean_squared_error: 0.3572\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35478\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3590 - mean_squared_error: 0.3590 - val_loss: 0.3552 - val_mean_squared_error: 0.3552\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35478\n",
      "Epoch 00006: early stopping\n",
      "temp/f99\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4462 - mean_squared_error: 0.4462 - val_loss: 0.3667 - val_mean_squared_error: 0.3667\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36668, saving model to temp/f99\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3658 - mean_squared_error: 0.3658 - val_loss: 0.3793 - val_mean_squared_error: 0.3793\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36668\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3601 - mean_squared_error: 0.3601 - val_loss: 0.3564 - val_mean_squared_error: 0.3564\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36668 to 0.35645, saving model to temp/f99\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3592 - mean_squared_error: 0.3592 - val_loss: 0.3564 - val_mean_squared_error: 0.3564\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35645 to 0.35644, saving model to temp/f99\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3585 - mean_squared_error: 0.3585 - val_loss: 0.3564 - val_mean_squared_error: 0.3564\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35644 to 0.35642, saving model to temp/f99\n",
      "Epoch 00005: early stopping\n",
      "temp/f100\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4559 - mean_squared_error: 0.4559 - val_loss: 0.3660 - val_mean_squared_error: 0.3660\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36598, saving model to temp/f100\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3653 - mean_squared_error: 0.3653 - val_loss: 0.3592 - val_mean_squared_error: 0.3592\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36598 to 0.35916, saving model to temp/f100\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3619 - mean_squared_error: 0.3619 - val_loss: 0.3632 - val_mean_squared_error: 0.3632\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35916\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3610 - mean_squared_error: 0.3610 - val_loss: 0.3619 - val_mean_squared_error: 0.3619\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35916\n",
      "Epoch 00004: early stopping\n",
      "temp/f101\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4450 - mean_squared_error: 0.4450 - val_loss: 0.3615 - val_mean_squared_error: 0.3615\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36145, saving model to temp/f101\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3646 - mean_squared_error: 0.3646 - val_loss: 0.3615 - val_mean_squared_error: 0.3615\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36145\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3645 - mean_squared_error: 0.3645 - val_loss: 0.3647 - val_mean_squared_error: 0.3647\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36145\n",
      "Epoch 00003: early stopping\n",
      "temp/f102\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4555 - mean_squared_error: 0.4555 - val_loss: 0.3652 - val_mean_squared_error: 0.3652\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36524, saving model to temp/f102\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3650 - mean_squared_error: 0.3650 - val_loss: 0.3611 - val_mean_squared_error: 0.3611\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36524 to 0.36107, saving model to temp/f102\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3623 - mean_squared_error: 0.3623 - val_loss: 0.3622 - val_mean_squared_error: 0.3622\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36107\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.3586 - mean_squared_error: 0.3586 - val_loss: 0.3717 - val_mean_squared_error: 0.3717\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36107\n",
      "Epoch 00004: early stopping\n",
      "temp/f103\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4547 - mean_squared_error: 0.4547 - val_loss: 0.3634 - val_mean_squared_error: 0.3634\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36337, saving model to temp/f103\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3638 - mean_squared_error: 0.3638 - val_loss: 0.3605 - val_mean_squared_error: 0.3605\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36337 to 0.36046, saving model to temp/f103\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3621 - mean_squared_error: 0.3621 - val_loss: 0.3639 - val_mean_squared_error: 0.3639\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36046\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3595 - mean_squared_error: 0.3595 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36046 to 0.35796, saving model to temp/f103\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3598 - mean_squared_error: 0.3598 - val_loss: 0.3596 - val_mean_squared_error: 0.3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35796\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3580 - mean_squared_error: 0.3580 - val_loss: 0.3540 - val_mean_squared_error: 0.3540\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35796 to 0.35397, saving model to temp/f103\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3591 - val_mean_squared_error: 0.3591\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35397\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3544 - mean_squared_error: 0.3544 - val_loss: 0.3560 - val_mean_squared_error: 0.3560\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35397\n",
      "Epoch 00008: early stopping\n",
      "temp/f104\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 0.4578 - mean_squared_error: 0.4578 - val_loss: 0.3662 - val_mean_squared_error: 0.3662\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36623, saving model to temp/f104\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3667 - mean_squared_error: 0.3667 - val_loss: 0.3581 - val_mean_squared_error: 0.3581\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36623 to 0.35814, saving model to temp/f104\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3636 - mean_squared_error: 0.3636 - val_loss: 0.3587 - val_mean_squared_error: 0.3587\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35814\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3595 - mean_squared_error: 0.3595 - val_loss: 0.3639 - val_mean_squared_error: 0.3639\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35814\n",
      "Epoch 00004: early stopping\n",
      "temp/f105\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4608 - mean_squared_error: 0.4608 - val_loss: 0.3661 - val_mean_squared_error: 0.3661\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36607, saving model to temp/f105\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3636 - mean_squared_error: 0.3636 - val_loss: 0.3604 - val_mean_squared_error: 0.3604\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36607 to 0.36039, saving model to temp/f105\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3614 - mean_squared_error: 0.3614 - val_loss: 0.3642 - val_mean_squared_error: 0.3642\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36039\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3597 - mean_squared_error: 0.3597 - val_loss: 0.3560 - val_mean_squared_error: 0.3560\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36039 to 0.35602, saving model to temp/f105\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3614 - val_mean_squared_error: 0.3614\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35602\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3592 - mean_squared_error: 0.3592 - val_loss: 0.3848 - val_mean_squared_error: 0.3848\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35602\n",
      "Epoch 00006: early stopping\n",
      "temp/f106\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4530 - mean_squared_error: 0.4530 - val_loss: 0.3709 - val_mean_squared_error: 0.3709\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37089, saving model to temp/f106\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3701 - mean_squared_error: 0.3701 - val_loss: 0.3618 - val_mean_squared_error: 0.3618\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37089 to 0.36180, saving model to temp/f106\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3614 - mean_squared_error: 0.3614 - val_loss: 0.3586 - val_mean_squared_error: 0.3586\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36180 to 0.35859, saving model to temp/f106\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3591 - mean_squared_error: 0.3591 - val_loss: 0.3585 - val_mean_squared_error: 0.3585\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35859 to 0.35853, saving model to temp/f106\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3596 - mean_squared_error: 0.3596 - val_loss: 0.3572 - val_mean_squared_error: 0.3572\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35853 to 0.35715, saving model to temp/f106\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3570 - mean_squared_error: 0.3570 - val_loss: 0.3544 - val_mean_squared_error: 0.3544\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35715 to 0.35440, saving model to temp/f106\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3566 - mean_squared_error: 0.3566 - val_loss: 0.3527 - val_mean_squared_error: 0.3527\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35440 to 0.35271, saving model to temp/f106\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3564 - mean_squared_error: 0.3564 - val_loss: 0.3532 - val_mean_squared_error: 0.3532\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35271\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3552 - mean_squared_error: 0.3552 - val_loss: 0.3587 - val_mean_squared_error: 0.3587\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35271\n",
      "Epoch 00009: early stopping\n",
      "temp/f107\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4604 - mean_squared_error: 0.4604 - val_loss: 0.3662 - val_mean_squared_error: 0.3662\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36623, saving model to temp/f107\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3668 - mean_squared_error: 0.3668 - val_loss: 0.3610 - val_mean_squared_error: 0.3610\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36623 to 0.36103, saving model to temp/f107\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3606 - mean_squared_error: 0.3606 - val_loss: 0.3591 - val_mean_squared_error: 0.3591\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36103 to 0.35908, saving model to temp/f107\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3597 - mean_squared_error: 0.3597 - val_loss: 0.3556 - val_mean_squared_error: 0.3556\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35908 to 0.35555, saving model to temp/f107\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3565 - mean_squared_error: 0.3565 - val_loss: 0.3585 - val_mean_squared_error: 0.3585\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35555\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3558 - mean_squared_error: 0.3558 - val_loss: 0.3587 - val_mean_squared_error: 0.3587\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35555\n",
      "Epoch 00006: early stopping\n",
      "temp/f108\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4559 - mean_squared_error: 0.4559 - val_loss: 0.3722 - val_mean_squared_error: 0.3722\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37224, saving model to temp/f108\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3657 - mean_squared_error: 0.3657 - val_loss: 0.3609 - val_mean_squared_error: 0.3609\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37224 to 0.36086, saving model to temp/f108\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3618 - mean_squared_error: 0.3618 - val_loss: 0.3608 - val_mean_squared_error: 0.3608\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36086 to 0.36079, saving model to temp/f108\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3636 - mean_squared_error: 0.3636 - val_loss: 0.3577 - val_mean_squared_error: 0.3577\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36079 to 0.35766, saving model to temp/f108\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3596 - mean_squared_error: 0.3596 - val_loss: 0.3591 - val_mean_squared_error: 0.3591\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35766\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3588 - mean_squared_error: 0.3588 - val_loss: 0.3529 - val_mean_squared_error: 0.3529\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35766 to 0.35288, saving model to temp/f108\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3539 - val_mean_squared_error: 0.3539\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35288\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3559 - mean_squared_error: 0.3559 - val_loss: 0.3565 - val_mean_squared_error: 0.3565\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35288\n",
      "Epoch 00008: early stopping\n",
      "temp/f109\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 0.4527 - mean_squared_error: 0.4527 - val_loss: 0.3699 - val_mean_squared_error: 0.3699\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36990, saving model to temp/f109\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3651 - mean_squared_error: 0.3651 - val_loss: 0.3599 - val_mean_squared_error: 0.3599\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36990 to 0.35989, saving model to temp/f109\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3603 - mean_squared_error: 0.3603 - val_loss: 0.3669 - val_mean_squared_error: 0.3669\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35989\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3585 - mean_squared_error: 0.3585 - val_loss: 0.3545 - val_mean_squared_error: 0.3545\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35989 to 0.35452, saving model to temp/f109\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3542 - val_mean_squared_error: 0.3542\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35452 to 0.35419, saving model to temp/f109\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3562 - mean_squared_error: 0.3562 - val_loss: 0.3581 - val_mean_squared_error: 0.3581\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35419\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3589 - mean_squared_error: 0.3589 - val_loss: 0.3555 - val_mean_squared_error: 0.3555\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35419\n",
      "Epoch 00007: early stopping\n",
      "temp/f110\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4573 - mean_squared_error: 0.4573 - val_loss: 0.3704 - val_mean_squared_error: 0.3704\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37043, saving model to temp/f110\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3666 - mean_squared_error: 0.3666 - val_loss: 0.3617 - val_mean_squared_error: 0.3617\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37043 to 0.36175, saving model to temp/f110\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3611 - mean_squared_error: 0.3611 - val_loss: 0.3583 - val_mean_squared_error: 0.3583\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36175 to 0.35832, saving model to temp/f110\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3558 - mean_squared_error: 0.3558 - val_loss: 0.3624 - val_mean_squared_error: 0.3624\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35832\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3583 - mean_squared_error: 0.3583 - val_loss: 0.3642 - val_mean_squared_error: 0.3642\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35832\n",
      "Epoch 00005: early stopping\n",
      "temp/f111\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4480 - mean_squared_error: 0.4480 - val_loss: 0.3613 - val_mean_squared_error: 0.3613\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36131, saving model to temp/f111\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3646 - mean_squared_error: 0.3646 - val_loss: 0.3683 - val_mean_squared_error: 0.3683\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36131\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3629 - mean_squared_error: 0.3629 - val_loss: 0.3586 - val_mean_squared_error: 0.3586\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36131 to 0.35860, saving model to temp/f111\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3593 - mean_squared_error: 0.3593 - val_loss: 0.3603 - val_mean_squared_error: 0.3603\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35860\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3592 - mean_squared_error: 0.3592 - val_loss: 0.3518 - val_mean_squared_error: 0.3518\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35860 to 0.35175, saving model to temp/f111\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3700 - val_mean_squared_error: 0.3700\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35175\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3546 - mean_squared_error: 0.3546 - val_loss: 0.3540 - val_mean_squared_error: 0.3540\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35175\n",
      "Epoch 00007: early stopping\n",
      "temp/f112\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4518 - mean_squared_error: 0.4518 - val_loss: 0.3635 - val_mean_squared_error: 0.3635\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36347, saving model to temp/f112\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3638 - mean_squared_error: 0.3638 - val_loss: 0.3620 - val_mean_squared_error: 0.3620\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36347 to 0.36196, saving model to temp/f112\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3631 - mean_squared_error: 0.3631 - val_loss: 0.3741 - val_mean_squared_error: 0.3741\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36196\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3605 - mean_squared_error: 0.3605 - val_loss: 0.3601 - val_mean_squared_error: 0.3601\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36196 to 0.36013, saving model to temp/f112\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3606 - mean_squared_error: 0.3606 - val_loss: 0.3540 - val_mean_squared_error: 0.3540\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.36013 to 0.35395, saving model to temp/f112\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3561 - mean_squared_error: 0.3561 - val_loss: 0.3532 - val_mean_squared_error: 0.3532\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35395 to 0.35320, saving model to temp/f112\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3560 - mean_squared_error: 0.3560 - val_loss: 0.3525 - val_mean_squared_error: 0.3525\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35320 to 0.35255, saving model to temp/f112\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3572 - mean_squared_error: 0.3572 - val_loss: 0.3626 - val_mean_squared_error: 0.3626\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35255\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3560 - val_mean_squared_error: 0.3560\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35255\n",
      "Epoch 00009: early stopping\n",
      "temp/f113\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4541 - mean_squared_error: 0.4541 - val_loss: 0.3636 - val_mean_squared_error: 0.3636\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36360, saving model to temp/f113\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3667 - mean_squared_error: 0.3667 - val_loss: 0.3638 - val_mean_squared_error: 0.3638\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36360\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3651 - mean_squared_error: 0.3651 - val_loss: 0.3592 - val_mean_squared_error: 0.3592\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36360 to 0.35924, saving model to temp/f113\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3579 - mean_squared_error: 0.3579 - val_loss: 0.3592 - val_mean_squared_error: 0.3592\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35924 to 0.35916, saving model to temp/f113\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3578 - mean_squared_error: 0.3578 - val_loss: 0.3688 - val_mean_squared_error: 0.3688\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35916\n",
      "Epoch 00005: early stopping\n",
      "temp/f114\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4488 - mean_squared_error: 0.4488 - val_loss: 0.3634 - val_mean_squared_error: 0.3634\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36344, saving model to temp/f114\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3680 - mean_squared_error: 0.3680 - val_loss: 0.3618 - val_mean_squared_error: 0.3618\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36344 to 0.36179, saving model to temp/f114\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3619 - mean_squared_error: 0.3619 - val_loss: 0.3656 - val_mean_squared_error: 0.3656\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36179\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3601 - mean_squared_error: 0.3601 - val_loss: 0.3711 - val_mean_squared_error: 0.3711\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36179\n",
      "Epoch 00004: early stopping\n",
      "temp/f115\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4532 - mean_squared_error: 0.4532 - val_loss: 0.3700 - val_mean_squared_error: 0.3700\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37000, saving model to temp/f115\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3644 - mean_squared_error: 0.3644 - val_loss: 0.3640 - val_mean_squared_error: 0.3640\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37000 to 0.36404, saving model to temp/f115\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3640 - mean_squared_error: 0.3640 - val_loss: 0.3575 - val_mean_squared_error: 0.3575\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36404 to 0.35750, saving model to temp/f115\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3639 - mean_squared_error: 0.3639 - val_loss: 0.3599 - val_mean_squared_error: 0.3599\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35750\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3546 - val_mean_squared_error: 0.3546\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35750 to 0.35460, saving model to temp/f115\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.3597 - mean_squared_error: 0.3597 - val_loss: 0.3552 - val_mean_squared_error: 0.3552\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35460\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3559 - mean_squared_error: 0.3559 - val_loss: 0.3536 - val_mean_squared_error: 0.3536\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35460 to 0.35359, saving model to temp/f115\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3584 - mean_squared_error: 0.3584 - val_loss: 0.3574 - val_mean_squared_error: 0.3574\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35359\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3567 - mean_squared_error: 0.3567 - val_loss: 0.3527 - val_mean_squared_error: 0.3527\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35359 to 0.35267, saving model to temp/f115\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3542 - mean_squared_error: 0.3542 - val_loss: 0.3552 - val_mean_squared_error: 0.3552\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35267\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3537 - mean_squared_error: 0.3537 - val_loss: 0.3549 - val_mean_squared_error: 0.3549\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35267\n",
      "Epoch 00011: early stopping\n",
      "temp/f116\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.4613 - mean_squared_error: 0.4613 - val_loss: 0.3642 - val_mean_squared_error: 0.3642\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36420, saving model to temp/f116\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3663 - mean_squared_error: 0.3663 - val_loss: 0.3620 - val_mean_squared_error: 0.3620\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36420 to 0.36199, saving model to temp/f116\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3635 - mean_squared_error: 0.3635 - val_loss: 0.3648 - val_mean_squared_error: 0.3648\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36199\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3573 - val_mean_squared_error: 0.3573\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36199 to 0.35729, saving model to temp/f116\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3532 - val_mean_squared_error: 0.3532\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35729 to 0.35315, saving model to temp/f116\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3523 - val_mean_squared_error: 0.3523\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35315 to 0.35231, saving model to temp/f116\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3561 - mean_squared_error: 0.3561 - val_loss: 0.3593 - val_mean_squared_error: 0.3593\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35231\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3554 - mean_squared_error: 0.3554 - val_loss: 0.3581 - val_mean_squared_error: 0.3581\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35231\n",
      "Epoch 00008: early stopping\n",
      "temp/f117\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4565 - mean_squared_error: 0.4565 - val_loss: 0.3664 - val_mean_squared_error: 0.3664\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36641, saving model to temp/f117\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.3649 - mean_squared_error: 0.3649 - val_loss: 0.3590 - val_mean_squared_error: 0.3590\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36641 to 0.35903, saving model to temp/f117\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3633 - mean_squared_error: 0.3633 - val_loss: 0.3777 - val_mean_squared_error: 0.3777\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35903\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3620 - mean_squared_error: 0.3620 - val_loss: 0.3692 - val_mean_squared_error: 0.3692\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35903\n",
      "Epoch 00004: early stopping\n",
      "temp/f118\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 0.4564 - mean_squared_error: 0.4564 - val_loss: 0.3677 - val_mean_squared_error: 0.3677\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36775, saving model to temp/f118\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3668 - mean_squared_error: 0.3668 - val_loss: 0.3613 - val_mean_squared_error: 0.3613\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36775 to 0.36133, saving model to temp/f118\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3642 - mean_squared_error: 0.3642 - val_loss: 0.3586 - val_mean_squared_error: 0.3586\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36133 to 0.35856, saving model to temp/f118\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3591 - mean_squared_error: 0.3591 - val_loss: 0.3581 - val_mean_squared_error: 0.3581\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35856 to 0.35806, saving model to temp/f118\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3584 - mean_squared_error: 0.3584 - val_loss: 0.3592 - val_mean_squared_error: 0.3592\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35806\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3595 - mean_squared_error: 0.3595 - val_loss: 0.3579 - val_mean_squared_error: 0.3579\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35806 to 0.35791, saving model to temp/f118\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3583 - mean_squared_error: 0.3583 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35791\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3565 - mean_squared_error: 0.3565 - val_loss: 0.3504 - val_mean_squared_error: 0.3504\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35791 to 0.35040, saving model to temp/f118\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3572 - mean_squared_error: 0.3572 - val_loss: 0.3612 - val_mean_squared_error: 0.3612\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35040\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3554 - mean_squared_error: 0.3554 - val_loss: 0.3482 - val_mean_squared_error: 0.3482\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.35040 to 0.34820, saving model to temp/f118\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3540 - mean_squared_error: 0.3540 - val_loss: 0.3490 - val_mean_squared_error: 0.3490\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.34820\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3528 - mean_squared_error: 0.3528 - val_loss: 0.3498 - val_mean_squared_error: 0.3498\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.34820\n",
      "Epoch 00012: early stopping\n",
      "temp/f119\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.4542 - mean_squared_error: 0.4542 - val_loss: 0.3679 - val_mean_squared_error: 0.3679\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36792, saving model to temp/f119\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3651 - mean_squared_error: 0.3651 - val_loss: 0.3675 - val_mean_squared_error: 0.3675\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36792 to 0.36749, saving model to temp/f119\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3622 - mean_squared_error: 0.3622 - val_loss: 0.3596 - val_mean_squared_error: 0.3596\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36749 to 0.35965, saving model to temp/f119\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3615 - mean_squared_error: 0.3615 - val_loss: 0.3586 - val_mean_squared_error: 0.3586\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35965 to 0.35858, saving model to temp/f119\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3579 - mean_squared_error: 0.3579 - val_loss: 0.3598 - val_mean_squared_error: 0.3598\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35858\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3590 - mean_squared_error: 0.3590 - val_loss: 0.3583 - val_mean_squared_error: 0.3583\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35858 to 0.35829, saving model to temp/f119\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3590 - mean_squared_error: 0.3590 - val_loss: 0.3531 - val_mean_squared_error: 0.3531\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35829 to 0.35311, saving model to temp/f119\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3586 - mean_squared_error: 0.3586 - val_loss: 0.3527 - val_mean_squared_error: 0.3527\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35311 to 0.35275, saving model to temp/f119\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3564 - mean_squared_error: 0.3564 - val_loss: 0.3546 - val_mean_squared_error: 0.3546\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35275\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3599 - mean_squared_error: 0.3599 - val_loss: 0.3503 - val_mean_squared_error: 0.3503\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.35275 to 0.35027, saving model to temp/f119\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3544 - mean_squared_error: 0.3544 - val_loss: 0.3523 - val_mean_squared_error: 0.3523\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35027\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3530 - mean_squared_error: 0.3530 - val_loss: 0.3572 - val_mean_squared_error: 0.3572\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35027\n",
      "Epoch 00012: early stopping\n",
      "temp/f120\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4523 - mean_squared_error: 0.4523 - val_loss: 0.3658 - val_mean_squared_error: 0.3658\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36580, saving model to temp/f120\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3641 - mean_squared_error: 0.3641 - val_loss: 0.3594 - val_mean_squared_error: 0.3594\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36580 to 0.35940, saving model to temp/f120\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3640 - mean_squared_error: 0.3640 - val_loss: 0.3646 - val_mean_squared_error: 0.3646\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35940\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3611 - mean_squared_error: 0.3611 - val_loss: 0.3721 - val_mean_squared_error: 0.3721\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35940\n",
      "Epoch 00004: early stopping\n",
      "temp/f121\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4516 - mean_squared_error: 0.4516 - val_loss: 0.3631 - val_mean_squared_error: 0.3631\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36307, saving model to temp/f121\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3641 - mean_squared_error: 0.3641 - val_loss: 0.3608 - val_mean_squared_error: 0.3608\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36307 to 0.36082, saving model to temp/f121\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3614 - mean_squared_error: 0.3614 - val_loss: 0.3638 - val_mean_squared_error: 0.3638\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36082\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3603 - mean_squared_error: 0.3603 - val_loss: 0.3556 - val_mean_squared_error: 0.3556\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36082 to 0.35564, saving model to temp/f121\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3589 - mean_squared_error: 0.3589 - val_loss: 0.3603 - val_mean_squared_error: 0.3603\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35564\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3593 - mean_squared_error: 0.3593 - val_loss: 0.3559 - val_mean_squared_error: 0.3559\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35564\n",
      "Epoch 00006: early stopping\n",
      "temp/f122\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 0.4599 - mean_squared_error: 0.4599 - val_loss: 0.3671 - val_mean_squared_error: 0.3671\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36711, saving model to temp/f122\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3647 - mean_squared_error: 0.3647 - val_loss: 0.3596 - val_mean_squared_error: 0.3596\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36711 to 0.35961, saving model to temp/f122\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3612 - mean_squared_error: 0.3612 - val_loss: 0.3719 - val_mean_squared_error: 0.3719\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35961\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3607 - mean_squared_error: 0.3607 - val_loss: 0.3552 - val_mean_squared_error: 0.3552\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35961 to 0.35524, saving model to temp/f122\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3589 - mean_squared_error: 0.3589 - val_loss: 0.3570 - val_mean_squared_error: 0.3570\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35524\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3593 - mean_squared_error: 0.3593 - val_loss: 0.3559 - val_mean_squared_error: 0.3559\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35524\n",
      "Epoch 00006: early stopping\n",
      "temp/f123\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.4499 - mean_squared_error: 0.4499 - val_loss: 0.3764 - val_mean_squared_error: 0.3764\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37644, saving model to temp/f123\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3667 - mean_squared_error: 0.3667 - val_loss: 0.3597 - val_mean_squared_error: 0.3597\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37644 to 0.35974, saving model to temp/f123\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3625 - mean_squared_error: 0.3625 - val_loss: 0.3626 - val_mean_squared_error: 0.3626\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35974\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3586 - mean_squared_error: 0.3586 - val_loss: 0.3558 - val_mean_squared_error: 0.3558\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35974 to 0.35583, saving model to temp/f123\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3594 - mean_squared_error: 0.3594 - val_loss: 0.3549 - val_mean_squared_error: 0.3549\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35583 to 0.35495, saving model to temp/f123\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3597 - mean_squared_error: 0.3597 - val_loss: 0.3506 - val_mean_squared_error: 0.3506\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35495 to 0.35055, saving model to temp/f123\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3584 - mean_squared_error: 0.3584 - val_loss: 0.3548 - val_mean_squared_error: 0.3548\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35055\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3537 - mean_squared_error: 0.3537 - val_loss: 0.3583 - val_mean_squared_error: 0.3583\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35055\n",
      "Epoch 00008: early stopping\n",
      "temp/f124\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4550 - mean_squared_error: 0.4550 - val_loss: 0.3635 - val_mean_squared_error: 0.3635\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36350, saving model to temp/f124\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3661 - mean_squared_error: 0.3661 - val_loss: 0.3611 - val_mean_squared_error: 0.3611\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36350 to 0.36111, saving model to temp/f124\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3622 - mean_squared_error: 0.3622 - val_loss: 0.3619 - val_mean_squared_error: 0.3619\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36111\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3613 - mean_squared_error: 0.3613 - val_loss: 0.3608 - val_mean_squared_error: 0.3608\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36111 to 0.36080, saving model to temp/f124\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3578 - mean_squared_error: 0.3578 - val_loss: 0.3591 - val_mean_squared_error: 0.3591\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.36080 to 0.35908, saving model to temp/f124\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3563 - mean_squared_error: 0.3563 - val_loss: 0.3558 - val_mean_squared_error: 0.3558\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35908 to 0.35580, saving model to temp/f124\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3588 - mean_squared_error: 0.3588 - val_loss: 0.3575 - val_mean_squared_error: 0.3575\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35580\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3555 - mean_squared_error: 0.3555 - val_loss: 0.3530 - val_mean_squared_error: 0.3530\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35580 to 0.35301, saving model to temp/f124\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3545 - mean_squared_error: 0.3545 - val_loss: 0.3543 - val_mean_squared_error: 0.3543\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35301\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3554 - mean_squared_error: 0.3554 - val_loss: 0.3524 - val_mean_squared_error: 0.3524\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.35301 to 0.35235, saving model to temp/f124\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3559 - mean_squared_error: 0.3559 - val_loss: 0.3525 - val_mean_squared_error: 0.3525\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35235\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3546 - mean_squared_error: 0.3546 - val_loss: 0.3585 - val_mean_squared_error: 0.3585\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35235\n",
      "Epoch 00012: early stopping\n",
      "temp/f125\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4535 - mean_squared_error: 0.4535 - val_loss: 0.3680 - val_mean_squared_error: 0.3680\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36802, saving model to temp/f125\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3638 - mean_squared_error: 0.3638 - val_loss: 0.3639 - val_mean_squared_error: 0.3639\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36802 to 0.36393, saving model to temp/f125\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3631 - mean_squared_error: 0.3631 - val_loss: 0.3620 - val_mean_squared_error: 0.3620\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36393 to 0.36204, saving model to temp/f125\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3608 - mean_squared_error: 0.3608 - val_loss: 0.3637 - val_mean_squared_error: 0.3637\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36204\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3592 - mean_squared_error: 0.3592 - val_loss: 0.3568 - val_mean_squared_error: 0.3568\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.36204 to 0.35679, saving model to temp/f125\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3597 - mean_squared_error: 0.3597 - val_loss: 0.3541 - val_mean_squared_error: 0.3541\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35679 to 0.35408, saving model to temp/f125\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3583 - mean_squared_error: 0.3583 - val_loss: 0.3553 - val_mean_squared_error: 0.3553\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35408\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3567 - mean_squared_error: 0.3567 - val_loss: 0.3533 - val_mean_squared_error: 0.3533\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35408 to 0.35330, saving model to temp/f125\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3567 - mean_squared_error: 0.3567 - val_loss: 0.3543 - val_mean_squared_error: 0.3543\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35330\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3563 - mean_squared_error: 0.3563 - val_loss: 0.3587 - val_mean_squared_error: 0.3587\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35330\n",
      "Epoch 00010: early stopping\n",
      "temp/f126\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4590 - mean_squared_error: 0.4590 - val_loss: 0.3706 - val_mean_squared_error: 0.3706\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37056, saving model to temp/f126\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3662 - mean_squared_error: 0.3662 - val_loss: 0.3604 - val_mean_squared_error: 0.3604\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37056 to 0.36036, saving model to temp/f126\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3639 - mean_squared_error: 0.3639 - val_loss: 0.3573 - val_mean_squared_error: 0.3573\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36036 to 0.35729, saving model to temp/f126\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3579 - mean_squared_error: 0.3579 - val_loss: 0.3584 - val_mean_squared_error: 0.3584\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35729\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3581 - mean_squared_error: 0.3581 - val_loss: 0.3535 - val_mean_squared_error: 0.3535\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35729 to 0.35351, saving model to temp/f126\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3572 - mean_squared_error: 0.3572 - val_loss: 0.3549 - val_mean_squared_error: 0.3549\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35351\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3581 - mean_squared_error: 0.3581 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35351\n",
      "Epoch 00007: early stopping\n",
      "temp/f127\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4478 - mean_squared_error: 0.4478 - val_loss: 0.3708 - val_mean_squared_error: 0.3708\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37075, saving model to temp/f127\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3666 - mean_squared_error: 0.3666 - val_loss: 0.3665 - val_mean_squared_error: 0.3665\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37075 to 0.36648, saving model to temp/f127\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3647 - mean_squared_error: 0.3647 - val_loss: 0.3604 - val_mean_squared_error: 0.3604\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36648 to 0.36040, saving model to temp/f127\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3611 - mean_squared_error: 0.3611 - val_loss: 0.3554 - val_mean_squared_error: 0.3554\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36040 to 0.35543, saving model to temp/f127\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3538 - val_mean_squared_error: 0.3538\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35543 to 0.35380, saving model to temp/f127\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3562 - mean_squared_error: 0.3562 - val_loss: 0.3548 - val_mean_squared_error: 0.3548\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35380\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3564 - mean_squared_error: 0.3564 - val_loss: 0.3534 - val_mean_squared_error: 0.3534\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35380 to 0.35344, saving model to temp/f127\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3560 - mean_squared_error: 0.3560 - val_loss: 0.3547 - val_mean_squared_error: 0.3547\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35344\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3564 - mean_squared_error: 0.3564 - val_loss: 0.3519 - val_mean_squared_error: 0.3519\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35344 to 0.35194, saving model to temp/f127\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3585 - mean_squared_error: 0.3585 - val_loss: 0.3532 - val_mean_squared_error: 0.3532\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35194\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3559 - mean_squared_error: 0.3559 - val_loss: 0.3592 - val_mean_squared_error: 0.3592\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35194\n",
      "Epoch 00011: early stopping\n",
      "temp/f128\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4593 - mean_squared_error: 0.4593 - val_loss: 0.3649 - val_mean_squared_error: 0.3649\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36486, saving model to temp/f128\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3665 - mean_squared_error: 0.3665 - val_loss: 0.3601 - val_mean_squared_error: 0.3601\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36486 to 0.36008, saving model to temp/f128\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3621 - mean_squared_error: 0.3621 - val_loss: 0.3559 - val_mean_squared_error: 0.3559\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36008 to 0.35591, saving model to temp/f128\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3623 - mean_squared_error: 0.3623 - val_loss: 0.3538 - val_mean_squared_error: 0.3538\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35591 to 0.35383, saving model to temp/f128\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3567 - mean_squared_error: 0.3567 - val_loss: 0.3568 - val_mean_squared_error: 0.3568\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35383\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3597 - mean_squared_error: 0.3597 - val_loss: 0.3567 - val_mean_squared_error: 0.3567\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35383\n",
      "Epoch 00006: early stopping\n",
      "temp/f129\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4546 - mean_squared_error: 0.4546 - val_loss: 0.3644 - val_mean_squared_error: 0.3644\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36435, saving model to temp/f129\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3673 - mean_squared_error: 0.3673 - val_loss: 0.3588 - val_mean_squared_error: 0.3588\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36435 to 0.35882, saving model to temp/f129\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3627 - mean_squared_error: 0.3627 - val_loss: 0.3606 - val_mean_squared_error: 0.3606\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35882\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3596 - mean_squared_error: 0.3596 - val_loss: 0.3571 - val_mean_squared_error: 0.3571\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35882 to 0.35708, saving model to temp/f129\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3591 - mean_squared_error: 0.3591 - val_loss: 0.3561 - val_mean_squared_error: 0.3561\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35708 to 0.35605, saving model to temp/f129\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3580 - mean_squared_error: 0.3580 - val_loss: 0.3557 - val_mean_squared_error: 0.3557\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35605 to 0.35572, saving model to temp/f129\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3626 - val_mean_squared_error: 0.3626\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35572\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3563 - mean_squared_error: 0.3563 - val_loss: 0.3533 - val_mean_squared_error: 0.3533\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35572 to 0.35329, saving model to temp/f129\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3541 - mean_squared_error: 0.3541 - val_loss: 0.3515 - val_mean_squared_error: 0.3515\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35329 to 0.35146, saving model to temp/f129\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3579 - mean_squared_error: 0.3579 - val_loss: 0.3601 - val_mean_squared_error: 0.3601\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35146\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3543 - mean_squared_error: 0.3543 - val_loss: 0.3525 - val_mean_squared_error: 0.3525\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35146\n",
      "Epoch 00011: early stopping\n",
      "temp/f130\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 0.4552 - mean_squared_error: 0.4552 - val_loss: 0.3672 - val_mean_squared_error: 0.3672\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36716, saving model to temp/f130\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3690 - mean_squared_error: 0.3690 - val_loss: 0.3657 - val_mean_squared_error: 0.3657\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36716 to 0.36566, saving model to temp/f130\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3621 - val_mean_squared_error: 0.3621\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36566 to 0.36212, saving model to temp/f130\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3611 - mean_squared_error: 0.3611 - val_loss: 0.3565 - val_mean_squared_error: 0.3565\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36212 to 0.35646, saving model to temp/f130\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3598 - mean_squared_error: 0.3598 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35646\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3577 - mean_squared_error: 0.3577 - val_loss: 0.3605 - val_mean_squared_error: 0.3605\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35646\n",
      "Epoch 00006: early stopping\n",
      "temp/f131\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.4538 - mean_squared_error: 0.4538 - val_loss: 0.3664 - val_mean_squared_error: 0.3664\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36640, saving model to temp/f131\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3650 - mean_squared_error: 0.3650 - val_loss: 0.3626 - val_mean_squared_error: 0.3626\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36640 to 0.36264, saving model to temp/f131\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3632 - mean_squared_error: 0.3632 - val_loss: 0.3572 - val_mean_squared_error: 0.3572\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36264 to 0.35724, saving model to temp/f131\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3612 - mean_squared_error: 0.3612 - val_loss: 0.3562 - val_mean_squared_error: 0.3562\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35724 to 0.35623, saving model to temp/f131\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3590 - mean_squared_error: 0.3590 - val_loss: 0.3576 - val_mean_squared_error: 0.3576\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35623\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3588 - mean_squared_error: 0.3588 - val_loss: 0.3524 - val_mean_squared_error: 0.3524\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35623 to 0.35236, saving model to temp/f131\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3553 - val_mean_squared_error: 0.3553\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35236\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3554 - mean_squared_error: 0.3554 - val_loss: 0.3568 - val_mean_squared_error: 0.3568\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35236\n",
      "Epoch 00008: early stopping\n",
      "temp/f132\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4520 - mean_squared_error: 0.4520 - val_loss: 0.3691 - val_mean_squared_error: 0.3691\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36905, saving model to temp/f132\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3682 - mean_squared_error: 0.3682 - val_loss: 0.3615 - val_mean_squared_error: 0.3615\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36905 to 0.36148, saving model to temp/f132\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3634 - mean_squared_error: 0.3634 - val_loss: 0.3574 - val_mean_squared_error: 0.3574\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36148 to 0.35743, saving model to temp/f132\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3622 - mean_squared_error: 0.3622 - val_loss: 0.3552 - val_mean_squared_error: 0.3552\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35743 to 0.35520, saving model to temp/f132\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3589 - mean_squared_error: 0.3589 - val_loss: 0.3571 - val_mean_squared_error: 0.3571\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35520\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3583 - mean_squared_error: 0.3583 - val_loss: 0.3544 - val_mean_squared_error: 0.3544\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35520 to 0.35444, saving model to temp/f132\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3586 - mean_squared_error: 0.3586 - val_loss: 0.3546 - val_mean_squared_error: 0.3546\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35444\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3646 - val_mean_squared_error: 0.3646\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35444\n",
      "Epoch 00008: early stopping\n",
      "temp/f133\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4548 - mean_squared_error: 0.4548 - val_loss: 0.3672 - val_mean_squared_error: 0.3672\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36715, saving model to temp/f133\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3664 - mean_squared_error: 0.3664 - val_loss: 0.3615 - val_mean_squared_error: 0.3615\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36715 to 0.36151, saving model to temp/f133\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3631 - mean_squared_error: 0.3631 - val_loss: 0.3570 - val_mean_squared_error: 0.3570\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36151 to 0.35700, saving model to temp/f133\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3633 - mean_squared_error: 0.3633 - val_loss: 0.3561 - val_mean_squared_error: 0.3561\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35700 to 0.35605, saving model to temp/f133\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3592 - mean_squared_error: 0.3592 - val_loss: 0.3623 - val_mean_squared_error: 0.3623\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35605\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3570 - mean_squared_error: 0.3570 - val_loss: 0.3555 - val_mean_squared_error: 0.3555\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35605 to 0.35546, saving model to temp/f133\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3584 - mean_squared_error: 0.3584 - val_loss: 0.3635 - val_mean_squared_error: 0.3635\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35546\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3565 - mean_squared_error: 0.3565 - val_loss: 0.3536 - val_mean_squared_error: 0.3536\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35546 to 0.35357, saving model to temp/f133\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3572 - mean_squared_error: 0.3572 - val_loss: 0.3659 - val_mean_squared_error: 0.3659\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35357\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3547 - mean_squared_error: 0.3547 - val_loss: 0.3516 - val_mean_squared_error: 0.3516\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.35357 to 0.35162, saving model to temp/f133\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3541 - mean_squared_error: 0.3541 - val_loss: 0.3494 - val_mean_squared_error: 0.3494\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.35162 to 0.34944, saving model to temp/f133\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3537 - mean_squared_error: 0.3537 - val_loss: 0.3544 - val_mean_squared_error: 0.3544\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.34944\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3555 - mean_squared_error: 0.3555 - val_loss: 0.3532 - val_mean_squared_error: 0.3532\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.34944\n",
      "Epoch 00013: early stopping\n",
      "temp/f134\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4507 - mean_squared_error: 0.4507 - val_loss: 0.3696 - val_mean_squared_error: 0.3696\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36964, saving model to temp/f134\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3651 - mean_squared_error: 0.3651 - val_loss: 0.3625 - val_mean_squared_error: 0.3625\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36964 to 0.36253, saving model to temp/f134\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3614 - mean_squared_error: 0.3614 - val_loss: 0.3606 - val_mean_squared_error: 0.3606\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36253 to 0.36059, saving model to temp/f134\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3599 - mean_squared_error: 0.3599 - val_loss: 0.3557 - val_mean_squared_error: 0.3557\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36059 to 0.35572, saving model to temp/f134\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3594 - mean_squared_error: 0.3594 - val_loss: 0.3527 - val_mean_squared_error: 0.3527\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35572 to 0.35267, saving model to temp/f134\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3585 - mean_squared_error: 0.3585 - val_loss: 0.3604 - val_mean_squared_error: 0.3604\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35267\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3579 - mean_squared_error: 0.3579 - val_loss: 0.3547 - val_mean_squared_error: 0.3547\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35267\n",
      "Epoch 00007: early stopping\n",
      "temp/f135\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4567 - mean_squared_error: 0.4567 - val_loss: 0.3742 - val_mean_squared_error: 0.3742\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37424, saving model to temp/f135\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3661 - mean_squared_error: 0.3661 - val_loss: 0.3633 - val_mean_squared_error: 0.3633\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37424 to 0.36325, saving model to temp/f135\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3648 - val_mean_squared_error: 0.3648\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36325\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3585 - mean_squared_error: 0.3585 - val_loss: 0.3686 - val_mean_squared_error: 0.3686\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36325\n",
      "Epoch 00004: early stopping\n",
      "temp/f136\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4522 - mean_squared_error: 0.4522 - val_loss: 0.3598 - val_mean_squared_error: 0.3598\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35983, saving model to temp/f136\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3667 - mean_squared_error: 0.3667 - val_loss: 0.3590 - val_mean_squared_error: 0.3590\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35983 to 0.35900, saving model to temp/f136\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3644 - mean_squared_error: 0.3644 - val_loss: 0.3566 - val_mean_squared_error: 0.3566\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35900 to 0.35664, saving model to temp/f136\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3614 - mean_squared_error: 0.3614 - val_loss: 0.3657 - val_mean_squared_error: 0.3657\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35664\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3542 - val_mean_squared_error: 0.3542\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35664 to 0.35416, saving model to temp/f136\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3582 - mean_squared_error: 0.3582 - val_loss: 0.3523 - val_mean_squared_error: 0.3523\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35416 to 0.35227, saving model to temp/f136\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3577 - mean_squared_error: 0.3577 - val_loss: 0.3583 - val_mean_squared_error: 0.3583\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35227\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3582 - mean_squared_error: 0.3582 - val_loss: 0.3576 - val_mean_squared_error: 0.3576\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35227\n",
      "Epoch 00008: early stopping\n",
      "temp/f137\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 0.4564 - mean_squared_error: 0.4564 - val_loss: 0.3640 - val_mean_squared_error: 0.3640\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36403, saving model to temp/f137\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3678 - mean_squared_error: 0.3678 - val_loss: 0.3603 - val_mean_squared_error: 0.3603\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36403 to 0.36026, saving model to temp/f137\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3624 - mean_squared_error: 0.3624 - val_loss: 0.3606 - val_mean_squared_error: 0.3606\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36026\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3615 - mean_squared_error: 0.3615 - val_loss: 0.3567 - val_mean_squared_error: 0.3567\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36026 to 0.35671, saving model to temp/f137\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3596 - mean_squared_error: 0.3596 - val_loss: 0.3665 - val_mean_squared_error: 0.3665\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35671\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3572 - mean_squared_error: 0.3572 - val_loss: 0.3666 - val_mean_squared_error: 0.3666\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35671\n",
      "Epoch 00006: early stopping\n",
      "temp/f138\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4558 - mean_squared_error: 0.4558 - val_loss: 0.3655 - val_mean_squared_error: 0.3655\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36550, saving model to temp/f138\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3664 - mean_squared_error: 0.3664 - val_loss: 0.3635 - val_mean_squared_error: 0.3635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_loss improved from 0.36550 to 0.36349, saving model to temp/f138\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3630 - mean_squared_error: 0.3630 - val_loss: 0.3539 - val_mean_squared_error: 0.3539\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36349 to 0.35386, saving model to temp/f138\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3618 - mean_squared_error: 0.3618 - val_loss: 0.3559 - val_mean_squared_error: 0.3559\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35386\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 142us/step - loss: 0.3588 - mean_squared_error: 0.3588 - val_loss: 0.3534 - val_mean_squared_error: 0.3534\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35386 to 0.35336, saving model to temp/f138\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3564 - mean_squared_error: 0.3564 - val_loss: 0.3541 - val_mean_squared_error: 0.3541\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35336\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3568 - mean_squared_error: 0.3568 - val_loss: 0.3551 - val_mean_squared_error: 0.3551\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35336\n",
      "Epoch 00007: early stopping\n",
      "temp/f139\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4548 - mean_squared_error: 0.4548 - val_loss: 0.3655 - val_mean_squared_error: 0.3655\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36548, saving model to temp/f139\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3650 - mean_squared_error: 0.3650 - val_loss: 0.3613 - val_mean_squared_error: 0.3613\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36548 to 0.36134, saving model to temp/f139\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3642 - mean_squared_error: 0.3642 - val_loss: 0.3639 - val_mean_squared_error: 0.3639\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36134\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3605 - mean_squared_error: 0.3605 - val_loss: 0.3569 - val_mean_squared_error: 0.3569\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36134 to 0.35694, saving model to temp/f139\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3584 - mean_squared_error: 0.3584 - val_loss: 0.3552 - val_mean_squared_error: 0.3552\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35694 to 0.35518, saving model to temp/f139\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3582 - mean_squared_error: 0.3582 - val_loss: 0.3559 - val_mean_squared_error: 0.3559\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35518\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3578 - mean_squared_error: 0.3578 - val_loss: 0.3519 - val_mean_squared_error: 0.3519\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35518 to 0.35193, saving model to temp/f139\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3599 - val_mean_squared_error: 0.3599\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35193\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3547 - mean_squared_error: 0.3547 - val_loss: 0.3494 - val_mean_squared_error: 0.3494\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35193 to 0.34943, saving model to temp/f139\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3582 - mean_squared_error: 0.3582 - val_loss: 0.3516 - val_mean_squared_error: 0.3516\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.34943\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3552 - mean_squared_error: 0.3552 - val_loss: 0.3502 - val_mean_squared_error: 0.3502\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.34943\n",
      "Epoch 00011: early stopping\n",
      "temp/f140\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 0.4526 - mean_squared_error: 0.4526 - val_loss: 0.3650 - val_mean_squared_error: 0.3650\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36498, saving model to temp/f140\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3640 - mean_squared_error: 0.3640 - val_loss: 0.3826 - val_mean_squared_error: 0.3826\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36498\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3651 - mean_squared_error: 0.3651 - val_loss: 0.3563 - val_mean_squared_error: 0.3563\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36498 to 0.35631, saving model to temp/f140\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3622 - mean_squared_error: 0.3622 - val_loss: 0.3557 - val_mean_squared_error: 0.3557\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35631 to 0.35571, saving model to temp/f140\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3595 - mean_squared_error: 0.3595 - val_loss: 0.3560 - val_mean_squared_error: 0.3560\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35571\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3575 - mean_squared_error: 0.3575 - val_loss: 0.3522 - val_mean_squared_error: 0.3522\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35571 to 0.35218, saving model to temp/f140\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3582 - val_mean_squared_error: 0.3582\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35218\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3581 - mean_squared_error: 0.3581 - val_loss: 0.3556 - val_mean_squared_error: 0.3556\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35218\n",
      "Epoch 00008: early stopping\n",
      "temp/f141\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4663 - mean_squared_error: 0.4663 - val_loss: 0.3667 - val_mean_squared_error: 0.3667\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36666, saving model to temp/f141\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3663 - mean_squared_error: 0.3663 - val_loss: 0.3641 - val_mean_squared_error: 0.3641\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36666 to 0.36407, saving model to temp/f141\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3613 - mean_squared_error: 0.3613 - val_loss: 0.3581 - val_mean_squared_error: 0.3581\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36407 to 0.35811, saving model to temp/f141\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3578 - val_mean_squared_error: 0.3578\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35811 to 0.35781, saving model to temp/f141\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3592 - mean_squared_error: 0.3592 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35781 to 0.35503, saving model to temp/f141\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3559 - mean_squared_error: 0.3559 - val_loss: 0.3571 - val_mean_squared_error: 0.3571\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35503\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3569 - mean_squared_error: 0.3569 - val_loss: 0.3529 - val_mean_squared_error: 0.3529\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35503 to 0.35293, saving model to temp/f141\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3541 - mean_squared_error: 0.3541 - val_loss: 0.3527 - val_mean_squared_error: 0.3527\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35293 to 0.35271, saving model to temp/f141\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3530 - mean_squared_error: 0.3530 - val_loss: 0.3576 - val_mean_squared_error: 0.3576\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35271\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3559 - mean_squared_error: 0.3559 - val_loss: 0.3502 - val_mean_squared_error: 0.3502\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.35271 to 0.35016, saving model to temp/f141\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3548 - mean_squared_error: 0.3548 - val_loss: 0.3586 - val_mean_squared_error: 0.3586\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35016\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3546 - mean_squared_error: 0.3546 - val_loss: 0.3538 - val_mean_squared_error: 0.3538\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35016\n",
      "Epoch 00012: early stopping\n",
      "temp/f142\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4459 - mean_squared_error: 0.4459 - val_loss: 0.3638 - val_mean_squared_error: 0.3638\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36375, saving model to temp/f142\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3679 - mean_squared_error: 0.3679 - val_loss: 0.3633 - val_mean_squared_error: 0.3633\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36375 to 0.36335, saving model to temp/f142\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3629 - mean_squared_error: 0.3629 - val_loss: 0.3546 - val_mean_squared_error: 0.3546\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36335 to 0.35464, saving model to temp/f142\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3592 - mean_squared_error: 0.3592 - val_loss: 0.3526 - val_mean_squared_error: 0.3526\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35464 to 0.35255, saving model to temp/f142\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3601 - mean_squared_error: 0.3601 - val_loss: 0.3548 - val_mean_squared_error: 0.3548\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35255\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3589 - mean_squared_error: 0.3589 - val_loss: 0.3536 - val_mean_squared_error: 0.3536\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35255\n",
      "Epoch 00006: early stopping\n",
      "temp/f143\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4520 - mean_squared_error: 0.4520 - val_loss: 0.3793 - val_mean_squared_error: 0.3793\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37929, saving model to temp/f143\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3677 - mean_squared_error: 0.3677 - val_loss: 0.3583 - val_mean_squared_error: 0.3583\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37929 to 0.35830, saving model to temp/f143\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3629 - mean_squared_error: 0.3629 - val_loss: 0.3593 - val_mean_squared_error: 0.3593\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35830\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3601 - mean_squared_error: 0.3601 - val_loss: 0.3620 - val_mean_squared_error: 0.3620\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35830\n",
      "Epoch 00004: early stopping\n",
      "temp/f144\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4590 - mean_squared_error: 0.4590 - val_loss: 0.3635 - val_mean_squared_error: 0.3635\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36349, saving model to temp/f144\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3633 - mean_squared_error: 0.3633 - val_loss: 0.3600 - val_mean_squared_error: 0.3600\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36349 to 0.36000, saving model to temp/f144\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3635 - mean_squared_error: 0.3635 - val_loss: 0.3564 - val_mean_squared_error: 0.3564\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36000 to 0.35645, saving model to temp/f144\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3561 - val_mean_squared_error: 0.3561\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35645 to 0.35611, saving model to temp/f144\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3592 - mean_squared_error: 0.3592 - val_loss: 0.3547 - val_mean_squared_error: 0.3547\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35611 to 0.35465, saving model to temp/f144\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3549 - val_mean_squared_error: 0.3549\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35465\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3571 - mean_squared_error: 0.3571 - val_loss: 0.3578 - val_mean_squared_error: 0.3578\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35465\n",
      "Epoch 00007: early stopping\n",
      "temp/f145\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4556 - mean_squared_error: 0.4556 - val_loss: 0.3640 - val_mean_squared_error: 0.3640\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36397, saving model to temp/f145\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3669 - mean_squared_error: 0.3669 - val_loss: 0.3653 - val_mean_squared_error: 0.3653\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36397\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3696 - val_mean_squared_error: 0.3696\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36397\n",
      "Epoch 00003: early stopping\n",
      "temp/f146\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.4474 - mean_squared_error: 0.4474 - val_loss: 0.3643 - val_mean_squared_error: 0.3643\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36432, saving model to temp/f146\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3651 - mean_squared_error: 0.3651 - val_loss: 0.3583 - val_mean_squared_error: 0.3583\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36432 to 0.35832, saving model to temp/f146\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3635 - mean_squared_error: 0.3635 - val_loss: 0.3711 - val_mean_squared_error: 0.3711\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35832\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3621 - mean_squared_error: 0.3621 - val_loss: 0.3571 - val_mean_squared_error: 0.3571\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35832 to 0.35712, saving model to temp/f146\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3596 - mean_squared_error: 0.3596 - val_loss: 0.3572 - val_mean_squared_error: 0.3572\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35712\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3572 - mean_squared_error: 0.3572 - val_loss: 0.3546 - val_mean_squared_error: 0.3546\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35712 to 0.35456, saving model to temp/f146\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3524 - val_mean_squared_error: 0.3524\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35456 to 0.35245, saving model to temp/f146\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3555 - mean_squared_error: 0.3555 - val_loss: 0.3549 - val_mean_squared_error: 0.3549\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35245\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3544 - mean_squared_error: 0.3544 - val_loss: 0.3523 - val_mean_squared_error: 0.3523\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35245 to 0.35233, saving model to temp/f146\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3554 - mean_squared_error: 0.3554 - val_loss: 0.3517 - val_mean_squared_error: 0.3517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00010: val_loss improved from 0.35233 to 0.35174, saving model to temp/f146\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3539 - mean_squared_error: 0.3539 - val_loss: 0.3549 - val_mean_squared_error: 0.3549\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35174\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3545 - mean_squared_error: 0.3545 - val_loss: 0.3571 - val_mean_squared_error: 0.3571\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35174\n",
      "Epoch 00012: early stopping\n",
      "temp/f147\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4653 - mean_squared_error: 0.4653 - val_loss: 0.3639 - val_mean_squared_error: 0.3639\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36387, saving model to temp/f147\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3656 - mean_squared_error: 0.3656 - val_loss: 0.3614 - val_mean_squared_error: 0.3614\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36387 to 0.36144, saving model to temp/f147\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3612 - mean_squared_error: 0.3612 - val_loss: 0.3683 - val_mean_squared_error: 0.3683\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36144\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3597 - mean_squared_error: 0.3597 - val_loss: 0.3567 - val_mean_squared_error: 0.3567\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36144 to 0.35668, saving model to temp/f147\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3584 - mean_squared_error: 0.3584 - val_loss: 0.3577 - val_mean_squared_error: 0.3577\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35668\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3596 - mean_squared_error: 0.3596 - val_loss: 0.3524 - val_mean_squared_error: 0.3524\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35668 to 0.35240, saving model to temp/f147\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3570 - mean_squared_error: 0.3570 - val_loss: 0.3537 - val_mean_squared_error: 0.3537\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35240\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3585 - mean_squared_error: 0.3585 - val_loss: 0.3553 - val_mean_squared_error: 0.3553\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35240\n",
      "Epoch 00008: early stopping\n",
      "temp/f148\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.4564 - mean_squared_error: 0.4564 - val_loss: 0.3657 - val_mean_squared_error: 0.3657\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36568, saving model to temp/f148\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3620 - mean_squared_error: 0.3620 - val_loss: 0.3674 - val_mean_squared_error: 0.3674\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36568\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3631 - mean_squared_error: 0.3631 - val_loss: 0.3670 - val_mean_squared_error: 0.3670\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36568\n",
      "Epoch 00003: early stopping\n",
      "temp/f149\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4540 - mean_squared_error: 0.4540 - val_loss: 0.3676 - val_mean_squared_error: 0.3676\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36764, saving model to temp/f149\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3685 - mean_squared_error: 0.3685 - val_loss: 0.3660 - val_mean_squared_error: 0.3660\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36764 to 0.36599, saving model to temp/f149\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3637 - mean_squared_error: 0.3637 - val_loss: 0.3602 - val_mean_squared_error: 0.3602\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36599 to 0.36020, saving model to temp/f149\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3611 - mean_squared_error: 0.3611 - val_loss: 0.3586 - val_mean_squared_error: 0.3586\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36020 to 0.35858, saving model to temp/f149\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3537 - val_mean_squared_error: 0.3537\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35858 to 0.35371, saving model to temp/f149\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3589 - mean_squared_error: 0.3589 - val_loss: 0.3540 - val_mean_squared_error: 0.3540\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35371\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3560 - mean_squared_error: 0.3560 - val_loss: 0.3519 - val_mean_squared_error: 0.3519\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35371 to 0.35190, saving model to temp/f149\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3559 - mean_squared_error: 0.3559 - val_loss: 0.3521 - val_mean_squared_error: 0.3521\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35190\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3575 - mean_squared_error: 0.3575 - val_loss: 0.3527 - val_mean_squared_error: 0.3527\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35190\n",
      "Epoch 00009: early stopping\n",
      "temp/f150\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4544 - mean_squared_error: 0.4544 - val_loss: 0.3643 - val_mean_squared_error: 0.3643\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36433, saving model to temp/f150\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3657 - mean_squared_error: 0.3657 - val_loss: 0.3602 - val_mean_squared_error: 0.3602\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36433 to 0.36017, saving model to temp/f150\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3620 - mean_squared_error: 0.3620 - val_loss: 0.3555 - val_mean_squared_error: 0.3555\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36017 to 0.35551, saving model to temp/f150\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3611 - mean_squared_error: 0.3611 - val_loss: 0.3556 - val_mean_squared_error: 0.3556\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35551\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3598 - val_mean_squared_error: 0.3598\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35551\n",
      "Epoch 00005: early stopping\n",
      "temp/f151\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4525 - mean_squared_error: 0.4525 - val_loss: 0.3635 - val_mean_squared_error: 0.3635\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36351, saving model to temp/f151\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3688 - val_mean_squared_error: 0.3688\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36351\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3635 - mean_squared_error: 0.3635 - val_loss: 0.3633 - val_mean_squared_error: 0.3633\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36351 to 0.36334, saving model to temp/f151\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3623 - mean_squared_error: 0.3623 - val_loss: 0.3625 - val_mean_squared_error: 0.3625\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36334 to 0.36246, saving model to temp/f151\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3602 - mean_squared_error: 0.3602 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.36246 to 0.35495, saving model to temp/f151\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3594 - mean_squared_error: 0.3594 - val_loss: 0.3560 - val_mean_squared_error: 0.3560\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35495\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3508 - val_mean_squared_error: 0.3508\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35495 to 0.35076, saving model to temp/f151\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3564 - mean_squared_error: 0.3564 - val_loss: 0.3522 - val_mean_squared_error: 0.3522\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35076\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3535 - mean_squared_error: 0.3535 - val_loss: 0.3519 - val_mean_squared_error: 0.3519\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35076\n",
      "Epoch 00009: early stopping\n",
      "temp/f152\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4473 - mean_squared_error: 0.4473 - val_loss: 0.3710 - val_mean_squared_error: 0.3710\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37104, saving model to temp/f152\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3699 - mean_squared_error: 0.3699 - val_loss: 0.3632 - val_mean_squared_error: 0.3632\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37104 to 0.36317, saving model to temp/f152\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3632 - mean_squared_error: 0.3632 - val_loss: 0.3603 - val_mean_squared_error: 0.3603\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36317 to 0.36026, saving model to temp/f152\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3601 - mean_squared_error: 0.3601 - val_loss: 0.3541 - val_mean_squared_error: 0.3541\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36026 to 0.35412, saving model to temp/f152\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3594 - mean_squared_error: 0.3594 - val_loss: 0.3610 - val_mean_squared_error: 0.3610\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35412\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3568 - mean_squared_error: 0.3568 - val_loss: 0.3565 - val_mean_squared_error: 0.3565\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35412\n",
      "Epoch 00006: early stopping\n",
      "temp/f153\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4559 - mean_squared_error: 0.4559 - val_loss: 0.3651 - val_mean_squared_error: 0.3651\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36505, saving model to temp/f153\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3641 - mean_squared_error: 0.3641 - val_loss: 0.3686 - val_mean_squared_error: 0.3686\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36505\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3639 - mean_squared_error: 0.3639 - val_loss: 0.3717 - val_mean_squared_error: 0.3717\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36505\n",
      "Epoch 00003: early stopping\n",
      "temp/f154\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4574 - mean_squared_error: 0.4574 - val_loss: 0.3693 - val_mean_squared_error: 0.3693\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36934, saving model to temp/f154\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3641 - mean_squared_error: 0.3641 - val_loss: 0.3576 - val_mean_squared_error: 0.3576\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36934 to 0.35760, saving model to temp/f154\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3641 - mean_squared_error: 0.3641 - val_loss: 0.3705 - val_mean_squared_error: 0.3705\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35760\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3606 - mean_squared_error: 0.3606 - val_loss: 0.3542 - val_mean_squared_error: 0.3542\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35760 to 0.35417, saving model to temp/f154\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3536 - val_mean_squared_error: 0.3536\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35417 to 0.35362, saving model to temp/f154\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3612 - val_mean_squared_error: 0.3612\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35362\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3570 - mean_squared_error: 0.3570 - val_loss: 0.3549 - val_mean_squared_error: 0.3549\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35362\n",
      "Epoch 00007: early stopping\n",
      "temp/f155\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4524 - mean_squared_error: 0.4524 - val_loss: 0.3656 - val_mean_squared_error: 0.3656\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36557, saving model to temp/f155\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3685 - mean_squared_error: 0.3685 - val_loss: 0.3596 - val_mean_squared_error: 0.3596\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36557 to 0.35961, saving model to temp/f155\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3624 - mean_squared_error: 0.3624 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35961 to 0.35797, saving model to temp/f155\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3592 - mean_squared_error: 0.3592 - val_loss: 0.3562 - val_mean_squared_error: 0.3562\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35797 to 0.35623, saving model to temp/f155\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3591 - mean_squared_error: 0.3591 - val_loss: 0.3558 - val_mean_squared_error: 0.3558\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35623 to 0.35585, saving model to temp/f155\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3570 - mean_squared_error: 0.3570 - val_loss: 0.3596 - val_mean_squared_error: 0.3596\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35585\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3597 - mean_squared_error: 0.3597 - val_loss: 0.3555 - val_mean_squared_error: 0.3555\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35585 to 0.35547, saving model to temp/f155\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3554 - mean_squared_error: 0.3554 - val_loss: 0.3520 - val_mean_squared_error: 0.3520\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35547 to 0.35205, saving model to temp/f155\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3611 - val_mean_squared_error: 0.3611\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35205\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3552 - mean_squared_error: 0.3552 - val_loss: 0.3554 - val_mean_squared_error: 0.3554\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35205\n",
      "Epoch 00010: early stopping\n",
      "temp/f156\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4572 - mean_squared_error: 0.4572 - val_loss: 0.3662 - val_mean_squared_error: 0.3662\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36617, saving model to temp/f156\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3651 - mean_squared_error: 0.3651 - val_loss: 0.3600 - val_mean_squared_error: 0.3600\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36617 to 0.36001, saving model to temp/f156\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3629 - mean_squared_error: 0.3629 - val_loss: 0.3604 - val_mean_squared_error: 0.3604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36001\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3609 - mean_squared_error: 0.3609 - val_loss: 0.3577 - val_mean_squared_error: 0.3577\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36001 to 0.35766, saving model to temp/f156\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3604 - mean_squared_error: 0.3604 - val_loss: 0.3533 - val_mean_squared_error: 0.3533\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35766 to 0.35330, saving model to temp/f156\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3582 - mean_squared_error: 0.3582 - val_loss: 0.3561 - val_mean_squared_error: 0.3561\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35330\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3565 - mean_squared_error: 0.3565 - val_loss: 0.3532 - val_mean_squared_error: 0.3532\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35330 to 0.35319, saving model to temp/f156\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3582 - mean_squared_error: 0.3582 - val_loss: 0.3536 - val_mean_squared_error: 0.3536\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35319\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3502 - val_mean_squared_error: 0.3502\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35319 to 0.35025, saving model to temp/f156\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3546 - mean_squared_error: 0.3546 - val_loss: 0.3544 - val_mean_squared_error: 0.3544\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35025\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3540 - mean_squared_error: 0.3540 - val_loss: 0.3555 - val_mean_squared_error: 0.3555\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35025\n",
      "Epoch 00011: early stopping\n",
      "temp/f157\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4516 - mean_squared_error: 0.4516 - val_loss: 0.3656 - val_mean_squared_error: 0.3656\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36556, saving model to temp/f157\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3671 - mean_squared_error: 0.3671 - val_loss: 0.3575 - val_mean_squared_error: 0.3575\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36556 to 0.35747, saving model to temp/f157\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3605 - mean_squared_error: 0.3605 - val_loss: 0.3628 - val_mean_squared_error: 0.3628\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35747\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3608 - mean_squared_error: 0.3608 - val_loss: 0.3536 - val_mean_squared_error: 0.3536\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35747 to 0.35364, saving model to temp/f157\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3560 - mean_squared_error: 0.3560 - val_loss: 0.3562 - val_mean_squared_error: 0.3562\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35364\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3595 - mean_squared_error: 0.3595 - val_loss: 0.3564 - val_mean_squared_error: 0.3564\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35364\n",
      "Epoch 00006: early stopping\n",
      "temp/f158\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4561 - mean_squared_error: 0.4561 - val_loss: 0.3614 - val_mean_squared_error: 0.3614\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36140, saving model to temp/f158\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3637 - mean_squared_error: 0.3637 - val_loss: 0.3631 - val_mean_squared_error: 0.3631\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36140\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3636 - mean_squared_error: 0.3636 - val_loss: 0.3644 - val_mean_squared_error: 0.3644\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36140\n",
      "Epoch 00003: early stopping\n",
      "temp/f159\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4502 - mean_squared_error: 0.4502 - val_loss: 0.3711 - val_mean_squared_error: 0.3711\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37108, saving model to temp/f159\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3655 - mean_squared_error: 0.3655 - val_loss: 0.3650 - val_mean_squared_error: 0.3650\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37108 to 0.36502, saving model to temp/f159\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3639 - mean_squared_error: 0.3639 - val_loss: 0.3615 - val_mean_squared_error: 0.3615\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36502 to 0.36146, saving model to temp/f159\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3601 - mean_squared_error: 0.3601 - val_loss: 0.3618 - val_mean_squared_error: 0.3618\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36146\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3594 - mean_squared_error: 0.3594 - val_loss: 0.3538 - val_mean_squared_error: 0.3538\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.36146 to 0.35383, saving model to temp/f159\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3578 - mean_squared_error: 0.3578 - val_loss: 0.3577 - val_mean_squared_error: 0.3577\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35383\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3567 - mean_squared_error: 0.3567 - val_loss: 0.3548 - val_mean_squared_error: 0.3548\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35383\n",
      "Epoch 00007: early stopping\n",
      "temp/f160\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 0.4490 - mean_squared_error: 0.4490 - val_loss: 0.3656 - val_mean_squared_error: 0.3656\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36558, saving model to temp/f160\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3644 - mean_squared_error: 0.3644 - val_loss: 0.3585 - val_mean_squared_error: 0.3585\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36558 to 0.35849, saving model to temp/f160\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3630 - mean_squared_error: 0.3630 - val_loss: 0.3605 - val_mean_squared_error: 0.3605\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35849\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3595 - mean_squared_error: 0.3595 - val_loss: 0.3571 - val_mean_squared_error: 0.3571\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35849 to 0.35715, saving model to temp/f160\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3615 - mean_squared_error: 0.3615 - val_loss: 0.3602 - val_mean_squared_error: 0.3602\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35715\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3530 - val_mean_squared_error: 0.3530\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35715 to 0.35302, saving model to temp/f160\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3570 - mean_squared_error: 0.3570 - val_loss: 0.3545 - val_mean_squared_error: 0.3545\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35302\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3555 - mean_squared_error: 0.3555 - val_loss: 0.3506 - val_mean_squared_error: 0.3506\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35302 to 0.35059, saving model to temp/f160\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3552 - mean_squared_error: 0.3552 - val_loss: 0.3621 - val_mean_squared_error: 0.3621\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35059\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3575 - mean_squared_error: 0.3575 - val_loss: 0.3595 - val_mean_squared_error: 0.3595\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35059\n",
      "Epoch 00010: early stopping\n",
      "temp/f161\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 0.4575 - mean_squared_error: 0.4575 - val_loss: 0.3704 - val_mean_squared_error: 0.3704\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37044, saving model to temp/f161\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3655 - mean_squared_error: 0.3655 - val_loss: 0.3601 - val_mean_squared_error: 0.3601\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37044 to 0.36005, saving model to temp/f161\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3622 - mean_squared_error: 0.3622 - val_loss: 0.3549 - val_mean_squared_error: 0.3549\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36005 to 0.35494, saving model to temp/f161\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3589 - mean_squared_error: 0.3589 - val_loss: 0.3591 - val_mean_squared_error: 0.3591\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35494\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3600 - mean_squared_error: 0.3600 - val_loss: 0.3542 - val_mean_squared_error: 0.3542\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35494 to 0.35424, saving model to temp/f161\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3590 - mean_squared_error: 0.3590 - val_loss: 0.3561 - val_mean_squared_error: 0.3561\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35424\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3571 - mean_squared_error: 0.3571 - val_loss: 0.3536 - val_mean_squared_error: 0.3536\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35424 to 0.35358, saving model to temp/f161\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3654 - val_mean_squared_error: 0.3654\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35358\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3562 - mean_squared_error: 0.3562 - val_loss: 0.3547 - val_mean_squared_error: 0.3547\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35358\n",
      "Epoch 00009: early stopping\n",
      "temp/f162\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4555 - mean_squared_error: 0.4555 - val_loss: 0.3664 - val_mean_squared_error: 0.3664\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36639, saving model to temp/f162\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3651 - mean_squared_error: 0.3651 - val_loss: 0.3597 - val_mean_squared_error: 0.3597\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36639 to 0.35971, saving model to temp/f162\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3617 - mean_squared_error: 0.3617 - val_loss: 0.3554 - val_mean_squared_error: 0.3554\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35971 to 0.35543, saving model to temp/f162\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3583 - val_mean_squared_error: 0.3583\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35543\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3604 - mean_squared_error: 0.3604 - val_loss: 0.3607 - val_mean_squared_error: 0.3607\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35543\n",
      "Epoch 00005: early stopping\n",
      "temp/f163\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4622 - mean_squared_error: 0.4622 - val_loss: 0.3637 - val_mean_squared_error: 0.3637\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36375, saving model to temp/f163\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3687 - mean_squared_error: 0.3687 - val_loss: 0.3602 - val_mean_squared_error: 0.3602\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36375 to 0.36021, saving model to temp/f163\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3632 - mean_squared_error: 0.3632 - val_loss: 0.3690 - val_mean_squared_error: 0.3690\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36021\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3602 - mean_squared_error: 0.3602 - val_loss: 0.3651 - val_mean_squared_error: 0.3651\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36021\n",
      "Epoch 00004: early stopping\n",
      "temp/f164\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4583 - mean_squared_error: 0.4583 - val_loss: 0.3667 - val_mean_squared_error: 0.3667\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36671, saving model to temp/f164\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3650 - mean_squared_error: 0.3650 - val_loss: 0.3591 - val_mean_squared_error: 0.3591\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36671 to 0.35909, saving model to temp/f164\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3627 - mean_squared_error: 0.3627 - val_loss: 0.3595 - val_mean_squared_error: 0.3595\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35909\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3604 - val_mean_squared_error: 0.3604\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35909\n",
      "Epoch 00004: early stopping\n",
      "temp/f165\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4639 - mean_squared_error: 0.4639 - val_loss: 0.3680 - val_mean_squared_error: 0.3680\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36801, saving model to temp/f165\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3669 - mean_squared_error: 0.3669 - val_loss: 0.3633 - val_mean_squared_error: 0.3633\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36801 to 0.36330, saving model to temp/f165\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3629 - mean_squared_error: 0.3629 - val_loss: 0.3609 - val_mean_squared_error: 0.3609\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36330 to 0.36093, saving model to temp/f165\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3596 - mean_squared_error: 0.3596 - val_loss: 0.3593 - val_mean_squared_error: 0.3593\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36093 to 0.35925, saving model to temp/f165\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3610 - mean_squared_error: 0.3610 - val_loss: 0.3560 - val_mean_squared_error: 0.3560\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35925 to 0.35600, saving model to temp/f165\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3554 - mean_squared_error: 0.3554 - val_loss: 0.3693 - val_mean_squared_error: 0.3693\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35600\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3563 - mean_squared_error: 0.3563 - val_loss: 0.3548 - val_mean_squared_error: 0.3548\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35600 to 0.35485, saving model to temp/f165\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35485\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3549 - mean_squared_error: 0.3549 - val_loss: 0.3508 - val_mean_squared_error: 0.3508\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35485 to 0.35077, saving model to temp/f165\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3566 - mean_squared_error: 0.3566 - val_loss: 0.3683 - val_mean_squared_error: 0.3683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35077\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3536 - mean_squared_error: 0.3536 - val_loss: 0.3508 - val_mean_squared_error: 0.3508\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35077\n",
      "Epoch 00011: early stopping\n",
      "temp/f166\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4573 - mean_squared_error: 0.4573 - val_loss: 0.3740 - val_mean_squared_error: 0.3740\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37398, saving model to temp/f166\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3659 - mean_squared_error: 0.3659 - val_loss: 0.3593 - val_mean_squared_error: 0.3593\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37398 to 0.35926, saving model to temp/f166\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3621 - mean_squared_error: 0.3621 - val_loss: 0.3639 - val_mean_squared_error: 0.3639\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35926\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 142us/step - loss: 0.3593 - mean_squared_error: 0.3593 - val_loss: 0.3549 - val_mean_squared_error: 0.3549\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35926 to 0.35487, saving model to temp/f166\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3575 - mean_squared_error: 0.3575 - val_loss: 0.3544 - val_mean_squared_error: 0.3544\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35487 to 0.35436, saving model to temp/f166\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3582 - mean_squared_error: 0.3582 - val_loss: 0.3551 - val_mean_squared_error: 0.3551\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35436\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3575 - mean_squared_error: 0.3575 - val_loss: 0.3578 - val_mean_squared_error: 0.3578\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35436\n",
      "Epoch 00007: early stopping\n",
      "temp/f167\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 0.4515 - mean_squared_error: 0.4515 - val_loss: 0.3647 - val_mean_squared_error: 0.3647\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36475, saving model to temp/f167\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3645 - mean_squared_error: 0.3645 - val_loss: 0.3621 - val_mean_squared_error: 0.3621\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36475 to 0.36205, saving model to temp/f167\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3604 - mean_squared_error: 0.3604 - val_loss: 0.3566 - val_mean_squared_error: 0.3566\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36205 to 0.35656, saving model to temp/f167\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3594 - mean_squared_error: 0.3594 - val_loss: 0.3600 - val_mean_squared_error: 0.3600\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35656\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3590 - mean_squared_error: 0.3590 - val_loss: 0.3608 - val_mean_squared_error: 0.3608\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35656\n",
      "Epoch 00005: early stopping\n",
      "temp/f168\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4581 - mean_squared_error: 0.4581 - val_loss: 0.3765 - val_mean_squared_error: 0.3765\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37654, saving model to temp/f168\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3665 - mean_squared_error: 0.3665 - val_loss: 0.3610 - val_mean_squared_error: 0.3610\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37654 to 0.36096, saving model to temp/f168\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3622 - mean_squared_error: 0.3622 - val_loss: 0.3678 - val_mean_squared_error: 0.3678\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36096\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3623 - mean_squared_error: 0.3623 - val_loss: 0.3592 - val_mean_squared_error: 0.3592\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36096 to 0.35922, saving model to temp/f168\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3614 - mean_squared_error: 0.3614 - val_loss: 0.3641 - val_mean_squared_error: 0.3641\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35922\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3605 - mean_squared_error: 0.3605 - val_loss: 0.3534 - val_mean_squared_error: 0.3534\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35922 to 0.35341, saving model to temp/f168\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3558 - mean_squared_error: 0.3558 - val_loss: 0.3561 - val_mean_squared_error: 0.3561\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35341\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3562 - mean_squared_error: 0.3562 - val_loss: 0.3568 - val_mean_squared_error: 0.3568\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35341\n",
      "Epoch 00008: early stopping\n",
      "temp/f169\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4524 - mean_squared_error: 0.4524 - val_loss: 0.3661 - val_mean_squared_error: 0.3661\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36605, saving model to temp/f169\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3662 - mean_squared_error: 0.3662 - val_loss: 0.3600 - val_mean_squared_error: 0.3600\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36605 to 0.35997, saving model to temp/f169\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3619 - mean_squared_error: 0.3619 - val_loss: 0.3554 - val_mean_squared_error: 0.3554\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35997 to 0.35537, saving model to temp/f169\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3604 - mean_squared_error: 0.3604 - val_loss: 0.3538 - val_mean_squared_error: 0.3538\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35537 to 0.35379, saving model to temp/f169\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3598 - mean_squared_error: 0.3598 - val_loss: 0.3575 - val_mean_squared_error: 0.3575\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35379\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3594 - mean_squared_error: 0.3594 - val_loss: 0.3542 - val_mean_squared_error: 0.3542\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35379\n",
      "Epoch 00006: early stopping\n",
      "temp/f170\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 0.4454 - mean_squared_error: 0.4454 - val_loss: 0.3665 - val_mean_squared_error: 0.3665\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36648, saving model to temp/f170\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3671 - mean_squared_error: 0.3671 - val_loss: 0.3608 - val_mean_squared_error: 0.3608\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36648 to 0.36078, saving model to temp/f170\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3645 - mean_squared_error: 0.3645 - val_loss: 0.3578 - val_mean_squared_error: 0.3578\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36078 to 0.35776, saving model to temp/f170\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3593 - mean_squared_error: 0.3593 - val_loss: 0.3632 - val_mean_squared_error: 0.3632\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35776\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3611 - mean_squared_error: 0.3611 - val_loss: 0.3560 - val_mean_squared_error: 0.3560\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35776 to 0.35597, saving model to temp/f170\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3568 - mean_squared_error: 0.3568 - val_loss: 0.3570 - val_mean_squared_error: 0.3570\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35597\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3563 - mean_squared_error: 0.3563 - val_loss: 0.3571 - val_mean_squared_error: 0.3571\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35597\n",
      "Epoch 00007: early stopping\n",
      "temp/f171\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4475 - mean_squared_error: 0.4475 - val_loss: 0.3667 - val_mean_squared_error: 0.3667\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36666, saving model to temp/f171\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3665 - mean_squared_error: 0.3665 - val_loss: 0.3592 - val_mean_squared_error: 0.3592\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36666 to 0.35919, saving model to temp/f171\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3614 - mean_squared_error: 0.3614 - val_loss: 0.3612 - val_mean_squared_error: 0.3612\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35919\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3603 - mean_squared_error: 0.3603 - val_loss: 0.3628 - val_mean_squared_error: 0.3628\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35919\n",
      "Epoch 00004: early stopping\n",
      "temp/f172\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.4591 - mean_squared_error: 0.4591 - val_loss: 0.3649 - val_mean_squared_error: 0.3649\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36493, saving model to temp/f172\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3619 - mean_squared_error: 0.3619 - val_loss: 0.3617 - val_mean_squared_error: 0.3617\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36493 to 0.36173, saving model to temp/f172\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3643 - mean_squared_error: 0.3643 - val_loss: 0.3623 - val_mean_squared_error: 0.3623\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36173\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3581 - mean_squared_error: 0.3581 - val_loss: 0.3573 - val_mean_squared_error: 0.3573\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36173 to 0.35726, saving model to temp/f172\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3585 - mean_squared_error: 0.3585 - val_loss: 0.3561 - val_mean_squared_error: 0.3561\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35726 to 0.35615, saving model to temp/f172\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3585 - mean_squared_error: 0.3585 - val_loss: 0.3555 - val_mean_squared_error: 0.3555\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35615 to 0.35549, saving model to temp/f172\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3563 - mean_squared_error: 0.3563 - val_loss: 0.3584 - val_mean_squared_error: 0.3584\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35549\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3601 - val_mean_squared_error: 0.3601\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35549\n",
      "Epoch 00008: early stopping\n",
      "temp/f173\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4504 - mean_squared_error: 0.4504 - val_loss: 0.3666 - val_mean_squared_error: 0.3666\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36664, saving model to temp/f173\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3660 - mean_squared_error: 0.3660 - val_loss: 0.3634 - val_mean_squared_error: 0.3634\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36664 to 0.36337, saving model to temp/f173\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3619 - mean_squared_error: 0.3619 - val_loss: 0.3615 - val_mean_squared_error: 0.3615\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36337 to 0.36152, saving model to temp/f173\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3601 - mean_squared_error: 0.3601 - val_loss: 0.3648 - val_mean_squared_error: 0.3648\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36152\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3612 - mean_squared_error: 0.3612 - val_loss: 0.3589 - val_mean_squared_error: 0.3589\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.36152 to 0.35886, saving model to temp/f173\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3557 - val_mean_squared_error: 0.3557\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35886 to 0.35569, saving model to temp/f173\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3585 - mean_squared_error: 0.3585 - val_loss: 0.3630 - val_mean_squared_error: 0.3630\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35569\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3567 - mean_squared_error: 0.3567 - val_loss: 0.3520 - val_mean_squared_error: 0.3520\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35569 to 0.35199, saving model to temp/f173\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3568 - mean_squared_error: 0.3568 - val_loss: 0.3542 - val_mean_squared_error: 0.3542\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35199\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3547 - mean_squared_error: 0.3547 - val_loss: 0.3577 - val_mean_squared_error: 0.3577\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35199\n",
      "Epoch 00010: early stopping\n",
      "temp/f174\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 0.4547 - mean_squared_error: 0.4547 - val_loss: 0.3694 - val_mean_squared_error: 0.3694\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36939, saving model to temp/f174\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3658 - mean_squared_error: 0.3658 - val_loss: 0.3587 - val_mean_squared_error: 0.3587\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36939 to 0.35866, saving model to temp/f174\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3617 - mean_squared_error: 0.3617 - val_loss: 0.3615 - val_mean_squared_error: 0.3615\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35866\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3602 - mean_squared_error: 0.3602 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35866 to 0.35495, saving model to temp/f174\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3609 - mean_squared_error: 0.3609 - val_loss: 0.3616 - val_mean_squared_error: 0.3616\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35495\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3589 - mean_squared_error: 0.3589 - val_loss: 0.3540 - val_mean_squared_error: 0.3540\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35495 to 0.35405, saving model to temp/f174\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3575 - mean_squared_error: 0.3575 - val_loss: 0.3535 - val_mean_squared_error: 0.3535\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35405 to 0.35346, saving model to temp/f174\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3556 - mean_squared_error: 0.3556 - val_loss: 0.3586 - val_mean_squared_error: 0.3586\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35346\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3555 - mean_squared_error: 0.3555 - val_loss: 0.3527 - val_mean_squared_error: 0.3527\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35346 to 0.35270, saving model to temp/f174\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3551 - mean_squared_error: 0.3551 - val_loss: 0.3517 - val_mean_squared_error: 0.3517\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.35270 to 0.35166, saving model to temp/f174\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3549 - mean_squared_error: 0.3549 - val_loss: 0.3513 - val_mean_squared_error: 0.3513\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.35166 to 0.35133, saving model to temp/f174\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3521 - mean_squared_error: 0.3521 - val_loss: 0.3519 - val_mean_squared_error: 0.3519\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35133\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3531 - mean_squared_error: 0.3531 - val_loss: 0.3508 - val_mean_squared_error: 0.3508\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.35133 to 0.35078, saving model to temp/f174\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3528 - mean_squared_error: 0.3528 - val_loss: 0.3504 - val_mean_squared_error: 0.3504\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.35078 to 0.35038, saving model to temp/f174\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3547 - mean_squared_error: 0.3547 - val_loss: 0.3507 - val_mean_squared_error: 0.3507\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.35038\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3527 - mean_squared_error: 0.3527 - val_loss: 0.3563 - val_mean_squared_error: 0.3563\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.35038\n",
      "Epoch 00016: early stopping\n",
      "temp/f175\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4576 - mean_squared_error: 0.4576 - val_loss: 0.3814 - val_mean_squared_error: 0.3814\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.38138, saving model to temp/f175\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3617 - mean_squared_error: 0.3617 - val_loss: 0.3597 - val_mean_squared_error: 0.3597\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.38138 to 0.35974, saving model to temp/f175\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3606 - mean_squared_error: 0.3606 - val_loss: 0.3613 - val_mean_squared_error: 0.3613\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35974\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3589 - mean_squared_error: 0.3589 - val_loss: 0.3566 - val_mean_squared_error: 0.3566\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35974 to 0.35658, saving model to temp/f175\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3601 - mean_squared_error: 0.3601 - val_loss: 0.3612 - val_mean_squared_error: 0.3612\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35658\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3569 - mean_squared_error: 0.3569 - val_loss: 0.3551 - val_mean_squared_error: 0.3551\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35658 to 0.35510, saving model to temp/f175\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3547 - mean_squared_error: 0.3547 - val_loss: 0.3532 - val_mean_squared_error: 0.3532\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35510 to 0.35321, saving model to temp/f175\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3551 - mean_squared_error: 0.3551 - val_loss: 0.3533 - val_mean_squared_error: 0.3533\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35321\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3565 - mean_squared_error: 0.3565 - val_loss: 0.3611 - val_mean_squared_error: 0.3611\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35321\n",
      "Epoch 00009: early stopping\n",
      "temp/f176\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4570 - mean_squared_error: 0.4570 - val_loss: 0.3636 - val_mean_squared_error: 0.3636\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36362, saving model to temp/f176\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3624 - mean_squared_error: 0.3624 - val_loss: 0.3608 - val_mean_squared_error: 0.3608\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36362 to 0.36078, saving model to temp/f176\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3628 - mean_squared_error: 0.3628 - val_loss: 0.3637 - val_mean_squared_error: 0.3637\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36078\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3621 - mean_squared_error: 0.3621 - val_loss: 0.3670 - val_mean_squared_error: 0.3670\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36078\n",
      "Epoch 00004: early stopping\n",
      "temp/f177\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4501 - mean_squared_error: 0.4501 - val_loss: 0.3646 - val_mean_squared_error: 0.3646\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36459, saving model to temp/f177\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3654 - mean_squared_error: 0.3654 - val_loss: 0.3578 - val_mean_squared_error: 0.3578\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36459 to 0.35783, saving model to temp/f177\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3644 - mean_squared_error: 0.3644 - val_loss: 0.3619 - val_mean_squared_error: 0.3619\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35783\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3600 - mean_squared_error: 0.3600 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35783 to 0.35498, saving model to temp/f177\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3569 - mean_squared_error: 0.3569 - val_loss: 0.3594 - val_mean_squared_error: 0.3594\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35498\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3577 - mean_squared_error: 0.3577 - val_loss: 0.3568 - val_mean_squared_error: 0.3568\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35498\n",
      "Epoch 00006: early stopping\n",
      "temp/f178\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4511 - mean_squared_error: 0.4511 - val_loss: 0.3640 - val_mean_squared_error: 0.3640\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36396, saving model to temp/f178\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3651 - mean_squared_error: 0.3651 - val_loss: 0.3579 - val_mean_squared_error: 0.3579\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36396 to 0.35793, saving model to temp/f178\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3636 - mean_squared_error: 0.3636 - val_loss: 0.3565 - val_mean_squared_error: 0.3565\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35793 to 0.35652, saving model to temp/f178\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3591 - mean_squared_error: 0.3591 - val_loss: 0.3664 - val_mean_squared_error: 0.3664\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35652\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3604 - mean_squared_error: 0.3604 - val_loss: 0.3604 - val_mean_squared_error: 0.3604\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35652\n",
      "Epoch 00005: early stopping\n",
      "temp/f179\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 0.4446 - mean_squared_error: 0.4446 - val_loss: 0.3808 - val_mean_squared_error: 0.3808\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.38079, saving model to temp/f179\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3683 - mean_squared_error: 0.3683 - val_loss: 0.3571 - val_mean_squared_error: 0.3571\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.38079 to 0.35708, saving model to temp/f179\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3662 - mean_squared_error: 0.3662 - val_loss: 0.3551 - val_mean_squared_error: 0.3551\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35708 to 0.35514, saving model to temp/f179\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3588 - val_mean_squared_error: 0.3588\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35514\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3596 - mean_squared_error: 0.3596 - val_loss: 0.3574 - val_mean_squared_error: 0.3574\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35514\n",
      "Epoch 00005: early stopping\n",
      "temp/f180\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4513 - mean_squared_error: 0.4513 - val_loss: 0.3641 - val_mean_squared_error: 0.3641\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36405, saving model to temp/f180\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3669 - mean_squared_error: 0.3669 - val_loss: 0.3609 - val_mean_squared_error: 0.3609\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36405 to 0.36090, saving model to temp/f180\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3629 - mean_squared_error: 0.3629 - val_loss: 0.3618 - val_mean_squared_error: 0.3618\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36090\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3625 - mean_squared_error: 0.3625 - val_loss: 0.3653 - val_mean_squared_error: 0.3653\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36090\n",
      "Epoch 00004: early stopping\n",
      "temp/f181\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 0.4504 - mean_squared_error: 0.4504 - val_loss: 0.3769 - val_mean_squared_error: 0.3769\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37693, saving model to temp/f181\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3676 - mean_squared_error: 0.3676 - val_loss: 0.3615 - val_mean_squared_error: 0.3615\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37693 to 0.36152, saving model to temp/f181\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3592 - mean_squared_error: 0.3592 - val_loss: 0.3575 - val_mean_squared_error: 0.3575\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36152 to 0.35748, saving model to temp/f181\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3603 - mean_squared_error: 0.3603 - val_loss: 0.3564 - val_mean_squared_error: 0.3564\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35748 to 0.35639, saving model to temp/f181\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3611 - mean_squared_error: 0.3611 - val_loss: 0.3548 - val_mean_squared_error: 0.3548\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35639 to 0.35483, saving model to temp/f181\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3580 - mean_squared_error: 0.3580 - val_loss: 0.3538 - val_mean_squared_error: 0.3538\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35483 to 0.35382, saving model to temp/f181\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3592 - val_mean_squared_error: 0.3592\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35382\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3560 - mean_squared_error: 0.3560 - val_loss: 0.3535 - val_mean_squared_error: 0.3535\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35382 to 0.35353, saving model to temp/f181\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3571 - mean_squared_error: 0.3571 - val_loss: 0.3521 - val_mean_squared_error: 0.3521\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35353 to 0.35211, saving model to temp/f181\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3564 - mean_squared_error: 0.3564 - val_loss: 0.3504 - val_mean_squared_error: 0.3504\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.35211 to 0.35042, saving model to temp/f181\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3548 - mean_squared_error: 0.3548 - val_loss: 0.3493 - val_mean_squared_error: 0.3493\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.35042 to 0.34928, saving model to temp/f181\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3540 - mean_squared_error: 0.3540 - val_loss: 0.3573 - val_mean_squared_error: 0.3573\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.34928\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3540 - mean_squared_error: 0.3540 - val_loss: 0.3499 - val_mean_squared_error: 0.3499\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.34928\n",
      "Epoch 00013: early stopping\n",
      "temp/f182\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 0.4598 - mean_squared_error: 0.4598 - val_loss: 0.3697 - val_mean_squared_error: 0.3697\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36972, saving model to temp/f182\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3653 - mean_squared_error: 0.3653 - val_loss: 0.3725 - val_mean_squared_error: 0.3725\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36972\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3606 - mean_squared_error: 0.3606 - val_loss: 0.3573 - val_mean_squared_error: 0.3573\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36972 to 0.35725, saving model to temp/f182\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3610 - mean_squared_error: 0.3610 - val_loss: 0.3539 - val_mean_squared_error: 0.3539\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35725 to 0.35393, saving model to temp/f182\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3602 - mean_squared_error: 0.3602 - val_loss: 0.3664 - val_mean_squared_error: 0.3664\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35393\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3584 - mean_squared_error: 0.3584 - val_loss: 0.3537 - val_mean_squared_error: 0.3537\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35393 to 0.35367, saving model to temp/f182\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3569 - val_mean_squared_error: 0.3569\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35367\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3523 - val_mean_squared_error: 0.3523\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35367 to 0.35233, saving model to temp/f182\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3547 - mean_squared_error: 0.3547 - val_loss: 0.3659 - val_mean_squared_error: 0.3659\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35233\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3551 - mean_squared_error: 0.3551 - val_loss: 0.3495 - val_mean_squared_error: 0.3495\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.35233 to 0.34954, saving model to temp/f182\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3546 - mean_squared_error: 0.3546 - val_loss: 0.3517 - val_mean_squared_error: 0.3517\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.34954\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3537 - mean_squared_error: 0.3537 - val_loss: 0.3559 - val_mean_squared_error: 0.3559\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.34954\n",
      "Epoch 00012: early stopping\n",
      "temp/f183\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 0.4599 - mean_squared_error: 0.4599 - val_loss: 0.3649 - val_mean_squared_error: 0.3649\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36491, saving model to temp/f183\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3681 - mean_squared_error: 0.3681 - val_loss: 0.3583 - val_mean_squared_error: 0.3583\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36491 to 0.35830, saving model to temp/f183\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3619 - mean_squared_error: 0.3619 - val_loss: 0.3603 - val_mean_squared_error: 0.3603\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35830\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3621 - mean_squared_error: 0.3621 - val_loss: 0.3878 - val_mean_squared_error: 0.3878\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35830\n",
      "Epoch 00004: early stopping\n",
      "temp/f184\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4498 - mean_squared_error: 0.4498 - val_loss: 0.3640 - val_mean_squared_error: 0.3640\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36404, saving model to temp/f184\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3636 - mean_squared_error: 0.3636 - val_loss: 0.3592 - val_mean_squared_error: 0.3592\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36404 to 0.35917, saving model to temp/f184\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3630 - mean_squared_error: 0.3630 - val_loss: 0.3588 - val_mean_squared_error: 0.3588\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35917 to 0.35881, saving model to temp/f184\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3608 - mean_squared_error: 0.3608 - val_loss: 0.3689 - val_mean_squared_error: 0.3689\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35881\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3584 - mean_squared_error: 0.3584 - val_loss: 0.3555 - val_mean_squared_error: 0.3555\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35881 to 0.35549, saving model to temp/f184\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3596 - mean_squared_error: 0.3596 - val_loss: 0.3564 - val_mean_squared_error: 0.3564\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35549\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3562 - mean_squared_error: 0.3562 - val_loss: 0.3737 - val_mean_squared_error: 0.3737\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35549\n",
      "Epoch 00007: early stopping\n",
      "temp/f185\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 0.4579 - mean_squared_error: 0.4579 - val_loss: 0.3615 - val_mean_squared_error: 0.3615\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36153, saving model to temp/f185\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3640 - mean_squared_error: 0.3640 - val_loss: 0.3616 - val_mean_squared_error: 0.3616\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36153\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3605 - mean_squared_error: 0.3605 - val_loss: 0.3620 - val_mean_squared_error: 0.3620\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36153\n",
      "Epoch 00003: early stopping\n",
      "temp/f186\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4578 - mean_squared_error: 0.4578 - val_loss: 0.3770 - val_mean_squared_error: 0.3770\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37698, saving model to temp/f186\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3679 - mean_squared_error: 0.3679 - val_loss: 0.3570 - val_mean_squared_error: 0.3570\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37698 to 0.35705, saving model to temp/f186\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3633 - mean_squared_error: 0.3633 - val_loss: 0.3573 - val_mean_squared_error: 0.3573\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35705\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3557 - val_mean_squared_error: 0.3557\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35705 to 0.35568, saving model to temp/f186\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3598 - mean_squared_error: 0.3598 - val_loss: 0.3593 - val_mean_squared_error: 0.3593\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35568\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3583 - mean_squared_error: 0.3583 - val_loss: 0.3608 - val_mean_squared_error: 0.3608\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35568\n",
      "Epoch 00006: early stopping\n",
      "temp/f187\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.4581 - mean_squared_error: 0.4581 - val_loss: 0.3639 - val_mean_squared_error: 0.3639\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36387, saving model to temp/f187\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3653 - mean_squared_error: 0.3653 - val_loss: 0.3624 - val_mean_squared_error: 0.3624\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36387 to 0.36242, saving model to temp/f187\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3635 - mean_squared_error: 0.3635 - val_loss: 0.3668 - val_mean_squared_error: 0.3668\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36242\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3600 - mean_squared_error: 0.3600 - val_loss: 0.3582 - val_mean_squared_error: 0.3582\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36242 to 0.35823, saving model to temp/f187\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3608 - mean_squared_error: 0.3608 - val_loss: 0.3566 - val_mean_squared_error: 0.3566\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35823 to 0.35662, saving model to temp/f187\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3578 - mean_squared_error: 0.3578 - val_loss: 0.3542 - val_mean_squared_error: 0.3542\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35662 to 0.35418, saving model to temp/f187\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3588 - mean_squared_error: 0.3588 - val_loss: 0.3561 - val_mean_squared_error: 0.3561\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35418\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3565 - mean_squared_error: 0.3565 - val_loss: 0.3506 - val_mean_squared_error: 0.3506\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35418 to 0.35058, saving model to temp/f187\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3566 - mean_squared_error: 0.3566 - val_loss: 0.3542 - val_mean_squared_error: 0.3542\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35058\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3551 - mean_squared_error: 0.3551 - val_loss: 0.3532 - val_mean_squared_error: 0.3532\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35058\n",
      "Epoch 00010: early stopping\n",
      "temp/f188\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4515 - mean_squared_error: 0.4515 - val_loss: 0.3690 - val_mean_squared_error: 0.3690\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36901, saving model to temp/f188\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3655 - mean_squared_error: 0.3655 - val_loss: 0.3592 - val_mean_squared_error: 0.3592\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36901 to 0.35917, saving model to temp/f188\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3600 - mean_squared_error: 0.3600 - val_loss: 0.3568 - val_mean_squared_error: 0.3568\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35917 to 0.35683, saving model to temp/f188\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3605 - mean_squared_error: 0.3605 - val_loss: 0.3689 - val_mean_squared_error: 0.3689\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35683\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3581 - mean_squared_error: 0.3581 - val_loss: 0.3633 - val_mean_squared_error: 0.3633\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35683\n",
      "Epoch 00005: early stopping\n",
      "temp/f189\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4451 - mean_squared_error: 0.4451 - val_loss: 0.3629 - val_mean_squared_error: 0.3629\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36289, saving model to temp/f189\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3662 - mean_squared_error: 0.3662 - val_loss: 0.3579 - val_mean_squared_error: 0.3579\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36289 to 0.35786, saving model to temp/f189\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3610 - mean_squared_error: 0.3610 - val_loss: 0.3604 - val_mean_squared_error: 0.3604\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35786\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3611 - mean_squared_error: 0.3611 - val_loss: 0.3564 - val_mean_squared_error: 0.3564\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35786 to 0.35638, saving model to temp/f189\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3585 - mean_squared_error: 0.3585 - val_loss: 0.3575 - val_mean_squared_error: 0.3575\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35638\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3579 - mean_squared_error: 0.3579 - val_loss: 0.3539 - val_mean_squared_error: 0.3539\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35638 to 0.35388, saving model to temp/f189\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3520 - val_mean_squared_error: 0.3520\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35388 to 0.35201, saving model to temp/f189\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3512 - val_mean_squared_error: 0.3512\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35201 to 0.35123, saving model to temp/f189\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3579 - mean_squared_error: 0.3579 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35123\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3531 - mean_squared_error: 0.3531 - val_loss: 0.3511 - val_mean_squared_error: 0.3511\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.35123 to 0.35111, saving model to temp/f189\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3547 - mean_squared_error: 0.3547 - val_loss: 0.3532 - val_mean_squared_error: 0.3532\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35111\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3546 - mean_squared_error: 0.3546 - val_loss: 0.3512 - val_mean_squared_error: 0.3512\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35111\n",
      "Epoch 00012: early stopping\n",
      "temp/f190\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4576 - mean_squared_error: 0.4576 - val_loss: 0.3666 - val_mean_squared_error: 0.3666\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36658, saving model to temp/f190\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3653 - mean_squared_error: 0.3653 - val_loss: 0.3633 - val_mean_squared_error: 0.3633\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36658 to 0.36329, saving model to temp/f190\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3641 - mean_squared_error: 0.3641 - val_loss: 0.3613 - val_mean_squared_error: 0.3613\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36329 to 0.36134, saving model to temp/f190\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3584 - mean_squared_error: 0.3584 - val_loss: 0.3552 - val_mean_squared_error: 0.3552\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36134 to 0.35520, saving model to temp/f190\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3570 - mean_squared_error: 0.3570 - val_loss: 0.3651 - val_mean_squared_error: 0.3651\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35520\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3580 - mean_squared_error: 0.3580 - val_loss: 0.3620 - val_mean_squared_error: 0.3620\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35520\n",
      "Epoch 00006: early stopping\n",
      "temp/f191\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4598 - mean_squared_error: 0.4598 - val_loss: 0.3706 - val_mean_squared_error: 0.3706\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37064, saving model to temp/f191\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3662 - mean_squared_error: 0.3662 - val_loss: 0.3620 - val_mean_squared_error: 0.3620\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37064 to 0.36203, saving model to temp/f191\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3603 - val_mean_squared_error: 0.3603\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36203 to 0.36027, saving model to temp/f191\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3606 - mean_squared_error: 0.3606 - val_loss: 0.3563 - val_mean_squared_error: 0.3563\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36027 to 0.35633, saving model to temp/f191\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3598 - mean_squared_error: 0.3598 - val_loss: 0.3555 - val_mean_squared_error: 0.3555\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35633 to 0.35548, saving model to temp/f191\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3581 - mean_squared_error: 0.3581 - val_loss: 0.3555 - val_mean_squared_error: 0.3555\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35548 to 0.35548, saving model to temp/f191\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3597 - mean_squared_error: 0.3597 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35548\n",
      "Epoch 00007: early stopping\n",
      "temp/f192\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4595 - mean_squared_error: 0.4595 - val_loss: 0.3616 - val_mean_squared_error: 0.3616\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36159, saving model to temp/f192\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3679 - mean_squared_error: 0.3679 - val_loss: 0.3567 - val_mean_squared_error: 0.3567\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36159 to 0.35673, saving model to temp/f192\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3621 - mean_squared_error: 0.3621 - val_loss: 0.3571 - val_mean_squared_error: 0.3571\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35673\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3588 - mean_squared_error: 0.3588 - val_loss: 0.3609 - val_mean_squared_error: 0.3609\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35673\n",
      "Epoch 00004: early stopping\n",
      "temp/f193\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4525 - mean_squared_error: 0.4525 - val_loss: 0.3692 - val_mean_squared_error: 0.3692\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36920, saving model to temp/f193\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3653 - mean_squared_error: 0.3653 - val_loss: 0.3600 - val_mean_squared_error: 0.3600\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36920 to 0.35996, saving model to temp/f193\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3640 - mean_squared_error: 0.3640 - val_loss: 0.3619 - val_mean_squared_error: 0.3619\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35996\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3607 - mean_squared_error: 0.3607 - val_loss: 0.3564 - val_mean_squared_error: 0.3564\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35996 to 0.35635, saving model to temp/f193\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3613 - mean_squared_error: 0.3613 - val_loss: 0.3602 - val_mean_squared_error: 0.3602\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35635\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3569 - mean_squared_error: 0.3569 - val_loss: 0.3579 - val_mean_squared_error: 0.3579\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35635\n",
      "Epoch 00006: early stopping\n",
      "temp/f194\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4526 - mean_squared_error: 0.4526 - val_loss: 0.3625 - val_mean_squared_error: 0.3625\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36246, saving model to temp/f194\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3674 - mean_squared_error: 0.3674 - val_loss: 0.3586 - val_mean_squared_error: 0.3586\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36246 to 0.35856, saving model to temp/f194\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3624 - mean_squared_error: 0.3624 - val_loss: 0.3611 - val_mean_squared_error: 0.3611\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35856\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3597 - mean_squared_error: 0.3597 - val_loss: 0.3650 - val_mean_squared_error: 0.3650\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35856\n",
      "Epoch 00004: early stopping\n",
      "temp/f195\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4583 - mean_squared_error: 0.4583 - val_loss: 0.3639 - val_mean_squared_error: 0.3639\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36389, saving model to temp/f195\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3663 - mean_squared_error: 0.3663 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36389 to 0.35804, saving model to temp/f195\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3597 - mean_squared_error: 0.3597 - val_loss: 0.3615 - val_mean_squared_error: 0.3615\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35804\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3614 - mean_squared_error: 0.3614 - val_loss: 0.3566 - val_mean_squared_error: 0.3566\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35804 to 0.35665, saving model to temp/f195\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3591 - mean_squared_error: 0.3591 - val_loss: 0.3543 - val_mean_squared_error: 0.3543\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35665 to 0.35426, saving model to temp/f195\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3604 - mean_squared_error: 0.3604 - val_loss: 0.3556 - val_mean_squared_error: 0.3556\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35426\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3530 - val_mean_squared_error: 0.3530\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35426 to 0.35299, saving model to temp/f195\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3563 - mean_squared_error: 0.3563 - val_loss: 0.3567 - val_mean_squared_error: 0.3567\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35299\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3549 - mean_squared_error: 0.3549 - val_loss: 0.3590 - val_mean_squared_error: 0.3590\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35299\n",
      "Epoch 00009: early stopping\n",
      "temp/f196\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4472 - mean_squared_error: 0.4472 - val_loss: 0.3658 - val_mean_squared_error: 0.3658\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36583, saving model to temp/f196\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3671 - mean_squared_error: 0.3671 - val_loss: 0.3635 - val_mean_squared_error: 0.3635\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36583 to 0.36352, saving model to temp/f196\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3626 - mean_squared_error: 0.3626 - val_loss: 0.3708 - val_mean_squared_error: 0.3708\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36352\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3578 - mean_squared_error: 0.3578 - val_loss: 0.3556 - val_mean_squared_error: 0.3556\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36352 to 0.35561, saving model to temp/f196\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3608 - val_mean_squared_error: 0.3608\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35561\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3577 - mean_squared_error: 0.3577 - val_loss: 0.3566 - val_mean_squared_error: 0.3566\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35561\n",
      "Epoch 00006: early stopping\n",
      "temp/f197\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4495 - mean_squared_error: 0.4495 - val_loss: 0.3725 - val_mean_squared_error: 0.3725\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37246, saving model to temp/f197\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3672 - mean_squared_error: 0.3672 - val_loss: 0.3661 - val_mean_squared_error: 0.3661\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37246 to 0.36605, saving model to temp/f197\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3621 - mean_squared_error: 0.3621 - val_loss: 0.3613 - val_mean_squared_error: 0.3613\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36605 to 0.36133, saving model to temp/f197\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3600 - mean_squared_error: 0.3600 - val_loss: 0.3559 - val_mean_squared_error: 0.3559\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36133 to 0.35586, saving model to temp/f197\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3591 - mean_squared_error: 0.3591 - val_loss: 0.3576 - val_mean_squared_error: 0.3576\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35586\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3582 - mean_squared_error: 0.3582 - val_loss: 0.3535 - val_mean_squared_error: 0.3535\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35586 to 0.35354, saving model to temp/f197\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3572 - mean_squared_error: 0.3572 - val_loss: 0.3545 - val_mean_squared_error: 0.3545\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35354\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3563 - mean_squared_error: 0.3563 - val_loss: 0.3632 - val_mean_squared_error: 0.3632\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35354\n",
      "Epoch 00008: early stopping\n",
      "temp/f198\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4587 - mean_squared_error: 0.4587 - val_loss: 0.3690 - val_mean_squared_error: 0.3690\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36896, saving model to temp/f198\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3632 - mean_squared_error: 0.3632 - val_loss: 0.3651 - val_mean_squared_error: 0.3651\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36896 to 0.36515, saving model to temp/f198\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3621 - mean_squared_error: 0.3621 - val_loss: 0.3739 - val_mean_squared_error: 0.3739\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36515\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3615 - mean_squared_error: 0.3615 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36515 to 0.35503, saving model to temp/f198\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3594 - mean_squared_error: 0.3594 - val_loss: 0.3558 - val_mean_squared_error: 0.3558\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35503\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3570 - mean_squared_error: 0.3570 - val_loss: 0.3534 - val_mean_squared_error: 0.3534\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35503 to 0.35343, saving model to temp/f198\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3579 - mean_squared_error: 0.3579 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35343\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3570 - mean_squared_error: 0.3570 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35343\n",
      "Epoch 00008: early stopping\n",
      "temp/f199\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4643 - mean_squared_error: 0.4643 - val_loss: 0.3647 - val_mean_squared_error: 0.3647\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36472, saving model to temp/f199\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3662 - mean_squared_error: 0.3662 - val_loss: 0.3634 - val_mean_squared_error: 0.3634\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36472 to 0.36339, saving model to temp/f199\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3643 - mean_squared_error: 0.3643 - val_loss: 0.3620 - val_mean_squared_error: 0.3620\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36339 to 0.36205, saving model to temp/f199\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3614 - mean_squared_error: 0.3614 - val_loss: 0.3551 - val_mean_squared_error: 0.3551\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36205 to 0.35511, saving model to temp/f199\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3583 - mean_squared_error: 0.3583 - val_loss: 0.3574 - val_mean_squared_error: 0.3574\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35511\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3565 - mean_squared_error: 0.3565 - val_loss: 0.3554 - val_mean_squared_error: 0.3554\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35511\n",
      "Epoch 00006: early stopping\n",
      "temp/f200\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4605 - mean_squared_error: 0.4605 - val_loss: 0.3709 - val_mean_squared_error: 0.3709\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37087, saving model to temp/f200\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3635 - mean_squared_error: 0.3635 - val_loss: 0.3591 - val_mean_squared_error: 0.3591\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37087 to 0.35906, saving model to temp/f200\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3614 - mean_squared_error: 0.3614 - val_loss: 0.3627 - val_mean_squared_error: 0.3627\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35906\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3592 - mean_squared_error: 0.3592 - val_loss: 0.3561 - val_mean_squared_error: 0.3561\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35906 to 0.35613, saving model to temp/f200\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3534 - val_mean_squared_error: 0.3534\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35613 to 0.35344, saving model to temp/f200\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3588 - mean_squared_error: 0.3588 - val_loss: 0.3563 - val_mean_squared_error: 0.3563\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35344\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3534 - val_mean_squared_error: 0.3534\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35344 to 0.35338, saving model to temp/f200\n",
      "Epoch 00007: early stopping\n",
      "temp/f201\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4610 - mean_squared_error: 0.4610 - val_loss: 0.3613 - val_mean_squared_error: 0.3613\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36128, saving model to temp/f201\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3675 - mean_squared_error: 0.3675 - val_loss: 0.3598 - val_mean_squared_error: 0.3598\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36128 to 0.35979, saving model to temp/f201\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3599 - mean_squared_error: 0.3599 - val_loss: 0.3608 - val_mean_squared_error: 0.3608\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35979\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3582 - mean_squared_error: 0.3582 - val_loss: 0.3630 - val_mean_squared_error: 0.3630\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35979\n",
      "Epoch 00004: early stopping\n",
      "temp/f202\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.4544 - mean_squared_error: 0.4544 - val_loss: 0.3681 - val_mean_squared_error: 0.3681\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36806, saving model to temp/f202\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3648 - mean_squared_error: 0.3648 - val_loss: 0.3630 - val_mean_squared_error: 0.3630\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36806 to 0.36303, saving model to temp/f202\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3634 - mean_squared_error: 0.3634 - val_loss: 0.3619 - val_mean_squared_error: 0.3619\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36303 to 0.36185, saving model to temp/f202\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3631 - mean_squared_error: 0.3631 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36185 to 0.35495, saving model to temp/f202\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3577 - mean_squared_error: 0.3577 - val_loss: 0.3602 - val_mean_squared_error: 0.3602\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35495\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3565 - mean_squared_error: 0.3565 - val_loss: 0.3521 - val_mean_squared_error: 0.3521\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35495 to 0.35212, saving model to temp/f202\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3532 - val_mean_squared_error: 0.3532\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35212\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3571 - mean_squared_error: 0.3571 - val_loss: 0.3538 - val_mean_squared_error: 0.3538\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35212\n",
      "Epoch 00008: early stopping\n",
      "temp/f203\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4526 - mean_squared_error: 0.4526 - val_loss: 0.3629 - val_mean_squared_error: 0.3629\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36287, saving model to temp/f203\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3664 - mean_squared_error: 0.3664 - val_loss: 0.3589 - val_mean_squared_error: 0.3589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_loss improved from 0.36287 to 0.35887, saving model to temp/f203\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3593 - mean_squared_error: 0.3593 - val_loss: 0.3603 - val_mean_squared_error: 0.3603\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35887\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3617 - mean_squared_error: 0.3617 - val_loss: 0.3614 - val_mean_squared_error: 0.3614\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35887\n",
      "Epoch 00004: early stopping\n",
      "temp/f204\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4490 - mean_squared_error: 0.4490 - val_loss: 0.3648 - val_mean_squared_error: 0.3648\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36481, saving model to temp/f204\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3664 - mean_squared_error: 0.3664 - val_loss: 0.3636 - val_mean_squared_error: 0.3636\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36481 to 0.36359, saving model to temp/f204\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3628 - mean_squared_error: 0.3628 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36359 to 0.35795, saving model to temp/f204\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3531 - val_mean_squared_error: 0.3531\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35795 to 0.35312, saving model to temp/f204\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3588 - mean_squared_error: 0.3588 - val_loss: 0.3538 - val_mean_squared_error: 0.3538\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35312\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3550 - mean_squared_error: 0.3550 - val_loss: 0.3547 - val_mean_squared_error: 0.3547\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35312\n",
      "Epoch 00006: early stopping\n",
      "temp/f205\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4444 - mean_squared_error: 0.4444 - val_loss: 0.3692 - val_mean_squared_error: 0.3692\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36921, saving model to temp/f205\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3669 - mean_squared_error: 0.3669 - val_loss: 0.3603 - val_mean_squared_error: 0.3603\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36921 to 0.36028, saving model to temp/f205\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3595 - mean_squared_error: 0.3595 - val_loss: 0.3640 - val_mean_squared_error: 0.3640\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36028\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3611 - mean_squared_error: 0.3611 - val_loss: 0.3558 - val_mean_squared_error: 0.3558\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36028 to 0.35576, saving model to temp/f205\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3610 - mean_squared_error: 0.3610 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35576 to 0.35504, saving model to temp/f205\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3596 - mean_squared_error: 0.3596 - val_loss: 0.3539 - val_mean_squared_error: 0.3539\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35504 to 0.35389, saving model to temp/f205\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3548 - mean_squared_error: 0.3548 - val_loss: 0.3545 - val_mean_squared_error: 0.3545\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35389\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3582 - mean_squared_error: 0.3582 - val_loss: 0.3528 - val_mean_squared_error: 0.3528\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35389 to 0.35277, saving model to temp/f205\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3563 - mean_squared_error: 0.3563 - val_loss: 0.3530 - val_mean_squared_error: 0.3530\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35277\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3544 - mean_squared_error: 0.3544 - val_loss: 0.3548 - val_mean_squared_error: 0.3548\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35277\n",
      "Epoch 00010: early stopping\n",
      "temp/f206\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 0.4522 - mean_squared_error: 0.4522 - val_loss: 0.3640 - val_mean_squared_error: 0.3640\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36396, saving model to temp/f206\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3659 - mean_squared_error: 0.3659 - val_loss: 0.3646 - val_mean_squared_error: 0.3646\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36396\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3630 - mean_squared_error: 0.3630 - val_loss: 0.3696 - val_mean_squared_error: 0.3696\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36396\n",
      "Epoch 00003: early stopping\n",
      "temp/f207\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4530 - mean_squared_error: 0.4530 - val_loss: 0.3761 - val_mean_squared_error: 0.3761\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37610, saving model to temp/f207\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3653 - mean_squared_error: 0.3653 - val_loss: 0.3608 - val_mean_squared_error: 0.3608\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37610 to 0.36078, saving model to temp/f207\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3613 - mean_squared_error: 0.3613 - val_loss: 0.3595 - val_mean_squared_error: 0.3595\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36078 to 0.35946, saving model to temp/f207\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3598 - mean_squared_error: 0.3598 - val_loss: 0.3560 - val_mean_squared_error: 0.3560\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35946 to 0.35597, saving model to temp/f207\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3588 - mean_squared_error: 0.3588 - val_loss: 0.3572 - val_mean_squared_error: 0.3572\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35597\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3588 - mean_squared_error: 0.3588 - val_loss: 0.3555 - val_mean_squared_error: 0.3555\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35597 to 0.35553, saving model to temp/f207\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3584 - mean_squared_error: 0.3584 - val_loss: 0.3556 - val_mean_squared_error: 0.3556\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35553\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3558 - mean_squared_error: 0.3558 - val_loss: 0.3579 - val_mean_squared_error: 0.3579\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35553\n",
      "Epoch 00008: early stopping\n",
      "temp/f208\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4526 - mean_squared_error: 0.4526 - val_loss: 0.3733 - val_mean_squared_error: 0.3733\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37329, saving model to temp/f208\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3660 - mean_squared_error: 0.3660 - val_loss: 0.3589 - val_mean_squared_error: 0.3589\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37329 to 0.35889, saving model to temp/f208\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3643 - mean_squared_error: 0.3643 - val_loss: 0.3588 - val_mean_squared_error: 0.3588\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35889 to 0.35883, saving model to temp/f208\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3611 - mean_squared_error: 0.3611 - val_loss: 0.3551 - val_mean_squared_error: 0.3551\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35883 to 0.35509, saving model to temp/f208\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3592 - mean_squared_error: 0.3592 - val_loss: 0.3564 - val_mean_squared_error: 0.3564\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35509\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3559 - mean_squared_error: 0.3559 - val_loss: 0.3604 - val_mean_squared_error: 0.3604\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35509\n",
      "Epoch 00006: early stopping\n",
      "temp/f209\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4563 - mean_squared_error: 0.4563 - val_loss: 0.3638 - val_mean_squared_error: 0.3638\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36382, saving model to temp/f209\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3635 - mean_squared_error: 0.3635 - val_loss: 0.3596 - val_mean_squared_error: 0.3596\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36382 to 0.35957, saving model to temp/f209\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3610 - mean_squared_error: 0.3610 - val_loss: 0.3639 - val_mean_squared_error: 0.3639\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35957\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3606 - mean_squared_error: 0.3606 - val_loss: 0.3629 - val_mean_squared_error: 0.3629\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35957\n",
      "Epoch 00004: early stopping\n",
      "temp/f210\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.4572 - mean_squared_error: 0.4572 - val_loss: 0.3736 - val_mean_squared_error: 0.3736\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37361, saving model to temp/f210\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3653 - mean_squared_error: 0.3653 - val_loss: 0.3582 - val_mean_squared_error: 0.3582\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37361 to 0.35816, saving model to temp/f210\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3625 - mean_squared_error: 0.3625 - val_loss: 0.3616 - val_mean_squared_error: 0.3616\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35816\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3615 - mean_squared_error: 0.3615 - val_loss: 0.3567 - val_mean_squared_error: 0.3567\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35816 to 0.35673, saving model to temp/f210\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3600 - mean_squared_error: 0.3600 - val_loss: 0.3658 - val_mean_squared_error: 0.3658\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35673\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3551 - mean_squared_error: 0.3551 - val_loss: 0.3541 - val_mean_squared_error: 0.3541\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35673 to 0.35408, saving model to temp/f210\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3583 - mean_squared_error: 0.3583 - val_loss: 0.3559 - val_mean_squared_error: 0.3559\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35408\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3558 - mean_squared_error: 0.3558 - val_loss: 0.3651 - val_mean_squared_error: 0.3651\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35408\n",
      "Epoch 00008: early stopping\n",
      "temp/f211\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4494 - mean_squared_error: 0.4494 - val_loss: 0.3653 - val_mean_squared_error: 0.3653\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36529, saving model to temp/f211\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3647 - mean_squared_error: 0.3647 - val_loss: 0.3671 - val_mean_squared_error: 0.3671\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36529\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3641 - mean_squared_error: 0.3641 - val_loss: 0.3587 - val_mean_squared_error: 0.3587\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36529 to 0.35873, saving model to temp/f211\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3626 - mean_squared_error: 0.3626 - val_loss: 0.3601 - val_mean_squared_error: 0.3601\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35873\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3527 - val_mean_squared_error: 0.3527\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35873 to 0.35273, saving model to temp/f211\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3688 - val_mean_squared_error: 0.3688\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35273\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3579 - mean_squared_error: 0.3579 - val_loss: 0.3514 - val_mean_squared_error: 0.3514\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35273 to 0.35143, saving model to temp/f211\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3570 - mean_squared_error: 0.3570 - val_loss: 0.3501 - val_mean_squared_error: 0.3501\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35143 to 0.35011, saving model to temp/f211\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3504 - val_mean_squared_error: 0.3504\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35011\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3546 - mean_squared_error: 0.3546 - val_loss: 0.3511 - val_mean_squared_error: 0.3511\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35011\n",
      "Epoch 00010: early stopping\n",
      "temp/f212\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4564 - mean_squared_error: 0.4564 - val_loss: 0.3636 - val_mean_squared_error: 0.3636\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36361, saving model to temp/f212\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3658 - mean_squared_error: 0.3658 - val_loss: 0.3608 - val_mean_squared_error: 0.3608\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36361 to 0.36078, saving model to temp/f212\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3632 - mean_squared_error: 0.3632 - val_loss: 0.3674 - val_mean_squared_error: 0.3674\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36078\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3613 - mean_squared_error: 0.3613 - val_loss: 0.3639 - val_mean_squared_error: 0.3639\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36078\n",
      "Epoch 00004: early stopping\n",
      "temp/f213\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 0.4578 - mean_squared_error: 0.4578 - val_loss: 0.3700 - val_mean_squared_error: 0.3700\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37002, saving model to temp/f213\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3625 - mean_squared_error: 0.3625 - val_loss: 0.3669 - val_mean_squared_error: 0.3669\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37002 to 0.36694, saving model to temp/f213\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3619 - mean_squared_error: 0.3619 - val_loss: 0.3620 - val_mean_squared_error: 0.3620\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36694 to 0.36196, saving model to temp/f213\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3623 - mean_squared_error: 0.3623 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36196 to 0.35499, saving model to temp/f213\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3585 - mean_squared_error: 0.3585 - val_loss: 0.3541 - val_mean_squared_error: 0.3541\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35499 to 0.35411, saving model to temp/f213\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3596 - mean_squared_error: 0.3596 - val_loss: 0.3581 - val_mean_squared_error: 0.3581\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35411\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3590 - mean_squared_error: 0.3590 - val_loss: 0.3560 - val_mean_squared_error: 0.3560\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35411\n",
      "Epoch 00007: early stopping\n",
      "temp/f214\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4590 - mean_squared_error: 0.4590 - val_loss: 0.3616 - val_mean_squared_error: 0.3616\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36164, saving model to temp/f214\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3642 - mean_squared_error: 0.3642 - val_loss: 0.3622 - val_mean_squared_error: 0.3622\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36164\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3623 - mean_squared_error: 0.3623 - val_loss: 0.3672 - val_mean_squared_error: 0.3672\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36164\n",
      "Epoch 00003: early stopping\n",
      "temp/f215\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.4572 - mean_squared_error: 0.4572 - val_loss: 0.3635 - val_mean_squared_error: 0.3635\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36351, saving model to temp/f215\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3660 - mean_squared_error: 0.3660 - val_loss: 0.3625 - val_mean_squared_error: 0.3625\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36351 to 0.36248, saving model to temp/f215\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3612 - mean_squared_error: 0.3612 - val_loss: 0.3573 - val_mean_squared_error: 0.3573\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36248 to 0.35731, saving model to temp/f215\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3604 - mean_squared_error: 0.3604 - val_loss: 0.3537 - val_mean_squared_error: 0.3537\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35731 to 0.35373, saving model to temp/f215\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3598 - mean_squared_error: 0.3598 - val_loss: 0.3551 - val_mean_squared_error: 0.3551\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35373\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3564 - mean_squared_error: 0.3564 - val_loss: 0.3529 - val_mean_squared_error: 0.3529\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35373 to 0.35288, saving model to temp/f215\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3568 - mean_squared_error: 0.3568 - val_loss: 0.3555 - val_mean_squared_error: 0.3555\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35288\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3561 - mean_squared_error: 0.3561 - val_loss: 0.3522 - val_mean_squared_error: 0.3522\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35288 to 0.35217, saving model to temp/f215\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3539 - mean_squared_error: 0.3539 - val_loss: 0.3532 - val_mean_squared_error: 0.3532\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35217\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3552 - mean_squared_error: 0.3552 - val_loss: 0.3520 - val_mean_squared_error: 0.3520\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.35217 to 0.35201, saving model to temp/f215\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3527 - mean_squared_error: 0.3527 - val_loss: 0.3521 - val_mean_squared_error: 0.3521\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35201\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3537 - mean_squared_error: 0.3537 - val_loss: 0.3518 - val_mean_squared_error: 0.3518\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.35201 to 0.35177, saving model to temp/f215\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3552 - mean_squared_error: 0.3552 - val_loss: 0.3513 - val_mean_squared_error: 0.3513\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.35177 to 0.35130, saving model to temp/f215\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3534 - mean_squared_error: 0.3534 - val_loss: 0.3603 - val_mean_squared_error: 0.3603\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.35130\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3520 - mean_squared_error: 0.3520 - val_loss: 0.3625 - val_mean_squared_error: 0.3625\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.35130\n",
      "Epoch 00015: early stopping\n",
      "temp/f216\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4603 - mean_squared_error: 0.4603 - val_loss: 0.3704 - val_mean_squared_error: 0.3704\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37039, saving model to temp/f216\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3660 - mean_squared_error: 0.3660 - val_loss: 0.3604 - val_mean_squared_error: 0.3604\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37039 to 0.36037, saving model to temp/f216\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3634 - mean_squared_error: 0.3634 - val_loss: 0.3682 - val_mean_squared_error: 0.3682\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36037\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3611 - mean_squared_error: 0.3611 - val_loss: 0.3579 - val_mean_squared_error: 0.3579\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36037 to 0.35786, saving model to temp/f216\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3590 - mean_squared_error: 0.3590 - val_loss: 0.3595 - val_mean_squared_error: 0.3595\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35786\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3585 - mean_squared_error: 0.3585 - val_loss: 0.3588 - val_mean_squared_error: 0.3588\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35786\n",
      "Epoch 00006: early stopping\n",
      "temp/f217\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4542 - mean_squared_error: 0.4542 - val_loss: 0.3707 - val_mean_squared_error: 0.3707\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37074, saving model to temp/f217\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3663 - mean_squared_error: 0.3663 - val_loss: 0.3635 - val_mean_squared_error: 0.3635\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37074 to 0.36348, saving model to temp/f217\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3650 - mean_squared_error: 0.3650 - val_loss: 0.3616 - val_mean_squared_error: 0.3616\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36348 to 0.36156, saving model to temp/f217\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3603 - mean_squared_error: 0.3603 - val_loss: 0.3606 - val_mean_squared_error: 0.3606\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36156 to 0.36056, saving model to temp/f217\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3592 - mean_squared_error: 0.3592 - val_loss: 0.3546 - val_mean_squared_error: 0.3546\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.36056 to 0.35462, saving model to temp/f217\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3596 - val_mean_squared_error: 0.3596\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35462\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3594 - mean_squared_error: 0.3594 - val_loss: 0.3507 - val_mean_squared_error: 0.3507\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35462 to 0.35068, saving model to temp/f217\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3558 - mean_squared_error: 0.3558 - val_loss: 0.3505 - val_mean_squared_error: 0.3505\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35068 to 0.35050, saving model to temp/f217\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3581 - mean_squared_error: 0.3581 - val_loss: 0.3522 - val_mean_squared_error: 0.3522\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35050\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3533 - mean_squared_error: 0.3533 - val_loss: 0.3525 - val_mean_squared_error: 0.3525\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35050\n",
      "Epoch 00010: early stopping\n",
      "temp/f218\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4535 - mean_squared_error: 0.4535 - val_loss: 0.3696 - val_mean_squared_error: 0.3696\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36963, saving model to temp/f218\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3654 - mean_squared_error: 0.3654 - val_loss: 0.3583 - val_mean_squared_error: 0.3583\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36963 to 0.35833, saving model to temp/f218\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3608 - mean_squared_error: 0.3608 - val_loss: 0.3564 - val_mean_squared_error: 0.3564\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35833 to 0.35642, saving model to temp/f218\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3600 - mean_squared_error: 0.3600 - val_loss: 0.3590 - val_mean_squared_error: 0.3590\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35642\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3594 - mean_squared_error: 0.3594 - val_loss: 0.3529 - val_mean_squared_error: 0.3529\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35642 to 0.35291, saving model to temp/f218\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3577 - mean_squared_error: 0.3577 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35291\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3557 - mean_squared_error: 0.3557 - val_loss: 0.3569 - val_mean_squared_error: 0.3569\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35291\n",
      "Epoch 00007: early stopping\n",
      "temp/f219\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4521 - mean_squared_error: 0.4521 - val_loss: 0.3651 - val_mean_squared_error: 0.3651\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36507, saving model to temp/f219\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3658 - mean_squared_error: 0.3658 - val_loss: 0.3720 - val_mean_squared_error: 0.3720\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36507\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3630 - mean_squared_error: 0.3630 - val_loss: 0.3603 - val_mean_squared_error: 0.3603\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36507 to 0.36035, saving model to temp/f219\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3584 - mean_squared_error: 0.3584 - val_loss: 0.3639 - val_mean_squared_error: 0.3639\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36035\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3590 - mean_squared_error: 0.3590 - val_loss: 0.3543 - val_mean_squared_error: 0.3543\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.36035 to 0.35435, saving model to temp/f219\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3569 - mean_squared_error: 0.3569 - val_loss: 0.3524 - val_mean_squared_error: 0.3524\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35435 to 0.35236, saving model to temp/f219\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3589 - mean_squared_error: 0.3589 - val_loss: 0.3545 - val_mean_squared_error: 0.3545\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35236\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35236\n",
      "Epoch 00008: early stopping\n",
      "temp/f220\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4602 - mean_squared_error: 0.4602 - val_loss: 0.3662 - val_mean_squared_error: 0.3662\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36624, saving model to temp/f220\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3668 - mean_squared_error: 0.3668 - val_loss: 0.3643 - val_mean_squared_error: 0.3643\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36624 to 0.36435, saving model to temp/f220\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3626 - mean_squared_error: 0.3626 - val_loss: 0.3613 - val_mean_squared_error: 0.3613\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36435 to 0.36135, saving model to temp/f220\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3603 - mean_squared_error: 0.3603 - val_loss: 0.3596 - val_mean_squared_error: 0.3596\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36135 to 0.35960, saving model to temp/f220\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3596 - mean_squared_error: 0.3596 - val_loss: 0.3595 - val_mean_squared_error: 0.3595\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35960 to 0.35949, saving model to temp/f220\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3580 - mean_squared_error: 0.3580 - val_loss: 0.3646 - val_mean_squared_error: 0.3646\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35949\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3594 - mean_squared_error: 0.3594 - val_loss: 0.3515 - val_mean_squared_error: 0.3515\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35949 to 0.35151, saving model to temp/f220\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3559 - mean_squared_error: 0.3559 - val_loss: 0.3523 - val_mean_squared_error: 0.3523\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35151\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3546 - mean_squared_error: 0.3546 - val_loss: 0.3596 - val_mean_squared_error: 0.3596\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35151\n",
      "Epoch 00009: early stopping\n",
      "temp/f221\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4557 - mean_squared_error: 0.4557 - val_loss: 0.3642 - val_mean_squared_error: 0.3642\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36424, saving model to temp/f221\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3676 - mean_squared_error: 0.3676 - val_loss: 0.3594 - val_mean_squared_error: 0.3594\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36424 to 0.35945, saving model to temp/f221\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3606 - mean_squared_error: 0.3606 - val_loss: 0.3622 - val_mean_squared_error: 0.3622\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35945\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3618 - mean_squared_error: 0.3618 - val_loss: 0.3548 - val_mean_squared_error: 0.3548\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35945 to 0.35475, saving model to temp/f221\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3580 - mean_squared_error: 0.3580 - val_loss: 0.3547 - val_mean_squared_error: 0.3547\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35475 to 0.35475, saving model to temp/f221\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3589 - mean_squared_error: 0.3589 - val_loss: 0.3631 - val_mean_squared_error: 0.3631\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35475\n",
      "Epoch 00006: early stopping\n",
      "temp/f222\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 0.4488 - mean_squared_error: 0.4488 - val_loss: 0.3724 - val_mean_squared_error: 0.3724\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37236, saving model to temp/f222\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3655 - mean_squared_error: 0.3655 - val_loss: 0.3606 - val_mean_squared_error: 0.3606\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37236 to 0.36063, saving model to temp/f222\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3652 - mean_squared_error: 0.3652 - val_loss: 0.3563 - val_mean_squared_error: 0.3563\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36063 to 0.35625, saving model to temp/f222\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3602 - mean_squared_error: 0.3602 - val_loss: 0.3577 - val_mean_squared_error: 0.3577\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35625\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3589 - mean_squared_error: 0.3589 - val_loss: 0.3560 - val_mean_squared_error: 0.3560\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35625 to 0.35602, saving model to temp/f222\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3594 - mean_squared_error: 0.3594 - val_loss: 0.3537 - val_mean_squared_error: 0.3537\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35602 to 0.35374, saving model to temp/f222\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3584 - mean_squared_error: 0.3584 - val_loss: 0.3514 - val_mean_squared_error: 0.3514\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35374 to 0.35139, saving model to temp/f222\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3549 - mean_squared_error: 0.3549 - val_loss: 0.3546 - val_mean_squared_error: 0.3546\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35139\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3554 - mean_squared_error: 0.3554 - val_loss: 0.3533 - val_mean_squared_error: 0.3533\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35139\n",
      "Epoch 00009: early stopping\n",
      "temp/f223\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4558 - mean_squared_error: 0.4558 - val_loss: 0.3650 - val_mean_squared_error: 0.3650\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36502, saving model to temp/f223\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3656 - mean_squared_error: 0.3656 - val_loss: 0.3625 - val_mean_squared_error: 0.3625\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36502 to 0.36254, saving model to temp/f223\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3637 - mean_squared_error: 0.3637 - val_loss: 0.3577 - val_mean_squared_error: 0.3577\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36254 to 0.35772, saving model to temp/f223\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3600 - mean_squared_error: 0.3600 - val_loss: 0.3633 - val_mean_squared_error: 0.3633\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35772\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3609 - mean_squared_error: 0.3609 - val_loss: 0.3544 - val_mean_squared_error: 0.3544\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35772 to 0.35440, saving model to temp/f223\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3579 - mean_squared_error: 0.3579 - val_loss: 0.3556 - val_mean_squared_error: 0.3556\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35440\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3603 - mean_squared_error: 0.3603 - val_loss: 0.3531 - val_mean_squared_error: 0.3531\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35440 to 0.35311, saving model to temp/f223\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3564 - val_mean_squared_error: 0.3564\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35311\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3549 - mean_squared_error: 0.3549 - val_loss: 0.3677 - val_mean_squared_error: 0.3677\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35311\n",
      "Epoch 00009: early stopping\n",
      "temp/f224\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4523 - mean_squared_error: 0.4523 - val_loss: 0.3728 - val_mean_squared_error: 0.3728\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37276, saving model to temp/f224\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3660 - mean_squared_error: 0.3660 - val_loss: 0.3608 - val_mean_squared_error: 0.3608\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37276 to 0.36075, saving model to temp/f224\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3614 - mean_squared_error: 0.3614 - val_loss: 0.3603 - val_mean_squared_error: 0.3603\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36075 to 0.36028, saving model to temp/f224\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3635 - mean_squared_error: 0.3635 - val_loss: 0.3563 - val_mean_squared_error: 0.3563\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36028 to 0.35634, saving model to temp/f224\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3592 - mean_squared_error: 0.3592 - val_loss: 0.3604 - val_mean_squared_error: 0.3604\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35634\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3581 - mean_squared_error: 0.3581 - val_loss: 0.3518 - val_mean_squared_error: 0.3518\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35634 to 0.35179, saving model to temp/f224\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3562 - mean_squared_error: 0.3562 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35179\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3584 - mean_squared_error: 0.3584 - val_loss: 0.3535 - val_mean_squared_error: 0.3535\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35179\n",
      "Epoch 00008: early stopping\n",
      "temp/f225\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4579 - mean_squared_error: 0.4579 - val_loss: 0.3661 - val_mean_squared_error: 0.3661\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36611, saving model to temp/f225\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3649 - mean_squared_error: 0.3649 - val_loss: 0.3629 - val_mean_squared_error: 0.3629\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36611 to 0.36292, saving model to temp/f225\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3606 - mean_squared_error: 0.3606 - val_loss: 0.3592 - val_mean_squared_error: 0.3592\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36292 to 0.35922, saving model to temp/f225\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3619 - mean_squared_error: 0.3619 - val_loss: 0.3578 - val_mean_squared_error: 0.3578\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35922 to 0.35784, saving model to temp/f225\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3594 - mean_squared_error: 0.3594 - val_loss: 0.3563 - val_mean_squared_error: 0.3563\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35784 to 0.35629, saving model to temp/f225\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3580 - mean_squared_error: 0.3580 - val_loss: 0.3761 - val_mean_squared_error: 0.3761\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35629\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3586 - mean_squared_error: 0.3586 - val_loss: 0.3592 - val_mean_squared_error: 0.3592\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35629\n",
      "Epoch 00007: early stopping\n",
      "temp/f226\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 0.4632 - mean_squared_error: 0.4632 - val_loss: 0.3704 - val_mean_squared_error: 0.3704\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37040, saving model to temp/f226\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3663 - mean_squared_error: 0.3663 - val_loss: 0.3618 - val_mean_squared_error: 0.3618\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37040 to 0.36175, saving model to temp/f226\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3611 - mean_squared_error: 0.3611 - val_loss: 0.3589 - val_mean_squared_error: 0.3589\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36175 to 0.35894, saving model to temp/f226\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3635 - mean_squared_error: 0.3635 - val_loss: 0.3571 - val_mean_squared_error: 0.3571\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35894 to 0.35708, saving model to temp/f226\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3590 - mean_squared_error: 0.3590 - val_loss: 0.3567 - val_mean_squared_error: 0.3567\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35708 to 0.35672, saving model to temp/f226\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3588 - mean_squared_error: 0.3588 - val_loss: 0.3545 - val_mean_squared_error: 0.3545\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35672 to 0.35447, saving model to temp/f226\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3564 - mean_squared_error: 0.3564 - val_loss: 0.3527 - val_mean_squared_error: 0.3527\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35447 to 0.35269, saving model to temp/f226\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3563 - mean_squared_error: 0.3563 - val_loss: 0.3578 - val_mean_squared_error: 0.3578\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35269\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3538 - mean_squared_error: 0.3538 - val_loss: 0.3525 - val_mean_squared_error: 0.3525\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35269 to 0.35249, saving model to temp/f226\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3543 - mean_squared_error: 0.3543 - val_loss: 0.3523 - val_mean_squared_error: 0.3523\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.35249 to 0.35226, saving model to temp/f226\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3541 - mean_squared_error: 0.3541 - val_loss: 0.3521 - val_mean_squared_error: 0.3521\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.35226 to 0.35206, saving model to temp/f226\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3538 - mean_squared_error: 0.3538 - val_loss: 0.3502 - val_mean_squared_error: 0.3502\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.35206 to 0.35016, saving model to temp/f226\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3537 - mean_squared_error: 0.3537 - val_loss: 0.3515 - val_mean_squared_error: 0.3515\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.35016\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3538 - mean_squared_error: 0.3538 - val_loss: 0.3496 - val_mean_squared_error: 0.3496\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.35016 to 0.34964, saving model to temp/f226\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3534 - mean_squared_error: 0.3534 - val_loss: 0.3552 - val_mean_squared_error: 0.3552\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.34964\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3545 - mean_squared_error: 0.3545 - val_loss: 0.3549 - val_mean_squared_error: 0.3549\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.34964\n",
      "Epoch 00016: early stopping\n",
      "temp/f227\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4454 - mean_squared_error: 0.4454 - val_loss: 0.3640 - val_mean_squared_error: 0.3640\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36396, saving model to temp/f227\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3682 - mean_squared_error: 0.3682 - val_loss: 0.3592 - val_mean_squared_error: 0.3592\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36396 to 0.35917, saving model to temp/f227\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3626 - mean_squared_error: 0.3626 - val_loss: 0.3578 - val_mean_squared_error: 0.3578\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35917 to 0.35781, saving model to temp/f227\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3605 - mean_squared_error: 0.3605 - val_loss: 0.3566 - val_mean_squared_error: 0.3566\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35781 to 0.35665, saving model to temp/f227\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3600 - mean_squared_error: 0.3600 - val_loss: 0.3588 - val_mean_squared_error: 0.3588\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35665\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3572 - mean_squared_error: 0.3572 - val_loss: 0.3587 - val_mean_squared_error: 0.3587\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35665\n",
      "Epoch 00006: early stopping\n",
      "temp/f228\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.4468 - mean_squared_error: 0.4468 - val_loss: 0.3641 - val_mean_squared_error: 0.3641\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36415, saving model to temp/f228\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3678 - mean_squared_error: 0.3678 - val_loss: 0.3573 - val_mean_squared_error: 0.3573\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36415 to 0.35726, saving model to temp/f228\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3617 - mean_squared_error: 0.3617 - val_loss: 0.3627 - val_mean_squared_error: 0.3627\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35726\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3603 - mean_squared_error: 0.3603 - val_loss: 0.3570 - val_mean_squared_error: 0.3570\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35726 to 0.35704, saving model to temp/f228\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3570 - mean_squared_error: 0.3570 - val_loss: 0.3608 - val_mean_squared_error: 0.3608\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35704\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3568 - mean_squared_error: 0.3568 - val_loss: 0.3521 - val_mean_squared_error: 0.3521\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35704 to 0.35212, saving model to temp/f228\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3583 - mean_squared_error: 0.3583 - val_loss: 0.3540 - val_mean_squared_error: 0.3540\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35212\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3525 - val_mean_squared_error: 0.3525\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35212\n",
      "Epoch 00008: early stopping\n",
      "temp/f229\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4523 - mean_squared_error: 0.4523 - val_loss: 0.3723 - val_mean_squared_error: 0.3723\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37231, saving model to temp/f229\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3639 - mean_squared_error: 0.3639 - val_loss: 0.3746 - val_mean_squared_error: 0.3746\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.37231\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3630 - mean_squared_error: 0.3630 - val_loss: 0.3574 - val_mean_squared_error: 0.3574\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.37231 to 0.35743, saving model to temp/f229\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3599 - mean_squared_error: 0.3599 - val_loss: 0.3559 - val_mean_squared_error: 0.3559\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35743 to 0.35591, saving model to temp/f229\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3572 - mean_squared_error: 0.3572 - val_loss: 0.3575 - val_mean_squared_error: 0.3575\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35591\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3601 - mean_squared_error: 0.3601 - val_loss: 0.3543 - val_mean_squared_error: 0.3543\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35591 to 0.35426, saving model to temp/f229\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3590 - mean_squared_error: 0.3590 - val_loss: 0.3540 - val_mean_squared_error: 0.3540\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35426 to 0.35399, saving model to temp/f229\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3545 - mean_squared_error: 0.3545 - val_loss: 0.3506 - val_mean_squared_error: 0.3506\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35399 to 0.35063, saving model to temp/f229\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3541 - mean_squared_error: 0.3541 - val_loss: 0.3538 - val_mean_squared_error: 0.3538\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35063\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3542 - mean_squared_error: 0.3542 - val_loss: 0.3528 - val_mean_squared_error: 0.3528\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35063\n",
      "Epoch 00010: early stopping\n",
      "temp/f230\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4597 - mean_squared_error: 0.4597 - val_loss: 0.3642 - val_mean_squared_error: 0.3642\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36425, saving model to temp/f230\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3696 - mean_squared_error: 0.3696 - val_loss: 0.3632 - val_mean_squared_error: 0.3632\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36425 to 0.36316, saving model to temp/f230\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3612 - mean_squared_error: 0.3612 - val_loss: 0.3591 - val_mean_squared_error: 0.3591\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36316 to 0.35912, saving model to temp/f230\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3630 - mean_squared_error: 0.3630 - val_loss: 0.3538 - val_mean_squared_error: 0.3538\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35912 to 0.35375, saving model to temp/f230\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3575 - mean_squared_error: 0.3575 - val_loss: 0.3606 - val_mean_squared_error: 0.3606\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35375\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3580 - mean_squared_error: 0.3580 - val_loss: 0.3602 - val_mean_squared_error: 0.3602\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35375\n",
      "Epoch 00006: early stopping\n",
      "temp/f231\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4541 - mean_squared_error: 0.4541 - val_loss: 0.3632 - val_mean_squared_error: 0.3632\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36322, saving model to temp/f231\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3651 - mean_squared_error: 0.3651 - val_loss: 0.3608 - val_mean_squared_error: 0.3608\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36322 to 0.36077, saving model to temp/f231\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3614 - mean_squared_error: 0.3614 - val_loss: 0.3547 - val_mean_squared_error: 0.3547\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36077 to 0.35468, saving model to temp/f231\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3605 - mean_squared_error: 0.3605 - val_loss: 0.3534 - val_mean_squared_error: 0.3534\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35468 to 0.35342, saving model to temp/f231\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3569 - mean_squared_error: 0.3569 - val_loss: 0.3538 - val_mean_squared_error: 0.3538\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35342\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3560 - mean_squared_error: 0.3560 - val_loss: 0.3545 - val_mean_squared_error: 0.3545\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35342\n",
      "Epoch 00006: early stopping\n",
      "temp/f232\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.4521 - mean_squared_error: 0.4521 - val_loss: 0.3634 - val_mean_squared_error: 0.3634\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36345, saving model to temp/f232\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3634 - mean_squared_error: 0.3634 - val_loss: 0.3676 - val_mean_squared_error: 0.3676\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36345\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3626 - mean_squared_error: 0.3626 - val_loss: 0.3633 - val_mean_squared_error: 0.3633\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36345 to 0.36334, saving model to temp/f232\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3599 - mean_squared_error: 0.3599 - val_loss: 0.3572 - val_mean_squared_error: 0.3572\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36334 to 0.35716, saving model to temp/f232\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3586 - mean_squared_error: 0.3586 - val_loss: 0.3540 - val_mean_squared_error: 0.3540\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35716 to 0.35402, saving model to temp/f232\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3578 - mean_squared_error: 0.3578 - val_loss: 0.3567 - val_mean_squared_error: 0.3567\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35402\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3559 - mean_squared_error: 0.3559 - val_loss: 0.3558 - val_mean_squared_error: 0.3558\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35402\n",
      "Epoch 00007: early stopping\n",
      "temp/f233\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4442 - mean_squared_error: 0.4442 - val_loss: 0.3641 - val_mean_squared_error: 0.3641\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36409, saving model to temp/f233\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3681 - mean_squared_error: 0.3681 - val_loss: 0.3598 - val_mean_squared_error: 0.3598\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36409 to 0.35979, saving model to temp/f233\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3601 - mean_squared_error: 0.3601 - val_loss: 0.3554 - val_mean_squared_error: 0.3554\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35979 to 0.35545, saving model to temp/f233\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3580 - mean_squared_error: 0.3580 - val_loss: 0.3558 - val_mean_squared_error: 0.3558\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35545\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3597 - mean_squared_error: 0.3597 - val_loss: 0.3566 - val_mean_squared_error: 0.3566\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35545\n",
      "Epoch 00005: early stopping\n",
      "temp/f234\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4645 - mean_squared_error: 0.4645 - val_loss: 0.3696 - val_mean_squared_error: 0.3696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36965, saving model to temp/f234\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3640 - mean_squared_error: 0.3640 - val_loss: 0.3655 - val_mean_squared_error: 0.3655\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36965 to 0.36553, saving model to temp/f234\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3599 - val_mean_squared_error: 0.3599\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36553 to 0.35992, saving model to temp/f234\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3572 - mean_squared_error: 0.3572 - val_loss: 0.3665 - val_mean_squared_error: 0.3665\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35992\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3529 - val_mean_squared_error: 0.3529\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35992 to 0.35286, saving model to temp/f234\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3561 - val_mean_squared_error: 0.3561\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35286\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3565 - mean_squared_error: 0.3565 - val_loss: 0.3599 - val_mean_squared_error: 0.3599\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35286\n",
      "Epoch 00007: early stopping\n",
      "temp/f235\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4633 - mean_squared_error: 0.4633 - val_loss: 0.3648 - val_mean_squared_error: 0.3648\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36482, saving model to temp/f235\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3668 - mean_squared_error: 0.3668 - val_loss: 0.3629 - val_mean_squared_error: 0.3629\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36482 to 0.36288, saving model to temp/f235\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3624 - mean_squared_error: 0.3624 - val_loss: 0.3581 - val_mean_squared_error: 0.3581\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36288 to 0.35807, saving model to temp/f235\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3610 - mean_squared_error: 0.3610 - val_loss: 0.3556 - val_mean_squared_error: 0.3556\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35807 to 0.35562, saving model to temp/f235\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3607 - mean_squared_error: 0.3607 - val_loss: 0.3669 - val_mean_squared_error: 0.3669\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35562\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3594 - mean_squared_error: 0.3594 - val_loss: 0.3592 - val_mean_squared_error: 0.3592\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35562\n",
      "Epoch 00006: early stopping\n",
      "temp/f236\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.4497 - mean_squared_error: 0.4497 - val_loss: 0.3778 - val_mean_squared_error: 0.3778\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37782, saving model to temp/f236\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3644 - mean_squared_error: 0.3644 - val_loss: 0.3672 - val_mean_squared_error: 0.3672\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37782 to 0.36718, saving model to temp/f236\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3645 - mean_squared_error: 0.3645 - val_loss: 0.3595 - val_mean_squared_error: 0.3595\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36718 to 0.35949, saving model to temp/f236\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3610 - mean_squared_error: 0.3610 - val_loss: 0.3607 - val_mean_squared_error: 0.3607\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35949\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3609 - mean_squared_error: 0.3609 - val_loss: 0.3616 - val_mean_squared_error: 0.3616\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35949\n",
      "Epoch 00005: early stopping\n",
      "temp/f237\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4562 - mean_squared_error: 0.4562 - val_loss: 0.3661 - val_mean_squared_error: 0.3661\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36613, saving model to temp/f237\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3629 - mean_squared_error: 0.3629 - val_loss: 0.3657 - val_mean_squared_error: 0.3657\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36613 to 0.36575, saving model to temp/f237\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3615 - mean_squared_error: 0.3615 - val_loss: 0.3612 - val_mean_squared_error: 0.3612\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36575 to 0.36117, saving model to temp/f237\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3590 - mean_squared_error: 0.3590 - val_loss: 0.3556 - val_mean_squared_error: 0.3556\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36117 to 0.35560, saving model to temp/f237\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3609 - mean_squared_error: 0.3609 - val_loss: 0.3570 - val_mean_squared_error: 0.3570\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35560\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3584 - mean_squared_error: 0.3584 - val_loss: 0.3616 - val_mean_squared_error: 0.3616\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35560\n",
      "Epoch 00006: early stopping\n",
      "temp/f238\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4558 - mean_squared_error: 0.4558 - val_loss: 0.3652 - val_mean_squared_error: 0.3652\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36523, saving model to temp/f238\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3663 - mean_squared_error: 0.3663 - val_loss: 0.3607 - val_mean_squared_error: 0.3607\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36523 to 0.36072, saving model to temp/f238\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3612 - mean_squared_error: 0.3612 - val_loss: 0.3736 - val_mean_squared_error: 0.3736\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36072\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3623 - mean_squared_error: 0.3623 - val_loss: 0.3597 - val_mean_squared_error: 0.3597\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36072 to 0.35968, saving model to temp/f238\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3577 - mean_squared_error: 0.3577 - val_loss: 0.3603 - val_mean_squared_error: 0.3603\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35968\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3591 - mean_squared_error: 0.3591 - val_loss: 0.3553 - val_mean_squared_error: 0.3553\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35968 to 0.35531, saving model to temp/f238\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3563 - mean_squared_error: 0.3563 - val_loss: 0.3520 - val_mean_squared_error: 0.3520\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35531 to 0.35202, saving model to temp/f238\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3554 - mean_squared_error: 0.3554 - val_loss: 0.3532 - val_mean_squared_error: 0.3532\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35202\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3589 - mean_squared_error: 0.3589 - val_loss: 0.3648 - val_mean_squared_error: 0.3648\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35202\n",
      "Epoch 00009: early stopping\n",
      "temp/f239\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.4490 - mean_squared_error: 0.4490 - val_loss: 0.3666 - val_mean_squared_error: 0.3666\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36664, saving model to temp/f239\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3652 - mean_squared_error: 0.3652 - val_loss: 0.3611 - val_mean_squared_error: 0.3611\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36664 to 0.36109, saving model to temp/f239\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3596 - mean_squared_error: 0.3596 - val_loss: 0.3683 - val_mean_squared_error: 0.3683\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36109\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3597 - mean_squared_error: 0.3597 - val_loss: 0.3583 - val_mean_squared_error: 0.3583\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36109 to 0.35834, saving model to temp/f239\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3612 - mean_squared_error: 0.3612 - val_loss: 0.3590 - val_mean_squared_error: 0.3590\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35834\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3537 - val_mean_squared_error: 0.3537\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35834 to 0.35366, saving model to temp/f239\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3537 - val_mean_squared_error: 0.3537\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35366\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3562 - mean_squared_error: 0.3562 - val_loss: 0.3571 - val_mean_squared_error: 0.3571\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35366\n",
      "Epoch 00008: early stopping\n",
      "temp/f240\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4497 - mean_squared_error: 0.4497 - val_loss: 0.3629 - val_mean_squared_error: 0.3629\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36292, saving model to temp/f240\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3684 - mean_squared_error: 0.3684 - val_loss: 0.3786 - val_mean_squared_error: 0.3786\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36292\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3624 - mean_squared_error: 0.3624 - val_loss: 0.3561 - val_mean_squared_error: 0.3561\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36292 to 0.35611, saving model to temp/f240\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3601 - mean_squared_error: 0.3601 - val_loss: 0.3668 - val_mean_squared_error: 0.3668\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35611\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3624 - mean_squared_error: 0.3624 - val_loss: 0.3543 - val_mean_squared_error: 0.3543\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35611 to 0.35430, saving model to temp/f240\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3567 - mean_squared_error: 0.3567 - val_loss: 0.3582 - val_mean_squared_error: 0.3582\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35430\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3565 - mean_squared_error: 0.3565 - val_loss: 0.3553 - val_mean_squared_error: 0.3553\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35430\n",
      "Epoch 00007: early stopping\n",
      "temp/f241\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.4513 - mean_squared_error: 0.4513 - val_loss: 0.3627 - val_mean_squared_error: 0.3627\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36270, saving model to temp/f241\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3638 - mean_squared_error: 0.3638 - val_loss: 0.3623 - val_mean_squared_error: 0.3623\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36270 to 0.36233, saving model to temp/f241\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3613 - mean_squared_error: 0.3613 - val_loss: 0.3578 - val_mean_squared_error: 0.3578\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36233 to 0.35780, saving model to temp/f241\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3626 - mean_squared_error: 0.3626 - val_loss: 0.3618 - val_mean_squared_error: 0.3618\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35780\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3601 - mean_squared_error: 0.3601 - val_loss: 0.3541 - val_mean_squared_error: 0.3541\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35780 to 0.35409, saving model to temp/f241\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3593 - mean_squared_error: 0.3593 - val_loss: 0.3572 - val_mean_squared_error: 0.3572\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35409\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3560 - mean_squared_error: 0.3560 - val_loss: 0.3545 - val_mean_squared_error: 0.3545\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35409\n",
      "Epoch 00007: early stopping\n",
      "temp/f242\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4594 - mean_squared_error: 0.4594 - val_loss: 0.3645 - val_mean_squared_error: 0.3645\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36452, saving model to temp/f242\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3666 - mean_squared_error: 0.3666 - val_loss: 0.3643 - val_mean_squared_error: 0.3643\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36452 to 0.36428, saving model to temp/f242\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3627 - mean_squared_error: 0.3627 - val_loss: 0.3594 - val_mean_squared_error: 0.3594\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36428 to 0.35941, saving model to temp/f242\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3601 - mean_squared_error: 0.3601 - val_loss: 0.3566 - val_mean_squared_error: 0.3566\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35941 to 0.35661, saving model to temp/f242\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3577 - mean_squared_error: 0.3577 - val_loss: 0.3569 - val_mean_squared_error: 0.3569\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35661\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3598 - mean_squared_error: 0.3598 - val_loss: 0.3577 - val_mean_squared_error: 0.3577\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35661\n",
      "Epoch 00006: early stopping\n",
      "temp/f243\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4558 - mean_squared_error: 0.4558 - val_loss: 0.3731 - val_mean_squared_error: 0.3731\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37305, saving model to temp/f243\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3651 - mean_squared_error: 0.3651 - val_loss: 0.3588 - val_mean_squared_error: 0.3588\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37305 to 0.35880, saving model to temp/f243\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3632 - mean_squared_error: 0.3632 - val_loss: 0.3561 - val_mean_squared_error: 0.3561\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35880 to 0.35610, saving model to temp/f243\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3619 - mean_squared_error: 0.3619 - val_loss: 0.3603 - val_mean_squared_error: 0.3603\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35610\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3607 - mean_squared_error: 0.3607 - val_loss: 0.3600 - val_mean_squared_error: 0.3600\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35610\n",
      "Epoch 00005: early stopping\n",
      "temp/f244\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4596 - mean_squared_error: 0.4596 - val_loss: 0.3653 - val_mean_squared_error: 0.3653\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36525, saving model to temp/f244\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3637 - mean_squared_error: 0.3637 - val_loss: 0.3642 - val_mean_squared_error: 0.3642\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36525 to 0.36423, saving model to temp/f244\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3634 - mean_squared_error: 0.3634 - val_loss: 0.3582 - val_mean_squared_error: 0.3582\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36423 to 0.35816, saving model to temp/f244\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3608 - mean_squared_error: 0.3608 - val_loss: 0.3554 - val_mean_squared_error: 0.3554\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35816 to 0.35538, saving model to temp/f244\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3593 - val_mean_squared_error: 0.3593\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35538\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3588 - mean_squared_error: 0.3588 - val_loss: 0.3546 - val_mean_squared_error: 0.3546\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35538 to 0.35463, saving model to temp/f244\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3556 - mean_squared_error: 0.3556 - val_loss: 0.3562 - val_mean_squared_error: 0.3562\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35463\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3563 - mean_squared_error: 0.3563 - val_loss: 0.3606 - val_mean_squared_error: 0.3606\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35463\n",
      "Epoch 00008: early stopping\n",
      "temp/f245\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4553 - mean_squared_error: 0.4553 - val_loss: 0.3649 - val_mean_squared_error: 0.3649\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36486, saving model to temp/f245\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3654 - mean_squared_error: 0.3654 - val_loss: 0.3640 - val_mean_squared_error: 0.3640\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36486 to 0.36400, saving model to temp/f245\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3639 - mean_squared_error: 0.3639 - val_loss: 0.3570 - val_mean_squared_error: 0.3570\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36400 to 0.35703, saving model to temp/f245\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3601 - mean_squared_error: 0.3601 - val_loss: 0.3627 - val_mean_squared_error: 0.3627\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35703\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3606 - mean_squared_error: 0.3606 - val_loss: 0.3563 - val_mean_squared_error: 0.3563\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35703 to 0.35627, saving model to temp/f245\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3592 - mean_squared_error: 0.3592 - val_loss: 0.3598 - val_mean_squared_error: 0.3598\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35627\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3547 - mean_squared_error: 0.3547 - val_loss: 0.3526 - val_mean_squared_error: 0.3526\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35627 to 0.35265, saving model to temp/f245\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3582 - val_mean_squared_error: 0.3582\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35265\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3556 - mean_squared_error: 0.3556 - val_loss: 0.3617 - val_mean_squared_error: 0.3617\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35265\n",
      "Epoch 00009: early stopping\n",
      "temp/f246\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4509 - mean_squared_error: 0.4509 - val_loss: 0.3649 - val_mean_squared_error: 0.3649\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36486, saving model to temp/f246\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3635 - mean_squared_error: 0.3635 - val_loss: 0.3591 - val_mean_squared_error: 0.3591\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36486 to 0.35905, saving model to temp/f246\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3637 - mean_squared_error: 0.3637 - val_loss: 0.3642 - val_mean_squared_error: 0.3642\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35905\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3597 - mean_squared_error: 0.3597 - val_loss: 0.3568 - val_mean_squared_error: 0.3568\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35905 to 0.35681, saving model to temp/f246\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3598 - mean_squared_error: 0.3598 - val_loss: 0.3567 - val_mean_squared_error: 0.3567\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35681 to 0.35667, saving model to temp/f246\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3592 - mean_squared_error: 0.3592 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35667\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3570 - mean_squared_error: 0.3570 - val_loss: 0.3676 - val_mean_squared_error: 0.3676\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35667\n",
      "Epoch 00007: early stopping\n",
      "temp/f247\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.4442 - mean_squared_error: 0.4442 - val_loss: 0.3640 - val_mean_squared_error: 0.3640\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36399, saving model to temp/f247\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3659 - mean_squared_error: 0.3659 - val_loss: 0.3599 - val_mean_squared_error: 0.3599\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36399 to 0.35992, saving model to temp/f247\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3637 - mean_squared_error: 0.3637 - val_loss: 0.3614 - val_mean_squared_error: 0.3614\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35992\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3627 - mean_squared_error: 0.3627 - val_loss: 0.3598 - val_mean_squared_error: 0.3598\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35992 to 0.35982, saving model to temp/f247\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3590 - mean_squared_error: 0.3590 - val_loss: 0.3594 - val_mean_squared_error: 0.3594\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35982 to 0.35942, saving model to temp/f247\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35942 to 0.35503, saving model to temp/f247\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3569 - mean_squared_error: 0.3569 - val_loss: 0.3515 - val_mean_squared_error: 0.3515\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35503 to 0.35154, saving model to temp/f247\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3575 - mean_squared_error: 0.3575 - val_loss: 0.3526 - val_mean_squared_error: 0.3526\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35154\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3529 - val_mean_squared_error: 0.3529\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35154\n",
      "Epoch 00009: early stopping\n",
      "temp/f248\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4479 - mean_squared_error: 0.4479 - val_loss: 0.3704 - val_mean_squared_error: 0.3704\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37043, saving model to temp/f248\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3668 - mean_squared_error: 0.3668 - val_loss: 0.3633 - val_mean_squared_error: 0.3633\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37043 to 0.36326, saving model to temp/f248\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3612 - mean_squared_error: 0.3612 - val_loss: 0.3591 - val_mean_squared_error: 0.3591\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36326 to 0.35909, saving model to temp/f248\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3609 - mean_squared_error: 0.3609 - val_loss: 0.3663 - val_mean_squared_error: 0.3663\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35909\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3614 - mean_squared_error: 0.3614 - val_loss: 0.3589 - val_mean_squared_error: 0.3589\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35909 to 0.35890, saving model to temp/f248\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3579 - mean_squared_error: 0.3579 - val_loss: 0.3543 - val_mean_squared_error: 0.3543\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35890 to 0.35428, saving model to temp/f248\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3574 - mean_squared_error: 0.3574 - val_loss: 0.3541 - val_mean_squared_error: 0.3541\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35428 to 0.35406, saving model to temp/f248\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3529 - mean_squared_error: 0.3529 - val_loss: 0.3511 - val_mean_squared_error: 0.3511\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35406 to 0.35105, saving model to temp/f248\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3546 - mean_squared_error: 0.3546 - val_loss: 0.3546 - val_mean_squared_error: 0.3546\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35105\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3543 - mean_squared_error: 0.3543 - val_loss: 0.3691 - val_mean_squared_error: 0.3691\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35105\n",
      "Epoch 00010: early stopping\n",
      "temp/f249\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.4410 - mean_squared_error: 0.4410 - val_loss: 0.3639 - val_mean_squared_error: 0.3639\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36392, saving model to temp/f249\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3649 - mean_squared_error: 0.3649 - val_loss: 0.3586 - val_mean_squared_error: 0.3586\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36392 to 0.35859, saving model to temp/f249\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3635 - mean_squared_error: 0.3635 - val_loss: 0.3618 - val_mean_squared_error: 0.3618\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35859\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3608 - mean_squared_error: 0.3608 - val_loss: 0.3597 - val_mean_squared_error: 0.3597\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35859\n",
      "Epoch 00004: early stopping\n",
      "temp/f250\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4526 - mean_squared_error: 0.4526 - val_loss: 0.3630 - val_mean_squared_error: 0.3630\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36299, saving model to temp/f250\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3654 - mean_squared_error: 0.3654 - val_loss: 0.3653 - val_mean_squared_error: 0.3653\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36299\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3617 - mean_squared_error: 0.3617 - val_loss: 0.3597 - val_mean_squared_error: 0.3597\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36299 to 0.35968, saving model to temp/f250\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3596 - mean_squared_error: 0.3596 - val_loss: 0.3615 - val_mean_squared_error: 0.3615\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35968\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3600 - mean_squared_error: 0.3600 - val_loss: 0.3556 - val_mean_squared_error: 0.3556\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35968 to 0.35558, saving model to temp/f250\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3591 - mean_squared_error: 0.3591 - val_loss: 0.3528 - val_mean_squared_error: 0.3528\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35558 to 0.35280, saving model to temp/f250\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3579 - mean_squared_error: 0.3579 - val_loss: 0.3571 - val_mean_squared_error: 0.3571\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35280\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3551 - mean_squared_error: 0.3551 - val_loss: 0.3533 - val_mean_squared_error: 0.3533\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35280\n",
      "Epoch 00008: early stopping\n",
      "temp/f251\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4530 - mean_squared_error: 0.4530 - val_loss: 0.3711 - val_mean_squared_error: 0.3711\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37106, saving model to temp/f251\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3665 - mean_squared_error: 0.3665 - val_loss: 0.3622 - val_mean_squared_error: 0.3622\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37106 to 0.36221, saving model to temp/f251\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3645 - mean_squared_error: 0.3645 - val_loss: 0.3605 - val_mean_squared_error: 0.3605\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36221 to 0.36052, saving model to temp/f251\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3595 - mean_squared_error: 0.3595 - val_loss: 0.3635 - val_mean_squared_error: 0.3635\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36052\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3579 - mean_squared_error: 0.3579 - val_loss: 0.3578 - val_mean_squared_error: 0.3578\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.36052 to 0.35777, saving model to temp/f251\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.3588 - mean_squared_error: 0.3588 - val_loss: 0.3557 - val_mean_squared_error: 0.3557\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35777 to 0.35573, saving model to temp/f251\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3631 - val_mean_squared_error: 0.3631\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35573\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3555 - mean_squared_error: 0.3555 - val_loss: 0.3499 - val_mean_squared_error: 0.3499\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35573 to 0.34993, saving model to temp/f251\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3554 - mean_squared_error: 0.3554 - val_loss: 0.3508 - val_mean_squared_error: 0.3508\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.34993\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3565 - mean_squared_error: 0.3565 - val_loss: 0.3530 - val_mean_squared_error: 0.3530\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.34993\n",
      "Epoch 00010: early stopping\n",
      "temp/f252\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 0.4549 - mean_squared_error: 0.4549 - val_loss: 0.3643 - val_mean_squared_error: 0.3643\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36429, saving model to temp/f252\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3678 - mean_squared_error: 0.3678 - val_loss: 0.3610 - val_mean_squared_error: 0.3610\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36429 to 0.36098, saving model to temp/f252\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3635 - mean_squared_error: 0.3635 - val_loss: 0.3587 - val_mean_squared_error: 0.3587\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36098 to 0.35867, saving model to temp/f252\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3610 - mean_squared_error: 0.3610 - val_loss: 0.3646 - val_mean_squared_error: 0.3646\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35867\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3606 - mean_squared_error: 0.3606 - val_loss: 0.3560 - val_mean_squared_error: 0.3560\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35867 to 0.35597, saving model to temp/f252\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3561 - mean_squared_error: 0.3561 - val_loss: 0.3576 - val_mean_squared_error: 0.3576\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35597\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3592 - mean_squared_error: 0.3592 - val_loss: 0.3601 - val_mean_squared_error: 0.3601\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35597\n",
      "Epoch 00007: early stopping\n",
      "temp/f253\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.4567 - mean_squared_error: 0.4567 - val_loss: 0.3665 - val_mean_squared_error: 0.3665\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36651, saving model to temp/f253\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3675 - mean_squared_error: 0.3675 - val_loss: 0.3627 - val_mean_squared_error: 0.3627\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36651 to 0.36266, saving model to temp/f253\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3639 - mean_squared_error: 0.3639 - val_loss: 0.3613 - val_mean_squared_error: 0.3613\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36266 to 0.36130, saving model to temp/f253\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3613 - mean_squared_error: 0.3613 - val_loss: 0.3644 - val_mean_squared_error: 0.3644\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36130\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3604 - mean_squared_error: 0.3604 - val_loss: 0.3624 - val_mean_squared_error: 0.3624\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.36130\n",
      "Epoch 00005: early stopping\n",
      "temp/f254\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4565 - mean_squared_error: 0.4565 - val_loss: 0.3639 - val_mean_squared_error: 0.3639\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36386, saving model to temp/f254\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3679 - mean_squared_error: 0.3679 - val_loss: 0.3646 - val_mean_squared_error: 0.3646\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36386\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3647 - mean_squared_error: 0.3647 - val_loss: 0.3576 - val_mean_squared_error: 0.3576\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36386 to 0.35759, saving model to temp/f254\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3607 - mean_squared_error: 0.3607 - val_loss: 0.3623 - val_mean_squared_error: 0.3623\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35759\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3580 - mean_squared_error: 0.3580 - val_loss: 0.3601 - val_mean_squared_error: 0.3601\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35759\n",
      "Epoch 00005: early stopping\n",
      "temp/f255\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4512 - mean_squared_error: 0.4512 - val_loss: 0.3629 - val_mean_squared_error: 0.3629\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36288, saving model to temp/f255\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3656 - mean_squared_error: 0.3656 - val_loss: 0.3606 - val_mean_squared_error: 0.3606\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36288 to 0.36060, saving model to temp/f255\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3601 - mean_squared_error: 0.3601 - val_loss: 0.3586 - val_mean_squared_error: 0.3586\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36060 to 0.35859, saving model to temp/f255\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3612 - mean_squared_error: 0.3612 - val_loss: 0.3629 - val_mean_squared_error: 0.3629\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35859\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3593 - mean_squared_error: 0.3593 - val_loss: 0.3529 - val_mean_squared_error: 0.3529\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35859 to 0.35285, saving model to temp/f255\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3577 - mean_squared_error: 0.3577 - val_loss: 0.3531 - val_mean_squared_error: 0.3531\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35285\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3568 - mean_squared_error: 0.3568 - val_loss: 0.3601 - val_mean_squared_error: 0.3601\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35285\n",
      "Epoch 00007: early stopping\n",
      "temp/f256\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4588 - mean_squared_error: 0.4588 - val_loss: 0.3629 - val_mean_squared_error: 0.3629\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36292, saving model to temp/f256\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3664 - mean_squared_error: 0.3664 - val_loss: 0.3652 - val_mean_squared_error: 0.3652\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36292\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3640 - mean_squared_error: 0.3640 - val_loss: 0.3577 - val_mean_squared_error: 0.3577\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36292 to 0.35774, saving model to temp/f256\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3606 - mean_squared_error: 0.3606 - val_loss: 0.3599 - val_mean_squared_error: 0.3599\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35774\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3606 - mean_squared_error: 0.3606 - val_loss: 0.3544 - val_mean_squared_error: 0.3544\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35774 to 0.35439, saving model to temp/f256\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3583 - val_mean_squared_error: 0.3583\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35439\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3575 - mean_squared_error: 0.3575 - val_loss: 0.3574 - val_mean_squared_error: 0.3574\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35439\n",
      "Epoch 00007: early stopping\n",
      "temp/f257\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4571 - mean_squared_error: 0.4571 - val_loss: 0.3703 - val_mean_squared_error: 0.3703\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37026, saving model to temp/f257\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3642 - mean_squared_error: 0.3642 - val_loss: 0.3648 - val_mean_squared_error: 0.3648\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37026 to 0.36481, saving model to temp/f257\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3611 - mean_squared_error: 0.3611 - val_loss: 0.3608 - val_mean_squared_error: 0.3608\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36481 to 0.36075, saving model to temp/f257\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3551 - val_mean_squared_error: 0.3551\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36075 to 0.35510, saving model to temp/f257\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3596 - mean_squared_error: 0.3596 - val_loss: 0.3574 - val_mean_squared_error: 0.3574\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35510\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3569 - mean_squared_error: 0.3569 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35510 to 0.35495, saving model to temp/f257\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3515 - val_mean_squared_error: 0.3515\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35495 to 0.35145, saving model to temp/f257\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3552 - mean_squared_error: 0.3552 - val_loss: 0.3589 - val_mean_squared_error: 0.3589\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35145\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3555 - mean_squared_error: 0.3555 - val_loss: 0.3516 - val_mean_squared_error: 0.3516\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35145\n",
      "Epoch 00009: early stopping\n",
      "temp/f258\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4563 - mean_squared_error: 0.4563 - val_loss: 0.3615 - val_mean_squared_error: 0.3615\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36153, saving model to temp/f258\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3670 - mean_squared_error: 0.3670 - val_loss: 0.3648 - val_mean_squared_error: 0.3648\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36153\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3611 - mean_squared_error: 0.3611 - val_loss: 0.3554 - val_mean_squared_error: 0.3554\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36153 to 0.35544, saving model to temp/f258\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3614 - mean_squared_error: 0.3614 - val_loss: 0.3715 - val_mean_squared_error: 0.3715\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35544\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3603 - mean_squared_error: 0.3603 - val_loss: 0.3551 - val_mean_squared_error: 0.3551\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35544 to 0.35508, saving model to temp/f258\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3594 - mean_squared_error: 0.3594 - val_loss: 0.3681 - val_mean_squared_error: 0.3681\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35508\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3590 - mean_squared_error: 0.3590 - val_loss: 0.3585 - val_mean_squared_error: 0.3585\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35508\n",
      "Epoch 00007: early stopping\n",
      "temp/f259\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4592 - mean_squared_error: 0.4592 - val_loss: 0.3680 - val_mean_squared_error: 0.3680\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36801, saving model to temp/f259\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3640 - mean_squared_error: 0.3640 - val_loss: 0.3586 - val_mean_squared_error: 0.3586\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36801 to 0.35863, saving model to temp/f259\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3620 - mean_squared_error: 0.3620 - val_loss: 0.3611 - val_mean_squared_error: 0.3611\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35863\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3601 - mean_squared_error: 0.3601 - val_loss: 0.3563 - val_mean_squared_error: 0.3563\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35863 to 0.35626, saving model to temp/f259\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3594 - mean_squared_error: 0.3594 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35626\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3591 - mean_squared_error: 0.3591 - val_loss: 0.3613 - val_mean_squared_error: 0.3613\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35626\n",
      "Epoch 00006: early stopping\n",
      "temp/f260\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4571 - mean_squared_error: 0.4571 - val_loss: 0.3657 - val_mean_squared_error: 0.3657\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36566, saving model to temp/f260\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3669 - mean_squared_error: 0.3669 - val_loss: 0.3604 - val_mean_squared_error: 0.3604\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36566 to 0.36036, saving model to temp/f260\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3637 - mean_squared_error: 0.3637 - val_loss: 0.3560 - val_mean_squared_error: 0.3560\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36036 to 0.35596, saving model to temp/f260\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3607 - mean_squared_error: 0.3607 - val_loss: 0.3619 - val_mean_squared_error: 0.3619\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35596\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3606 - mean_squared_error: 0.3606 - val_loss: 0.3591 - val_mean_squared_error: 0.3591\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35596\n",
      "Epoch 00005: early stopping\n",
      "temp/f261\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4511 - mean_squared_error: 0.4511 - val_loss: 0.3608 - val_mean_squared_error: 0.3608\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36077, saving model to temp/f261\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3672 - mean_squared_error: 0.3672 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36077 to 0.35796, saving model to temp/f261\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3620 - mean_squared_error: 0.3620 - val_loss: 0.3592 - val_mean_squared_error: 0.3592\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35796\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3620 - mean_squared_error: 0.3620 - val_loss: 0.3564 - val_mean_squared_error: 0.3564\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35796 to 0.35641, saving model to temp/f261\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3564 - mean_squared_error: 0.3564 - val_loss: 0.3584 - val_mean_squared_error: 0.3584\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35641\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3590 - mean_squared_error: 0.3590 - val_loss: 0.3558 - val_mean_squared_error: 0.3558\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35641 to 0.35576, saving model to temp/f261\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3582 - mean_squared_error: 0.3582 - val_loss: 0.3600 - val_mean_squared_error: 0.3600\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35576\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3575 - mean_squared_error: 0.3575 - val_loss: 0.3531 - val_mean_squared_error: 0.3531\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35576 to 0.35308, saving model to temp/f261\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3546 - mean_squared_error: 0.3546 - val_loss: 0.3543 - val_mean_squared_error: 0.3543\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35308\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3555 - mean_squared_error: 0.3555 - val_loss: 0.3516 - val_mean_squared_error: 0.3516\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.35308 to 0.35162, saving model to temp/f261\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3517 - mean_squared_error: 0.3517 - val_loss: 0.3597 - val_mean_squared_error: 0.3597\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35162\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3550 - mean_squared_error: 0.3550 - val_loss: 0.3523 - val_mean_squared_error: 0.3523\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35162\n",
      "Epoch 00012: early stopping\n",
      "temp/f262\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4565 - mean_squared_error: 0.4565 - val_loss: 0.3639 - val_mean_squared_error: 0.3639\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36393, saving model to temp/f262\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3636 - mean_squared_error: 0.3636 - val_loss: 0.3623 - val_mean_squared_error: 0.3623\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36393 to 0.36225, saving model to temp/f262\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3626 - mean_squared_error: 0.3626 - val_loss: 0.3573 - val_mean_squared_error: 0.3573\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36225 to 0.35728, saving model to temp/f262\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3623 - mean_squared_error: 0.3623 - val_loss: 0.3542 - val_mean_squared_error: 0.3542\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35728 to 0.35418, saving model to temp/f262\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3580 - mean_squared_error: 0.3580 - val_loss: 0.3557 - val_mean_squared_error: 0.3557\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35418\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3572 - mean_squared_error: 0.3572 - val_loss: 0.3567 - val_mean_squared_error: 0.3567\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35418\n",
      "Epoch 00006: early stopping\n",
      "temp/f263\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4536 - mean_squared_error: 0.4536 - val_loss: 0.3608 - val_mean_squared_error: 0.3608\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36084, saving model to temp/f263\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3674 - mean_squared_error: 0.3674 - val_loss: 0.3625 - val_mean_squared_error: 0.3625\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36084\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3628 - mean_squared_error: 0.3628 - val_loss: 0.3582 - val_mean_squared_error: 0.3582\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36084 to 0.35821, saving model to temp/f263\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3617 - mean_squared_error: 0.3617 - val_loss: 0.3556 - val_mean_squared_error: 0.3556\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35821 to 0.35556, saving model to temp/f263\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3591 - mean_squared_error: 0.3591 - val_loss: 0.3578 - val_mean_squared_error: 0.3578\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35556\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3581 - mean_squared_error: 0.3581 - val_loss: 0.3548 - val_mean_squared_error: 0.3548\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35556 to 0.35484, saving model to temp/f263\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3538 - val_mean_squared_error: 0.3538\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35484 to 0.35379, saving model to temp/f263\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3545 - mean_squared_error: 0.3545 - val_loss: 0.3627 - val_mean_squared_error: 0.3627\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35379\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3591 - mean_squared_error: 0.3591 - val_loss: 0.3527 - val_mean_squared_error: 0.3527\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35379 to 0.35274, saving model to temp/f263\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3552 - mean_squared_error: 0.3552 - val_loss: 0.3599 - val_mean_squared_error: 0.3599\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35274\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3581 - mean_squared_error: 0.3581 - val_loss: 0.3502 - val_mean_squared_error: 0.3502\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.35274 to 0.35022, saving model to temp/f263\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3553 - val_mean_squared_error: 0.3553\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35022\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3550 - mean_squared_error: 0.3550 - val_loss: 0.3557 - val_mean_squared_error: 0.3557\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.35022\n",
      "Epoch 00013: early stopping\n",
      "temp/f264\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4472 - mean_squared_error: 0.4472 - val_loss: 0.3748 - val_mean_squared_error: 0.3748\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37483, saving model to temp/f264\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3658 - mean_squared_error: 0.3658 - val_loss: 0.3628 - val_mean_squared_error: 0.3628\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37483 to 0.36284, saving model to temp/f264\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3629 - mean_squared_error: 0.3629 - val_loss: 0.3589 - val_mean_squared_error: 0.3589\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36284 to 0.35890, saving model to temp/f264\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3603 - mean_squared_error: 0.3603 - val_loss: 0.3654 - val_mean_squared_error: 0.3654\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35890\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3593 - mean_squared_error: 0.3593 - val_loss: 0.3549 - val_mean_squared_error: 0.3549\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35890 to 0.35495, saving model to temp/f264\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3572 - mean_squared_error: 0.3572 - val_loss: 0.3553 - val_mean_squared_error: 0.3553\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35495\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3583 - mean_squared_error: 0.3583 - val_loss: 0.3561 - val_mean_squared_error: 0.3561\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35495\n",
      "Epoch 00007: early stopping\n",
      "temp/f265\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4612 - mean_squared_error: 0.4612 - val_loss: 0.3616 - val_mean_squared_error: 0.3616\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36162, saving model to temp/f265\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3667 - mean_squared_error: 0.3667 - val_loss: 0.3593 - val_mean_squared_error: 0.3593\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36162 to 0.35929, saving model to temp/f265\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.3624 - mean_squared_error: 0.3624 - val_loss: 0.3586 - val_mean_squared_error: 0.3586\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35929 to 0.35859, saving model to temp/f265\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3595 - mean_squared_error: 0.3595 - val_loss: 0.3558 - val_mean_squared_error: 0.3558\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35859 to 0.35581, saving model to temp/f265\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3595 - mean_squared_error: 0.3595 - val_loss: 0.3566 - val_mean_squared_error: 0.3566\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35581\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3578 - mean_squared_error: 0.3578 - val_loss: 0.3630 - val_mean_squared_error: 0.3630\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35581\n",
      "Epoch 00006: early stopping\n",
      "temp/f266\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 0.4669 - mean_squared_error: 0.4669 - val_loss: 0.3630 - val_mean_squared_error: 0.3630\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36298, saving model to temp/f266\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3665 - mean_squared_error: 0.3665 - val_loss: 0.3611 - val_mean_squared_error: 0.3611\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36298 to 0.36109, saving model to temp/f266\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3627 - mean_squared_error: 0.3627 - val_loss: 0.3575 - val_mean_squared_error: 0.3575\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36109 to 0.35752, saving model to temp/f266\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3636 - mean_squared_error: 0.3636 - val_loss: 0.3635 - val_mean_squared_error: 0.3635\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35752\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3584 - mean_squared_error: 0.3584 - val_loss: 0.3585 - val_mean_squared_error: 0.3585\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35752\n",
      "Epoch 00005: early stopping\n",
      "temp/f267\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.4534 - mean_squared_error: 0.4534 - val_loss: 0.3720 - val_mean_squared_error: 0.3720\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37196, saving model to temp/f267\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3677 - mean_squared_error: 0.3677 - val_loss: 0.3620 - val_mean_squared_error: 0.3620\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37196 to 0.36196, saving model to temp/f267\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3621 - mean_squared_error: 0.3621 - val_loss: 0.3597 - val_mean_squared_error: 0.3597\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36196 to 0.35969, saving model to temp/f267\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3584 - mean_squared_error: 0.3584 - val_loss: 0.3657 - val_mean_squared_error: 0.3657\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35969\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3615 - mean_squared_error: 0.3615 - val_loss: 0.3603 - val_mean_squared_error: 0.3603\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35969\n",
      "Epoch 00005: early stopping\n",
      "temp/f268\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4508 - mean_squared_error: 0.4508 - val_loss: 0.3698 - val_mean_squared_error: 0.3698\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36975, saving model to temp/f268\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3647 - mean_squared_error: 0.3647 - val_loss: 0.3600 - val_mean_squared_error: 0.3600\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36975 to 0.36003, saving model to temp/f268\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36003 to 0.35799, saving model to temp/f268\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3586 - mean_squared_error: 0.3586 - val_loss: 0.3639 - val_mean_squared_error: 0.3639\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35799\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3595 - mean_squared_error: 0.3595 - val_loss: 0.3544 - val_mean_squared_error: 0.3544\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35799 to 0.35441, saving model to temp/f268\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3604 - mean_squared_error: 0.3604 - val_loss: 0.3602 - val_mean_squared_error: 0.3602\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35441\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3533 - val_mean_squared_error: 0.3533\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35441 to 0.35334, saving model to temp/f268\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3568 - mean_squared_error: 0.3568 - val_loss: 0.3535 - val_mean_squared_error: 0.3535\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35334\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3556 - mean_squared_error: 0.3556 - val_loss: 0.3524 - val_mean_squared_error: 0.3524\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35334 to 0.35242, saving model to temp/f268\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3555 - mean_squared_error: 0.3555 - val_loss: 0.3503 - val_mean_squared_error: 0.3503\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.35242 to 0.35034, saving model to temp/f268\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3541 - mean_squared_error: 0.3541 - val_loss: 0.3506 - val_mean_squared_error: 0.3506\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35034\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3551 - mean_squared_error: 0.3551 - val_loss: 0.3563 - val_mean_squared_error: 0.3563\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35034\n",
      "Epoch 00012: early stopping\n",
      "temp/f269\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 0.4558 - mean_squared_error: 0.4558 - val_loss: 0.3704 - val_mean_squared_error: 0.3704\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37037, saving model to temp/f269\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3652 - mean_squared_error: 0.3652 - val_loss: 0.3574 - val_mean_squared_error: 0.3574\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37037 to 0.35736, saving model to temp/f269\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3649 - mean_squared_error: 0.3649 - val_loss: 0.3737 - val_mean_squared_error: 0.3737\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35736\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3611 - val_mean_squared_error: 0.3611\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35736\n",
      "Epoch 00004: early stopping\n",
      "temp/f270\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4523 - mean_squared_error: 0.4523 - val_loss: 0.3708 - val_mean_squared_error: 0.3708\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37079, saving model to temp/f270\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3641 - mean_squared_error: 0.3641 - val_loss: 0.3615 - val_mean_squared_error: 0.3615\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37079 to 0.36146, saving model to temp/f270\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3625 - mean_squared_error: 0.3625 - val_loss: 0.3572 - val_mean_squared_error: 0.3572\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36146 to 0.35717, saving model to temp/f270\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3606 - mean_squared_error: 0.3606 - val_loss: 0.3562 - val_mean_squared_error: 0.3562\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35717 to 0.35621, saving model to temp/f270\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3611 - mean_squared_error: 0.3611 - val_loss: 0.3592 - val_mean_squared_error: 0.3592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35621\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3566 - mean_squared_error: 0.3566 - val_loss: 0.3552 - val_mean_squared_error: 0.3552\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35621 to 0.35520, saving model to temp/f270\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3571 - mean_squared_error: 0.3571 - val_loss: 0.3567 - val_mean_squared_error: 0.3567\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35520\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3564 - val_mean_squared_error: 0.3564\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35520\n",
      "Epoch 00008: early stopping\n",
      "temp/f271\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4556 - mean_squared_error: 0.4556 - val_loss: 0.3633 - val_mean_squared_error: 0.3633\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36327, saving model to temp/f271\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3679 - mean_squared_error: 0.3679 - val_loss: 0.3595 - val_mean_squared_error: 0.3595\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36327 to 0.35946, saving model to temp/f271\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3628 - mean_squared_error: 0.3628 - val_loss: 0.3669 - val_mean_squared_error: 0.3669\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35946\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3603 - mean_squared_error: 0.3603 - val_loss: 0.3627 - val_mean_squared_error: 0.3627\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35946\n",
      "Epoch 00004: early stopping\n",
      "temp/f272\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 0.4575 - mean_squared_error: 0.4575 - val_loss: 0.3693 - val_mean_squared_error: 0.3693\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36930, saving model to temp/f272\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3655 - mean_squared_error: 0.3655 - val_loss: 0.3728 - val_mean_squared_error: 0.3728\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36930\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3641 - mean_squared_error: 0.3641 - val_loss: 0.3563 - val_mean_squared_error: 0.3563\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36930 to 0.35629, saving model to temp/f272\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3589 - mean_squared_error: 0.3589 - val_loss: 0.3530 - val_mean_squared_error: 0.3530\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35629 to 0.35304, saving model to temp/f272\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3563 - mean_squared_error: 0.3563 - val_loss: 0.3566 - val_mean_squared_error: 0.3566\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35304\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3588 - mean_squared_error: 0.3588 - val_loss: 0.3564 - val_mean_squared_error: 0.3564\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35304\n",
      "Epoch 00006: early stopping\n",
      "temp/f273\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4488 - mean_squared_error: 0.4488 - val_loss: 0.3629 - val_mean_squared_error: 0.3629\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36290, saving model to temp/f273\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3643 - mean_squared_error: 0.3643 - val_loss: 0.3634 - val_mean_squared_error: 0.3634\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36290\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3565 - val_mean_squared_error: 0.3565\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36290 to 0.35645, saving model to temp/f273\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3604 - mean_squared_error: 0.3604 - val_loss: 0.3569 - val_mean_squared_error: 0.3569\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35645\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3580 - mean_squared_error: 0.3580 - val_loss: 0.3540 - val_mean_squared_error: 0.3540\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35645 to 0.35404, saving model to temp/f273\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3582 - mean_squared_error: 0.3582 - val_loss: 0.3525 - val_mean_squared_error: 0.3525\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35404 to 0.35247, saving model to temp/f273\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3585 - mean_squared_error: 0.3585 - val_loss: 0.3543 - val_mean_squared_error: 0.3543\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35247\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3554 - mean_squared_error: 0.3554 - val_loss: 0.3519 - val_mean_squared_error: 0.3519\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35247 to 0.35190, saving model to temp/f273\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3535 - mean_squared_error: 0.3535 - val_loss: 0.3517 - val_mean_squared_error: 0.3517\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35190 to 0.35171, saving model to temp/f273\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3556 - mean_squared_error: 0.3556 - val_loss: 0.3514 - val_mean_squared_error: 0.3514\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.35171 to 0.35144, saving model to temp/f273\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3545 - mean_squared_error: 0.3545 - val_loss: 0.3485 - val_mean_squared_error: 0.3485\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.35144 to 0.34854, saving model to temp/f273\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3528 - mean_squared_error: 0.3528 - val_loss: 0.3529 - val_mean_squared_error: 0.3529\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.34854\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3529 - mean_squared_error: 0.3529 - val_loss: 0.3604 - val_mean_squared_error: 0.3604\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.34854\n",
      "Epoch 00013: early stopping\n",
      "temp/f274\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 0.4570 - mean_squared_error: 0.4570 - val_loss: 0.3639 - val_mean_squared_error: 0.3639\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36387, saving model to temp/f274\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3665 - mean_squared_error: 0.3665 - val_loss: 0.3651 - val_mean_squared_error: 0.3651\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36387\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3630 - mean_squared_error: 0.3630 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36387 to 0.35801, saving model to temp/f274\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3610 - mean_squared_error: 0.3610 - val_loss: 0.3537 - val_mean_squared_error: 0.3537\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35801 to 0.35366, saving model to temp/f274\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3589 - mean_squared_error: 0.3589 - val_loss: 0.3533 - val_mean_squared_error: 0.3533\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35366 to 0.35334, saving model to temp/f274\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3580 - mean_squared_error: 0.3580 - val_loss: 0.3545 - val_mean_squared_error: 0.3545\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35334\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3569 - mean_squared_error: 0.3569 - val_loss: 0.3534 - val_mean_squared_error: 0.3534\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35334\n",
      "Epoch 00007: early stopping\n",
      "temp/f275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4618 - mean_squared_error: 0.4618 - val_loss: 0.3622 - val_mean_squared_error: 0.3622\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36220, saving model to temp/f275\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3660 - mean_squared_error: 0.3660 - val_loss: 0.3621 - val_mean_squared_error: 0.3621\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36220 to 0.36212, saving model to temp/f275\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3631 - mean_squared_error: 0.3631 - val_loss: 0.3594 - val_mean_squared_error: 0.3594\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36212 to 0.35939, saving model to temp/f275\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3606 - mean_squared_error: 0.3606 - val_loss: 0.3578 - val_mean_squared_error: 0.3578\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35939 to 0.35783, saving model to temp/f275\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3601 - mean_squared_error: 0.3601 - val_loss: 0.3553 - val_mean_squared_error: 0.3553\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35783 to 0.35532, saving model to temp/f275\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3585 - mean_squared_error: 0.3585 - val_loss: 0.3644 - val_mean_squared_error: 0.3644\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35532\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3575 - mean_squared_error: 0.3575 - val_loss: 0.3577 - val_mean_squared_error: 0.3577\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35532\n",
      "Epoch 00007: early stopping\n",
      "temp/f276\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4477 - mean_squared_error: 0.4477 - val_loss: 0.3622 - val_mean_squared_error: 0.3622\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36222, saving model to temp/f276\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3674 - mean_squared_error: 0.3674 - val_loss: 0.3547 - val_mean_squared_error: 0.3547\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36222 to 0.35468, saving model to temp/f276\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3635 - mean_squared_error: 0.3635 - val_loss: 0.3638 - val_mean_squared_error: 0.3638\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35468\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3619 - mean_squared_error: 0.3619 - val_loss: 0.3556 - val_mean_squared_error: 0.3556\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35468\n",
      "Epoch 00004: early stopping\n",
      "temp/f277\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.4490 - mean_squared_error: 0.4490 - val_loss: 0.3658 - val_mean_squared_error: 0.3658\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36576, saving model to temp/f277\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3636 - mean_squared_error: 0.3636 - val_loss: 0.3584 - val_mean_squared_error: 0.3584\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36576 to 0.35840, saving model to temp/f277\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3614 - mean_squared_error: 0.3614 - val_loss: 0.3567 - val_mean_squared_error: 0.3567\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35840 to 0.35668, saving model to temp/f277\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3629 - mean_squared_error: 0.3629 - val_loss: 0.3614 - val_mean_squared_error: 0.3614\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35668\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3594 - mean_squared_error: 0.3594 - val_loss: 0.3565 - val_mean_squared_error: 0.3565\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35668 to 0.35653, saving model to temp/f277\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3575 - mean_squared_error: 0.3575 - val_loss: 0.3586 - val_mean_squared_error: 0.3586\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35653\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3589 - mean_squared_error: 0.3589 - val_loss: 0.3537 - val_mean_squared_error: 0.3537\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35653 to 0.35372, saving model to temp/f277\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3540 - mean_squared_error: 0.3540 - val_loss: 0.3547 - val_mean_squared_error: 0.3547\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35372\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3562 - mean_squared_error: 0.3562 - val_loss: 0.3510 - val_mean_squared_error: 0.3510\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35372 to 0.35105, saving model to temp/f277\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3564 - mean_squared_error: 0.3564 - val_loss: 0.3511 - val_mean_squared_error: 0.3511\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35105\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3529 - mean_squared_error: 0.3529 - val_loss: 0.3508 - val_mean_squared_error: 0.3508\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.35105 to 0.35079, saving model to temp/f277\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3543 - mean_squared_error: 0.3543 - val_loss: 0.3512 - val_mean_squared_error: 0.3512\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35079\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3538 - mean_squared_error: 0.3538 - val_loss: 0.3532 - val_mean_squared_error: 0.3532\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.35079\n",
      "Epoch 00013: early stopping\n",
      "temp/f278\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4473 - mean_squared_error: 0.4473 - val_loss: 0.3732 - val_mean_squared_error: 0.3732\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37320, saving model to temp/f278\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3650 - mean_squared_error: 0.3650 - val_loss: 0.3599 - val_mean_squared_error: 0.3599\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37320 to 0.35986, saving model to temp/f278\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3632 - mean_squared_error: 0.3632 - val_loss: 0.3648 - val_mean_squared_error: 0.3648\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35986\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3593 - mean_squared_error: 0.3593 - val_loss: 0.3564 - val_mean_squared_error: 0.3564\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35986 to 0.35635, saving model to temp/f278\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3604 - mean_squared_error: 0.3604 - val_loss: 0.3552 - val_mean_squared_error: 0.3552\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35635 to 0.35518, saving model to temp/f278\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3579 - mean_squared_error: 0.3579 - val_loss: 0.3552 - val_mean_squared_error: 0.3552\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35518 to 0.35516, saving model to temp/f278\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3573 - mean_squared_error: 0.3573 - val_loss: 0.3512 - val_mean_squared_error: 0.3512\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35516 to 0.35119, saving model to temp/f278\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3570 - mean_squared_error: 0.3570 - val_loss: 0.3536 - val_mean_squared_error: 0.3536\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35119\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3531 - val_mean_squared_error: 0.3531\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35119\n",
      "Epoch 00009: early stopping\n",
      "temp/f279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4443 - mean_squared_error: 0.4443 - val_loss: 0.3711 - val_mean_squared_error: 0.3711\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37115, saving model to temp/f279\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3675 - mean_squared_error: 0.3675 - val_loss: 0.3638 - val_mean_squared_error: 0.3638\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37115 to 0.36378, saving model to temp/f279\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3630 - mean_squared_error: 0.3630 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36378 to 0.35802, saving model to temp/f279\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3615 - mean_squared_error: 0.3615 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35802 to 0.35802, saving model to temp/f279\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3586 - mean_squared_error: 0.3586 - val_loss: 0.3551 - val_mean_squared_error: 0.3551\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35802 to 0.35511, saving model to temp/f279\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3567 - mean_squared_error: 0.3567 - val_loss: 0.3557 - val_mean_squared_error: 0.3557\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35511\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3562 - mean_squared_error: 0.3562 - val_loss: 0.3552 - val_mean_squared_error: 0.3552\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35511\n",
      "Epoch 00007: early stopping\n",
      "temp/f280\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4546 - mean_squared_error: 0.4546 - val_loss: 0.3702 - val_mean_squared_error: 0.3702\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37017, saving model to temp/f280\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3673 - mean_squared_error: 0.3673 - val_loss: 0.3601 - val_mean_squared_error: 0.3601\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37017 to 0.36012, saving model to temp/f280\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3602 - mean_squared_error: 0.3602 - val_loss: 0.3604 - val_mean_squared_error: 0.3604\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36012\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3583 - mean_squared_error: 0.3583 - val_loss: 0.3607 - val_mean_squared_error: 0.3607\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36012\n",
      "Epoch 00004: early stopping\n",
      "temp/f281\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4611 - mean_squared_error: 0.4611 - val_loss: 0.3644 - val_mean_squared_error: 0.3644\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36445, saving model to temp/f281\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3641 - mean_squared_error: 0.3641 - val_loss: 0.3593 - val_mean_squared_error: 0.3593\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36445 to 0.35933, saving model to temp/f281\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3636 - mean_squared_error: 0.3636 - val_loss: 0.3573 - val_mean_squared_error: 0.3573\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35933 to 0.35726, saving model to temp/f281\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3605 - mean_squared_error: 0.3605 - val_loss: 0.3564 - val_mean_squared_error: 0.3564\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35726 to 0.35645, saving model to temp/f281\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3590 - mean_squared_error: 0.3590 - val_loss: 0.3667 - val_mean_squared_error: 0.3667\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35645\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3595 - mean_squared_error: 0.3595 - val_loss: 0.3541 - val_mean_squared_error: 0.3541\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35645 to 0.35412, saving model to temp/f281\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3559 - mean_squared_error: 0.3559 - val_loss: 0.3606 - val_mean_squared_error: 0.3606\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35412\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3542 - val_mean_squared_error: 0.3542\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35412\n",
      "Epoch 00008: early stopping\n",
      "temp/f282\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 0.4569 - mean_squared_error: 0.4569 - val_loss: 0.3663 - val_mean_squared_error: 0.3663\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36626, saving model to temp/f282\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3668 - mean_squared_error: 0.3668 - val_loss: 0.3675 - val_mean_squared_error: 0.3675\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36626\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3633 - mean_squared_error: 0.3633 - val_loss: 0.3646 - val_mean_squared_error: 0.3646\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36626 to 0.36459, saving model to temp/f282\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3623 - mean_squared_error: 0.3623 - val_loss: 0.3549 - val_mean_squared_error: 0.3549\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36459 to 0.35489, saving model to temp/f282\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3591 - mean_squared_error: 0.3591 - val_loss: 0.3525 - val_mean_squared_error: 0.3525\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35489 to 0.35255, saving model to temp/f282\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3594 - mean_squared_error: 0.3594 - val_loss: 0.3546 - val_mean_squared_error: 0.3546\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35255\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3567 - mean_squared_error: 0.3567 - val_loss: 0.3619 - val_mean_squared_error: 0.3619\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35255\n",
      "Epoch 00007: early stopping\n",
      "temp/f283\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4513 - mean_squared_error: 0.4513 - val_loss: 0.3793 - val_mean_squared_error: 0.3793\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37928, saving model to temp/f283\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3656 - mean_squared_error: 0.3656 - val_loss: 0.3586 - val_mean_squared_error: 0.3586\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37928 to 0.35857, saving model to temp/f283\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3637 - mean_squared_error: 0.3637 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35857 to 0.35802, saving model to temp/f283\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3612 - mean_squared_error: 0.3612 - val_loss: 0.3631 - val_mean_squared_error: 0.3631\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35802\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.3608 - mean_squared_error: 0.3608 - val_loss: 0.3576 - val_mean_squared_error: 0.3576\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35802 to 0.35760, saving model to temp/f283\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3603 - mean_squared_error: 0.3603 - val_loss: 0.3540 - val_mean_squared_error: 0.3540\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35760 to 0.35404, saving model to temp/f283\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3561 - mean_squared_error: 0.3561 - val_loss: 0.3541 - val_mean_squared_error: 0.3541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35404\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3558 - mean_squared_error: 0.3558 - val_loss: 0.3515 - val_mean_squared_error: 0.3515\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35404 to 0.35153, saving model to temp/f283\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3535 - mean_squared_error: 0.3535 - val_loss: 0.3545 - val_mean_squared_error: 0.3545\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35153\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3548 - mean_squared_error: 0.3548 - val_loss: 0.3563 - val_mean_squared_error: 0.3563\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35153\n",
      "Epoch 00010: early stopping\n",
      "temp/f284\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4523 - mean_squared_error: 0.4523 - val_loss: 0.3642 - val_mean_squared_error: 0.3642\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36420, saving model to temp/f284\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3652 - mean_squared_error: 0.3652 - val_loss: 0.3582 - val_mean_squared_error: 0.3582\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36420 to 0.35822, saving model to temp/f284\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3642 - mean_squared_error: 0.3642 - val_loss: 0.3562 - val_mean_squared_error: 0.3562\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35822 to 0.35624, saving model to temp/f284\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3597 - mean_squared_error: 0.3597 - val_loss: 0.3620 - val_mean_squared_error: 0.3620\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35624\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3583 - mean_squared_error: 0.3583 - val_loss: 0.3588 - val_mean_squared_error: 0.3588\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35624\n",
      "Epoch 00005: early stopping\n",
      "temp/f285\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4427 - mean_squared_error: 0.4427 - val_loss: 0.3638 - val_mean_squared_error: 0.3638\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36380, saving model to temp/f285\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3642 - mean_squared_error: 0.3642 - val_loss: 0.3600 - val_mean_squared_error: 0.3600\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36380 to 0.36002, saving model to temp/f285\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3613 - mean_squared_error: 0.3613 - val_loss: 0.3619 - val_mean_squared_error: 0.3619\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36002\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3604 - mean_squared_error: 0.3604 - val_loss: 0.3660 - val_mean_squared_error: 0.3660\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36002\n",
      "Epoch 00004: early stopping\n",
      "temp/f286\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 0.4534 - mean_squared_error: 0.4534 - val_loss: 0.3668 - val_mean_squared_error: 0.3668\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36683, saving model to temp/f286\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3686 - mean_squared_error: 0.3686 - val_loss: 0.3568 - val_mean_squared_error: 0.3568\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36683 to 0.35683, saving model to temp/f286\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3627 - mean_squared_error: 0.3627 - val_loss: 0.3566 - val_mean_squared_error: 0.3566\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35683 to 0.35659, saving model to temp/f286\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3593 - mean_squared_error: 0.3593 - val_loss: 0.3608 - val_mean_squared_error: 0.3608\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35659\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3606 - mean_squared_error: 0.3606 - val_loss: 0.3566 - val_mean_squared_error: 0.3566\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35659 to 0.35658, saving model to temp/f286\n",
      "Epoch 00005: early stopping\n",
      "temp/f287\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4518 - mean_squared_error: 0.4518 - val_loss: 0.3649 - val_mean_squared_error: 0.3649\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36488, saving model to temp/f287\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3673 - mean_squared_error: 0.3673 - val_loss: 0.3597 - val_mean_squared_error: 0.3597\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36488 to 0.35974, saving model to temp/f287\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3617 - mean_squared_error: 0.3617 - val_loss: 0.3551 - val_mean_squared_error: 0.3551\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35974 to 0.35513, saving model to temp/f287\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3588 - mean_squared_error: 0.3588 - val_loss: 0.3573 - val_mean_squared_error: 0.3573\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35513\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3578 - mean_squared_error: 0.3578 - val_loss: 0.3567 - val_mean_squared_error: 0.3567\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35513\n",
      "Epoch 00005: early stopping\n",
      "temp/f288\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4613 - mean_squared_error: 0.4613 - val_loss: 0.3684 - val_mean_squared_error: 0.3684\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36837, saving model to temp/f288\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3672 - mean_squared_error: 0.3672 - val_loss: 0.3657 - val_mean_squared_error: 0.3657\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36837 to 0.36571, saving model to temp/f288\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3640 - mean_squared_error: 0.3640 - val_loss: 0.3635 - val_mean_squared_error: 0.3635\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36571 to 0.36345, saving model to temp/f288\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3610 - mean_squared_error: 0.3610 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36345 to 0.35796, saving model to temp/f288\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3580 - mean_squared_error: 0.3580 - val_loss: 0.3560 - val_mean_squared_error: 0.3560\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35796 to 0.35601, saving model to temp/f288\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3569 - mean_squared_error: 0.3569 - val_loss: 0.3534 - val_mean_squared_error: 0.3534\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35601 to 0.35343, saving model to temp/f288\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3555 - mean_squared_error: 0.3555 - val_loss: 0.3576 - val_mean_squared_error: 0.3576\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35343\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3572 - mean_squared_error: 0.3572 - val_loss: 0.3533 - val_mean_squared_error: 0.3533\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35343 to 0.35334, saving model to temp/f288\n",
      "Epoch 00008: early stopping\n",
      "temp/f289\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4526 - mean_squared_error: 0.4526 - val_loss: 0.3626 - val_mean_squared_error: 0.3626\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36264, saving model to temp/f289\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3653 - mean_squared_error: 0.3653 - val_loss: 0.3640 - val_mean_squared_error: 0.3640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36264\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3627 - mean_squared_error: 0.3627 - val_loss: 0.3611 - val_mean_squared_error: 0.3611\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36264 to 0.36105, saving model to temp/f289\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3600 - mean_squared_error: 0.3600 - val_loss: 0.3594 - val_mean_squared_error: 0.3594\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36105 to 0.35937, saving model to temp/f289\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3602 - mean_squared_error: 0.3602 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35937 to 0.35804, saving model to temp/f289\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3628 - mean_squared_error: 0.3628 - val_loss: 0.3595 - val_mean_squared_error: 0.3595\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35804\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3584 - mean_squared_error: 0.3584 - val_loss: 0.3536 - val_mean_squared_error: 0.3536\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35804 to 0.35360, saving model to temp/f289\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3556 - mean_squared_error: 0.3556 - val_loss: 0.3641 - val_mean_squared_error: 0.3641\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35360\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3547 - mean_squared_error: 0.3547 - val_loss: 0.3538 - val_mean_squared_error: 0.3538\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35360\n",
      "Epoch 00009: early stopping\n",
      "temp/f290\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4626 - mean_squared_error: 0.4626 - val_loss: 0.3679 - val_mean_squared_error: 0.3679\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36785, saving model to temp/f290\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3651 - mean_squared_error: 0.3651 - val_loss: 0.3669 - val_mean_squared_error: 0.3669\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36785 to 0.36687, saving model to temp/f290\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3636 - mean_squared_error: 0.3636 - val_loss: 0.3565 - val_mean_squared_error: 0.3565\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36687 to 0.35647, saving model to temp/f290\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3595 - mean_squared_error: 0.3595 - val_loss: 0.3567 - val_mean_squared_error: 0.3567\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35647\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3603 - mean_squared_error: 0.3603 - val_loss: 0.3548 - val_mean_squared_error: 0.3548\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35647 to 0.35483, saving model to temp/f290\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3518 - val_mean_squared_error: 0.3518\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35483 to 0.35177, saving model to temp/f290\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3553 - mean_squared_error: 0.3553 - val_loss: 0.3565 - val_mean_squared_error: 0.3565\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35177\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3557 - mean_squared_error: 0.3557 - val_loss: 0.3556 - val_mean_squared_error: 0.3556\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35177\n",
      "Epoch 00008: early stopping\n",
      "temp/f291\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.4508 - mean_squared_error: 0.4508 - val_loss: 0.3663 - val_mean_squared_error: 0.3663\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36634, saving model to temp/f291\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3662 - mean_squared_error: 0.3662 - val_loss: 0.3665 - val_mean_squared_error: 0.3665\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36634\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3605 - mean_squared_error: 0.3605 - val_loss: 0.3687 - val_mean_squared_error: 0.3687\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36634\n",
      "Epoch 00003: early stopping\n",
      "temp/f292\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.4559 - mean_squared_error: 0.4559 - val_loss: 0.3785 - val_mean_squared_error: 0.3785\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37845, saving model to temp/f292\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3673 - mean_squared_error: 0.3673 - val_loss: 0.3601 - val_mean_squared_error: 0.3601\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37845 to 0.36011, saving model to temp/f292\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3654 - mean_squared_error: 0.3654 - val_loss: 0.3634 - val_mean_squared_error: 0.3634\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36011\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3603 - mean_squared_error: 0.3603 - val_loss: 0.3605 - val_mean_squared_error: 0.3605\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36011\n",
      "Epoch 00004: early stopping\n",
      "temp/f293\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4540 - mean_squared_error: 0.4540 - val_loss: 0.3650 - val_mean_squared_error: 0.3650\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36504, saving model to temp/f293\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3633 - mean_squared_error: 0.3633 - val_loss: 0.3586 - val_mean_squared_error: 0.3586\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36504 to 0.35859, saving model to temp/f293\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3632 - mean_squared_error: 0.3632 - val_loss: 0.3636 - val_mean_squared_error: 0.3636\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35859\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3597 - mean_squared_error: 0.3597 - val_loss: 0.3611 - val_mean_squared_error: 0.3611\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35859\n",
      "Epoch 00004: early stopping\n",
      "temp/f294\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4525 - mean_squared_error: 0.4525 - val_loss: 0.3676 - val_mean_squared_error: 0.3676\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36759, saving model to temp/f294\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3658 - mean_squared_error: 0.3658 - val_loss: 0.3688 - val_mean_squared_error: 0.3688\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36759\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3622 - mean_squared_error: 0.3622 - val_loss: 0.3582 - val_mean_squared_error: 0.3582\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36759 to 0.35816, saving model to temp/f294\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3599 - mean_squared_error: 0.3599 - val_loss: 0.3621 - val_mean_squared_error: 0.3621\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35816\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3594 - mean_squared_error: 0.3594 - val_loss: 0.3554 - val_mean_squared_error: 0.3554\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35816 to 0.35542, saving model to temp/f294\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3589 - mean_squared_error: 0.3589 - val_loss: 0.3557 - val_mean_squared_error: 0.3557\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35542\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3568 - mean_squared_error: 0.3568 - val_loss: 0.3530 - val_mean_squared_error: 0.3530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: val_loss improved from 0.35542 to 0.35300, saving model to temp/f294\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3554 - mean_squared_error: 0.3554 - val_loss: 0.3540 - val_mean_squared_error: 0.3540\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35300\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3550 - mean_squared_error: 0.3550 - val_loss: 0.3532 - val_mean_squared_error: 0.3532\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35300\n",
      "Epoch 00009: early stopping\n",
      "temp/f295\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4474 - mean_squared_error: 0.4474 - val_loss: 0.3611 - val_mean_squared_error: 0.3611\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36111, saving model to temp/f295\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.3666 - mean_squared_error: 0.3666 - val_loss: 0.3632 - val_mean_squared_error: 0.3632\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36111\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3625 - mean_squared_error: 0.3625 - val_loss: 0.3576 - val_mean_squared_error: 0.3576\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36111 to 0.35758, saving model to temp/f295\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3609 - mean_squared_error: 0.3609 - val_loss: 0.3552 - val_mean_squared_error: 0.3552\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35758 to 0.35520, saving model to temp/f295\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3602 - mean_squared_error: 0.3602 - val_loss: 0.3654 - val_mean_squared_error: 0.3654\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35520\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3555 - mean_squared_error: 0.3555 - val_loss: 0.3545 - val_mean_squared_error: 0.3545\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35520 to 0.35452, saving model to temp/f295\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3572 - mean_squared_error: 0.3572 - val_loss: 0.3528 - val_mean_squared_error: 0.3528\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35452 to 0.35278, saving model to temp/f295\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3566 - mean_squared_error: 0.3566 - val_loss: 0.3544 - val_mean_squared_error: 0.3544\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35278\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3559 - mean_squared_error: 0.3559 - val_loss: 0.3521 - val_mean_squared_error: 0.3521\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35278 to 0.35208, saving model to temp/f295\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3542 - mean_squared_error: 0.3542 - val_loss: 0.3523 - val_mean_squared_error: 0.3523\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35208\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3550 - mean_squared_error: 0.3550 - val_loss: 0.3573 - val_mean_squared_error: 0.3573\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35208\n",
      "Epoch 00011: early stopping\n",
      "temp/f296\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4660 - mean_squared_error: 0.4660 - val_loss: 0.3622 - val_mean_squared_error: 0.3622\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36223, saving model to temp/f296\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3660 - mean_squared_error: 0.3660 - val_loss: 0.3619 - val_mean_squared_error: 0.3619\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36223 to 0.36192, saving model to temp/f296\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3615 - mean_squared_error: 0.3615 - val_loss: 0.3596 - val_mean_squared_error: 0.3596\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36192 to 0.35961, saving model to temp/f296\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3578 - mean_squared_error: 0.3578 - val_loss: 0.3571 - val_mean_squared_error: 0.3571\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35961 to 0.35712, saving model to temp/f296\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3587 - mean_squared_error: 0.3587 - val_loss: 0.3614 - val_mean_squared_error: 0.3614\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35712\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3554 - mean_squared_error: 0.3554 - val_loss: 0.3551 - val_mean_squared_error: 0.3551\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35712 to 0.35512, saving model to temp/f296\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3582 - mean_squared_error: 0.3582 - val_loss: 0.3536 - val_mean_squared_error: 0.3536\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35512 to 0.35360, saving model to temp/f296\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3576 - mean_squared_error: 0.3576 - val_loss: 0.3542 - val_mean_squared_error: 0.3542\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35360\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.3549 - mean_squared_error: 0.3549 - val_loss: 0.3535 - val_mean_squared_error: 0.3535\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35360 to 0.35348, saving model to temp/f296\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3575 - mean_squared_error: 0.3575 - val_loss: 0.3513 - val_mean_squared_error: 0.3513\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.35348 to 0.35131, saving model to temp/f296\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3536 - mean_squared_error: 0.3536 - val_loss: 0.3552 - val_mean_squared_error: 0.3552\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35131\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3544 - mean_squared_error: 0.3544 - val_loss: 0.3719 - val_mean_squared_error: 0.3719\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35131\n",
      "Epoch 00012: early stopping\n",
      "temp/f297\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4546 - mean_squared_error: 0.4546 - val_loss: 0.3740 - val_mean_squared_error: 0.3740\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37400, saving model to temp/f297\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3662 - mean_squared_error: 0.3662 - val_loss: 0.3611 - val_mean_squared_error: 0.3611\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37400 to 0.36106, saving model to temp/f297\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3616 - mean_squared_error: 0.3616 - val_loss: 0.3582 - val_mean_squared_error: 0.3582\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36106 to 0.35823, saving model to temp/f297\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.3611 - mean_squared_error: 0.3611 - val_loss: 0.3580 - val_mean_squared_error: 0.3580\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35823 to 0.35796, saving model to temp/f297\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3583 - mean_squared_error: 0.3583 - val_loss: 0.3644 - val_mean_squared_error: 0.3644\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35796\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3569 - mean_squared_error: 0.3569 - val_loss: 0.3530 - val_mean_squared_error: 0.3530\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35796 to 0.35302, saving model to temp/f297\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3563 - mean_squared_error: 0.3563 - val_loss: 0.3557 - val_mean_squared_error: 0.3557\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35302\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3582 - mean_squared_error: 0.3582 - val_loss: 0.3512 - val_mean_squared_error: 0.3512\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35302 to 0.35119, saving model to temp/f297\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3546 - mean_squared_error: 0.3546 - val_loss: 0.3565 - val_mean_squared_error: 0.3565\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35119\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3544 - mean_squared_error: 0.3544 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35119\n",
      "Epoch 00010: early stopping\n",
      "temp/f298\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 0.4638 - mean_squared_error: 0.4638 - val_loss: 0.3701 - val_mean_squared_error: 0.3701\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37008, saving model to temp/f298\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3666 - mean_squared_error: 0.3666 - val_loss: 0.3624 - val_mean_squared_error: 0.3624\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37008 to 0.36244, saving model to temp/f298\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3615 - mean_squared_error: 0.3615 - val_loss: 0.3598 - val_mean_squared_error: 0.3598\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36244 to 0.35981, saving model to temp/f298\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3634 - mean_squared_error: 0.3634 - val_loss: 0.3665 - val_mean_squared_error: 0.3665\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35981\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3605 - mean_squared_error: 0.3605 - val_loss: 0.3531 - val_mean_squared_error: 0.3531\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35981 to 0.35314, saving model to temp/f298\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3599 - mean_squared_error: 0.3599 - val_loss: 0.3617 - val_mean_squared_error: 0.3617\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35314\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3563 - mean_squared_error: 0.3563 - val_loss: 0.3534 - val_mean_squared_error: 0.3534\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35314\n",
      "Epoch 00007: early stopping\n",
      "temp/f299\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4560 - mean_squared_error: 0.4560 - val_loss: 0.3740 - val_mean_squared_error: 0.3740\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37399, saving model to temp/f299\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3642 - mean_squared_error: 0.3642 - val_loss: 0.3617 - val_mean_squared_error: 0.3617\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37399 to 0.36172, saving model to temp/f299\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3607 - mean_squared_error: 0.3607 - val_loss: 0.3637 - val_mean_squared_error: 0.3637\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36172\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.3621 - mean_squared_error: 0.3621 - val_loss: 0.3552 - val_mean_squared_error: 0.3552\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36172 to 0.35518, saving model to temp/f299\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3602 - mean_squared_error: 0.3602 - val_loss: 0.3552 - val_mean_squared_error: 0.3552\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35518\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3595 - mean_squared_error: 0.3595 - val_loss: 0.3565 - val_mean_squared_error: 0.3565\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35518\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "for idx, model_name in enumerate(model_names):\n",
    "    print(model_name)\n",
    "\n",
    "    if type(models[idx]) is list:\n",
    "        #clear session\n",
    "        keras.backend.clear_session() \n",
    "        #get model according to specification\n",
    "        model = get_model(models[idx], [0.2] * len(models), 6)\n",
    "        callbacks = [ModelCheckpoint(model_name, verbose= verbosity, monitor='val_loss',save_best_only=True), \n",
    "                     EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose= verbosity, mode='auto')]\n",
    "        model.compile(optimizer = optimizers.SGD(lr = 0.001, momentum = 0.9, ), loss='mean_squared_error', metrics = ['mse'])\n",
    "        #print(len(X), len(y))\n",
    "        model.fit(X, y, epochs = 20, validation_data = (x_val, y_val), callbacks = callbacks, batch_size = 32, verbose = verbosity)\n",
    "    else:\n",
    "        models[idx].fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "1 1\n",
      "1 2\n",
      "1 3\n",
      "2 1\n",
      "2 2\n",
      "2 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n            setA = get_MB(get_CG(perturbed_df, tetrad), \\'g\\', pc)\\n            if setA != {\\'f\\'}:\\n                print(\"Error in SETA markov blanket\")\\n                #setA = {\\'f\\'}\\n            setC = get_MB(get_CG(test_df2, tetrad), \\'g\\', pc)\\n\\n            if setA != setC:\\n                causal_dicts[idx][str(m) + \\'_\\' + str(v)].append(1)\\n            else:\\n                causal_dicts[idx][str(m) + \\'_\\' + str(v)].append(0)\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_test = 2000\n",
    "metrics_dicts = []\n",
    "for m in models:\n",
    "    metrics_dicts.append(defaultdict(list))\n",
    "\n",
    "\n",
    "#means = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "#variances = [1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0]\n",
    "means = [0, 1, 2]\n",
    "variances = [1,2,3]\n",
    "\n",
    "\n",
    "# ok at this point we need to check the model on various variances and means\n",
    "for m in means:\n",
    "    for v in variances:\n",
    "        print(m,v)\n",
    "        #t0 = time.time()\n",
    "        perturbed_df = gen_data(mean =m, var = v, SIZE = nb_test)\n",
    "        y_test2 = perturbed_df['g']\n",
    "        x_test2 = perturbed_df[['a', 'b', 'c', 'd', 'e', 'f']]\n",
    "        #t1 = time.time()\n",
    "        #print(\"Time for gen_data = \", t1 - t0)\n",
    "        for idx, model_name in enumerate(model_names):\n",
    "            #t0 = time.time()\n",
    "            if type(models[idx]) is list:\n",
    "                keras.backend.clear_session()\n",
    "                model = load_model(model_name)\n",
    "            else:\n",
    "                model = models[idx]\n",
    "            #t1 = time.time()\n",
    "            #print(\"Time to load model = \", t1 - t0)\n",
    "            \n",
    "            y_pred2 = model.predict(x_test2)\n",
    "            metrics_dicts[idx][str(m) + '_' + str(v)].append(mean_squared_error(y_test2, y_pred2))\n",
    "\n",
    "            test_df2 = pd.DataFrame(x_test2, columns = ['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "            test_targets2 = pd.DataFrame(model.predict(x_test2), columns = ['g'])\n",
    "            test_df2 = test_df2.join(test_targets2)\n",
    "'''\n",
    "            setA = get_MB(get_CG(perturbed_df, tetrad), 'g', pc)\n",
    "            if setA != {'f'}:\n",
    "                print(\"Error in SETA markov blanket\")\n",
    "                #setA = {'f'}\n",
    "            setC = get_MB(get_CG(test_df2, tetrad), 'g', pc)\n",
    "\n",
    "            if setA != setC:\n",
    "                causal_dicts[idx][str(m) + '_' + str(v)].append(1)\n",
    "            else:\n",
    "                causal_dicts[idx][str(m) + '_' + str(v)].append(0)\n",
    "\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times =  0\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20729.957148028232\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20748.12977880152\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20793.12883928121\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20759.157240566416\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20844.25999276072\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20820.49151088665\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20794.39760694454\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20805.944152164688\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20802.49091273295\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20766.436360762928\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20762.3364087828\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20805.07874467237\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20826.10500473472\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20821.28411382604\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20758.927598172755\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20766.903780558565\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20819.366617407326\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20878.13774384666\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20833.884714156025\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20775.161993160687\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20809.284278319497\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20755.990457727923\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20821.789196578502\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20782.89813492555\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20829.72335417196\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20789.530103505036\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20829.46702305291\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20841.29668856489\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20817.91778003411\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20705.63640827487\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20811.161375154894\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20807.45388464053\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20773.33223909468\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20710.64861460458\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20835.328767528543\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20770.691628740307\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20815.012533136418\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20753.59143420965\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20860.60024519276\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20779.962017100188\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20724.29911777576\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20725.550344564537\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20846.67734210829\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20776.60527934272\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20817.00098134496\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20806.860921908545\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20780.195285597594\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20851.16149009573\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20870.497357369703\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20709.57583287602\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20778.116186296873\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20675.265121802553\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20748.125656864693\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20813.83543248519\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20854.53300982476\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20832.989811449082\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20767.09021920491\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20752.332822431108\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20708.252369627327\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20745.239938173254\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20815.95979504346\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20759.753199442486\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20831.14435050203\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20789.474528186496\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20770.472919363474\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20849.953378712104\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20860.810884567814\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20790.70498745171\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20802.42888287377\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20762.817487348875\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20678.6510552751\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20770.2293687215\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20789.193580357\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20803.8766301507\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20761.82094418633\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20768.58091540521\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20769.566530354532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20753.943538253778\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20850.882229394825\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20797.854992559463\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20728.21403875032\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20813.930937764482\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20838.412744218946\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20774.9339317368\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20728.361158389635\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20787.00974283071\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20840.87126641623\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20820.243924667055\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20788.299715048135\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20787.23710895777\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20796.96537658385\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20811.55948585602\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20749.428222690545\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20753.787138659358\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20817.64977025981\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20840.222263078278\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20811.20879153366\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20741.34466634221\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20780.44154212812\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20852.901245845424\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20856.152376555125\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20769.93938302684\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20702.423061001995\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20733.094154369173\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20794.42277508819\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20787.485445512513\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20755.963848305706\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20743.660574690264\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20797.397446127834\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20779.66629282944\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20862.18599770234\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20828.443603178795\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20739.745328870726\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20702.325500375347\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20840.445295268764\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20812.262259597337\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20835.099776316274\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20839.099845716424\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20780.279221159257\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20772.132363286702\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20742.973385024397\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20804.429441803986\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20862.86977662139\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20839.092209550414\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20761.232774808865\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20743.77673511127\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20739.86931897885\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20753.641574083165\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20801.998332328785\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20730.55091965301\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20766.06094678635\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20727.904597662677\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20729.737640967738\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20729.174204594892\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20788.990998418838\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20650.65421314092\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20768.00850906946\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20768.527595584044\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20744.225722929128\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20814.285067744255\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20756.491279969425\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20745.121172446426\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20789.68733613374\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20781.365067666466\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20832.498156584905\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20867.975225880113\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20822.93540402461\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20737.169788572086\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20773.57019724053\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20736.695942945284\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20854.286361360537\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20774.830022224392\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20805.818809690845\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20848.58635468717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20837.19301262122\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20802.368508889962\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20800.897318915755\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20832.682235144457\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20779.774219223604\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20817.148523326596\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20851.161022844874\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20825.02094605726\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20798.47653092977\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20747.232054870005\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20776.98910186922\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20721.335843662622\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20782.748901013238\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20776.735338048777\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20721.301732141565\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20749.5554624762\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20803.093533787756\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20793.438697542948\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20700.661672622227\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20818.939207348947\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20733.629215759698\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20744.176196316625\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20731.21664294617\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20731.90307292196\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20742.753511361727\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20744.742010237977\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20799.34037590614\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20760.56440927236\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20829.94985116733\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20905.272704379764\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20816.23098153717\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20694.702791567153\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20774.330429928104\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20783.918065939513\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20755.04718661021\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20731.645696263662\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20740.428165636353\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20747.156899505364\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20773.64342116083\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20684.628946323755\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20778.825458268828\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20819.561654859823\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20753.292966429894\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20845.36336879358\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20764.903491772133\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20768.177656718766\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20825.469299302655\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20751.439603376322\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20822.468045765752\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20874.012009472084\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20766.56746856771\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20866.6105995699\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20782.047833769946\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20854.93493573484\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20828.43867889227\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20745.031727083293\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20705.587984508267\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20848.042877985547\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20883.617892247243\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20755.136163000596\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20797.08812675505\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20846.888089974607\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20884.976547408845\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20834.604937083706\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20785.72966085678\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20818.03437122346\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20738.357918943362\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20805.870975824328\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20835.221751661793\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20831.145058758186\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20789.37346819104\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20685.053754579654\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20738.14932913983\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20798.41496484932\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20715.13833081697\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20772.01703794348\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20828.203739628545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20751.92225230748\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20796.15640651757\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20720.60457033376\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20825.294575911234\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20774.53887169133\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20819.9545734649\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20792.35904164047\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20775.430897856728\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20794.196820410685\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20865.83931716923\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20879.334291051717\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20755.14770800651\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20810.45675983775\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20820.029383183908\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20737.95117595252\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20731.489631879336\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20737.449428855405\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20790.06100924987\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20786.457890178463\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20771.79272812685\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20760.784675741277\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20868.38619991372\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20893.387963966474\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20825.058834380307\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20817.30364409211\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20773.419296601583\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20765.559648933613\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20772.45123267926\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20844.92688621877\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20816.38976412261\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20804.345485217615\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20801.767350826256\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20777.677969007047\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20864.88276906694\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20819.25533321839\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20781.566915446598\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20847.601357122563\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20773.555505585828\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20737.98771064321\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20925.24464618125\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20768.51011014555\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20794.567257030267\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20717.225085777933\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20793.760580767914\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20836.53736785877\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20803.325618846706\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20738.71132317595\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20859.40450753084\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20844.073306552316\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20882.874052206986\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20766.769331058807\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20793.902848250127\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20736.7177874444\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20852.394841888272\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20810.56192387119\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20791.89504733788\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20790.7083460747\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20816.235891960732\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20784.229862744356\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20812.513628235116\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20693.45770347728\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20817.6025442663\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20763.336711504206\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20814.8957158489\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20727.451609687443\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20814.445224935153\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20860.347286276454\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20797.598170427184\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20922.883649990938 -20821.274718928682\n",
      "Times =  1\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20869.360023640726\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20912.93935190002\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20929.028927149902\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20916.772475488186\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20977.798220071985\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20984.487257542405\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20930.140312513482\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20926.973913389054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20941.86109590164\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20906.802024757304\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20937.882345426595\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20938.01953058893\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20969.456412247346\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20952.928826624364\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20899.25172819701\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20890.42822733717\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20950.676269961245\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -21009.360901579203\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20975.886652831665\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20928.99553233144\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20945.181500429975\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20895.87244954103\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20958.18914143179\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20921.270587386818\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20968.709311865838\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20929.21248550396\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20984.601805188948\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20967.815587282363\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20969.10751594787\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20890.623910514583\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20944.506722872495\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20924.06525006496\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20914.49953840359\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20852.284208472294\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20949.206442666684\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20862.932146498177\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20945.014565525846\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20893.979454387267\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -21003.373481161034\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20918.078246321333\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20855.794713545325\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20838.182736840936\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20986.447179172283\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20909.28109716754\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20943.646990205503\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20932.512915129482\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20941.98884754893\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20986.966987174932\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -21008.489884584356\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20828.39505730796\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20908.453367197944\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20838.2844275209\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20890.393642329273\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20946.99841868011\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20988.538475070472\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20952.7146128369\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20898.38701151294\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20907.34960363553\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20835.876548855253\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20881.528858455014\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20956.547748958754\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20915.489019385743\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20945.627525701406\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20921.917297315325\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20893.1194094485\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20971.64238641708\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20991.48686436836\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20912.30285851312\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20924.528328958382\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20891.414264166906\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20809.724204032667\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20895.678048713293\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20918.797780292603\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20943.341244213756\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20911.302731261105\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20923.492807248636\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20908.02141718151\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20884.068694367663\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20955.615181393707\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20940.987949245842\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20881.16110522795\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20934.34522324759\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20979.41620399567\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20907.828733367358\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20844.394770102324\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20928.072605858033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20976.697421284658\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20926.254966989138\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20902.029146722958\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20915.08957116417\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20931.12471877463\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20962.18085939226\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20896.183943791366\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20893.222955605674\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20946.918981503368\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20989.60497382083\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20948.347489371892\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20851.670932314824\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20920.644031878437\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -21001.087768888217\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20981.002415215\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20889.47047402063\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20815.555153792608\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20874.69872859733\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20938.573279461314\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20927.157544684873\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20844.31450409094\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20877.18598995367\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20932.767018540813\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20900.03643281461\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20988.357789842088\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20943.008414067175\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20911.510040824796\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20841.09604625885\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20966.598066589217\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20950.8212396382\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20946.021688143668\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20958.73695221614\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20901.13543592126\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20908.788509320842\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20896.70911134296\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20940.18151522745\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -21006.604537823005\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20985.725710629547\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20903.620430525883\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20887.82478080543\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20882.400975108627\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20866.101492959453\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20896.848324925828\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20866.83970454199\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20883.84326661827\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20875.435985605596\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20889.78469378397\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20875.487082465446\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20907.684334360776\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20759.46617334453\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20921.226455387005\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20903.44699170144\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20850.412689591536\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20931.67900591461\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20890.753960095688\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20871.26283263221\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20926.724399393177\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20893.337768199668\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20942.914736908482\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -21001.1610954377\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20983.357195493812\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20863.504835880667\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20928.826943835935\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20889.430139490647\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -21003.152348177893\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20939.0640833641\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20954.89097141218\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -21004.70632563222\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20959.282548354906\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20948.496247190174\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20924.236185212336\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20964.487694562813\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20908.68929449508\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20954.36704542341\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -21017.24619567325\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20967.361877152052\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20926.67624885438\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20870.91922547156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20908.54926414703\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20861.0392541108\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20934.918789724194\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20917.326595493454\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20837.685817456244\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20888.335594797587\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20932.675473605184\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20953.68039329267\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20839.58523483378\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20968.153974545035\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20875.150410911094\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20855.634586295288\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20852.174834871643\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20868.358266521624\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20896.52296245888\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20883.884777771105\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20936.172987577353\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20901.67611094805\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20972.909238225104\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -21016.03166971217\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20947.367630117147\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20810.725537969472\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20904.876891955297\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20901.30456027635\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20900.740474624214\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20855.489365316094\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20860.296845400597\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20892.93884989104\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20907.956439439295\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20830.18531089348\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20933.921857456196\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20974.567597734713\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20890.51344517803\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20991.11904030432\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20896.5506778124\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20890.26213126605\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20964.920845252833\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20889.396185150727\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20963.821196691173\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -21001.151697738424\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20901.364892443853\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -21001.94350842942\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20900.05407426083\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20977.260742556224\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20951.569403118225\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20895.038572263285\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20856.774037098075\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20986.352623251645\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -21026.206684007797\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20910.049637813903\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20925.09559076629\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20966.434486725924\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -21022.633174115075\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20969.965585897284\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20916.012717802914\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20931.18645245981\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20883.619793949616\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20949.98433032273\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20981.273001112306\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20953.377033149663\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20914.380386460925\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20824.660329270042\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20866.789744775677\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20933.811529651957\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20857.127333609813\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20919.888313764593\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20957.42395689591\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20882.67952915819\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20927.24899000208\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20860.673331474263\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20926.753409888137\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20929.13501515597\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20953.257388511574\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20909.5835147116\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20916.061901104666\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20910.47733170232\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20957.28997175336\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -21008.344495359786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20890.229331309354\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20938.122694099173\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20967.740025537427\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20855.56755773707\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20867.078856837066\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20854.17127330148\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20909.74852436571\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20914.93318448451\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20912.93764748392\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20913.3846669755\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -21000.23915483005\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -21035.46210434638\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20964.81878473965\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20942.301202691706\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20892.856518450226\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20900.735054671524\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20894.640652636335\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20983.8547762065\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20956.421019927613\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20936.72821905815\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20964.286867721028\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20898.833000264898\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20988.17335765272\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20952.209005711375\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20884.37745452172\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20979.693579093335\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20912.30661978698\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20875.471670859388\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -21050.488732282978\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20905.465260477376\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20932.259239236504\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20877.50550159006\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20942.40854985803\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20964.82000553039\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20922.806321954504\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20896.583870082264\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20964.52737616973\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20969.04127175247\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -21016.205243276854\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20872.3663683388\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20933.971463880865\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20871.642778056466\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20984.847224342207\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20958.377600572552\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20926.621407143317\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20930.249774538093\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20950.073165246984\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20934.650848221132\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20952.06862163479\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20837.756520898343\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20968.322956715936\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20908.494302703824\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20948.20934156127\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20858.04078177399\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20972.302168348528\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -21004.51151954312\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20939.494787391395\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-21114.29309838869 -20945.871510002402\n",
      "Times =  2\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20768.874372764705\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20809.160706803596\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20836.913315387454\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20805.880132155784\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20873.566564620018\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20855.666752557954\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20841.136042028458\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20826.600358099768\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20817.657234626447\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20794.287010709108\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20790.109473779376\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20826.49739367642\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20858.166461816123\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20856.592044272278\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20807.629290782144\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20790.895744636495\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20848.038776828194\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20888.41132383497\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20863.95697266139\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20822.08113672876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20837.627465686804\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20811.351422521973\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20862.640306916786\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20815.28901282431\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20846.437885978536\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20840.519461318385\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20860.228050664777\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20887.213468907023\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20859.344395749733\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20768.85289665826\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20828.780957936997\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20826.51070275395\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20798.486735938623\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20773.149499901454\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20861.51909233274\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20799.567845174224\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20851.9970898665\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20792.42674881948\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20900.329357150073\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20808.983898952785\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20755.473018475495\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20765.00790563121\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20898.667634965816\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20811.4982295427\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20847.680066325487\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20837.72804935246\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20822.352093264784\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20900.761065282935\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20914.514137968974\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20732.75028369431\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20804.16484785441\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20719.703455983785\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20797.87021847335\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20843.60065176264\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20880.515264310376\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20855.11572516896\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20803.99194950552\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20803.252659370355\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20729.695443560646\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20791.906346957097\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20852.408049172598\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20810.739121096194\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20856.88676938208\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20832.65596967903\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20791.885761183865\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20856.859203951266\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20893.54120780276\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20833.758165694282\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20829.95337178821\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20796.45306991223\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20718.489415371034\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20785.058995035502\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20830.697343890904\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20850.649353571876\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20790.264426922105\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20804.835407656206\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20810.58873031304\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20818.756071176103\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20863.69588620123\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20851.831126960467\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20788.37953244416\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20819.446512006318\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20877.02869440154\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20799.152116531193\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20752.604679008018\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20821.959826319508\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20887.532556980488\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20818.603080378296\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20816.32982550712\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20812.26782784965\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20827.427128050742\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20851.98059605454\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20799.509083766126\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20806.831817142873\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20859.40114716534\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20850.27850881479\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20859.633572200626\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20773.404210684297\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20805.917770597225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20904.185755769755\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20885.643727016093\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20771.511679272804\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20747.56591504613\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20753.143835087634\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20833.275146470678\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20820.343914851892\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20761.654748354507\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20772.188633679783\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20835.921508933112\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20823.410578776788\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20884.304893592427\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20842.663008126583\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20797.389185655913\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20716.463442298194\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20861.350570577222\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20842.2363415713\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20854.25550066262\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20855.65325193495\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20818.12415475298\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20802.281582470096\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20766.026456843396\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20843.167678415884\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20916.273014567996\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20873.24654703551\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20808.36578182198\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20797.67872494727\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20769.696680776025\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20775.07921386678\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20828.044998844147\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20767.34521261951\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20793.041803541535\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20768.507916741408\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20775.525741161917\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20755.74690645939\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20793.698167782077\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20683.347265624732\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20813.389294921653\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20779.547906267297\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20773.138428017282\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20840.387571676387\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20808.34560237416\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20785.22577867513\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20834.790395011336\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20792.164325555415\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20869.021560538273\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20912.49695016532\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20868.84085136924\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20771.193229751556\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20834.52245442946\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20778.156143602246\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20908.985336908405\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20808.918648165956\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20835.482514256444\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20917.11618174261\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20866.829261927327\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20841.87824275528\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20810.74438372828\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20890.9984676155\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20828.86338998618\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20853.50226568171\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20877.712047201323\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20867.590812674258\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20845.71546805093\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20773.362866976255\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20803.07817760372\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20764.972281012982\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20818.338092783044\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20809.440715703637\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20739.373913299158\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20780.264141286556\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20839.674245414113\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20843.320316241785\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20735.361844192736\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20870.354268161915\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20763.654660275868\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20774.768178750215\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20758.609630279443\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20756.42187148837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20795.677129405758\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20777.901201856628\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20822.67681952635\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20812.457880627386\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20858.276196442966\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20914.29438900757\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20859.21328098754\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20734.903391680702\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20795.86521078105\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20817.47092561726\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20803.782674676346\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20754.379461335244\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20760.630375624594\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20796.827699458998\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20824.049664923867\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20727.963320980627\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20850.07569239001\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20869.33061536047\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20793.437752364414\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20871.98659793481\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20791.74463433253\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20798.529274099597\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20844.47703530114\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20790.073865170547\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20866.25115855939\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20900.44365087011\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20805.194846388797\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20905.923694604993\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20815.562391811804\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20875.909795079304\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20849.59305075855\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20788.08638422761\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20740.848427138113\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20876.79214128896\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20914.91406784039\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20779.044970344417\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20824.31733332669\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20877.385288847185\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20903.061369292802\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20865.083698279093\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20811.785402791487\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20822.972935928396\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20781.346923738394\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20831.310650214837\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20867.187846808774\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20856.673101537905\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20829.163893737616\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20713.89981494476\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20766.295784467366\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20852.285831425346\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20745.240624028207\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20782.30224214054\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20874.831205343973\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20808.642464911834\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20818.42453961009\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20754.18305654351\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20831.906767552064\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20856.354251712502\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20859.089471287094\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20824.34861232023\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20805.331527354767\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20821.75223209094\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20863.6516876796\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20908.078604876988\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20795.080371578395\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20831.13779953529\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20845.669398300706\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20770.493007830974\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20776.17633143757\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20772.892351449504\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20821.596041352554\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20821.10333118667\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20818.346514856436\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20812.681240317233\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20900.663818994202\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20944.724938233136\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20867.86764973801\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20836.017381361846\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20773.667155992665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20785.65490166617\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20814.386228849555\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20885.681390905585\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20846.933027708794\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20822.687362456447\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20859.003263497212\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20792.897430427212\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20891.78229977592\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20849.844251357594\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20789.272649367853\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20871.04532059204\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20807.44115813818\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20765.98461404913\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20952.484211077328\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20789.37164222732\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20822.45035319087\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20757.622901721996\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20829.156496588694\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20862.953331273755\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20819.510248494506\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20792.18512893399\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20863.081042519945\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20850.071197536978\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20898.326665427783\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20813.807440768207\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20835.37122550729\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20787.066102369983\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20885.042313631366\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20863.04948911894\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20824.394810228245\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20814.402238027636\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20846.758180761186\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20827.18032678225\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20818.50725441089\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20740.534819039625\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20852.054440114563\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20826.44286337629\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20854.86898627264\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20772.088618999693\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20855.806614142908\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20892.828002111204\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20851.844304326838\n",
      "['a --> g', 'b --> g', 'c --> g', 'd --> g', 'g --> e', 'g --> f']\n",
      "-20937.4242195377 -20864.67023380806\n"
     ]
    }
   ],
   "source": [
    "#the number of times to sample\n",
    "times = 3\n",
    "## the size of the test set\n",
    "\n",
    "\n",
    "violations = np.zeros(len(models))\n",
    "violation_mean = np.zeros((len(models), times))\n",
    "violation_mean2 = np.zeros((len(models), times))\n",
    "mean = np.zeros((len(models), times))\n",
    "\n",
    "fold = 0\n",
    "\n",
    "from pycausal import prior as p\n",
    "def get_bic(df, prior):\n",
    "\n",
    "    tetrad.run(algoId = 'gfci', dfs = df,  scoreId = 'sem-bic-deterministic', dataType = 'continuous',\n",
    "               structurePrior = 1.0, samplePrior = 1, maxDegree = -1, maxPathLength = -1, priorKnowledge = prior,\n",
    "               completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True,\n",
    "               penaltyDiscount = 200\n",
    "               )\n",
    "    BIC = tetrad.getTetradGraph().getAllAttributes().toString()\n",
    "    BIC = float(BIC.split('=')[-1].split('}')[0])\n",
    "    return BIC #/ len(df)\n",
    "import itertools\n",
    "def get_pairs(lst):\n",
    "    a = set()\n",
    "    for i in itertools.permutations(lst,2):\n",
    "        a.add(i)\n",
    "    return a\n",
    "full_conx = get_pairs(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n",
    "forced_conx = set({('a','g'), ('b','g'),('c','g'),('d','g'),('g','e'),('g','f')})\n",
    "restricted_conx = full_conx.difference(forced_conx)   \n",
    "\n",
    "prior = p.knowledge(requiredirect =  list(map(list, forced_conx)),\n",
    "                       forbiddirect = list(map(list, restricted_conx))\n",
    "                       )\n",
    "\n",
    "\n",
    "for t in range(times):\n",
    "    print(\"Times = \", t)\n",
    "    df_test = gen_data(SIZE = nb_test)\n",
    "    x_test = df_test[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "    y_test = df_test['g'].values\n",
    "    bic_orig = get_bic(df_test,prior)\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        if type(models[idx]) is list:\n",
    "            keras.backend.clear_session()\n",
    "            model = load_model(model_name)\n",
    "        else:\n",
    "            model = models[idx]\n",
    "        test_df = pd.DataFrame(x_test, columns = ['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "        test_targets = pd.DataFrame(model.predict(x_test), columns = ['g'])\n",
    "        test_df = test_df.join(test_targets)\n",
    "       \n",
    "        mean[idx][t] = mean_squared_error(y_test, model.predict(x_test)) \n",
    "        bic_pred = get_bic(test_df,prior)\n",
    "        print(tetrad.getEdges())\n",
    "        print(bic_orig, bic_pred)\n",
    "        violation_mean[idx][t] = bic_pred\n",
    "        violation_mean2[idx][t] = bic_pred\n",
    "        #print(bic_orig - bic_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_name =  temp/f0 Violations =  0.0\n",
      "Average_violations =  -20789.39718147789 58.732045144971714\n",
      "MSE =  0.3607879692318871 0.007912628923386606\n",
      "Model_name =  temp/f1 Violations =  0.0\n",
      "Average_violations =  -20823.409945835043 68.03346948847428\n",
      "MSE =  0.3583079598592036 0.006465954772247917\n",
      "Model_name =  temp/f2 Violations =  0.0\n",
      "Average_violations =  -20853.02369393952 56.638424492552275\n",
      "MSE =  0.35679508516696673 0.006468380753862783\n",
      "Model_name =  temp/f3 Violations =  0.0\n",
      "Average_violations =  -20827.26994940346 66.09984256803229\n",
      "MSE =  0.3623526560406787 0.004452357532421517\n",
      "Model_name =  temp/f4 Violations =  0.0\n",
      "Average_violations =  -20898.541592484242 57.30577952803071\n",
      "MSE =  0.36170260403902743 0.005467573266527397\n",
      "Model_name =  temp/f5 Violations =  0.0\n",
      "Average_violations =  -20886.881840329002 70.495567277715\n",
      "MSE =  0.3644926397068962 0.004355216523014997\n",
      "Model_name =  temp/f6 Violations =  0.0\n",
      "Average_violations =  -20855.224653828827 56.30504575163248\n",
      "MSE =  0.3610697944286499 0.0036839872909837238\n",
      "Model_name =  temp/f7 Violations =  0.0\n",
      "Average_violations =  -20853.172807884504 52.862224011597824\n",
      "MSE =  0.3656113464806028 0.007607409548524458\n",
      "Model_name =  temp/f8 Violations =  0.0\n",
      "Average_violations =  -20854.003081087012 62.43277667657112\n",
      "MSE =  0.36429229716849143 0.007434879543753967\n",
      "Model_name =  temp/f9 Violations =  0.0\n",
      "Average_violations =  -20822.50846540978 60.679308863412516\n",
      "MSE =  0.3607527398468949 0.007361629457221442\n",
      "Model_name =  temp/f10 Violations =  0.0\n",
      "Average_violations =  -20830.10940932959 77.04583095384508\n",
      "MSE =  0.36729121794063185 0.004624764849689175\n",
      "Model_name =  temp/f11 Violations =  0.0\n",
      "Average_violations =  -20856.531889645907 58.28016444230869\n",
      "MSE =  0.3603554581244466 0.0023174927774560453\n",
      "Model_name =  temp/f12 Violations =  0.0\n",
      "Average_violations =  -20884.575959599395 61.430191756368274\n",
      "MSE =  0.3638010210177134 0.009141717672536387\n",
      "Model_name =  temp/f13 Violations =  0.0\n",
      "Average_violations =  -20876.934994907562 55.6354762859718\n",
      "MSE =  0.36203983775224646 0.0029208185167950112\n",
      "Model_name =  temp/f14 Violations =  0.0\n",
      "Average_violations =  -20821.936205717302 58.173483237568135\n",
      "MSE =  0.36008876046416666 0.0011833115983857637\n",
      "Model_name =  temp/f15 Violations =  0.0\n",
      "Average_violations =  -20816.07591751074 53.47961024211474\n",
      "MSE =  0.3669827266705945 0.0020951119581352137\n",
      "Model_name =  temp/f16 Violations =  0.0\n",
      "Average_violations =  -20872.69388806559 56.37057208658849\n",
      "MSE =  0.3570483904939679 0.004769220811088186\n",
      "Model_name =  temp/f17 Violations =  0.0\n",
      "Average_violations =  -20925.303323086944 59.5854791479194\n",
      "MSE =  0.3608836992348567 0.005786195086607755\n",
      "Model_name =  temp/f18 Violations =  0.0\n",
      "Average_violations =  -20891.24277988303 61.09841298382039\n",
      "MSE =  0.36214071573507006 0.004424744795238051\n",
      "Model_name =  temp/f19 Violations =  0.0\n",
      "Average_violations =  -20842.07955407363 64.37464286334297\n",
      "MSE =  0.3573681217027375 0.0019007246692140773\n",
      "Model_name =  temp/f20 Violations =  0.0\n",
      "Average_violations =  -20864.03108147876 58.53703636447039\n",
      "MSE =  0.3654727988295845 0.003261621933337634\n",
      "Model_name =  temp/f21 Violations =  0.0\n",
      "Average_violations =  -20821.07144326364 57.51870426689912\n",
      "MSE =  0.35889762186431895 0.006613634916872616\n",
      "Model_name =  temp/f22 Violations =  0.0\n",
      "Average_violations =  -20880.872881642357 57.15800512519516\n",
      "MSE =  0.3581012043053698 0.007639653241858648\n",
      "Model_name =  temp/f23 Violations =  0.0\n",
      "Average_violations =  -20839.819245045557 59.09333363224607\n",
      "MSE =  0.35913553810858007 0.0031227354090539905\n",
      "Model_name =  temp/f24 Violations =  0.0\n",
      "Average_violations =  -20881.62351733878 61.95587461887774\n",
      "MSE =  0.3631605871897256 0.0019773692463215717\n",
      "Model_name =  temp/f25 Violations =  0.0\n",
      "Average_violations =  -20853.087350109126 57.71340590489955\n",
      "MSE =  0.36255030671062105 0.0077948223813332955\n",
      "Model_name =  temp/f26 Violations =  0.0\n",
      "Average_violations =  -20891.43229296888 67.06702473609961\n",
      "MSE =  0.3630789263589953 0.005870536608249068\n",
      "Model_name =  temp/f27 Violations =  0.0\n",
      "Average_violations =  -20898.775248251422 52.29412962195696\n",
      "MSE =  0.3612442161013665 0.0023974500327555927\n",
      "Model_name =  temp/f28 Violations =  0.0\n",
      "Average_violations =  -20882.12323057724 63.78997072943018\n",
      "MSE =  0.35883417193497563 0.0020818877756881944\n",
      "Model_name =  temp/f29 Violations =  0.0\n",
      "Average_violations =  -20788.371071815905 76.77158053748526\n",
      "MSE =  0.3585295294349659 0.0030422801478318445\n",
      "Model_name =  temp/f30 Violations =  0.0\n",
      "Average_violations =  -20861.483018654795 59.14566211692898\n",
      "MSE =  0.36189874679033474 0.0036456221169143487\n",
      "Model_name =  temp/f31 Violations =  0.0\n",
      "Average_violations =  -20852.67661248648 51.07539368838737\n",
      "MSE =  0.3631943815355314 0.0015638325508063229\n",
      "Model_name =  temp/f32 Violations =  0.0\n",
      "Average_violations =  -20828.7728378123 61.48163722375291\n",
      "MSE =  0.36204195324126337 0.007091532708325315\n",
      "Model_name =  temp/f33 Violations =  0.0\n",
      "Average_violations =  -20778.69410765944 57.95525497575724\n",
      "MSE =  0.36326654217053606 0.006561654403210739\n",
      "Model_name =  temp/f34 Violations =  0.0\n",
      "Average_violations =  -20882.018100842655 48.69762647285259\n",
      "MSE =  0.35590511950243425 0.006962207189235794\n",
      "Model_name =  temp/f35 Violations =  0.0\n",
      "Average_violations =  -20811.063873470903 38.524427566662\n",
      "MSE =  0.36062276816167554 0.005481771631354142\n",
      "Model_name =  temp/f36 Violations =  0.0\n",
      "Average_violations =  -20870.67472950959 54.69169817149713\n",
      "MSE =  0.3679971515475647 0.00390164587747262\n",
      "Model_name =  temp/f37 Violations =  0.0\n",
      "Average_violations =  -20813.332545805464 59.18889711796816\n",
      "MSE =  0.3640683134787979 0.003851070925736423\n",
      "Model_name =  temp/f38 Violations =  0.0\n",
      "Average_violations =  -20921.434361167954 60.16707362646813\n",
      "MSE =  0.36884812770037767 0.002557712502355823\n",
      "Model_name =  temp/f39 Violations =  0.0\n",
      "Average_violations =  -20835.674720791434 59.46048083326084\n",
      "MSE =  0.363017679495053 0.0018800538412094617\n",
      "Model_name =  temp/f40 Violations =  0.0\n",
      "Average_violations =  -20778.522283265527 56.102432402519376\n",
      "MSE =  0.36109711278408335 0.00791356329885158\n",
      "Model_name =  temp/f41 Violations =  0.0\n",
      "Average_violations =  -20776.246995678895 46.66370328425228\n",
      "MSE =  0.36628427631107 0.0026465430218203835\n",
      "Model_name =  temp/f42 Violations =  0.0\n",
      "Average_violations =  -20910.59738541546 57.680967673152274\n",
      "MSE =  0.35773578054799665 0.0031431689052144438\n",
      "Model_name =  temp/f43 Violations =  0.0\n",
      "Average_violations =  -20832.461535350987 56.15640837090636\n",
      "MSE =  0.35876895883690435 0.0016369025866485342\n",
      "Model_name =  temp/f44 Violations =  0.0\n",
      "Average_violations =  -20869.442679291984 53.94448617126844\n",
      "MSE =  0.3635472076789395 0.0012054035833408705\n",
      "Model_name =  temp/f45 Violations =  0.0\n",
      "Average_violations =  -20859.033962130165 53.46377146920421\n",
      "MSE =  0.3713147925632246 0.00583001649869493\n",
      "Model_name =  temp/f46 Violations =  0.0\n",
      "Average_violations =  -20848.1787421371 68.53004701689058\n",
      "MSE =  0.35800105030144747 0.0038152385584031964\n",
      "Model_name =  temp/f47 Violations =  0.0\n",
      "Average_violations =  -20912.963180851195 56.109725701749404\n",
      "MSE =  0.363805470595472 0.0026847091496561533\n",
      "Model_name =  temp/f48 Violations =  0.0\n",
      "Average_violations =  -20931.167126641012 57.55273474854742\n",
      "MSE =  0.36308208876281384 0.006954137320973434\n",
      "Model_name =  temp/f49 Violations =  0.0\n",
      "Average_violations =  -20756.90705795943 51.42738784489519\n",
      "MSE =  0.3649965943614322 0.0036120687433455773\n",
      "Model_name =  temp/f50 Violations =  0.0\n",
      "Average_violations =  -20830.244800449742 56.31499579354676\n",
      "MSE =  0.3615962695616694 0.003992180651317017\n",
      "Model_name =  temp/f51 Violations =  0.0\n",
      "Average_violations =  -20744.417668435744 68.80851561671409\n",
      "MSE =  0.3581692353946815 0.00587152385021389\n",
      "Model_name =  temp/f52 Violations =  0.0\n",
      "Average_violations =  -20812.129839222438 58.949399261673825\n",
      "MSE =  0.3634590089901842 0.0019923058174166307\n",
      "Model_name =  temp/f53 Violations =  0.0\n",
      "Average_violations =  -20868.14483430931 57.06667384589301\n",
      "MSE =  0.35652792679687134 0.003887557242237249\n",
      "Model_name =  temp/f54 Violations =  0.0\n",
      "Average_violations =  -20907.862249735204 58.02447407660075\n",
      "MSE =  0.35835043753600976 0.006997998803359425\n",
      "Model_name =  temp/f55 Violations =  0.0\n",
      "Average_violations =  -20880.27338315165 52.01402268118626\n",
      "MSE =  0.3598606400341831 0.007347071016575327\n",
      "Model_name =  temp/f56 Violations =  0.0\n",
      "Average_violations =  -20823.15639340779 55.2881473142112\n",
      "MSE =  0.36487743266957606 0.00715099653370347\n",
      "Model_name =  temp/f57 Violations =  0.0\n",
      "Average_violations =  -20820.97836181233 64.51460304479316\n",
      "MSE =  0.3634875793458705 0.0038873429914469486\n",
      "Model_name =  temp/f58 Violations =  0.0\n",
      "Average_violations =  -20757.94145401441 55.799406274117004\n",
      "MSE =  0.36469032288823017 0.00213822114946233\n",
      "Model_name =  temp/f59 Violations =  0.0\n",
      "Average_violations =  -20806.22504786179 56.55343399293876\n",
      "MSE =  0.3590845634756204 0.005340017429097831\n",
      "Model_name =  temp/f60 Violations =  0.0\n",
      "Average_violations =  -20874.971864391606 59.57117599673055\n",
      "MSE =  0.35544292168961295 0.002276339633728984\n",
      "Model_name =  temp/f61 Violations =  0.0\n",
      "Average_violations =  -20828.660446641476 64.8294780447112\n",
      "MSE =  0.36171391025979044 0.005510280338580511\n",
      "Model_name =  temp/f62 Violations =  0.0\n",
      "Average_violations =  -20877.886215195173 49.03965670268334\n",
      "MSE =  0.36237015773361864 0.008112044563075928\n",
      "Model_name =  temp/f63 Violations =  0.0\n",
      "Average_violations =  -20848.01593172695 55.149603197202225\n",
      "MSE =  0.3600471832385432 0.002870212180109587\n",
      "Model_name =  temp/f64 Violations =  0.0\n",
      "Average_violations =  -20818.492696665282 53.48823636606602\n",
      "MSE =  0.36044519790425017 0.007565583147525256\n",
      "Model_name =  temp/f65 Violations =  0.0\n",
      "Average_violations =  -20892.818323026815 55.80828691495324\n",
      "MSE =  0.3569581418707439 0.0036941753533762988\n",
      "Model_name =  temp/f66 Violations =  0.0\n",
      "Average_violations =  -20915.27965224631 55.51860288424115\n",
      "MSE =  0.3612430892157983 0.006087824598171886\n",
      "Model_name =  temp/f67 Violations =  0.0\n",
      "Average_violations =  -20845.58867055304 50.34203807009711\n",
      "MSE =  0.3624224523674451 0.0036418174137159286\n",
      "Model_name =  temp/f68 Violations =  0.0\n",
      "Average_violations =  -20852.303527873457 52.29222881281904\n",
      "MSE =  0.3614789897248875 0.0017065126891227913\n",
      "Model_name =  temp/f69 Violations =  0.0\n",
      "Average_violations =  -20816.894940476002 54.45294792207408\n",
      "MSE =  0.3635659568684759 0.004211330129474348\n",
      "Model_name =  temp/f70 Violations =  0.0\n",
      "Average_violations =  -20735.621558226267 54.86453207926473\n",
      "MSE =  0.3678504004556547 0.0042947065497520595\n",
      "Model_name =  temp/f71 Violations =  0.0\n",
      "Average_violations =  -20816.988804156765 55.97009535344786\n",
      "MSE =  0.3575810963120724 0.004944315944478079\n",
      "Model_name =  temp/f72 Violations =  0.0\n",
      "Average_violations =  -20846.22956818017 54.03856433921928\n",
      "MSE =  0.3582579588858197 0.006736042600212297\n",
      "Model_name =  temp/f73 Violations =  0.0\n",
      "Average_violations =  -20865.955742645445 57.955780866639806\n",
      "MSE =  0.3593264329540691 0.00255267358085926\n",
      "Model_name =  temp/f74 Violations =  0.0\n",
      "Average_violations =  -20821.129367456513 64.8109280004102\n",
      "MSE =  0.3635867463461248 0.006326019226254485\n",
      "Model_name =  temp/f75 Violations =  0.0\n",
      "Average_violations =  -20832.303043436685 66.15777515482222\n",
      "MSE =  0.3667938723972542 0.006869975090829713\n",
      "Model_name =  temp/f76 Violations =  0.0\n",
      "Average_violations =  -20829.392225949694 58.06672889980994\n",
      "MSE =  0.36799062762897333 0.00830583738660583\n",
      "Model_name =  temp/f77 Violations =  0.0\n",
      "Average_violations =  -20818.922767932516 53.12350329984018\n",
      "MSE =  0.3586275097506552 0.004302101459837919\n",
      "Model_name =  temp/f78 Violations =  0.0\n",
      "Average_violations =  -20890.06443232992 46.64563557508679\n",
      "MSE =  0.36147367255178436 0.001974336296975697\n",
      "Model_name =  temp/f79 Violations =  0.0\n",
      "Average_violations =  -20863.558022921923 59.0192109351654\n",
      "MSE =  0.35663719637638214 0.008009522771964392\n",
      "Model_name =  temp/f80 Violations =  0.0\n",
      "Average_violations =  -20799.25155880748 62.91185370003394\n",
      "MSE =  0.36096143510806594 0.0054242117746207425\n",
      "Model_name =  temp/f81 Violations =  0.0\n",
      "Average_violations =  -20855.907557672796 55.50949423550085\n",
      "MSE =  0.36036878798692057 0.003548788870758031\n",
      "Model_name =  temp/f82 Violations =  0.0\n",
      "Average_violations =  -20898.285880872052 59.49450898883048\n",
      "MSE =  0.36444949058144144 0.008150326469274926\n",
      "Model_name =  temp/f83 Violations =  0.0\n",
      "Average_violations =  -20827.304927211782 57.7909602909154\n",
      "MSE =  0.35546822771839354 0.005318644733312794\n",
      "Model_name =  temp/f84 Violations =  0.0\n",
      "Average_violations =  -20775.12020249999 49.974402516762204\n",
      "MSE =  0.36185350698255236 0.009342551207393914\n",
      "Model_name =  temp/f85 Violations =  0.0\n",
      "Average_violations =  -20845.68072500275 59.98162833802743\n",
      "MSE =  0.36123921139475695 0.007852264240168718\n",
      "Model_name =  temp/f86 Violations =  0.0\n",
      "Average_violations =  -20901.70041489379 56.34851216594931\n",
      "MSE =  0.3565534071556857 0.0041625955490355035\n",
      "Model_name =  temp/f87 Violations =  0.0\n",
      "Average_violations =  -20855.033990678163 50.365290247095956\n",
      "MSE =  0.36216813373250883 0.008148992355494886\n",
      "Model_name =  temp/f88 Violations =  0.0\n",
      "Average_violations =  -20835.552895759403 48.37865045460133\n",
      "MSE =  0.3632425409875979 0.004986386891921532\n",
      "Model_name =  temp/f89 Violations =  0.0\n",
      "Average_violations =  -20838.198169323863 55.32238835400446\n",
      "MSE =  0.36013761939287064 0.009154547510702606\n",
      "Model_name =  temp/f90 Violations =  0.0\n",
      "Average_violations =  -20851.83907446974 57.42612425391346\n",
      "MSE =  0.3564757678284441 0.002545711523736486\n",
      "Model_name =  temp/f91 Violations =  0.0\n",
      "Average_violations =  -20875.240313767605 63.6524961152591\n",
      "MSE =  0.35638506253024665 0.004092847582595776\n",
      "Model_name =  temp/f92 Violations =  0.0\n",
      "Average_violations =  -20815.040416749343 60.91101239859228\n",
      "MSE =  0.36164215487341694 0.004554306016201152\n",
      "Model_name =  temp/f93 Violations =  0.0\n",
      "Average_violations =  -20817.947303802634 57.464495094125326\n",
      "MSE =  0.3585459898211027 0.0067608683112153675\n",
      "Model_name =  temp/f94 Violations =  0.0\n",
      "Average_violations =  -20874.65663297617 53.86513794887065\n",
      "MSE =  0.35709220055929075 0.0030403708040550544\n",
      "Model_name =  temp/f95 Violations =  0.0\n",
      "Average_violations =  -20893.368581904633 68.17313432902206\n",
      "MSE =  0.35854398578176133 0.004949998611737416\n",
      "Model_name =  temp/f96 Violations =  0.0\n",
      "Average_violations =  -20873.063284368727 56.78628641893519\n",
      "MSE =  0.362017911463376 0.007982304499498537\n",
      "Model_name =  temp/f97 Violations =  0.0\n",
      "Average_violations =  -20788.806603113775 46.33858370932183\n",
      "MSE =  0.36265175783743314 0.005279842485949427\n",
      "Model_name =  temp/f98 Violations =  0.0\n",
      "Average_violations =  -20835.667781534594 60.9807723583271\n",
      "MSE =  0.3626246627828426 0.0026092243075511302\n",
      "Model_name =  temp/f99 Violations =  0.0\n",
      "Average_violations =  -20919.391590167797 61.444958853072535\n",
      "MSE =  0.3614771623341266 0.0009348534017271059\n",
      "Model_name =  temp/f100 Violations =  0.0\n",
      "Average_violations =  -20907.599506262075 53.281799394456904\n",
      "MSE =  0.3585181633020949 0.004800359395202047\n",
      "Model_name =  temp/f101 Violations =  0.0\n",
      "Average_violations =  -20810.307178773422 55.980583030597344\n",
      "MSE =  0.36796553846170443 0.0031787735220596717\n",
      "Model_name =  temp/f102 Violations =  0.0\n",
      "Average_violations =  -20755.18137661358 46.49884619328319\n",
      "MSE =  0.3654491096252072 0.007044846668304776\n",
      "Model_name =  temp/f103 Violations =  0.0\n",
      "Average_violations =  -20786.978906018045 62.56502153351544\n",
      "MSE =  0.36301328172842434 0.005474610527621682\n",
      "Model_name =  temp/f104 Violations =  0.0\n",
      "Average_violations =  -20855.423733673393 60.897520004587456\n",
      "MSE =  0.36747799200266 0.006214829042602707\n",
      "Model_name =  temp/f105 Violations =  0.0\n",
      "Average_violations =  -20844.99563501643 59.62580147389348\n",
      "MSE =  0.3646864355556509 0.0032275960895019958\n",
      "Model_name =  temp/f106 Violations =  0.0\n",
      "Average_violations =  -20787.31103358372 40.37444177729433\n",
      "MSE =  0.35684011353467143 0.0017085318017546795\n",
      "Model_name =  temp/f107 Violations =  0.0\n",
      "Average_violations =  -20797.678399441236 57.41402412446244\n",
      "MSE =  0.36203372541039697 0.002829724725589497\n",
      "Model_name =  temp/f108 Violations =  0.0\n",
      "Average_violations =  -20855.361991200585 56.948395042718026\n",
      "MSE =  0.3606623525240933 0.002912000852463599\n",
      "Model_name =  temp/f109 Violations =  0.0\n",
      "Average_violations =  -20834.371101473615 49.74831617487572\n",
      "MSE =  0.36122605167533717 0.005258276897147223\n",
      "Model_name =  temp/f110 Violations =  0.0\n",
      "Average_violations =  -20911.616227045623 55.01067765553905\n",
      "MSE =  0.36060687610715 0.003385361682744549\n",
      "Model_name =  temp/f111 Violations =  0.0\n",
      "Average_violations =  -20871.371675124185 50.986368400824766\n",
      "MSE =  0.3551631132355422 0.007865198238088544\n",
      "Model_name =  temp/f112 Violations =  0.0\n",
      "Average_violations =  -20816.21485178381 71.37498790209145\n",
      "MSE =  0.3608482800713185 0.004037434912349172\n",
      "Model_name =  temp/f113 Violations =  0.0\n",
      "Average_violations =  -20753.294996310797 62.35243181271348\n",
      "MSE =  0.369549613897355 0.006321470839052076\n",
      "Model_name =  temp/f114 Violations =  0.0\n",
      "Average_violations =  -20889.46464414507 55.205261086187875\n",
      "MSE =  0.36826666039014117 0.005898759373196514\n",
      "Model_name =  temp/f115 Violations =  0.0\n",
      "Average_violations =  -20868.439946935614 59.5237736150162\n",
      "MSE =  0.3579686932395414 0.0021792994709807505\n",
      "Model_name =  temp/f116 Violations =  0.0\n",
      "Average_violations =  -20878.458988374186 48.40987670516574\n",
      "MSE =  0.3630050039348209 0.007637490236778088\n",
      "Model_name =  temp/f117 Violations =  0.0\n",
      "Average_violations =  -20884.49668328917 52.92898992751379\n",
      "MSE =  0.36466103342831097 0.0015575572728459375\n",
      "Model_name =  temp/f118 Violations =  0.0\n",
      "Average_violations =  -20833.1796039445 50.47478627604535\n",
      "MSE =  0.35757168464294437 0.0031567831435700745\n",
      "Model_name =  temp/f119 Violations =  0.0\n",
      "Average_violations =  -20827.734151692548 58.62081838530176\n",
      "MSE =  0.3560409360405865 0.004153103593196725\n",
      "Model_name =  temp/f120 Violations =  0.0\n",
      "Average_violations =  -20801.902984403583 67.69545680967909\n",
      "MSE =  0.3653350633568744 0.0015592873704403543\n",
      "Model_name =  temp/f121 Violations =  0.0\n",
      "Average_violations =  -20862.59287848244 57.097344701317965\n",
      "MSE =  0.361412290213799 0.0026843736899880327\n",
      "Model_name =  temp/f122 Violations =  0.0\n",
      "Average_violations =  -20928.58244300413 59.32150771144987\n",
      "MSE =  0.355753155207275 0.0028437032148993834\n",
      "Model_name =  temp/f123 Violations =  0.0\n",
      "Average_violations =  -20899.35482240516 62.64491169421546\n",
      "MSE =  0.3585456108467395 0.00829634957783053\n",
      "Model_name =  temp/f124 Violations =  0.0\n",
      "Average_violations =  -20824.406329052243 59.22575730052922\n",
      "MSE =  0.35956657724162616 0.002709024208226126\n",
      "Model_name =  temp/f125 Violations =  0.0\n",
      "Average_violations =  -20809.76008028799 59.424625800129775\n",
      "MSE =  0.35812289851949436 0.0007326654260948089\n",
      "Model_name =  temp/f126 Violations =  0.0\n",
      "Average_violations =  -20797.3223249545 61.37969486174307\n",
      "MSE =  0.3651307401753799 0.007548936817109863\n",
      "Model_name =  temp/f127 Violations =  0.0\n",
      "Average_violations =  -20798.274093636468 48.753189120049605\n",
      "MSE =  0.3601108528369405 0.0025747225431654096\n",
      "Model_name =  temp/f128 Violations =  0.0\n",
      "Average_violations =  -20842.297218699587 40.01228646320701\n",
      "MSE =  0.36379241009380237 0.003900965801599768\n",
      "Model_name =  temp/f129 Violations =  0.0\n",
      "Average_violations =  -20788.24527893817 57.568902489231576\n",
      "MSE =  0.36025664998465246 0.004651018056182563\n",
      "Model_name =  temp/f130 Violations =  0.0\n",
      "Average_violations =  -20814.31533898205 50.382478428195554\n",
      "MSE =  0.3583304347746638 0.003667151548109822\n",
      "Model_name =  temp/f131 Violations =  0.0\n",
      "Average_violations =  -20790.616166669894 62.22517520139443\n",
      "MSE =  0.36230283702490573 0.0011953083083203443\n",
      "Model_name =  temp/f132 Violations =  0.0\n",
      "Average_violations =  -20798.349358637875 67.30256515570659\n",
      "MSE =  0.35529269027656457 0.00691406777157686\n",
      "Model_name =  temp/f133 Violations =  0.0\n",
      "Average_violations =  -20786.802731173244 63.64072461447984\n",
      "MSE =  0.35728159224067574 0.006078894244273087\n",
      "Model_name =  temp/f134 Violations =  0.0\n",
      "Average_violations =  -20830.12450018723 54.87674230263369\n",
      "MSE =  0.3597730912398261 0.0042897395754534365\n",
      "Model_name =  temp/f135 Violations =  0.0\n",
      "Average_violations =  -20697.822550703393 45.58626313133778\n",
      "MSE =  0.37219400235565675 0.0016618896388166742\n",
      "Model_name =  temp/f136 Violations =  0.0\n",
      "Average_violations =  -20834.208086459374 64.25989588776605\n",
      "MSE =  0.3576479590928073 0.002855268400239499\n",
      "Model_name =  temp/f137 Violations =  0.0\n",
      "Average_violations =  -20817.174164517593 61.169776549148644\n",
      "MSE =  0.35863097203852407 0.005231180345772087\n",
      "Model_name =  temp/f138 Violations =  0.0\n",
      "Average_violations =  -20789.25894684598 44.82425918867804\n",
      "MSE =  0.362393808682091 0.0029171198937556165\n",
      "Model_name =  temp/f139 Violations =  0.0\n",
      "Average_violations =  -20862.11721511175 50.328701106872956\n",
      "MSE =  0.3581959063671159 0.00742994646938659\n",
      "Model_name =  temp/f140 Violations =  0.0\n",
      "Average_violations =  -20818.53028081309 55.28358752740353\n",
      "MSE =  0.35835477834403323 0.006665759925169674\n",
      "Model_name =  temp/f141 Violations =  0.0\n",
      "Average_violations =  -20800.536594584588 52.62284308464225\n",
      "MSE =  0.3584845837291586 0.0029838065285587288\n",
      "Model_name =  temp/f142 Violations =  0.0\n",
      "Average_violations =  -20850.400710179416 57.02368298305516\n",
      "MSE =  0.3625272508518031 0.00844604502574852\n",
      "Model_name =  temp/f143 Violations =  0.0\n",
      "Average_violations =  -20822.289053807184 50.43210521983453\n",
      "MSE =  0.36093534586788206 0.0034336849961320404\n",
      "Model_name =  temp/f144 Violations =  0.0\n",
      "Average_violations =  -20881.478151343883 45.92987621336197\n",
      "MSE =  0.3600411280649758 0.0035536407964435316\n",
      "Model_name =  temp/f145 Violations =  0.0\n",
      "Average_violations =  -20927.21109049438 55.35942200246187\n",
      "MSE =  0.369229809810741 0.0026686160924427457\n",
      "Model_name =  temp/f146 Violations =  0.0\n",
      "Average_violations =  -20891.711150295887 67.45900347269024\n",
      "MSE =  0.3599979017013264 0.0037318304508853037\n",
      "Model_name =  temp/f147 Violations =  0.0\n",
      "Average_violations =  -20790.6226180681 53.37453757727392\n",
      "MSE =  0.36040501604052694 0.005023655469477895\n",
      "Model_name =  temp/f148 Violations =  0.0\n",
      "Average_violations =  -20845.639865168643 63.868938496503425\n",
      "MSE =  0.3695173527437569 0.002167314848127717\n",
      "Model_name =  temp/f149 Violations =  0.0\n",
      "Average_violations =  -20801.427408679392 64.4882290670555\n",
      "MSE =  0.36106587843498067 0.008784290815803484\n",
      "Model_name =  temp/f150 Violations =  0.0\n",
      "Average_violations =  -20922.14134881561 61.482143720776094\n",
      "MSE =  0.36481058455517074 0.006715622908041103\n",
      "Model_name =  temp/f151 Violations =  0.0\n",
      "Average_violations =  -20840.937584584815 70.76777009654721\n",
      "MSE =  0.3553944141415581 0.000691650174854149\n",
      "Model_name =  temp/f152 Violations =  0.0\n",
      "Average_violations =  -20865.39743178649 64.42982781388358\n",
      "MSE =  0.35946834991803445 0.001717494179298489\n",
      "Model_name =  temp/f153 Violations =  0.0\n",
      "Average_violations =  -20923.469620687334 63.89384932208117\n",
      "MSE =  0.3677343666060881 0.00541226240359797\n",
      "Model_name =  temp/f154 Violations =  0.0\n",
      "Average_violations =  -20887.76827430115 51.99548290194739\n",
      "MSE =  0.35884212761149104 0.007502829576108652\n",
      "Model_name =  temp/f155 Violations =  0.0\n",
      "Average_violations =  -20864.247666278472 61.71775707068327\n",
      "MSE =  0.3557084536398077 0.006627994462265061\n",
      "Model_name =  temp/f156 Violations =  0.0\n",
      "Average_violations =  -20845.292629285457 55.96609055574444\n",
      "MSE =  0.3608251114587047 0.0075507567557906476\n",
      "Model_name =  temp/f157 Violations =  0.0\n",
      "Average_violations =  -20896.056132440925 53.928067912950546\n",
      "MSE =  0.3623142100580887 0.004651772676941894\n",
      "Model_name =  temp/f158 Violations =  0.0\n",
      "Average_violations =  -20839.108967901622 53.125656439418634\n",
      "MSE =  0.37037483195600873 0.005602728411264751\n",
      "Model_name =  temp/f159 Violations =  0.0\n",
      "Average_violations =  -20875.00594481057 58.04617049479487\n",
      "MSE =  0.3645826967241863 0.004669197975670223\n",
      "Model_name =  temp/f160 Violations =  0.0\n",
      "Average_violations =  -20915.37308857315 72.84612406191488\n",
      "MSE =  0.36236611133629265 0.003975863844150918\n",
      "Model_name =  temp/f161 Violations =  0.0\n",
      "Average_violations =  -20886.657878627855 59.65400192941711\n",
      "MSE =  0.3607202520831903 0.00502074028319962\n",
      "Model_name =  temp/f162 Violations =  0.0\n",
      "Average_violations =  -20856.956082611694 52.93741890676269\n",
      "MSE =  0.3610746406671291 0.0014918412715506269\n",
      "Model_name =  temp/f163 Violations =  0.0\n",
      "Average_violations =  -20797.171382439275 53.22758119164133\n",
      "MSE =  0.36071671365747854 0.006193457614137408\n",
      "Model_name =  temp/f164 Violations =  0.0\n",
      "Average_violations =  -20829.538847873322 56.87497620792701\n",
      "MSE =  0.36265351088299996 0.0014740570890379727\n",
      "Model_name =  temp/f165 Violations =  0.0\n",
      "Average_violations =  -20782.449126262134 58.3571807213144\n",
      "MSE =  0.35754964039531173 0.005017242518021808\n",
      "Model_name =  temp/f166 Violations =  0.0\n",
      "Average_violations =  -20845.33526117349 64.99002017439717\n",
      "MSE =  0.35859148607367003 0.0013633375240498001\n",
      "Model_name =  temp/f167 Violations =  0.0\n",
      "Average_violations =  -20834.500883081957 60.06931778781866\n",
      "MSE =  0.35839763925341855 0.005765089501864579\n",
      "Model_name =  temp/f168 Violations =  0.0\n",
      "Average_violations =  -20766.12048763232 51.13934074371874\n",
      "MSE =  0.35830844788344135 0.006379014931321256\n",
      "Model_name =  temp/f169 Violations =  0.0\n",
      "Average_violations =  -20806.051732853448 59.51879922008391\n",
      "MSE =  0.360350532290866 0.0023929692905973713\n",
      "Model_name =  temp/f170 Violations =  0.0\n",
      "Average_violations =  -20858.481084269017 54.54748801119783\n",
      "MSE =  0.3623777887408582 0.007418557789176798\n",
      "Model_name =  temp/f171 Violations =  0.0\n",
      "Average_violations =  -20863.479802359136 66.95348596637007\n",
      "MSE =  0.3668852468595916 0.005477827483804546\n",
      "Model_name =  temp/f172 Violations =  0.0\n",
      "Average_violations =  -20758.536250549583 59.035180849393434\n",
      "MSE =  0.35776300163862035 0.004153823476874404\n",
      "Model_name =  temp/f173 Violations =  0.0\n",
      "Average_violations =  -20885.8158166853 61.88999003286094\n",
      "MSE =  0.3586470577352973 0.003901890609407817\n",
      "Model_name =  temp/f174 Violations =  0.0\n",
      "Average_violations =  -20790.81142898222 60.88338434634732\n",
      "MSE =  0.35924133061068475 0.005680947173953063\n",
      "Model_name =  temp/f175 Violations =  0.0\n",
      "Average_violations =  -20791.526320454042 47.02034778818704\n",
      "MSE =  0.3611856841904851 0.004432388063128414\n",
      "Model_name =  temp/f176 Violations =  0.0\n",
      "Average_violations =  -20780.66703603242 51.785570125814644\n",
      "MSE =  0.36569819544308246 0.0038428875059690463\n",
      "Model_name =  temp/f177 Violations =  0.0\n",
      "Average_violations =  -20785.56107031065 59.395985532603284\n",
      "MSE =  0.3624038666477046 0.009146665955857202\n",
      "Model_name =  temp/f178 Violations =  0.0\n",
      "Average_violations =  -20811.651201075452 63.7842156386742\n",
      "MSE =  0.35844819889466156 0.004278178364510811\n",
      "Model_name =  temp/f179 Violations =  0.0\n",
      "Average_violations =  -20802.175996621903 59.34153511589642\n",
      "MSE =  0.35915511195954997 0.008187042695531683\n",
      "Model_name =  temp/f180 Violations =  0.0\n",
      "Average_violations =  -20852.73006100328 59.76726471151374\n",
      "MSE =  0.36372611588216736 0.0018507999118909382\n",
      "Model_name =  temp/f181 Violations =  0.0\n",
      "Average_violations =  -20824.899466949264 58.27648405149295\n",
      "MSE =  0.35331902899825257 0.004051544418854646\n",
      "Model_name =  temp/f182 Violations =  0.0\n",
      "Average_violations =  -20887.045095278467 61.80660019846331\n",
      "MSE =  0.3612207175109714 0.003951332602430824\n",
      "Model_name =  temp/f183 Violations =  0.0\n",
      "Average_violations =  -20945.199587699837 50.22108176209544\n",
      "MSE =  0.364895030087168 0.002603127983562719\n",
      "Model_name =  temp/f184 Violations =  0.0\n",
      "Average_violations =  -20874.27063088062 54.58478414081505\n",
      "MSE =  0.3578827502262771 0.002789062976602673\n",
      "Model_name =  temp/f185 Violations =  0.0\n",
      "Average_violations =  -20746.777240405776 48.10447403546903\n",
      "MSE =  0.3700447665153456 0.0038280543653676567\n",
      "Model_name =  temp/f186 Violations =  0.0\n",
      "Average_violations =  -20825.024177554817 57.14472092526467\n",
      "MSE =  0.3630689705673494 0.0016248221279058096\n",
      "Model_name =  temp/f187 Violations =  0.0\n",
      "Average_violations =  -20834.231183944376 49.36649982573639\n",
      "MSE =  0.35866364705677395 0.005721343369653401\n",
      "Model_name =  temp/f188 Violations =  0.0\n",
      "Average_violations =  -20819.856778636924 60.55529790576828\n",
      "MSE =  0.35613396403806385 0.004985211746573201\n",
      "Model_name =  temp/f189 Violations =  0.0\n",
      "Average_violations =  -20780.504840971666 53.828215568618575\n",
      "MSE =  0.3577100153160724 0.005917925136040136\n",
      "Model_name =  temp/f190 Violations =  0.0\n",
      "Average_violations =  -20787.118462220515 52.3980861338812\n",
      "MSE =  0.3611938774616408 0.007000282698347036\n",
      "Model_name =  temp/f191 Violations =  0.0\n",
      "Average_violations =  -20812.307816285134 60.51346834810764\n",
      "MSE =  0.35749488831929455 0.0012410789374927551\n",
      "Model_name =  temp/f192 Violations =  0.0\n",
      "Average_violations =  -20835.216508507994 55.398679385536596\n",
      "MSE =  0.3629712917536969 0.00510714416033592\n",
      "Model_name =  temp/f193 Violations =  0.0\n",
      "Average_violations =  -20747.592526065953 61.02263561389937\n",
      "MSE =  0.3657285319270139 0.004382917060132989\n",
      "Model_name =  temp/f194 Violations =  0.0\n",
      "Average_violations =  -20854.274336038343 63.38740525049736\n",
      "MSE =  0.36137195227140895 0.0035289712604358067\n",
      "Model_name =  temp/f195 Violations =  0.0\n",
      "Average_violations =  -20887.819955985 64.61734735699002\n",
      "MSE =  0.3614223849986913 0.004321847209295299\n",
      "Model_name =  temp/f196 Violations =  0.0\n",
      "Average_violations =  -20812.414721324112 57.60473915769922\n",
      "MSE =  0.35971626016905334 0.003386953543585193\n",
      "Model_name =  temp/f197 Violations =  0.0\n",
      "Average_violations =  -20902.823002344234 63.37371601362198\n",
      "MSE =  0.3620375981414398 0.007613033833980873\n",
      "Model_name =  temp/f198 Violations =  0.0\n",
      "Average_violations =  -20817.73293463902 56.799584580027165\n",
      "MSE =  0.3559441473611169 0.0034719771244296066\n",
      "Model_name =  temp/f199 Violations =  0.0\n",
      "Average_violations =  -20818.989687361467 51.89814456867404\n",
      "MSE =  0.36634356376133576 0.004380467356835183\n",
      "Model_name =  temp/f200 Violations =  0.0\n",
      "Average_violations =  -20878.28905995221 61.747459733050356\n",
      "MSE =  0.3572178849431569 0.0034689463564338044\n",
      "Model_name =  temp/f201 Violations =  0.0\n",
      "Average_violations =  -20810.3032178992 58.1086605569521\n",
      "MSE =  0.36435383642048014 0.007025267392655587\n",
      "Model_name =  temp/f202 Violations =  0.0\n",
      "Average_violations =  -20884.180133672107 59.08335597882299\n",
      "MSE =  0.36083106926736636 0.0026391787549014598\n",
      "Model_name =  temp/f203 Violations =  0.0\n",
      "Average_violations =  -20925.20245269354 54.7775731719612\n",
      "MSE =  0.36736928096625027 0.0024501628159657204\n",
      "Model_name =  temp/f204 Violations =  0.0\n",
      "Average_violations =  -20824.37573580012 56.677545582403425\n",
      "MSE =  0.36256387878129265 0.009808811833417549\n",
      "Model_name =  temp/f205 Violations =  0.0\n",
      "Average_violations =  -20924.82593420144 56.843176379789796\n",
      "MSE =  0.35921905056293885 0.007969812213171425\n",
      "Model_name =  temp/f206 Violations =  0.0\n",
      "Average_violations =  -20832.554766614194 49.65161158691381\n",
      "MSE =  0.36697458015077805 0.0010485931307659955\n",
      "Model_name =  temp/f207 Violations =  0.0\n",
      "Average_violations =  -20902.701824456788 53.411986059614264\n",
      "MSE =  0.36425937497616684 0.010504257486169928\n",
      "Model_name =  temp/f208 Violations =  0.0\n",
      "Average_violations =  -20876.533710923013 53.75650775906016\n",
      "MSE =  0.3646520145581933 0.00423648027428378\n",
      "Model_name =  temp/f209 Violations =  0.0\n",
      "Average_violations =  -20809.385561191397 63.06480584665587\n",
      "MSE =  0.36995575948928633 0.0031755219060092725\n",
      "Model_name =  temp/f210 Violations =  0.0\n",
      "Average_violations =  -20767.736816248154 64.58351041414885\n",
      "MSE =  0.3604333670089372 0.003823156378651797\n",
      "Model_name =  temp/f211 Violations =  0.0\n",
      "Average_violations =  -20903.729214175382 59.59083168105465\n",
      "MSE =  0.3602588262495479 0.003764535974045358\n",
      "Model_name =  temp/f212 Violations =  0.0\n",
      "Average_violations =  -20941.57954803181 61.18919708201737\n",
      "MSE =  0.36415946502101365 0.007210651418867569\n",
      "Model_name =  temp/f213 Violations =  0.0\n",
      "Average_violations =  -20814.743590386304 68.09473682233514\n",
      "MSE =  0.35785221458892574 0.00252992130618317\n",
      "Model_name =  temp/f214 Violations =  0.0\n",
      "Average_violations =  -20848.833683616012 55.05915778674613\n",
      "MSE =  0.3608343363425291 0.002583453186474211\n",
      "Model_name =  temp/f215 Violations =  0.0\n",
      "Average_violations =  -20896.90262184924 50.71837247448318\n",
      "MSE =  0.3561683441638161 0.0031058215860724878\n",
      "Model_name =  temp/f216 Violations =  0.0\n",
      "Average_violations =  -20936.890363605573 61.07720448304078\n",
      "MSE =  0.3676131568525196 0.004073479215766104\n",
      "Model_name =  temp/f217 Violations =  0.0\n",
      "Average_violations =  -20889.884740420028 57.976691179964526\n",
      "MSE =  0.35992563541469796 0.0008496835092930815\n",
      "Model_name =  temp/f218 Violations =  0.0\n",
      "Average_violations =  -20837.84259381706 56.28884812533036\n",
      "MSE =  0.35981506206107766 0.005876680923905437\n",
      "Model_name =  temp/f219 Violations =  0.0\n",
      "Average_violations =  -20857.397919870553 52.21531072774716\n",
      "MSE =  0.35546841939382484 0.0066384600862215044\n",
      "Model_name =  temp/f220 Violations =  0.0\n",
      "Average_violations =  -20801.108212210456 60.926920564121744\n",
      "MSE =  0.35935789748722424 0.006213145882168795\n",
      "Model_name =  temp/f221 Violations =  0.0\n",
      "Average_violations =  -20862.388652120633 62.80417389546725\n",
      "MSE =  0.36209541043772947 0.0005318621060193032\n",
      "Model_name =  temp/f222 Violations =  0.0\n",
      "Average_violations =  -20894.560866527627 62.688135628737044\n",
      "MSE =  0.35944551070836345 0.0029044415991015915\n",
      "Model_name =  temp/f223 Violations =  0.0\n",
      "Average_violations =  -20880.39839781525 52.64555159109284\n",
      "MSE =  0.3575682257071389 0.002605232393438138\n",
      "Model_name =  temp/f224 Violations =  0.0\n",
      "Average_violations =  -20844.30591612986 52.14494565106592\n",
      "MSE =  0.35552015795634445 0.0026754752700473192\n",
      "Model_name =  temp/f225 Violations =  0.0\n",
      "Average_violations =  -20741.204632931487 60.17565254132724\n",
      "MSE =  0.35939371105117357 0.003488057038881428\n",
      "Model_name =  temp/f226 Violations =  0.0\n",
      "Average_violations =  -20790.411619460956 55.21635782277754\n",
      "MSE =  0.35396665645175784 0.006713369780910666\n",
      "Model_name =  temp/f227 Violations =  0.0\n",
      "Average_violations =  -20861.504108642206 55.658422041496195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE =  0.3620798443887164 0.00615609418012659\n",
      "Model_name =  temp/f228 Violations =  0.0\n",
      "Average_violations =  -20772.502096151664 61.08797003927961\n",
      "MSE =  0.36332328691706556 0.008172640153338412\n",
      "Model_name =  temp/f229 Violations =  0.0\n",
      "Average_violations =  -20824.735864616207 67.41383532849636\n",
      "MSE =  0.35954864053023367 0.005620071415815755\n",
      "Model_name =  temp/f230 Violations =  0.0\n",
      "Average_violations =  -20886.819633956144 53.430690014248135\n",
      "MSE =  0.35974108002457683 0.0009589148459566961\n",
      "Model_name =  temp/f231 Violations =  0.0\n",
      "Average_violations =  -20814.4147487925 53.53725063450511\n",
      "MSE =  0.35935138231103864 0.00912358336339055\n",
      "Model_name =  temp/f232 Violations =  0.0\n",
      "Average_violations =  -20847.27664537658 57.275063605560334\n",
      "MSE =  0.3591805996056779 0.004623840186186572\n",
      "Model_name =  temp/f233 Violations =  0.0\n",
      "Average_violations =  -20778.486986117176 59.70943647587769\n",
      "MSE =  0.36519986759946965 0.004369722657268866\n",
      "Model_name =  temp/f234 Violations =  0.0\n",
      "Average_violations =  -20861.318251117144 46.34832087654299\n",
      "MSE =  0.36301756940917435 0.0031638584427280056\n",
      "Model_name =  temp/f235 Violations =  0.0\n",
      "Average_violations =  -20853.342712853268 63.14952582699\n",
      "MSE =  0.3610554568103915 0.0024270512971766877\n",
      "Model_name =  temp/f236 Violations =  0.0\n",
      "Average_violations =  -20877.433811087856 55.94518881242209\n",
      "MSE =  0.3594870271527179 0.005333141258297821\n",
      "Model_name =  temp/f237 Violations =  0.0\n",
      "Average_violations =  -20842.097056224102 49.47490756237098\n",
      "MSE =  0.36368249643164013 0.0060922420201765465\n",
      "Model_name =  temp/f238 Violations =  0.0\n",
      "Average_violations =  -20832.27477543872 60.49090141839766\n",
      "MSE =  0.35962682038502813 0.004600718617369482\n",
      "Model_name =  temp/f239 Violations =  0.0\n",
      "Average_violations =  -20842.142128067982 49.61249990359945\n",
      "MSE =  0.3597484819168962 0.0024986126356450313\n",
      "Model_name =  temp/f240 Violations =  0.0\n",
      "Average_violations =  -20895.593658867398 43.635021855445814\n",
      "MSE =  0.3602929000227249 0.004688820035707865\n",
      "Model_name =  temp/f241 Violations =  0.0\n",
      "Average_violations =  -20931.9191304295 55.30030860593426\n",
      "MSE =  0.3635050298373835 0.005989295996223356\n",
      "Model_name =  temp/f242 Violations =  0.0\n",
      "Average_violations =  -20813.48580363142 56.66175185741453\n",
      "MSE =  0.3644336757963014 0.0055268855351082585\n",
      "Model_name =  temp/f243 Violations =  0.0\n",
      "Average_violations =  -20859.905751157403 55.9484523057294\n",
      "MSE =  0.36199121381034055 0.002273945478354319\n",
      "Model_name =  temp/f244 Violations =  0.0\n",
      "Average_violations =  -20877.812935674014 64.44384486912719\n",
      "MSE =  0.36187219061583736 0.004411021741214425\n",
      "Model_name =  temp/f245 Violations =  0.0\n",
      "Average_violations =  -20788.003913840188 49.58747952168316\n",
      "MSE =  0.3604358418609583 0.0071856397089090475\n",
      "Model_name =  temp/f246 Violations =  0.0\n",
      "Average_violations =  -20791.58160671799 56.41572689128816\n",
      "MSE =  0.3645687393739621 0.006343034628425412\n",
      "Model_name =  temp/f247 Violations =  0.0\n",
      "Average_violations =  -20788.171017868797 48.86086012605856\n",
      "MSE =  0.36136169727636985 0.003082975353264809\n",
      "Model_name =  temp/f248 Violations =  0.0\n",
      "Average_violations =  -20840.468524989377 50.65177389205542\n",
      "MSE =  0.35812117526276693 0.0008599360221589404\n",
      "Model_name =  temp/f249 Violations =  0.0\n",
      "Average_violations =  -20840.831468616547 54.27322769466572\n",
      "MSE =  0.35771377335355825 0.005464763428968166\n",
      "Model_name =  temp/f250 Violations =  0.0\n",
      "Average_violations =  -20834.35896348907 58.72404929548569\n",
      "MSE =  0.36251602736237937 0.0035608265933999234\n",
      "Model_name =  temp/f251 Violations =  0.0\n",
      "Average_violations =  -20828.95019434467 63.35191909764262\n",
      "MSE =  0.36228122750861086 0.005824185869986299\n",
      "Model_name =  temp/f252 Violations =  0.0\n",
      "Average_violations =  -20923.09639124599 56.11723237067124\n",
      "MSE =  0.3587152830801575 0.004966295818487127\n",
      "Model_name =  temp/f253 Violations =  0.0\n",
      "Average_violations =  -20957.858335515328 58.740275418156834\n",
      "MSE =  0.36904536780875197 0.004975649225050893\n",
      "Model_name =  temp/f254 Violations =  0.0\n",
      "Average_violations =  -20885.91508961932 58.466477550894034\n",
      "MSE =  0.360712035669116 0.0037158500752208254\n",
      "Model_name =  temp/f255 Violations =  0.0\n",
      "Average_violations =  -20865.207409381885 55.04628789072564\n",
      "MSE =  0.35938218906052716 0.001443513578924353\n",
      "Model_name =  temp/f256 Violations =  0.0\n",
      "Average_violations =  -20813.31432368149 56.24491633347783\n",
      "MSE =  0.3604103586757128 0.0031750045048980174\n",
      "Model_name =  temp/f257 Violations =  0.0\n",
      "Average_violations =  -20817.316535090435 59.553571694856736\n",
      "MSE =  0.3577905186846859 0.008080165894589679\n",
      "Model_name =  temp/f258 Violations =  0.0\n",
      "Average_violations =  -20827.15937138838 50.694697060603204\n",
      "MSE =  0.36391699030351216 0.007809802882817396\n",
      "Model_name =  temp/f259 Violations =  0.0\n",
      "Average_violations =  -20904.821017776954 58.309425434154974\n",
      "MSE =  0.36057513338877717 0.005340511266066133\n",
      "Model_name =  temp/f260 Violations =  0.0\n",
      "Average_violations =  -20873.247937253003 60.11956974131673\n",
      "MSE =  0.36074505985780675 0.007559057170824147\n",
      "Model_name =  temp/f261 Violations =  0.0\n",
      "Average_violations =  -20854.58702224407 58.56328927293449\n",
      "MSE =  0.3585599826583144 0.009294843831312355\n",
      "Model_name =  temp/f262 Violations =  0.0\n",
      "Average_violations =  -20875.019160681502 67.3078998082234\n",
      "MSE =  0.3626166451376415 0.005705533828496944\n",
      "Model_name =  temp/f263 Violations =  0.0\n",
      "Average_violations =  -20823.136133233053 53.88518510257194\n",
      "MSE =  0.3558587071113837 0.007008373770210025\n",
      "Model_name =  temp/f264 Violations =  0.0\n",
      "Average_violations =  -20914.946142165194 52.9311817623177\n",
      "MSE =  0.36429066556021744 0.002492253192056521\n",
      "Model_name =  temp/f265 Violations =  0.0\n",
      "Average_violations =  -20873.769530095786 56.853519265847666\n",
      "MSE =  0.36417355587267 0.00709350562502405\n",
      "Model_name =  temp/f266 Violations =  0.0\n",
      "Average_violations =  -20818.405673112055 46.75504636285864\n",
      "MSE =  0.36774225619447654 0.003022020106720918\n",
      "Model_name =  temp/f267 Violations =  0.0\n",
      "Average_violations =  -20899.446752269312 57.54459005723616\n",
      "MSE =  0.36374617348739124 0.004444752710635565\n",
      "Model_name =  temp/f268 Violations =  0.0\n",
      "Average_violations =  -20831.10109450366 59.06387708591117\n",
      "MSE =  0.35877450263914357 0.0027700967773863125\n",
      "Model_name =  temp/f269 Violations =  0.0\n",
      "Average_violations =  -20793.147998517245 59.32310916218454\n",
      "MSE =  0.3638749173831542 0.005741113282646448\n",
      "Model_name =  temp/f270 Violations =  0.0\n",
      "Average_violations =  -20976.072529847184 53.78244362029647\n",
      "MSE =  0.363661842559295 0.004143050957642407\n",
      "Model_name =  temp/f271 Violations =  0.0\n",
      "Average_violations =  -20821.115670950083 60.2491539097818\n",
      "MSE =  0.36547585643459524 0.005075025553495027\n",
      "Model_name =  temp/f272 Violations =  0.0\n",
      "Average_violations =  -20849.758949819214 59.43674552736921\n",
      "MSE =  0.3570904911342918 0.003014779830443136\n",
      "Model_name =  temp/f273 Violations =  0.0\n",
      "Average_violations =  -20784.117829696665 68.06339608293803\n",
      "MSE =  0.3568150000992925 0.002633028519930251\n",
      "Model_name =  temp/f274 Violations =  0.0\n",
      "Average_violations =  -20855.10854240488 63.399191320753296\n",
      "MSE =  0.36165611647956997 0.005038296107501771\n",
      "Model_name =  temp/f275 Violations =  0.0\n",
      "Average_violations =  -20888.103568220973 55.30828506877189\n",
      "MSE =  0.36111343478914426 0.005283737808992237\n",
      "Model_name =  temp/f276 Violations =  0.0\n",
      "Average_violations =  -20848.547396431906 52.923067270880814\n",
      "MSE =  0.36293953510315996 0.009293733390529784\n",
      "Model_name =  temp/f277 Violations =  0.0\n",
      "Average_violations =  -20809.1601073974 65.55937607516067\n",
      "MSE =  0.3550249602733359 0.004336742728544415\n",
      "Model_name =  temp/f278 Violations =  0.0\n",
      "Average_violations =  -20895.67097540684 48.71195727195157\n",
      "MSE =  0.35994839219628894 0.0058945518014816315\n",
      "Model_name =  temp/f279 Violations =  0.0\n",
      "Average_violations =  -20887.728591947252 57.54886385727705\n",
      "MSE =  0.3634966652571885 0.0032888353648472127\n",
      "Model_name =  temp/f280 Violations =  0.0\n",
      "Average_violations =  -20932.468653637206 59.54582631439871\n",
      "MSE =  0.3686661116540735 0.005803550151444137\n",
      "Model_name =  temp/f281 Violations =  0.0\n",
      "Average_violations =  -20817.647713388604 43.195249282973144\n",
      "MSE =  0.35688593011193115 0.0026530875694627167\n",
      "Model_name =  temp/f282 Violations =  0.0\n",
      "Average_violations =  -20854.415179212763 58.746962478809486\n",
      "MSE =  0.35952903776671796 0.003859007781741516\n",
      "Model_name =  temp/f283 Violations =  0.0\n",
      "Average_violations =  -20798.47555595695 55.67057868258893\n",
      "MSE =  0.35903954713981684 0.004051975448992346\n",
      "Model_name =  temp/f284 Violations =  0.0\n",
      "Average_violations =  -20907.428126620613 56.3427124647198\n",
      "MSE =  0.36539229985085325 0.010589909604707811\n",
      "Model_name =  temp/f285 Violations =  0.0\n",
      "Average_violations =  -20877.329671187563 61.18448207187292\n",
      "MSE =  0.3618132068740684 0.011488725121460514\n",
      "Model_name =  temp/f286 Violations =  0.0\n",
      "Average_violations =  -20847.637088236483 57.404707251032725\n",
      "MSE =  0.3567199111082618 0.006713344890217266\n",
      "Model_name =  temp/f287 Violations =  0.0\n",
      "Average_violations =  -20845.12011954681 60.96799022531124\n",
      "MSE =  0.3592788624103957 0.0010042310352332158\n",
      "Model_name =  temp/f288 Violations =  0.0\n",
      "Average_violations =  -20871.0224126563 57.269355634985324\n",
      "MSE =  0.35911458242621813 0.0045771985347276035\n",
      "Model_name =  temp/f289 Violations =  0.0\n",
      "Average_violations =  -20848.68701258258 63.26410972284715\n",
      "MSE =  0.3589159365098514 0.0056369256843529945\n",
      "Model_name =  temp/f290 Violations =  0.0\n",
      "Average_violations =  -20861.029834760266 64.4206303672587\n",
      "MSE =  0.36283114657066323 0.0037638714723680737\n",
      "Model_name =  temp/f291 Violations =  0.0\n",
      "Average_violations =  -20757.249681138415 60.08370349501586\n",
      "MSE =  0.37002278320018966 0.004751673857509512\n",
      "Model_name =  temp/f292 Violations =  0.0\n",
      "Average_violations =  -20879.326647032267 64.4825073688905\n",
      "MSE =  0.3675495640424669 0.007320339781637204\n",
      "Model_name =  temp/f293 Violations =  0.0\n",
      "Average_violations =  -20832.757959194772 59.42834282908212\n",
      "MSE =  0.36375455462450623 0.004328685679051189\n",
      "Model_name =  temp/f294 Violations =  0.0\n",
      "Average_violations =  -20872.658014560937 55.85975203266254\n",
      "MSE =  0.3589843267353688 0.0036583126737464336\n",
      "Model_name =  temp/f295 Violations =  0.0\n",
      "Average_violations =  -20785.860336820377 54.19488367833174\n",
      "MSE =  0.35277575727883576 0.0032038553918934403\n",
      "Model_name =  temp/f296 Violations =  0.0\n",
      "Average_violations =  -20880.851335808864 66.83378491176067\n",
      "MSE =  0.3590677761098506 0.008909983411336841\n",
      "Model_name =  temp/f297 Violations =  0.0\n",
      "Average_violations =  -20919.228935976924 61.74457341919838\n",
      "MSE =  0.3568735372007179 0.0056247487643138125\n",
      "Model_name =  temp/f298 Violations =  0.0\n",
      "Average_violations =  -20862.979087381802 58.46166847734761\n",
      "MSE =  0.35800836429970945 0.00556353402569488\n",
      "Model_name =  temp/f299 Violations =  0.0\n",
      "Average_violations =  -20877.272154246384 51.64104559693689\n",
      "MSE =  0.36731835097936383 0.007931307437872208\n",
      "[0.36078797 0.35830796 0.35679509 0.36235266 0.3617026  0.36449264\n",
      " 0.36106979 0.36561135 0.3642923  0.36075274 0.36729122 0.36035546\n",
      " 0.36380102 0.36203984 0.36008876 0.36698273 0.35704839 0.3608837\n",
      " 0.36214072 0.35736812 0.3654728  0.35889762 0.3581012  0.35913554\n",
      " 0.36316059 0.36255031 0.36307893 0.36124422 0.35883417 0.35852953\n",
      " 0.36189875 0.36319438 0.36204195 0.36326654 0.35590512 0.36062277\n",
      " 0.36799715 0.36406831 0.36884813 0.36301768 0.36109711 0.36628428\n",
      " 0.35773578 0.35876896 0.36354721 0.37131479 0.35800105 0.36380547\n",
      " 0.36308209 0.36499659 0.36159627 0.35816924 0.36345901 0.35652793\n",
      " 0.35835044 0.35986064 0.36487743 0.36348758 0.36469032 0.35908456\n",
      " 0.35544292 0.36171391 0.36237016 0.36004718 0.3604452  0.35695814\n",
      " 0.36124309 0.36242245 0.36147899 0.36356596 0.3678504  0.3575811\n",
      " 0.35825796 0.35932643 0.36358675 0.36679387 0.36799063 0.35862751\n",
      " 0.36147367 0.3566372  0.36096144 0.36036879 0.36444949 0.35546823\n",
      " 0.36185351 0.36123921 0.35655341 0.36216813 0.36324254 0.36013762\n",
      " 0.35647577 0.35638506 0.36164215 0.35854599 0.3570922  0.35854399\n",
      " 0.36201791 0.36265176 0.36262466 0.36147716 0.35851816 0.36796554\n",
      " 0.36544911 0.36301328 0.36747799 0.36468644 0.35684011 0.36203373\n",
      " 0.36066235 0.36122605 0.36060688 0.35516311 0.36084828 0.36954961\n",
      " 0.36826666 0.35796869 0.363005   0.36466103 0.35757168 0.35604094\n",
      " 0.36533506 0.36141229 0.35575316 0.35854561 0.35956658 0.3581229\n",
      " 0.36513074 0.36011085 0.36379241 0.36025665 0.35833043 0.36230284\n",
      " 0.35529269 0.35728159 0.35977309 0.372194   0.35764796 0.35863097\n",
      " 0.36239381 0.35819591 0.35835478 0.35848458 0.36252725 0.36093535\n",
      " 0.36004113 0.36922981 0.3599979  0.36040502 0.36951735 0.36106588\n",
      " 0.36481058 0.35539441 0.35946835 0.36773437 0.35884213 0.35570845\n",
      " 0.36082511 0.36231421 0.37037483 0.3645827  0.36236611 0.36072025\n",
      " 0.36107464 0.36071671 0.36265351 0.35754964 0.35859149 0.35839764\n",
      " 0.35830845 0.36035053 0.36237779 0.36688525 0.357763   0.35864706\n",
      " 0.35924133 0.36118568 0.3656982  0.36240387 0.3584482  0.35915511\n",
      " 0.36372612 0.35331903 0.36122072 0.36489503 0.35788275 0.37004477\n",
      " 0.36306897 0.35866365 0.35613396 0.35771002 0.36119388 0.35749489\n",
      " 0.36297129 0.36572853 0.36137195 0.36142238 0.35971626 0.3620376\n",
      " 0.35594415 0.36634356 0.35721788 0.36435384 0.36083107 0.36736928\n",
      " 0.36256388 0.35921905 0.36697458 0.36425937 0.36465201 0.36995576\n",
      " 0.36043337 0.36025883 0.36415947 0.35785221 0.36083434 0.35616834\n",
      " 0.36761316 0.35992564 0.35981506 0.35546842 0.3593579  0.36209541\n",
      " 0.35944551 0.35756823 0.35552016 0.35939371 0.35396666 0.36207984\n",
      " 0.36332329 0.35954864 0.35974108 0.35935138 0.3591806  0.36519987\n",
      " 0.36301757 0.36105546 0.35948703 0.3636825  0.35962682 0.35974848\n",
      " 0.3602929  0.36350503 0.36443368 0.36199121 0.36187219 0.36043584\n",
      " 0.36456874 0.3613617  0.35812118 0.35771377 0.36251603 0.36228123\n",
      " 0.35871528 0.36904537 0.36071204 0.35938219 0.36041036 0.35779052\n",
      " 0.36391699 0.36057513 0.36074506 0.35855998 0.36261665 0.35585871\n",
      " 0.36429067 0.36417356 0.36774226 0.36374617 0.3587745  0.36387492\n",
      " 0.36366184 0.36547586 0.35709049 0.356815   0.36165612 0.36111343\n",
      " 0.36293954 0.35502496 0.35994839 0.36349667 0.36866611 0.35688593\n",
      " 0.35952904 0.35903955 0.3653923  0.36181321 0.35671991 0.35927886\n",
      " 0.35911458 0.35891594 0.36283115 0.37002278 0.36754956 0.36375455\n",
      " 0.35898433 0.35277576 0.35906778 0.35687354 0.35800836 0.36731835] [0.00791263 0.00646595 0.00646838 0.00445236 0.00546757 0.00435522\n",
      " 0.00368399 0.00760741 0.00743488 0.00736163 0.00462476 0.00231749\n",
      " 0.00914172 0.00292082 0.00118331 0.00209511 0.00476922 0.0057862\n",
      " 0.00442474 0.00190072 0.00326162 0.00661363 0.00763965 0.00312274\n",
      " 0.00197737 0.00779482 0.00587054 0.00239745 0.00208189 0.00304228\n",
      " 0.00364562 0.00156383 0.00709153 0.00656165 0.00696221 0.00548177\n",
      " 0.00390165 0.00385107 0.00255771 0.00188005 0.00791356 0.00264654\n",
      " 0.00314317 0.0016369  0.0012054  0.00583002 0.00381524 0.00268471\n",
      " 0.00695414 0.00361207 0.00399218 0.00587152 0.00199231 0.00388756\n",
      " 0.006998   0.00734707 0.007151   0.00388734 0.00213822 0.00534002\n",
      " 0.00227634 0.00551028 0.00811204 0.00287021 0.00756558 0.00369418\n",
      " 0.00608782 0.00364182 0.00170651 0.00421133 0.00429471 0.00494432\n",
      " 0.00673604 0.00255267 0.00632602 0.00686998 0.00830584 0.0043021\n",
      " 0.00197434 0.00800952 0.00542421 0.00354879 0.00815033 0.00531864\n",
      " 0.00934255 0.00785226 0.0041626  0.00814899 0.00498639 0.00915455\n",
      " 0.00254571 0.00409285 0.00455431 0.00676087 0.00304037 0.00495\n",
      " 0.0079823  0.00527984 0.00260922 0.00093485 0.00480036 0.00317877\n",
      " 0.00704485 0.00547461 0.00621483 0.0032276  0.00170853 0.00282972\n",
      " 0.002912   0.00525828 0.00338536 0.0078652  0.00403743 0.00632147\n",
      " 0.00589876 0.0021793  0.00763749 0.00155756 0.00315678 0.0041531\n",
      " 0.00155929 0.00268437 0.0028437  0.00829635 0.00270902 0.00073267\n",
      " 0.00754894 0.00257472 0.00390097 0.00465102 0.00366715 0.00119531\n",
      " 0.00691407 0.00607889 0.00428974 0.00166189 0.00285527 0.00523118\n",
      " 0.00291712 0.00742995 0.00666576 0.00298381 0.00844605 0.00343368\n",
      " 0.00355364 0.00266862 0.00373183 0.00502366 0.00216731 0.00878429\n",
      " 0.00671562 0.00069165 0.00171749 0.00541226 0.00750283 0.00662799\n",
      " 0.00755076 0.00465177 0.00560273 0.0046692  0.00397586 0.00502074\n",
      " 0.00149184 0.00619346 0.00147406 0.00501724 0.00136334 0.00576509\n",
      " 0.00637901 0.00239297 0.00741856 0.00547783 0.00415382 0.00390189\n",
      " 0.00568095 0.00443239 0.00384289 0.00914667 0.00427818 0.00818704\n",
      " 0.0018508  0.00405154 0.00395133 0.00260313 0.00278906 0.00382805\n",
      " 0.00162482 0.00572134 0.00498521 0.00591793 0.00700028 0.00124108\n",
      " 0.00510714 0.00438292 0.00352897 0.00432185 0.00338695 0.00761303\n",
      " 0.00347198 0.00438047 0.00346895 0.00702527 0.00263918 0.00245016\n",
      " 0.00980881 0.00796981 0.00104859 0.01050426 0.00423648 0.00317552\n",
      " 0.00382316 0.00376454 0.00721065 0.00252992 0.00258345 0.00310582\n",
      " 0.00407348 0.00084968 0.00587668 0.00663846 0.00621315 0.00053186\n",
      " 0.00290444 0.00260523 0.00267548 0.00348806 0.00671337 0.00615609\n",
      " 0.00817264 0.00562007 0.00095891 0.00912358 0.00462384 0.00436972\n",
      " 0.00316386 0.00242705 0.00533314 0.00609224 0.00460072 0.00249861\n",
      " 0.00468882 0.0059893  0.00552689 0.00227395 0.00441102 0.00718564\n",
      " 0.00634303 0.00308298 0.00085994 0.00546476 0.00356083 0.00582419\n",
      " 0.0049663  0.00497565 0.00371585 0.00144351 0.003175   0.00808017\n",
      " 0.0078098  0.00534051 0.00755906 0.00929484 0.00570553 0.00700837\n",
      " 0.00249225 0.00709351 0.00302202 0.00444475 0.0027701  0.00574111\n",
      " 0.00414305 0.00507503 0.00301478 0.00263303 0.0050383  0.00528374\n",
      " 0.00929373 0.00433674 0.00589455 0.00328884 0.00580355 0.00265309\n",
      " 0.00385901 0.00405198 0.01058991 0.01148873 0.00671334 0.00100423\n",
      " 0.0045772  0.00563693 0.00376387 0.00475167 0.00732034 0.00432869\n",
      " 0.00365831 0.00320386 0.00890998 0.00562475 0.00556353 0.00793131] [-20789.39718148 -20823.40994584 -20853.02369394 -20827.2699494\n",
      " -20898.54159248 -20886.88184033 -20855.22465383 -20853.17280788\n",
      " -20854.00308109 -20822.50846541 -20830.10940933 -20856.53188965\n",
      " -20884.5759596  -20876.93499491 -20821.93620572 -20816.07591751\n",
      " -20872.69388807 -20925.30332309 -20891.24277988 -20842.07955407\n",
      " -20864.03108148 -20821.07144326 -20880.87288164 -20839.81924505\n",
      " -20881.62351734 -20853.08735011 -20891.43229297 -20898.77524825\n",
      " -20882.12323058 -20788.37107182 -20861.48301865 -20852.67661249\n",
      " -20828.77283781 -20778.69410766 -20882.01810084 -20811.06387347\n",
      " -20870.67472951 -20813.33254581 -20921.43436117 -20835.67472079\n",
      " -20778.52228327 -20776.24699568 -20910.59738542 -20832.46153535\n",
      " -20869.44267929 -20859.03396213 -20848.17874214 -20912.96318085\n",
      " -20931.16712664 -20756.90705796 -20830.24480045 -20744.41766844\n",
      " -20812.12983922 -20868.14483431 -20907.86224974 -20880.27338315\n",
      " -20823.15639341 -20820.97836181 -20757.94145401 -20806.22504786\n",
      " -20874.97186439 -20828.66044664 -20877.8862152  -20848.01593173\n",
      " -20818.49269667 -20892.81832303 -20915.27965225 -20845.58867055\n",
      " -20852.30352787 -20816.89494048 -20735.62155823 -20816.98880416\n",
      " -20846.22956818 -20865.95574265 -20821.12936746 -20832.30304344\n",
      " -20829.39222595 -20818.92276793 -20890.06443233 -20863.55802292\n",
      " -20799.25155881 -20855.90755767 -20898.28588087 -20827.30492721\n",
      " -20775.1202025  -20845.680725   -20901.70041489 -20855.03399068\n",
      " -20835.55289576 -20838.19816932 -20851.83907447 -20875.24031377\n",
      " -20815.04041675 -20817.9473038  -20874.65663298 -20893.3685819\n",
      " -20873.06328437 -20788.80660311 -20835.66778153 -20919.39159017\n",
      " -20907.59950626 -20810.30717877 -20755.18137661 -20786.97890602\n",
      " -20855.42373367 -20844.99563502 -20787.31103358 -20797.67839944\n",
      " -20855.3619912  -20834.37110147 -20911.61622705 -20871.37167512\n",
      " -20816.21485178 -20753.29499631 -20889.46464415 -20868.43994694\n",
      " -20878.45898837 -20884.49668329 -20833.17960394 -20827.73415169\n",
      " -20801.9029844  -20862.59287848 -20928.582443   -20899.35482241\n",
      " -20824.40632905 -20809.76008029 -20797.32232495 -20798.27409364\n",
      " -20842.2972187  -20788.24527894 -20814.31533898 -20790.61616667\n",
      " -20798.34935864 -20786.80273117 -20830.12450019 -20697.8225507\n",
      " -20834.20808646 -20817.17416452 -20789.25894685 -20862.11721511\n",
      " -20818.53028081 -20800.53659458 -20850.40071018 -20822.28905381\n",
      " -20881.47815134 -20927.21109049 -20891.7111503  -20790.62261807\n",
      " -20845.63986517 -20801.42740868 -20922.14134882 -20840.93758458\n",
      " -20865.39743179 -20923.46962069 -20887.7682743  -20864.24766628\n",
      " -20845.29262929 -20896.05613244 -20839.1089679  -20875.00594481\n",
      " -20915.37308857 -20886.65787863 -20856.95608261 -20797.17138244\n",
      " -20829.53884787 -20782.44912626 -20845.33526117 -20834.50088308\n",
      " -20766.12048763 -20806.05173285 -20858.48108427 -20863.47980236\n",
      " -20758.53625055 -20885.81581669 -20790.81142898 -20791.52632045\n",
      " -20780.66703603 -20785.56107031 -20811.65120108 -20802.17599662\n",
      " -20852.730061   -20824.89946695 -20887.04509528 -20945.1995877\n",
      " -20874.27063088 -20746.77724041 -20825.02417755 -20834.23118394\n",
      " -20819.85677864 -20780.50484097 -20787.11846222 -20812.30781629\n",
      " -20835.21650851 -20747.59252607 -20854.27433604 -20887.81995598\n",
      " -20812.41472132 -20902.82300234 -20817.73293464 -20818.98968736\n",
      " -20878.28905995 -20810.3032179  -20884.18013367 -20925.20245269\n",
      " -20824.3757358  -20924.8259342  -20832.55476661 -20902.70182446\n",
      " -20876.53371092 -20809.38556119 -20767.73681625 -20903.72921418\n",
      " -20941.57954803 -20814.74359039 -20848.83368362 -20896.90262185\n",
      " -20936.89036361 -20889.88474042 -20837.84259382 -20857.39791987\n",
      " -20801.10821221 -20862.38865212 -20894.56086653 -20880.39839782\n",
      " -20844.30591613 -20741.20463293 -20790.41161946 -20861.50410864\n",
      " -20772.50209615 -20824.73586462 -20886.81963396 -20814.41474879\n",
      " -20847.27664538 -20778.48698612 -20861.31825112 -20853.34271285\n",
      " -20877.43381109 -20842.09705622 -20832.27477544 -20842.14212807\n",
      " -20895.59365887 -20931.91913043 -20813.48580363 -20859.90575116\n",
      " -20877.81293567 -20788.00391384 -20791.58160672 -20788.17101787\n",
      " -20840.46852499 -20840.83146862 -20834.35896349 -20828.95019434\n",
      " -20923.09639125 -20957.85833552 -20885.91508962 -20865.20740938\n",
      " -20813.31432368 -20817.31653509 -20827.15937139 -20904.82101778\n",
      " -20873.24793725 -20854.58702224 -20875.01916068 -20823.13613323\n",
      " -20914.94614217 -20873.7695301  -20818.40567311 -20899.44675227\n",
      " -20831.1010945  -20793.14799852 -20976.07252985 -20821.11567095\n",
      " -20849.75894982 -20784.1178297  -20855.1085424  -20888.10356822\n",
      " -20848.54739643 -20809.1601074  -20895.67097541 -20887.72859195\n",
      " -20932.46865364 -20817.64771339 -20854.41517921 -20798.47555596\n",
      " -20907.42812662 -20877.32967119 -20847.63708824 -20845.12011955\n",
      " -20871.02241266 -20848.68701258 -20861.02983476 -20757.24968114\n",
      " -20879.32664703 -20832.75795919 -20872.65801456 -20785.86033682\n",
      " -20880.85133581 -20919.22893598 -20862.97908738 -20877.27215425] [58.73204514 68.03346949 56.63842449 66.09984257 57.30577953 70.49556728\n",
      " 56.30504575 52.86222401 62.43277668 60.67930886 77.04583095 58.28016444\n",
      " 61.43019176 55.63547629 58.17348324 53.47961024 56.37057209 59.58547915\n",
      " 61.09841298 64.37464286 58.53703636 57.51870427 57.15800513 59.09333363\n",
      " 61.95587462 57.7134059  67.06702474 52.29412962 63.78997073 76.77158054\n",
      " 59.14566212 51.07539369 61.48163722 57.95525498 48.69762647 38.52442757\n",
      " 54.69169817 59.18889712 60.16707363 59.46048083 56.1024324  46.66370328\n",
      " 57.68096767 56.15640837 53.94448617 53.46377147 68.53004702 56.1097257\n",
      " 57.55273475 51.42738784 56.31499579 68.80851562 58.94939926 57.06667385\n",
      " 58.02447408 52.01402268 55.28814731 64.51460304 55.79940627 56.55343399\n",
      " 59.571176   64.82947804 49.0396567  55.1496032  53.48823637 55.80828691\n",
      " 55.51860288 50.34203807 52.29222881 54.45294792 54.86453208 55.97009535\n",
      " 54.03856434 57.95578087 64.810928   66.15777515 58.0667289  53.1235033\n",
      " 46.64563558 59.01921094 62.9118537  55.50949424 59.49450899 57.79096029\n",
      " 49.97440252 59.98162834 56.34851217 50.36529025 48.37865045 55.32238835\n",
      " 57.42612425 63.65249612 60.9110124  57.46449509 53.86513795 68.17313433\n",
      " 56.78628642 46.33858371 60.98077236 61.44495885 53.28179939 55.98058303\n",
      " 46.49884619 62.56502153 60.89752    59.62580147 40.37444178 57.41402412\n",
      " 56.94839504 49.74831617 55.01067766 50.9863684  71.3749879  62.35243181\n",
      " 55.20526109 59.52377362 48.40987671 52.92898993 50.47478628 58.62081839\n",
      " 67.69545681 57.0973447  59.32150771 62.64491169 59.2257573  59.4246258\n",
      " 61.37969486 48.75318912 40.01228646 57.56890249 50.38247843 62.2251752\n",
      " 67.30256516 63.64072461 54.8767423  45.58626313 64.25989589 61.16977655\n",
      " 44.82425919 50.32870111 55.28358753 52.62284308 57.02368298 50.43210522\n",
      " 45.92987621 55.359422   67.45900347 53.37453758 63.8689385  64.48822907\n",
      " 61.48214372 70.7677701  64.42982781 63.89384932 51.9954829  61.71775707\n",
      " 55.96609056 53.92806791 53.12565644 58.04617049 72.84612406 59.65400193\n",
      " 52.93741891 53.22758119 56.87497621 58.35718072 64.99002017 60.06931779\n",
      " 51.13934074 59.51879922 54.54748801 66.95348597 59.03518085 61.88999003\n",
      " 60.88338435 47.02034779 51.78557013 59.39598553 63.78421564 59.34153512\n",
      " 59.76726471 58.27648405 61.8066002  50.22108176 54.58478414 48.10447404\n",
      " 57.14472093 49.36649983 60.55529791 53.82821557 52.39808613 60.51346835\n",
      " 55.39867939 61.02263561 63.38740525 64.61734736 57.60473916 63.37371601\n",
      " 56.79958458 51.89814457 61.74745973 58.10866056 59.08335598 54.77757317\n",
      " 56.67754558 56.84317638 49.65161159 53.41198606 53.75650776 63.06480585\n",
      " 64.58351041 59.59083168 61.18919708 68.09473682 55.05915779 50.71837247\n",
      " 61.07720448 57.97669118 56.28884813 52.21531073 60.92692056 62.8041739\n",
      " 62.68813563 52.64555159 52.14494565 60.17565254 55.21635782 55.65842204\n",
      " 61.08797004 67.41383533 53.43069001 53.53725063 57.27506361 59.70943648\n",
      " 46.34832088 63.14952583 55.94518881 49.47490756 60.49090142 49.6124999\n",
      " 43.63502186 55.30030861 56.66175186 55.94845231 64.44384487 49.58747952\n",
      " 56.41572689 48.86086013 50.65177389 54.27322769 58.7240493  63.3519191\n",
      " 56.11723237 58.74027542 58.46647755 55.04628789 56.24491633 59.55357169\n",
      " 50.69469706 58.30942543 60.11956974 58.56328927 67.30789981 53.8851851\n",
      " 52.93118176 56.85351927 46.75504636 57.54459006 59.06387709 59.32310916\n",
      " 53.78244362 60.24915391 59.43674553 68.06339608 63.39919132 55.30828507\n",
      " 52.92306727 65.55937608 48.71195727 57.54886386 59.54582631 43.19524928\n",
      " 58.74696248 55.67057868 56.34271246 61.18448207 57.40470725 60.96799023\n",
      " 57.26935563 63.26410972 64.42063037 60.0837035  64.48250737 59.42834283\n",
      " 55.85975203 54.19488368 66.83378491 61.74457342 58.46166848 51.6410456 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1332x756 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp/f0\n",
      "Area under surface (rectangular approx) =  16.715719834521195\n",
      "Violations =  0.0\n",
      "Average_violations =  -20789.39718147789\n",
      "MSE =  0.3607879692318871\n",
      "temp/f1\n",
      "Area under surface (rectangular approx) =  17.492330436785426\n",
      "Violations =  0.0\n",
      "Average_violations =  -20823.409945835043\n",
      "MSE =  0.3583079598592036\n",
      "temp/f2\n",
      "Area under surface (rectangular approx) =  17.568158577096316\n",
      "Violations =  0.0\n",
      "Average_violations =  -20853.02369393952\n",
      "MSE =  0.35679508516696673\n",
      "temp/f3\n",
      "Area under surface (rectangular approx) =  18.237619536290733\n",
      "Violations =  0.0\n",
      "Average_violations =  -20827.26994940346\n",
      "MSE =  0.3623526560406787\n",
      "temp/f4\n",
      "Area under surface (rectangular approx) =  18.419080910982235\n",
      "Violations =  0.0\n",
      "Average_violations =  -20898.541592484242\n",
      "MSE =  0.36170260403902743\n",
      "temp/f5\n",
      "Area under surface (rectangular approx) =  19.06392887388705\n",
      "Violations =  0.0\n",
      "Average_violations =  -20886.881840329002\n",
      "MSE =  0.3644926397068962\n",
      "temp/f6\n",
      "Area under surface (rectangular approx) =  18.419773316101363\n",
      "Violations =  0.0\n",
      "Average_violations =  -20855.224653828827\n",
      "MSE =  0.3610697944286499\n",
      "temp/f7\n",
      "Area under surface (rectangular approx) =  19.02101272937106\n",
      "Violations =  0.0\n",
      "Average_violations =  -20853.172807884504\n",
      "MSE =  0.3656113464806028\n",
      "temp/f8\n",
      "Area under surface (rectangular approx) =  19.039369562098216\n",
      "Violations =  0.0\n",
      "Average_violations =  -20854.003081087012\n",
      "MSE =  0.36429229716849143\n",
      "temp/f9\n",
      "Area under surface (rectangular approx) =  17.016592633454714\n",
      "Violations =  0.0\n",
      "Average_violations =  -20822.50846540978\n",
      "MSE =  0.3607527398468949\n",
      "temp/f10\n",
      "Area under surface (rectangular approx) =  19.05104650134528\n",
      "Violations =  0.0\n",
      "Average_violations =  -20830.10940932959\n",
      "MSE =  0.36729121794063185\n",
      "temp/f11\n",
      "Area under surface (rectangular approx) =  20.322554676516642\n",
      "Violations =  0.0\n",
      "Average_violations =  -20856.531889645907\n",
      "MSE =  0.3603554581244466\n",
      "temp/f12\n",
      "Area under surface (rectangular approx) =  19.444033725677098\n",
      "Violations =  0.0\n",
      "Average_violations =  -20884.575959599395\n",
      "MSE =  0.3638010210177134\n",
      "temp/f13\n",
      "Area under surface (rectangular approx) =  18.278749405386577\n",
      "Violations =  0.0\n",
      "Average_violations =  -20876.934994907562\n",
      "MSE =  0.36203983775224646\n",
      "temp/f14\n",
      "Area under surface (rectangular approx) =  17.570124869390312\n",
      "Violations =  0.0\n",
      "Average_violations =  -20821.936205717302\n",
      "MSE =  0.36008876046416666\n",
      "temp/f15\n",
      "Area under surface (rectangular approx) =  20.216216212629\n",
      "Violations =  0.0\n",
      "Average_violations =  -20816.07591751074\n",
      "MSE =  0.3669827266705945\n",
      "temp/f16\n",
      "Area under surface (rectangular approx) =  19.27385076181263\n",
      "Violations =  0.0\n",
      "Average_violations =  -20872.69388806559\n",
      "MSE =  0.3570483904939679\n",
      "temp/f17\n",
      "Area under surface (rectangular approx) =  19.459496697697677\n",
      "Violations =  0.0\n",
      "Average_violations =  -20925.303323086944\n",
      "MSE =  0.3608836992348567\n",
      "temp/f18\n",
      "Area under surface (rectangular approx) =  19.09915721056399\n",
      "Violations =  0.0\n",
      "Average_violations =  -20891.24277988303\n",
      "MSE =  0.36214071573507006\n",
      "temp/f19\n",
      "Area under surface (rectangular approx) =  18.887730341858337\n",
      "Violations =  0.0\n",
      "Average_violations =  -20842.07955407363\n",
      "MSE =  0.3573681217027375\n",
      "temp/f20\n",
      "Area under surface (rectangular approx) =  19.901154452819046\n",
      "Violations =  0.0\n",
      "Average_violations =  -20864.03108147876\n",
      "MSE =  0.3654727988295845\n",
      "temp/f21\n",
      "Area under surface (rectangular approx) =  18.08702047180411\n",
      "Violations =  0.0\n",
      "Average_violations =  -20821.07144326364\n",
      "MSE =  0.35889762186431895\n",
      "temp/f22\n",
      "Area under surface (rectangular approx) =  18.936933663336976\n",
      "Violations =  0.0\n",
      "Average_violations =  -20880.872881642357\n",
      "MSE =  0.3581012043053698\n",
      "temp/f23\n",
      "Area under surface (rectangular approx) =  18.554344698660685\n",
      "Violations =  0.0\n",
      "Average_violations =  -20839.819245045557\n",
      "MSE =  0.35913553810858007\n",
      "temp/f24\n",
      "Area under surface (rectangular approx) =  18.854603451132807\n",
      "Violations =  0.0\n",
      "Average_violations =  -20881.62351733878\n",
      "MSE =  0.3631605871897256\n",
      "temp/f25\n",
      "Area under surface (rectangular approx) =  19.475067421759448\n",
      "Violations =  0.0\n",
      "Average_violations =  -20853.087350109126\n",
      "MSE =  0.36255030671062105\n",
      "temp/f26\n",
      "Area under surface (rectangular approx) =  18.96555890239391\n",
      "Violations =  0.0\n",
      "Average_violations =  -20891.43229296888\n",
      "MSE =  0.3630789263589953\n",
      "temp/f27\n",
      "Area under surface (rectangular approx) =  19.718964668283938\n",
      "Violations =  0.0\n",
      "Average_violations =  -20898.775248251422\n",
      "MSE =  0.3612442161013665\n",
      "temp/f28\n",
      "Area under surface (rectangular approx) =  19.681671185357796\n",
      "Violations =  0.0\n",
      "Average_violations =  -20882.12323057724\n",
      "MSE =  0.35883417193497563\n",
      "temp/f29\n",
      "Area under surface (rectangular approx) =  18.29747172287055\n",
      "Violations =  0.0\n",
      "Average_violations =  -20788.371071815905\n",
      "MSE =  0.3585295294349659\n",
      "temp/f30\n",
      "Area under surface (rectangular approx) =  17.987689968782245\n",
      "Violations =  0.0\n",
      "Average_violations =  -20861.483018654795\n",
      "MSE =  0.36189874679033474\n",
      "temp/f31\n",
      "Area under surface (rectangular approx) =  18.595061259314548\n",
      "Violations =  0.0\n",
      "Average_violations =  -20852.67661248648\n",
      "MSE =  0.3631943815355314\n",
      "temp/f32\n",
      "Area under surface (rectangular approx) =  18.449160231985882\n",
      "Violations =  0.0\n",
      "Average_violations =  -20828.7728378123\n",
      "MSE =  0.36204195324126337\n",
      "temp/f33\n",
      "Area under surface (rectangular approx) =  17.233091365188592\n",
      "Violations =  0.0\n",
      "Average_violations =  -20778.69410765944\n",
      "MSE =  0.36326654217053606\n",
      "temp/f34\n",
      "Area under surface (rectangular approx) =  18.55646920109351\n",
      "Violations =  0.0\n",
      "Average_violations =  -20882.018100842655\n",
      "MSE =  0.35590511950243425\n",
      "temp/f35\n",
      "Area under surface (rectangular approx) =  18.835379538890166\n",
      "Violations =  0.0\n",
      "Average_violations =  -20811.063873470903\n",
      "MSE =  0.36062276816167554\n",
      "temp/f36\n",
      "Area under surface (rectangular approx) =  20.935041558960467\n",
      "Violations =  0.0\n",
      "Average_violations =  -20870.67472950959\n",
      "MSE =  0.3679971515475647\n",
      "temp/f37\n",
      "Area under surface (rectangular approx) =  17.350336858071074\n",
      "Violations =  0.0\n",
      "Average_violations =  -20813.332545805464\n",
      "MSE =  0.3640683134787979\n",
      "temp/f38\n",
      "Area under surface (rectangular approx) =  21.152529269000233\n",
      "Violations =  0.0\n",
      "Average_violations =  -20921.434361167954\n",
      "MSE =  0.36884812770037767\n",
      "temp/f39\n",
      "Area under surface (rectangular approx) =  19.80879514383955\n",
      "Violations =  0.0\n",
      "Average_violations =  -20835.674720791434\n",
      "MSE =  0.363017679495053\n",
      "temp/f40\n",
      "Area under surface (rectangular approx) =  17.116855485552534\n",
      "Violations =  0.0\n",
      "Average_violations =  -20778.522283265527\n",
      "MSE =  0.36109711278408335\n",
      "temp/f41\n",
      "Area under surface (rectangular approx) =  17.63605769082061\n",
      "Violations =  0.0\n",
      "Average_violations =  -20776.246995678895\n",
      "MSE =  0.36628427631107\n",
      "temp/f42\n",
      "Area under surface (rectangular approx) =  19.51946103055537\n",
      "Violations =  0.0\n",
      "Average_violations =  -20910.59738541546\n",
      "MSE =  0.35773578054799665\n",
      "temp/f43\n",
      "Area under surface (rectangular approx) =  19.487478989548766\n",
      "Violations =  0.0\n",
      "Average_violations =  -20832.461535350987\n",
      "MSE =  0.35876895883690435\n",
      "temp/f44\n",
      "Area under surface (rectangular approx) =  18.464887959520606\n",
      "Violations =  0.0\n",
      "Average_violations =  -20869.442679291984\n",
      "MSE =  0.3635472076789395\n",
      "temp/f45\n",
      "Area under surface (rectangular approx) =  18.03183080186874\n",
      "Violations =  0.0\n",
      "Average_violations =  -20859.033962130165\n",
      "MSE =  0.3713147925632246\n",
      "temp/f46\n",
      "Area under surface (rectangular approx) =  19.0110464123468\n",
      "Violations =  0.0\n",
      "Average_violations =  -20848.1787421371\n",
      "MSE =  0.35800105030144747\n",
      "temp/f47\n",
      "Area under surface (rectangular approx) =  19.55147487692132\n",
      "Violations =  0.0\n",
      "Average_violations =  -20912.963180851195\n",
      "MSE =  0.363805470595472\n",
      "temp/f48\n",
      "Area under surface (rectangular approx) =  20.776620782882553\n",
      "Violations =  0.0\n",
      "Average_violations =  -20931.167126641012\n",
      "MSE =  0.36308208876281384\n",
      "temp/f49\n",
      "Area under surface (rectangular approx) =  17.645999059069748\n",
      "Violations =  0.0\n",
      "Average_violations =  -20756.90705795943\n",
      "MSE =  0.3649965943614322\n",
      "temp/f50\n",
      "Area under surface (rectangular approx) =  18.429226659155574\n",
      "Violations =  0.0\n",
      "Average_violations =  -20830.244800449742\n",
      "MSE =  0.3615962695616694\n",
      "temp/f51\n",
      "Area under surface (rectangular approx) =  16.864126132809147\n",
      "Violations =  0.0\n",
      "Average_violations =  -20744.417668435744\n",
      "MSE =  0.3581692353946815\n",
      "temp/f52\n",
      "Area under surface (rectangular approx) =  17.858009298576317\n",
      "Violations =  0.0\n",
      "Average_violations =  -20812.129839222438\n",
      "MSE =  0.3634590089901842\n",
      "temp/f53\n",
      "Area under surface (rectangular approx) =  17.386777805033788\n",
      "Violations =  0.0\n",
      "Average_violations =  -20868.14483430931\n",
      "MSE =  0.35652792679687134\n",
      "temp/f54\n",
      "Area under surface (rectangular approx) =  20.218978959026632\n",
      "Violations =  0.0\n",
      "Average_violations =  -20907.862249735204\n",
      "MSE =  0.35835043753600976\n",
      "temp/f55\n",
      "Area under surface (rectangular approx) =  18.786325002660508\n",
      "Violations =  0.0\n",
      "Average_violations =  -20880.27338315165\n",
      "MSE =  0.3598606400341831\n",
      "temp/f56\n",
      "Area under surface (rectangular approx) =  17.014050566769576\n",
      "Violations =  0.0\n",
      "Average_violations =  -20823.15639340779\n",
      "MSE =  0.36487743266957606\n",
      "temp/f57\n",
      "Area under surface (rectangular approx) =  17.274536267628456\n",
      "Violations =  0.0\n",
      "Average_violations =  -20820.97836181233\n",
      "MSE =  0.3634875793458705\n",
      "temp/f58\n",
      "Area under surface (rectangular approx) =  17.547316151422372\n",
      "Violations =  0.0\n",
      "Average_violations =  -20757.94145401441\n",
      "MSE =  0.36469032288823017\n",
      "temp/f59\n",
      "Area under surface (rectangular approx) =  17.653107917079776\n",
      "Violations =  0.0\n",
      "Average_violations =  -20806.22504786179\n",
      "MSE =  0.3590845634756204\n",
      "temp/f60\n",
      "Area under surface (rectangular approx) =  19.17806305200503\n",
      "Violations =  0.0\n",
      "Average_violations =  -20874.971864391606\n",
      "MSE =  0.35544292168961295\n",
      "temp/f61\n",
      "Area under surface (rectangular approx) =  18.41260849380248\n",
      "Violations =  0.0\n",
      "Average_violations =  -20828.660446641476\n",
      "MSE =  0.36171391025979044\n",
      "temp/f62\n",
      "Area under surface (rectangular approx) =  17.146809726275183\n",
      "Violations =  0.0\n",
      "Average_violations =  -20877.886215195173\n",
      "MSE =  0.36237015773361864\n",
      "temp/f63\n",
      "Area under surface (rectangular approx) =  19.01242302809998\n",
      "Violations =  0.0\n",
      "Average_violations =  -20848.01593172695\n",
      "MSE =  0.3600471832385432\n",
      "temp/f64\n",
      "Area under surface (rectangular approx) =  16.913411432251706\n",
      "Violations =  0.0\n",
      "Average_violations =  -20818.492696665282\n",
      "MSE =  0.36044519790425017\n",
      "temp/f65\n",
      "Area under surface (rectangular approx) =  19.266240091956533\n",
      "Violations =  0.0\n",
      "Average_violations =  -20892.818323026815\n",
      "MSE =  0.3569581418707439\n",
      "temp/f66\n",
      "Area under surface (rectangular approx) =  18.32097710238782\n",
      "Violations =  0.0\n",
      "Average_violations =  -20915.27965224631\n",
      "MSE =  0.3612430892157983\n",
      "temp/f67\n",
      "Area under surface (rectangular approx) =  17.81915409112831\n",
      "Violations =  0.0\n",
      "Average_violations =  -20845.58867055304\n",
      "MSE =  0.3624224523674451\n",
      "temp/f68\n",
      "Area under surface (rectangular approx) =  18.244441176221383\n",
      "Violations =  0.0\n",
      "Average_violations =  -20852.303527873457\n",
      "MSE =  0.3614789897248875\n",
      "temp/f69\n",
      "Area under surface (rectangular approx) =  17.826489655132427\n",
      "Violations =  0.0\n",
      "Average_violations =  -20816.894940476002\n",
      "MSE =  0.3635659568684759\n",
      "temp/f70\n",
      "Area under surface (rectangular approx) =  18.12937941543166\n",
      "Violations =  0.0\n",
      "Average_violations =  -20735.621558226267\n",
      "MSE =  0.3678504004556547\n",
      "temp/f71\n",
      "Area under surface (rectangular approx) =  17.168902494398328\n",
      "Violations =  0.0\n",
      "Average_violations =  -20816.988804156765\n",
      "MSE =  0.3575810963120724\n",
      "temp/f72\n",
      "Area under surface (rectangular approx) =  19.401286284841618\n",
      "Violations =  0.0\n",
      "Average_violations =  -20846.22956818017\n",
      "MSE =  0.3582579588858197\n",
      "temp/f73\n",
      "Area under surface (rectangular approx) =  18.43454882819222\n",
      "Violations =  0.0\n",
      "Average_violations =  -20865.955742645445\n",
      "MSE =  0.3593264329540691\n",
      "temp/f74\n",
      "Area under surface (rectangular approx) =  17.554794553618493\n",
      "Violations =  0.0\n",
      "Average_violations =  -20821.129367456513\n",
      "MSE =  0.3635867463461248\n",
      "temp/f75\n",
      "Area under surface (rectangular approx) =  17.54794966756047\n",
      "Violations =  0.0\n",
      "Average_violations =  -20832.303043436685\n",
      "MSE =  0.3667938723972542\n",
      "temp/f76\n",
      "Area under surface (rectangular approx) =  19.121308992958262\n",
      "Violations =  0.0\n",
      "Average_violations =  -20829.392225949694\n",
      "MSE =  0.36799062762897333\n",
      "temp/f77\n",
      "Area under surface (rectangular approx) =  19.09170676317457\n",
      "Violations =  0.0\n",
      "Average_violations =  -20818.922767932516\n",
      "MSE =  0.3586275097506552\n",
      "temp/f78\n",
      "Area under surface (rectangular approx) =  19.76473969532067\n",
      "Violations =  0.0\n",
      "Average_violations =  -20890.06443232992\n",
      "MSE =  0.36147367255178436\n",
      "temp/f79\n",
      "Area under surface (rectangular approx) =  17.22024826203789\n",
      "Violations =  0.0\n",
      "Average_violations =  -20863.558022921923\n",
      "MSE =  0.35663719637638214\n",
      "temp/f80\n",
      "Area under surface (rectangular approx) =  17.9075648182082\n",
      "Violations =  0.0\n",
      "Average_violations =  -20799.25155880748\n",
      "MSE =  0.36096143510806594\n",
      "temp/f81\n",
      "Area under surface (rectangular approx) =  19.06805190207913\n",
      "Violations =  0.0\n",
      "Average_violations =  -20855.907557672796\n",
      "MSE =  0.36036878798692057\n",
      "temp/f82\n",
      "Area under surface (rectangular approx) =  20.119298253954835\n",
      "Violations =  0.0\n",
      "Average_violations =  -20898.285880872052\n",
      "MSE =  0.36444949058144144\n",
      "temp/f83\n",
      "Area under surface (rectangular approx) =  17.779089481306137\n",
      "Violations =  0.0\n",
      "Average_violations =  -20827.304927211782\n",
      "MSE =  0.35546822771839354\n",
      "temp/f84\n",
      "Area under surface (rectangular approx) =  18.03726016278207\n",
      "Violations =  0.0\n",
      "Average_violations =  -20775.12020249999\n",
      "MSE =  0.36185350698255236\n",
      "temp/f85\n",
      "Area under surface (rectangular approx) =  18.141202255151335\n",
      "Violations =  0.0\n",
      "Average_violations =  -20845.68072500275\n",
      "MSE =  0.36123921139475695\n",
      "temp/f86\n",
      "Area under surface (rectangular approx) =  19.958959338377046\n",
      "Violations =  0.0\n",
      "Average_violations =  -20901.70041489379\n",
      "MSE =  0.3565534071556857\n",
      "temp/f87\n",
      "Area under surface (rectangular approx) =  18.113852740613794\n",
      "Violations =  0.0\n",
      "Average_violations =  -20855.033990678163\n",
      "MSE =  0.36216813373250883\n",
      "temp/f88\n",
      "Area under surface (rectangular approx) =  18.100621299436813\n",
      "Violations =  0.0\n",
      "Average_violations =  -20835.552895759403\n",
      "MSE =  0.3632425409875979\n",
      "temp/f89\n",
      "Area under surface (rectangular approx) =  17.47388767038185\n",
      "Violations =  0.0\n",
      "Average_violations =  -20838.198169323863\n",
      "MSE =  0.36013761939287064\n",
      "temp/f90\n",
      "Area under surface (rectangular approx) =  19.34000029090491\n",
      "Violations =  0.0\n",
      "Average_violations =  -20851.83907446974\n",
      "MSE =  0.3564757678284441\n",
      "temp/f91\n",
      "Area under surface (rectangular approx) =  19.28915144613999\n",
      "Violations =  0.0\n",
      "Average_violations =  -20875.240313767605\n",
      "MSE =  0.35638506253024665\n",
      "temp/f92\n",
      "Area under surface (rectangular approx) =  17.700989572070707\n",
      "Violations =  0.0\n",
      "Average_violations =  -20815.040416749343\n",
      "MSE =  0.36164215487341694\n",
      "temp/f93\n",
      "Area under surface (rectangular approx) =  17.918359564227803\n",
      "Violations =  0.0\n",
      "Average_violations =  -20817.947303802634\n",
      "MSE =  0.3585459898211027\n",
      "temp/f94\n",
      "Area under surface (rectangular approx) =  18.928202775552773\n",
      "Violations =  0.0\n",
      "Average_violations =  -20874.65663297617\n",
      "MSE =  0.35709220055929075\n",
      "temp/f95\n",
      "Area under surface (rectangular approx) =  20.029190867263758\n",
      "Violations =  0.0\n",
      "Average_violations =  -20893.368581904633\n",
      "MSE =  0.35854398578176133\n",
      "temp/f96\n",
      "Area under surface (rectangular approx) =  19.555377680231096\n",
      "Violations =  0.0\n",
      "Average_violations =  -20873.063284368727\n",
      "MSE =  0.362017911463376\n",
      "temp/f97\n",
      "Area under surface (rectangular approx) =  17.221978043897224\n",
      "Violations =  0.0\n",
      "Average_violations =  -20788.806603113775\n",
      "MSE =  0.36265175783743314\n",
      "temp/f98\n",
      "Area under surface (rectangular approx) =  19.18392338112493\n",
      "Violations =  0.0\n",
      "Average_violations =  -20835.667781534594\n",
      "MSE =  0.3626246627828426\n",
      "temp/f99\n",
      "Area under surface (rectangular approx) =  20.698722850408444\n",
      "Violations =  0.0\n",
      "Average_violations =  -20919.391590167797\n",
      "MSE =  0.3614771623341266\n",
      "temp/f100\n",
      "Area under surface (rectangular approx) =  19.36562617727701\n",
      "Violations =  0.0\n",
      "Average_violations =  -20907.599506262075\n",
      "MSE =  0.3585181633020949\n",
      "temp/f101\n",
      "Area under surface (rectangular approx) =  18.722898338877172\n",
      "Violations =  0.0\n",
      "Average_violations =  -20810.307178773422\n",
      "MSE =  0.36796553846170443\n",
      "temp/f102\n",
      "Area under surface (rectangular approx) =  17.477870852017197\n",
      "Violations =  0.0\n",
      "Average_violations =  -20755.18137661358\n",
      "MSE =  0.3654491096252072\n",
      "temp/f103\n",
      "Area under surface (rectangular approx) =  17.01763684019179\n",
      "Violations =  0.0\n",
      "Average_violations =  -20786.978906018045\n",
      "MSE =  0.36301328172842434\n",
      "temp/f104\n",
      "Area under surface (rectangular approx) =  18.176840628597493\n",
      "Violations =  0.0\n",
      "Average_violations =  -20855.423733673393\n",
      "MSE =  0.36747799200266\n",
      "temp/f105\n",
      "Area under surface (rectangular approx) =  19.087545814705052\n",
      "Violations =  0.0\n",
      "Average_violations =  -20844.99563501643\n",
      "MSE =  0.3646864355556509\n",
      "temp/f106\n",
      "Area under surface (rectangular approx) =  17.500157346596\n",
      "Violations =  0.0\n",
      "Average_violations =  -20787.31103358372\n",
      "MSE =  0.35684011353467143\n",
      "temp/f107\n",
      "Area under surface (rectangular approx) =  18.270632038978487\n",
      "Violations =  0.0\n",
      "Average_violations =  -20797.678399441236\n",
      "MSE =  0.36203372541039697\n",
      "temp/f108\n",
      "Area under surface (rectangular approx) =  17.571432471902995\n",
      "Violations =  0.0\n",
      "Average_violations =  -20855.361991200585\n",
      "MSE =  0.3606623525240933\n",
      "temp/f109\n",
      "Area under surface (rectangular approx) =  17.36030999248459\n",
      "Violations =  0.0\n",
      "Average_violations =  -20834.371101473615\n",
      "MSE =  0.36122605167533717\n",
      "temp/f110\n",
      "Area under surface (rectangular approx) =  20.979254634837584\n",
      "Violations =  0.0\n",
      "Average_violations =  -20911.616227045623\n",
      "MSE =  0.36060687610715\n",
      "temp/f111\n",
      "Area under surface (rectangular approx) =  18.238878248070083\n",
      "Violations =  0.0\n",
      "Average_violations =  -20871.371675124185\n",
      "MSE =  0.3551631132355422\n",
      "temp/f112\n",
      "Area under surface (rectangular approx) =  17.736283205158877\n",
      "Violations =  0.0\n",
      "Average_violations =  -20816.21485178381\n",
      "MSE =  0.3608482800713185\n",
      "temp/f113\n",
      "Area under surface (rectangular approx) =  16.892986129519688\n",
      "Violations =  0.0\n",
      "Average_violations =  -20753.294996310797\n",
      "MSE =  0.369549613897355\n",
      "temp/f114\n",
      "Area under surface (rectangular approx) =  21.18702543647913\n",
      "Violations =  0.0\n",
      "Average_violations =  -20889.46464414507\n",
      "MSE =  0.36826666039014117\n",
      "temp/f115\n",
      "Area under surface (rectangular approx) =  17.845307836796472\n",
      "Violations =  0.0\n",
      "Average_violations =  -20868.439946935614\n",
      "MSE =  0.3579686932395414\n",
      "temp/f116\n",
      "Area under surface (rectangular approx) =  18.269458308748945\n",
      "Violations =  0.0\n",
      "Average_violations =  -20878.458988374186\n",
      "MSE =  0.3630050039348209\n",
      "temp/f117\n",
      "Area under surface (rectangular approx) =  18.50086110727804\n",
      "Violations =  0.0\n",
      "Average_violations =  -20884.49668328917\n",
      "MSE =  0.36466103342831097\n",
      "temp/f118\n",
      "Area under surface (rectangular approx) =  17.776978234023\n",
      "Violations =  0.0\n",
      "Average_violations =  -20833.1796039445\n",
      "MSE =  0.35757168464294437\n",
      "temp/f119\n",
      "Area under surface (rectangular approx) =  17.93585625721106\n",
      "Violations =  0.0\n",
      "Average_violations =  -20827.734151692548\n",
      "MSE =  0.3560409360405865\n",
      "temp/f120\n",
      "Area under surface (rectangular approx) =  18.38493311664501\n",
      "Violations =  0.0\n",
      "Average_violations =  -20801.902984403583\n",
      "MSE =  0.3653350633568744\n",
      "temp/f121\n",
      "Area under surface (rectangular approx) =  19.35710088821183\n",
      "Violations =  0.0\n",
      "Average_violations =  -20862.59287848244\n",
      "MSE =  0.361412290213799\n",
      "temp/f122\n",
      "Area under surface (rectangular approx) =  20.814135272847537\n",
      "Violations =  0.0\n",
      "Average_violations =  -20928.58244300413\n",
      "MSE =  0.355753155207275\n",
      "temp/f123\n",
      "Area under surface (rectangular approx) =  18.059214792082507\n",
      "Violations =  0.0\n",
      "Average_violations =  -20899.35482240516\n",
      "MSE =  0.3585456108467395\n",
      "temp/f124\n",
      "Area under surface (rectangular approx) =  17.43743282615873\n",
      "Violations =  0.0\n",
      "Average_violations =  -20824.406329052243\n",
      "MSE =  0.35956657724162616\n",
      "temp/f125\n",
      "Area under surface (rectangular approx) =  18.540059179186255\n",
      "Violations =  0.0\n",
      "Average_violations =  -20809.76008028799\n",
      "MSE =  0.35812289851949436\n",
      "temp/f126\n",
      "Area under surface (rectangular approx) =  17.892262551313756\n",
      "Violations =  0.0\n",
      "Average_violations =  -20797.3223249545\n",
      "MSE =  0.3651307401753799\n",
      "temp/f127\n",
      "Area under surface (rectangular approx) =  16.970705795407127\n",
      "Violations =  0.0\n",
      "Average_violations =  -20798.274093636468\n",
      "MSE =  0.3601108528369405\n",
      "temp/f128\n",
      "Area under surface (rectangular approx) =  18.296237103114727\n",
      "Violations =  0.0\n",
      "Average_violations =  -20842.297218699587\n",
      "MSE =  0.36379241009380237\n",
      "temp/f129\n",
      "Area under surface (rectangular approx) =  18.55858056872664\n",
      "Violations =  0.0\n",
      "Average_violations =  -20788.24527893817\n",
      "MSE =  0.36025664998465246\n",
      "temp/f130\n",
      "Area under surface (rectangular approx) =  19.09073267611121\n",
      "Violations =  0.0\n",
      "Average_violations =  -20814.31533898205\n",
      "MSE =  0.3583304347746638\n",
      "temp/f131\n",
      "Area under surface (rectangular approx) =  17.8748249731854\n",
      "Violations =  0.0\n",
      "Average_violations =  -20790.616166669894\n",
      "MSE =  0.36230283702490573\n",
      "temp/f132\n",
      "Area under surface (rectangular approx) =  17.103004378879124\n",
      "Violations =  0.0\n",
      "Average_violations =  -20798.349358637875\n",
      "MSE =  0.35529269027656457\n",
      "temp/f133\n",
      "Area under surface (rectangular approx) =  17.117472228132126\n",
      "Violations =  0.0\n",
      "Average_violations =  -20786.802731173244\n",
      "MSE =  0.35728159224067574\n",
      "temp/f134\n",
      "Area under surface (rectangular approx) =  18.481158992120807\n",
      "Violations =  0.0\n",
      "Average_violations =  -20830.12450018723\n",
      "MSE =  0.3597730912398261\n",
      "temp/f135\n",
      "Area under surface (rectangular approx) =  17.490522918768555\n",
      "Violations =  0.0\n",
      "Average_violations =  -20697.822550703393\n",
      "MSE =  0.37219400235565675\n",
      "temp/f136\n",
      "Area under surface (rectangular approx) =  18.079283837412333\n",
      "Violations =  0.0\n",
      "Average_violations =  -20834.208086459374\n",
      "MSE =  0.3576479590928073\n",
      "temp/f137\n",
      "Area under surface (rectangular approx) =  19.56698054978949\n",
      "Violations =  0.0\n",
      "Average_violations =  -20817.174164517593\n",
      "MSE =  0.35863097203852407\n",
      "temp/f138\n",
      "Area under surface (rectangular approx) =  17.56371685317378\n",
      "Violations =  0.0\n",
      "Average_violations =  -20789.25894684598\n",
      "MSE =  0.362393808682091\n",
      "temp/f139\n",
      "Area under surface (rectangular approx) =  17.60671205432949\n",
      "Violations =  0.0\n",
      "Average_violations =  -20862.11721511175\n",
      "MSE =  0.3581959063671159\n",
      "temp/f140\n",
      "Area under surface (rectangular approx) =  18.692862715202484\n",
      "Violations =  0.0\n",
      "Average_violations =  -20818.53028081309\n",
      "MSE =  0.35835477834403323\n",
      "temp/f141\n",
      "Area under surface (rectangular approx) =  16.938818052316673\n",
      "Violations =  0.0\n",
      "Average_violations =  -20800.536594584588\n",
      "MSE =  0.3584845837291586\n",
      "temp/f142\n",
      "Area under surface (rectangular approx) =  18.950220638255104\n",
      "Violations =  0.0\n",
      "Average_violations =  -20850.400710179416\n",
      "MSE =  0.3625272508518031\n",
      "temp/f143\n",
      "Area under surface (rectangular approx) =  18.38813140138995\n",
      "Violations =  0.0\n",
      "Average_violations =  -20822.289053807184\n",
      "MSE =  0.36093534586788206\n",
      "temp/f144\n",
      "Area under surface (rectangular approx) =  19.277488839457554\n",
      "Violations =  0.0\n",
      "Average_violations =  -20881.478151343883\n",
      "MSE =  0.3600411280649758\n",
      "temp/f145\n",
      "Area under surface (rectangular approx) =  22.335362633093848\n",
      "Violations =  0.0\n",
      "Average_violations =  -20927.21109049438\n",
      "MSE =  0.369229809810741\n",
      "temp/f146\n",
      "Area under surface (rectangular approx) =  18.02862131805061\n",
      "Violations =  0.0\n",
      "Average_violations =  -20891.711150295887\n",
      "MSE =  0.3599979017013264\n",
      "temp/f147\n",
      "Area under surface (rectangular approx) =  17.461763320115175\n",
      "Violations =  0.0\n",
      "Average_violations =  -20790.6226180681\n",
      "MSE =  0.36040501604052694\n",
      "temp/f148\n",
      "Area under surface (rectangular approx) =  18.323839441709993\n",
      "Violations =  0.0\n",
      "Average_violations =  -20845.639865168643\n",
      "MSE =  0.3695173527437569\n",
      "temp/f149\n",
      "Area under surface (rectangular approx) =  17.71120731620137\n",
      "Violations =  0.0\n",
      "Average_violations =  -20801.427408679392\n",
      "MSE =  0.36106587843498067\n",
      "temp/f150\n",
      "Area under surface (rectangular approx) =  19.226224694582662\n",
      "Violations =  0.0\n",
      "Average_violations =  -20922.14134881561\n",
      "MSE =  0.36481058455517074\n",
      "temp/f151\n",
      "Area under surface (rectangular approx) =  18.548255345683028\n",
      "Violations =  0.0\n",
      "Average_violations =  -20840.937584584815\n",
      "MSE =  0.3553944141415581\n",
      "temp/f152\n",
      "Area under surface (rectangular approx) =  19.711640244962346\n",
      "Violations =  0.0\n",
      "Average_violations =  -20865.39743178649\n",
      "MSE =  0.35946834991803445\n",
      "temp/f153\n",
      "Area under surface (rectangular approx) =  19.55494402247722\n",
      "Violations =  0.0\n",
      "Average_violations =  -20923.469620687334\n",
      "MSE =  0.3677343666060881\n",
      "temp/f154\n",
      "Area under surface (rectangular approx) =  19.373559723808878\n",
      "Violations =  0.0\n",
      "Average_violations =  -20887.76827430115\n",
      "MSE =  0.35884212761149104\n",
      "temp/f155\n",
      "Area under surface (rectangular approx) =  19.484047520332545\n",
      "Violations =  0.0\n",
      "Average_violations =  -20864.247666278472\n",
      "MSE =  0.3557084536398077\n",
      "temp/f156\n",
      "Area under surface (rectangular approx) =  18.186499220771314\n",
      "Violations =  0.0\n",
      "Average_violations =  -20845.292629285457\n",
      "MSE =  0.3608251114587047\n",
      "temp/f157\n",
      "Area under surface (rectangular approx) =  18.467481725582857\n",
      "Violations =  0.0\n",
      "Average_violations =  -20896.056132440925\n",
      "MSE =  0.3623142100580887\n",
      "temp/f158\n",
      "Area under surface (rectangular approx) =  18.988999498270456\n",
      "Violations =  0.0\n",
      "Average_violations =  -20839.108967901622\n",
      "MSE =  0.37037483195600873\n",
      "temp/f159\n",
      "Area under surface (rectangular approx) =  18.093609852700055\n",
      "Violations =  0.0\n",
      "Average_violations =  -20875.00594481057\n",
      "MSE =  0.3645826967241863\n",
      "temp/f160\n",
      "Area under surface (rectangular approx) =  19.32552060701414\n",
      "Violations =  0.0\n",
      "Average_violations =  -20915.37308857315\n",
      "MSE =  0.36236611133629265\n",
      "temp/f161\n",
      "Area under surface (rectangular approx) =  19.25109029698795\n",
      "Violations =  0.0\n",
      "Average_violations =  -20886.657878627855\n",
      "MSE =  0.3607202520831903\n",
      "temp/f162\n",
      "Area under surface (rectangular approx) =  19.73796072454638\n",
      "Violations =  0.0\n",
      "Average_violations =  -20856.956082611694\n",
      "MSE =  0.3610746406671291\n",
      "temp/f163\n",
      "Area under surface (rectangular approx) =  17.95453945706764\n",
      "Violations =  0.0\n",
      "Average_violations =  -20797.171382439275\n",
      "MSE =  0.36071671365747854\n",
      "temp/f164\n",
      "Area under surface (rectangular approx) =  18.870382025889224\n",
      "Violations =  0.0\n",
      "Average_violations =  -20829.538847873322\n",
      "MSE =  0.36265351088299996\n",
      "temp/f165\n",
      "Area under surface (rectangular approx) =  17.28196384370745\n",
      "Violations =  0.0\n",
      "Average_violations =  -20782.449126262134\n",
      "MSE =  0.35754964039531173\n",
      "temp/f166\n",
      "Area under surface (rectangular approx) =  18.722223564303235\n",
      "Violations =  0.0\n",
      "Average_violations =  -20845.33526117349\n",
      "MSE =  0.35859148607367003\n",
      "temp/f167\n",
      "Area under surface (rectangular approx) =  18.849164430985567\n",
      "Violations =  0.0\n",
      "Average_violations =  -20834.500883081957\n",
      "MSE =  0.35839763925341855\n",
      "temp/f168\n",
      "Area under surface (rectangular approx) =  17.32350887401795\n",
      "Violations =  0.0\n",
      "Average_violations =  -20766.12048763232\n",
      "MSE =  0.35830844788344135\n",
      "temp/f169\n",
      "Area under surface (rectangular approx) =  18.406456446437957\n",
      "Violations =  0.0\n",
      "Average_violations =  -20806.051732853448\n",
      "MSE =  0.360350532290866\n",
      "temp/f170\n",
      "Area under surface (rectangular approx) =  18.26197762940506\n",
      "Violations =  0.0\n",
      "Average_violations =  -20858.481084269017\n",
      "MSE =  0.3623777887408582\n",
      "temp/f171\n",
      "Area under surface (rectangular approx) =  18.310009597879194\n",
      "Violations =  0.0\n",
      "Average_violations =  -20863.479802359136\n",
      "MSE =  0.3668852468595916\n",
      "temp/f172\n",
      "Area under surface (rectangular approx) =  17.141954896449953\n",
      "Violations =  0.0\n",
      "Average_violations =  -20758.536250549583\n",
      "MSE =  0.35776300163862035\n",
      "temp/f173\n",
      "Area under surface (rectangular approx) =  19.533556280002717\n",
      "Violations =  0.0\n",
      "Average_violations =  -20885.8158166853\n",
      "MSE =  0.3586470577352973\n",
      "temp/f174\n",
      "Area under surface (rectangular approx) =  16.610151653418495\n",
      "Violations =  0.0\n",
      "Average_violations =  -20790.81142898222\n",
      "MSE =  0.35924133061068475\n",
      "temp/f175\n",
      "Area under surface (rectangular approx) =  18.144215756564197\n",
      "Violations =  0.0\n",
      "Average_violations =  -20791.526320454042\n",
      "MSE =  0.3611856841904851\n",
      "temp/f176\n",
      "Area under surface (rectangular approx) =  18.114663375016175\n",
      "Violations =  0.0\n",
      "Average_violations =  -20780.66703603242\n",
      "MSE =  0.36569819544308246\n",
      "temp/f177\n",
      "Area under surface (rectangular approx) =  17.432174410761636\n",
      "Violations =  0.0\n",
      "Average_violations =  -20785.56107031065\n",
      "MSE =  0.3624038666477046\n",
      "temp/f178\n",
      "Area under surface (rectangular approx) =  18.29069753414762\n",
      "Violations =  0.0\n",
      "Average_violations =  -20811.651201075452\n",
      "MSE =  0.35844819889466156\n",
      "temp/f179\n",
      "Area under surface (rectangular approx) =  18.27441734989235\n",
      "Violations =  0.0\n",
      "Average_violations =  -20802.175996621903\n",
      "MSE =  0.35915511195954997\n",
      "temp/f180\n",
      "Area under surface (rectangular approx) =  18.54636226946799\n",
      "Violations =  0.0\n",
      "Average_violations =  -20852.73006100328\n",
      "MSE =  0.36372611588216736\n",
      "temp/f181\n",
      "Area under surface (rectangular approx) =  17.821256981125117\n",
      "Violations =  0.0\n",
      "Average_violations =  -20824.899466949264\n",
      "MSE =  0.35331902899825257\n",
      "temp/f182\n",
      "Area under surface (rectangular approx) =  17.965114265485578\n",
      "Violations =  0.0\n",
      "Average_violations =  -20887.045095278467\n",
      "MSE =  0.3612207175109714\n",
      "temp/f183\n",
      "Area under surface (rectangular approx) =  20.564897516003715\n",
      "Violations =  0.0\n",
      "Average_violations =  -20945.199587699837\n",
      "MSE =  0.364895030087168\n",
      "temp/f184\n",
      "Area under surface (rectangular approx) =  21.110316078128626\n",
      "Violations =  0.0\n",
      "Average_violations =  -20874.27063088062\n",
      "MSE =  0.3578827502262771\n",
      "temp/f185\n",
      "Area under surface (rectangular approx) =  18.14019555123884\n",
      "Violations =  0.0\n",
      "Average_violations =  -20746.777240405776\n",
      "MSE =  0.3700447665153456\n",
      "temp/f186\n",
      "Area under surface (rectangular approx) =  19.505217838850747\n",
      "Violations =  0.0\n",
      "Average_violations =  -20825.024177554817\n",
      "MSE =  0.3630689705673494\n",
      "temp/f187\n",
      "Area under surface (rectangular approx) =  17.938089670784144\n",
      "Violations =  0.0\n",
      "Average_violations =  -20834.231183944376\n",
      "MSE =  0.35866364705677395\n",
      "temp/f188\n",
      "Area under surface (rectangular approx) =  18.11861498010051\n",
      "Violations =  0.0\n",
      "Average_violations =  -20819.856778636924\n",
      "MSE =  0.35613396403806385\n",
      "temp/f189\n",
      "Area under surface (rectangular approx) =  16.60247003058981\n",
      "Violations =  0.0\n",
      "Average_violations =  -20780.504840971666\n",
      "MSE =  0.3577100153160724\n",
      "temp/f190\n",
      "Area under surface (rectangular approx) =  17.916344089291766\n",
      "Violations =  0.0\n",
      "Average_violations =  -20787.118462220515\n",
      "MSE =  0.3611938774616408\n",
      "temp/f191\n",
      "Area under surface (rectangular approx) =  18.77070561093382\n",
      "Violations =  0.0\n",
      "Average_violations =  -20812.307816285134\n",
      "MSE =  0.35749488831929455\n",
      "temp/f192\n",
      "Area under surface (rectangular approx) =  18.862645731361987\n",
      "Violations =  0.0\n",
      "Average_violations =  -20835.216508507994\n",
      "MSE =  0.3629712917536969\n",
      "temp/f193\n",
      "Area under surface (rectangular approx) =  17.58600945673829\n",
      "Violations =  0.0\n",
      "Average_violations =  -20747.592526065953\n",
      "MSE =  0.3657285319270139\n",
      "temp/f194\n",
      "Area under surface (rectangular approx) =  19.533680219120278\n",
      "Violations =  0.0\n",
      "Average_violations =  -20854.274336038343\n",
      "MSE =  0.36137195227140895\n",
      "temp/f195\n",
      "Area under surface (rectangular approx) =  17.884475957607293\n",
      "Violations =  0.0\n",
      "Average_violations =  -20887.819955985\n",
      "MSE =  0.3614223849986913\n",
      "temp/f196\n",
      "Area under surface (rectangular approx) =  17.958932921753647\n",
      "Violations =  0.0\n",
      "Average_violations =  -20812.414721324112\n",
      "MSE =  0.35971626016905334\n",
      "temp/f197\n",
      "Area under surface (rectangular approx) =  20.40939311232127\n",
      "Violations =  0.0\n",
      "Average_violations =  -20902.823002344234\n",
      "MSE =  0.3620375981414398\n",
      "temp/f198\n",
      "Area under surface (rectangular approx) =  17.94894743559153\n",
      "Violations =  0.0\n",
      "Average_violations =  -20817.73293463902\n",
      "MSE =  0.3559441473611169\n",
      "temp/f199\n",
      "Area under surface (rectangular approx) =  18.13322852273042\n",
      "Violations =  0.0\n",
      "Average_violations =  -20818.989687361467\n",
      "MSE =  0.36634356376133576\n",
      "temp/f200\n",
      "Area under surface (rectangular approx) =  20.000859828053418\n",
      "Violations =  0.0\n",
      "Average_violations =  -20878.28905995221\n",
      "MSE =  0.3572178849431569\n",
      "temp/f201\n",
      "Area under surface (rectangular approx) =  18.939338697024482\n",
      "Violations =  0.0\n",
      "Average_violations =  -20810.3032178992\n",
      "MSE =  0.36435383642048014\n",
      "temp/f202\n",
      "Area under surface (rectangular approx) =  19.049850794263087\n",
      "Violations =  0.0\n",
      "Average_violations =  -20884.180133672107\n",
      "MSE =  0.36083106926736636\n",
      "temp/f203\n",
      "Area under surface (rectangular approx) =  19.77764476844963\n",
      "Violations =  0.0\n",
      "Average_violations =  -20925.20245269354\n",
      "MSE =  0.36736928096625027\n",
      "temp/f204\n",
      "Area under surface (rectangular approx) =  17.48839177713488\n",
      "Violations =  0.0\n",
      "Average_violations =  -20824.37573580012\n",
      "MSE =  0.36256387878129265\n",
      "temp/f205\n",
      "Area under surface (rectangular approx) =  19.48619269350327\n",
      "Violations =  0.0\n",
      "Average_violations =  -20924.82593420144\n",
      "MSE =  0.35921905056293885\n",
      "temp/f206\n",
      "Area under surface (rectangular approx) =  19.39867896256662\n",
      "Violations =  0.0\n",
      "Average_violations =  -20832.554766614194\n",
      "MSE =  0.36697458015077805\n",
      "temp/f207\n",
      "Area under surface (rectangular approx) =  17.783441659499097\n",
      "Violations =  0.0\n",
      "Average_violations =  -20902.701824456788\n",
      "MSE =  0.36425937497616684\n",
      "temp/f208\n",
      "Area under surface (rectangular approx) =  18.55427548201551\n",
      "Violations =  0.0\n",
      "Average_violations =  -20876.533710923013\n",
      "MSE =  0.3646520145581933\n",
      "temp/f209\n",
      "Area under surface (rectangular approx) =  17.28478180027453\n",
      "Violations =  0.0\n",
      "Average_violations =  -20809.385561191397\n",
      "MSE =  0.36995575948928633\n",
      "temp/f210\n",
      "Area under surface (rectangular approx) =  17.50264007830844\n",
      "Violations =  0.0\n",
      "Average_violations =  -20767.736816248154\n",
      "MSE =  0.3604333670089372\n",
      "temp/f211\n",
      "Area under surface (rectangular approx) =  18.99280914096744\n",
      "Violations =  0.0\n",
      "Average_violations =  -20903.729214175382\n",
      "MSE =  0.3602588262495479\n",
      "temp/f212\n",
      "Area under surface (rectangular approx) =  19.906803263619636\n",
      "Violations =  0.0\n",
      "Average_violations =  -20941.57954803181\n",
      "MSE =  0.36415946502101365\n",
      "temp/f213\n",
      "Area under surface (rectangular approx) =  18.35402069403493\n",
      "Violations =  0.0\n",
      "Average_violations =  -20814.743590386304\n",
      "MSE =  0.35785221458892574\n",
      "temp/f214\n",
      "Area under surface (rectangular approx) =  19.41758273508823\n",
      "Violations =  0.0\n",
      "Average_violations =  -20848.833683616012\n",
      "MSE =  0.3608343363425291\n",
      "temp/f215\n",
      "Area under surface (rectangular approx) =  20.708158456661955\n",
      "Violations =  0.0\n",
      "Average_violations =  -20896.90262184924\n",
      "MSE =  0.3561683441638161\n",
      "temp/f216\n",
      "Area under surface (rectangular approx) =  18.77228669245622\n",
      "Violations =  0.0\n",
      "Average_violations =  -20936.890363605573\n",
      "MSE =  0.3676131568525196\n",
      "temp/f217\n",
      "Area under surface (rectangular approx) =  19.236864483906892\n",
      "Violations =  0.0\n",
      "Average_violations =  -20889.884740420028\n",
      "MSE =  0.35992563541469796\n",
      "temp/f218\n",
      "Area under surface (rectangular approx) =  17.3933116351764\n",
      "Violations =  0.0\n",
      "Average_violations =  -20837.84259381706\n",
      "MSE =  0.35981506206107766\n",
      "temp/f219\n",
      "Area under surface (rectangular approx) =  18.631146381603585\n",
      "Violations =  0.0\n",
      "Average_violations =  -20857.397919870553\n",
      "MSE =  0.35546841939382484\n",
      "temp/f220\n",
      "Area under surface (rectangular approx) =  17.133405409840694\n",
      "Violations =  0.0\n",
      "Average_violations =  -20801.108212210456\n",
      "MSE =  0.35935789748722424\n",
      "temp/f221\n",
      "Area under surface (rectangular approx) =  19.910675745354805\n",
      "Violations =  0.0\n",
      "Average_violations =  -20862.388652120633\n",
      "MSE =  0.36209541043772947\n",
      "temp/f222\n",
      "Area under surface (rectangular approx) =  19.312429109717975\n",
      "Violations =  0.0\n",
      "Average_violations =  -20894.560866527627\n",
      "MSE =  0.35944551070836345\n",
      "temp/f223\n",
      "Area under surface (rectangular approx) =  19.665567337530675\n",
      "Violations =  0.0\n",
      "Average_violations =  -20880.39839781525\n",
      "MSE =  0.3575682257071389\n",
      "temp/f224\n",
      "Area under surface (rectangular approx) =  19.352209373495565\n",
      "Violations =  0.0\n",
      "Average_violations =  -20844.30591612986\n",
      "MSE =  0.35552015795634445\n",
      "temp/f225\n",
      "Area under surface (rectangular approx) =  17.45292499859499\n",
      "Violations =  0.0\n",
      "Average_violations =  -20741.204632931487\n",
      "MSE =  0.35939371105117357\n",
      "temp/f226\n",
      "Area under surface (rectangular approx) =  16.9246340260285\n",
      "Violations =  0.0\n",
      "Average_violations =  -20790.411619460956\n",
      "MSE =  0.35396665645175784\n",
      "temp/f227\n",
      "Area under surface (rectangular approx) =  18.18055750379983\n",
      "Violations =  0.0\n",
      "Average_violations =  -20861.504108642206\n",
      "MSE =  0.3620798443887164\n",
      "temp/f228\n",
      "Area under surface (rectangular approx) =  16.60780956353529\n",
      "Violations =  0.0\n",
      "Average_violations =  -20772.502096151664\n",
      "MSE =  0.36332328691706556\n",
      "temp/f229\n",
      "Area under surface (rectangular approx) =  17.543366897113756\n",
      "Violations =  0.0\n",
      "Average_violations =  -20824.735864616207\n",
      "MSE =  0.35954864053023367\n",
      "temp/f230\n",
      "Area under surface (rectangular approx) =  20.51744958884414\n",
      "Violations =  0.0\n",
      "Average_violations =  -20886.819633956144\n",
      "MSE =  0.35974108002457683\n",
      "temp/f231\n",
      "Area under surface (rectangular approx) =  17.30951350571413\n",
      "Violations =  0.0\n",
      "Average_violations =  -20814.4147487925\n",
      "MSE =  0.35935138231103864\n",
      "temp/f232\n",
      "Area under surface (rectangular approx) =  17.88757915169963\n",
      "Violations =  0.0\n",
      "Average_violations =  -20847.27664537658\n",
      "MSE =  0.3591805996056779\n",
      "temp/f233\n",
      "Area under surface (rectangular approx) =  17.41447508156567\n",
      "Violations =  0.0\n",
      "Average_violations =  -20778.486986117176\n",
      "MSE =  0.36519986759946965\n",
      "temp/f234\n",
      "Area under surface (rectangular approx) =  18.87520074235826\n",
      "Violations =  0.0\n",
      "Average_violations =  -20861.318251117144\n",
      "MSE =  0.36301756940917435\n",
      "temp/f235\n",
      "Area under surface (rectangular approx) =  17.683744501858513\n",
      "Violations =  0.0\n",
      "Average_violations =  -20853.342712853268\n",
      "MSE =  0.3610554568103915\n",
      "temp/f236\n",
      "Area under surface (rectangular approx) =  18.068227381218986\n",
      "Violations =  0.0\n",
      "Average_violations =  -20877.433811087856\n",
      "MSE =  0.3594870271527179\n",
      "temp/f237\n",
      "Area under surface (rectangular approx) =  17.239379170447187\n",
      "Violations =  0.0\n",
      "Average_violations =  -20842.097056224102\n",
      "MSE =  0.36368249643164013\n",
      "temp/f238\n",
      "Area under surface (rectangular approx) =  17.36992419018823\n",
      "Violations =  0.0\n",
      "Average_violations =  -20832.27477543872\n",
      "MSE =  0.35962682038502813\n",
      "temp/f239\n",
      "Area under surface (rectangular approx) =  17.8682105298789\n",
      "Violations =  0.0\n",
      "Average_violations =  -20842.142128067982\n",
      "MSE =  0.3597484819168962\n",
      "temp/f240\n",
      "Area under surface (rectangular approx) =  19.01530434823963\n",
      "Violations =  0.0\n",
      "Average_violations =  -20895.593658867398\n",
      "MSE =  0.3602929000227249\n",
      "temp/f241\n",
      "Area under surface (rectangular approx) =  20.15422853491256\n",
      "Violations =  0.0\n",
      "Average_violations =  -20931.9191304295\n",
      "MSE =  0.3635050298373835\n",
      "temp/f242\n",
      "Area under surface (rectangular approx) =  17.179784023567915\n",
      "Violations =  0.0\n",
      "Average_violations =  -20813.48580363142\n",
      "MSE =  0.3644336757963014\n",
      "temp/f243\n",
      "Area under surface (rectangular approx) =  20.140080962660825\n",
      "Violations =  0.0\n",
      "Average_violations =  -20859.905751157403\n",
      "MSE =  0.36199121381034055\n",
      "temp/f244\n",
      "Area under surface (rectangular approx) =  18.22322164585738\n",
      "Violations =  0.0\n",
      "Average_violations =  -20877.812935674014\n",
      "MSE =  0.36187219061583736\n",
      "temp/f245\n",
      "Area under surface (rectangular approx) =  17.21371616268039\n",
      "Violations =  0.0\n",
      "Average_violations =  -20788.003913840188\n",
      "MSE =  0.3604358418609583\n",
      "temp/f246\n",
      "Area under surface (rectangular approx) =  17.19231122340825\n",
      "Violations =  0.0\n",
      "Average_violations =  -20791.58160671799\n",
      "MSE =  0.3645687393739621\n",
      "temp/f247\n",
      "Area under surface (rectangular approx) =  17.499871288582213\n",
      "Violations =  0.0\n",
      "Average_violations =  -20788.171017868797\n",
      "MSE =  0.36136169727636985\n",
      "temp/f248\n",
      "Area under surface (rectangular approx) =  17.56643160535384\n",
      "Violations =  0.0\n",
      "Average_violations =  -20840.468524989377\n",
      "MSE =  0.35812117526276693\n",
      "temp/f249\n",
      "Area under surface (rectangular approx) =  18.22229940415377\n",
      "Violations =  0.0\n",
      "Average_violations =  -20840.831468616547\n",
      "MSE =  0.35771377335355825\n",
      "temp/f250\n",
      "Area under surface (rectangular approx) =  17.365540389684952\n",
      "Violations =  0.0\n",
      "Average_violations =  -20834.35896348907\n",
      "MSE =  0.36251602736237937\n",
      "temp/f251\n",
      "Area under surface (rectangular approx) =  17.254269052725768\n",
      "Violations =  0.0\n",
      "Average_violations =  -20828.95019434467\n",
      "MSE =  0.36228122750861086\n",
      "temp/f252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under surface (rectangular approx) =  18.708582650572836\n",
      "Violations =  0.0\n",
      "Average_violations =  -20923.09639124599\n",
      "MSE =  0.3587152830801575\n",
      "temp/f253\n",
      "Area under surface (rectangular approx) =  20.803903389939414\n",
      "Violations =  0.0\n",
      "Average_violations =  -20957.858335515328\n",
      "MSE =  0.36904536780875197\n",
      "temp/f254\n",
      "Area under surface (rectangular approx) =  20.890296616408527\n",
      "Violations =  0.0\n",
      "Average_violations =  -20885.91508961932\n",
      "MSE =  0.360712035669116\n",
      "temp/f255\n",
      "Area under surface (rectangular approx) =  18.480472804978454\n",
      "Violations =  0.0\n",
      "Average_violations =  -20865.207409381885\n",
      "MSE =  0.35938218906052716\n",
      "temp/f256\n",
      "Area under surface (rectangular approx) =  17.770366395898478\n",
      "Violations =  0.0\n",
      "Average_violations =  -20813.31432368149\n",
      "MSE =  0.3604103586757128\n",
      "temp/f257\n",
      "Area under surface (rectangular approx) =  17.975753164652936\n",
      "Violations =  0.0\n",
      "Average_violations =  -20817.316535090435\n",
      "MSE =  0.3577905186846859\n",
      "temp/f258\n",
      "Area under surface (rectangular approx) =  17.17475379587459\n",
      "Violations =  0.0\n",
      "Average_violations =  -20827.15937138838\n",
      "MSE =  0.36391699030351216\n",
      "temp/f259\n",
      "Area under surface (rectangular approx) =  18.818598429861872\n",
      "Violations =  0.0\n",
      "Average_violations =  -20904.821017776954\n",
      "MSE =  0.36057513338877717\n",
      "temp/f260\n",
      "Area under surface (rectangular approx) =  20.170148458852413\n",
      "Violations =  0.0\n",
      "Average_violations =  -20873.247937253003\n",
      "MSE =  0.36074505985780675\n",
      "temp/f261\n",
      "Area under surface (rectangular approx) =  17.138439652379393\n",
      "Violations =  0.0\n",
      "Average_violations =  -20854.58702224407\n",
      "MSE =  0.3585599826583144\n",
      "temp/f262\n",
      "Area under surface (rectangular approx) =  18.33798400430795\n",
      "Violations =  0.0\n",
      "Average_violations =  -20875.019160681502\n",
      "MSE =  0.3626166451376415\n",
      "temp/f263\n",
      "Area under surface (rectangular approx) =  18.26480238266135\n",
      "Violations =  0.0\n",
      "Average_violations =  -20823.136133233053\n",
      "MSE =  0.3558587071113837\n",
      "temp/f264\n",
      "Area under surface (rectangular approx) =  20.213508525029134\n",
      "Violations =  0.0\n",
      "Average_violations =  -20914.946142165194\n",
      "MSE =  0.36429066556021744\n",
      "temp/f265\n",
      "Area under surface (rectangular approx) =  17.485433995498006\n",
      "Violations =  0.0\n",
      "Average_violations =  -20873.769530095786\n",
      "MSE =  0.36417355587267\n",
      "temp/f266\n",
      "Area under surface (rectangular approx) =  18.407439877769352\n",
      "Violations =  0.0\n",
      "Average_violations =  -20818.405673112055\n",
      "MSE =  0.36774225619447654\n",
      "temp/f267\n",
      "Area under surface (rectangular approx) =  21.615625542945146\n",
      "Violations =  0.0\n",
      "Average_violations =  -20899.446752269312\n",
      "MSE =  0.36374617348739124\n",
      "temp/f268\n",
      "Area under surface (rectangular approx) =  18.244353075839445\n",
      "Violations =  0.0\n",
      "Average_violations =  -20831.10109450366\n",
      "MSE =  0.35877450263914357\n",
      "temp/f269\n",
      "Area under surface (rectangular approx) =  17.454734987626086\n",
      "Violations =  0.0\n",
      "Average_violations =  -20793.147998517245\n",
      "MSE =  0.3638749173831542\n",
      "temp/f270\n",
      "Area under surface (rectangular approx) =  20.122212561073965\n",
      "Violations =  0.0\n",
      "Average_violations =  -20976.072529847184\n",
      "MSE =  0.363661842559295\n",
      "temp/f271\n",
      "Area under surface (rectangular approx) =  18.146801181613267\n",
      "Violations =  0.0\n",
      "Average_violations =  -20821.115670950083\n",
      "MSE =  0.36547585643459524\n",
      "temp/f272\n",
      "Area under surface (rectangular approx) =  18.400960651961114\n",
      "Violations =  0.0\n",
      "Average_violations =  -20849.758949819214\n",
      "MSE =  0.3570904911342918\n",
      "temp/f273\n",
      "Area under surface (rectangular approx) =  16.688285247466556\n",
      "Violations =  0.0\n",
      "Average_violations =  -20784.117829696665\n",
      "MSE =  0.3568150000992925\n",
      "temp/f274\n",
      "Area under surface (rectangular approx) =  19.652816512865183\n",
      "Violations =  0.0\n",
      "Average_violations =  -20855.10854240488\n",
      "MSE =  0.36165611647956997\n",
      "temp/f275\n",
      "Area under surface (rectangular approx) =  18.01377162577531\n",
      "Violations =  0.0\n",
      "Average_violations =  -20888.103568220973\n",
      "MSE =  0.36111343478914426\n",
      "temp/f276\n",
      "Area under surface (rectangular approx) =  18.869012599868867\n",
      "Violations =  0.0\n",
      "Average_violations =  -20848.547396431906\n",
      "MSE =  0.36293953510315996\n",
      "temp/f277\n",
      "Area under surface (rectangular approx) =  17.262850407979364\n",
      "Violations =  0.0\n",
      "Average_violations =  -20809.1601073974\n",
      "MSE =  0.3550249602733359\n",
      "temp/f278\n",
      "Area under surface (rectangular approx) =  17.747271345150985\n",
      "Violations =  0.0\n",
      "Average_violations =  -20895.67097540684\n",
      "MSE =  0.35994839219628894\n",
      "temp/f279\n",
      "Area under surface (rectangular approx) =  17.730970259065213\n",
      "Violations =  0.0\n",
      "Average_violations =  -20887.728591947252\n",
      "MSE =  0.3634966652571885\n",
      "temp/f280\n",
      "Area under surface (rectangular approx) =  20.738578405518776\n",
      "Violations =  0.0\n",
      "Average_violations =  -20932.468653637206\n",
      "MSE =  0.3686661116540735\n",
      "temp/f281\n",
      "Area under surface (rectangular approx) =  18.933395543199524\n",
      "Violations =  0.0\n",
      "Average_violations =  -20817.647713388604\n",
      "MSE =  0.35688593011193115\n",
      "temp/f282\n",
      "Area under surface (rectangular approx) =  18.872956919032404\n",
      "Violations =  0.0\n",
      "Average_violations =  -20854.415179212763\n",
      "MSE =  0.35952903776671796\n",
      "temp/f283\n",
      "Area under surface (rectangular approx) =  17.951257996046834\n",
      "Violations =  0.0\n",
      "Average_violations =  -20798.47555595695\n",
      "MSE =  0.35903954713981684\n",
      "temp/f284\n",
      "Area under surface (rectangular approx) =  18.403689899353683\n",
      "Violations =  0.0\n",
      "Average_violations =  -20907.428126620613\n",
      "MSE =  0.36539229985085325\n",
      "temp/f285\n",
      "Area under surface (rectangular approx) =  17.973188779872785\n",
      "Violations =  0.0\n",
      "Average_violations =  -20877.329671187563\n",
      "MSE =  0.3618132068740684\n",
      "temp/f286\n",
      "Area under surface (rectangular approx) =  19.210357156659228\n",
      "Violations =  0.0\n",
      "Average_violations =  -20847.637088236483\n",
      "MSE =  0.3567199111082618\n",
      "temp/f287\n",
      "Area under surface (rectangular approx) =  18.377453483511445\n",
      "Violations =  0.0\n",
      "Average_violations =  -20845.12011954681\n",
      "MSE =  0.3592788624103957\n",
      "temp/f288\n",
      "Area under surface (rectangular approx) =  18.752754938746836\n",
      "Violations =  0.0\n",
      "Average_violations =  -20871.0224126563\n",
      "MSE =  0.35911458242621813\n",
      "temp/f289\n",
      "Area under surface (rectangular approx) =  17.214357092796003\n",
      "Violations =  0.0\n",
      "Average_violations =  -20848.68701258258\n",
      "MSE =  0.3589159365098514\n",
      "temp/f290\n",
      "Area under surface (rectangular approx) =  17.71271330952912\n",
      "Violations =  0.0\n",
      "Average_violations =  -20861.029834760266\n",
      "MSE =  0.36283114657066323\n",
      "temp/f291\n",
      "Area under surface (rectangular approx) =  17.924876890272408\n",
      "Violations =  0.0\n",
      "Average_violations =  -20757.249681138415\n",
      "MSE =  0.37002278320018966\n",
      "temp/f292\n",
      "Area under surface (rectangular approx) =  18.253580615467918\n",
      "Violations =  0.0\n",
      "Average_violations =  -20879.326647032267\n",
      "MSE =  0.3675495640424669\n",
      "temp/f293\n",
      "Area under surface (rectangular approx) =  17.56919600626287\n",
      "Violations =  0.0\n",
      "Average_violations =  -20832.757959194772\n",
      "MSE =  0.36375455462450623\n",
      "temp/f294\n",
      "Area under surface (rectangular approx) =  18.47086548867373\n",
      "Violations =  0.0\n",
      "Average_violations =  -20872.658014560937\n",
      "MSE =  0.3589843267353688\n",
      "temp/f295\n",
      "Area under surface (rectangular approx) =  17.728579165261213\n",
      "Violations =  0.0\n",
      "Average_violations =  -20785.860336820377\n",
      "MSE =  0.35277575727883576\n",
      "temp/f296\n",
      "Area under surface (rectangular approx) =  18.1282840847058\n",
      "Violations =  0.0\n",
      "Average_violations =  -20880.851335808864\n",
      "MSE =  0.3590677761098506\n",
      "temp/f297\n",
      "Area under surface (rectangular approx) =  19.87997164559818\n",
      "Violations =  0.0\n",
      "Average_violations =  -20919.228935976924\n",
      "MSE =  0.3568735372007179\n",
      "temp/f298\n",
      "Area under surface (rectangular approx) =  18.775906223573553\n",
      "Violations =  0.0\n",
      "Average_violations =  -20862.979087381802\n",
      "MSE =  0.35800836429970945\n",
      "temp/f299\n",
      "Area under surface (rectangular approx) =  17.922515925787337\n",
      "Violations =  0.0\n",
      "Average_violations =  -20877.272154246384\n",
      "MSE =  0.36731835097936383\n"
     ]
    }
   ],
   "source": [
    "metric = []\n",
    "metric_err = []\n",
    "viol = []\n",
    "viol_err = []\n",
    "\n",
    "#normalize the violations for prettier graphing.\n",
    "#also violations are always positive, so just divide by max.\n",
    "\n",
    "#TMK\n",
    "#violation_mean = violation_mean / np.max(violation_mean)\n",
    "\n",
    "for i in range(len(violations)):\n",
    "    print(\"Model_name = \", model_names[i], \"Violations = \", violations[i])\n",
    "    print(\"Average_violations = \", np.mean(violation_mean[i]), np.std(violation_mean[i]))\n",
    "    print(\"MSE = \", np.mean(mean[i]), np.std(mean[i]))\n",
    "    #print(\"mean = \", mean[i])\n",
    "    metric.append(np.mean(mean[i]))\n",
    "    metric_err.append(np.std(mean[i]))\n",
    "    viol.append(np.mean(violation_mean[i]))\n",
    "    #viol.append(violations[i]/times)\n",
    "    viol_err.append(np.std(violation_mean[i]))\n",
    "print(np.array(metric), \n",
    "         np.array(metric_err), \n",
    "         np.array(viol), \n",
    "         np.array(viol_err))    \n",
    "\n",
    "bar_plot(model_names, \n",
    "         np.array(metric), \n",
    "         np.array(metric_err), \n",
    "         np.array(viol), \n",
    "         np.array(viol_err))\n",
    "\n",
    "\n",
    "def heat_plot(x,y,z, xlab = 'Mean', ylab = 'Variance', clim_low = 0, clim_high = 1):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    cax = ax.scatter(x, y, c=z, s=450, edgecolor='')\n",
    "    cax.set_clim(clim_low, clim_high)\n",
    "    ax.set_xlabel(xlab)\n",
    "    ax.set_ylabel(ylab)\n",
    "    plt.colorbar(cax)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "MSE = []\n",
    "VIO = []\n",
    "VIO2 = []\n",
    "AUS = []\n",
    "for i, m in enumerate(models):\n",
    "    print(model_names[i])\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "    \n",
    "    \n",
    "    rectangular_approx = 0\n",
    "    for k, v in metrics_dicts[i].items():\n",
    "        x.append(float(k.split('_')[0]))\n",
    "        y.append(float(k.split('_')[-1]))\n",
    "        z.append(np.mean(v))\n",
    "        rectangular_approx += np.mean(v)\n",
    "    print(\"Area under surface (rectangular approx) = \", rectangular_approx)\n",
    "    print(\"Violations = \", violations[i])\n",
    "    print(\"Average_violations = \", np.mean(violation_mean[i]))\n",
    "    print(\"MSE = \", np.mean(mean[i]))   \n",
    "    MSE.append(np.mean(mean[i]))\n",
    "    VIO.append(np.mean(violation_mean[i]))\n",
    "    VIO2.append(np.mean(violation_mean2[i]))\n",
    "    #VIO.append(violations[i]/times)\n",
    "    AUS.append(rectangular_approx)\n",
    "    \n",
    "    #heat_plot(x,y,z, clim_low = 0, clim_high = 10)\n",
    "    \n",
    "#heat_plot(MSE,VIO,AUS, xlab = 'MSE', ylab='Violations', clim_low = np.min(AUS), clim_high = np.max(AUS))\n",
    "    \n",
    "#VIO = np.abs(VIO)\n",
    "#VIO2 = np.abs(VIO2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6582792015503192\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4321675582327473\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-297.36951528128725 -0.015152412360513344\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3522930701993604 -4.318004031486778e-07\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-129.2394771376972 107.95537538424502\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.4321675582327473"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.polynomial.polynomial import polyfit  \n",
    "from scipy.stats import pearsonr\n",
    "from pylab import text\n",
    "\n",
    "\n",
    "print(pearsonr(VIO,AUS)[0])\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(VIO,AUS, 1)\n",
    "ax.plot(VIO,AUS, '.')\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(VIO,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "plt.plot(VIO, b + m * np.array(VIO), '-')\n",
    "ax.set_xlabel(\"Proposed metric\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "fig.savefig('Ex4VIOVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "METRIC = (VIO/np.max(VIO)) + np.array(MSE)\n",
    "print(pearsonr(METRIC,AUS)[0])\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(METRIC,AUS, 1)\n",
    "ax.plot(METRIC,AUS, '.')\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(METRIC,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "plt.plot(METRIC, b + m * np.array(METRIC), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"Proposed metric\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "fig.savefig('Ex4ProposedVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(MSE,AUS, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(MSE,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(MSE,AUS, '.')\n",
    "plt.plot(MSE, b + m * np.array(MSE), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(VIO2,AUS, 1)\n",
    "print(b,m)\n",
    "ax.plot(VIO2,AUS, '.')\n",
    "plt.plot(VIO2, b + m * np.array(VIO2), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"Violations2\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(VIO,MSE, 1)\n",
    "print(b,m)\n",
    "ax.plot(VIO,MSE, '.')\n",
    "plt.plot(VIO, b + m * np.array(VIO), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"Violations\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(METRIC,AUS, 1)\n",
    "print(b,m)\n",
    "ax.plot(METRIC,AUS, '.')\n",
    "plt.plot(METRIC, b + m * np.array(METRIC), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"MSExVIO\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "plt.show()\n",
    "\n",
    "MSE = np.array(MSE)\n",
    "METRIC = VIO/np.max(VIO)+ MSE\n",
    "x = []\n",
    "y1 = []\n",
    "y2 = []\n",
    "y3 = []\n",
    "for split in range(10, len(AUS), 5):\n",
    "    #print(\"******\", split, \"*******\")\n",
    "    sorted_aus = [AUS for _,AUS in sorted(zip(VIO,AUS))]\n",
    "    sorted_mse = [MSE for _,MSE in sorted(zip(VIO,MSE))]\n",
    "\n",
    "    low = []\n",
    "    high = []\n",
    "    low = sorted_aus[:split]\n",
    "    high = sorted_aus[split:]\n",
    "\n",
    "    x.append(split)\n",
    "    \n",
    "    \n",
    "    #print(\"Low Violations = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High Violations = \", np.mean(high), \"for\", len(high))\n",
    "    y1.append(np.mean(low))\n",
    "    sorted_aus_by_mse = [AUS for _,AUS in sorted(zip(MSE,AUS))]\n",
    "    low = sorted_aus_by_mse[:split]\n",
    "    high = sorted_aus_by_mse[split:]\n",
    "    #print(\"Low AUS by MSE = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High AUS by MSE = \", np.mean(high), \"for\", len(high))\n",
    "    y2.append(np.mean(low))\n",
    "    sorted_aus = [AUS for _,AUS in sorted(zip(METRIC,AUS))]\n",
    "    sorted_mse = [MSE for _,MSE in sorted(zip(METRIC,MSE))]\n",
    "\n",
    "    low = []\n",
    "    high = []\n",
    "    low = sorted_aus[:split]\n",
    "    high = sorted_aus[split:]\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"Low Metric = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High Metric = \", np.mean(high), \"for\", len(high))\n",
    "    y3.append(np.mean(low))\n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x,y1, '-', label = 'Violations')\n",
    "ax.plot(x,y2, '-', label = 'MSE')\n",
    "ax.plot(x,y3, '-', label = 'METRIC')\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "plt.show()  \n",
    "pearsonr(METRIC,AUS)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(mean = 0, var = 1, SIZE = 2000):\n",
    "    a = np.random.normal(mean, var, SIZE)\n",
    "    b = np.random.normal(mean, var, SIZE)\n",
    "    c = np.random.normal(mean, var, SIZE)\n",
    "    g = a + b + c + np.random.normal(mean,var, SIZE)\n",
    "    d = g + np.random.normal(mean, var, SIZE)\n",
    "    e = g + np.random.normal(mean, var, SIZE)\n",
    "    f = g + np.random.normal(mean, var, SIZE)\n",
    "    \n",
    "    #g = np.rint(g)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
