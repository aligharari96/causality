{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes \n",
    "### Required installing Oracle JAVA 8 to get javabridge installed\n",
    "### Then, I was able to install py-causal from https://bd2kccd.github.io/docs/py-causal/\n",
    "### GFCI is slower than RFCI, but more accurate (SPIRTES), GFCI and RFCI account for unobserved variables, FGES assumes no unobserved variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure Learning Performance Guarantees If the assumptions in the previous section hold, then in the large sample limit, the CBN structure output by GFCId will contain an edge of one of four kinds between Xand Y   if and only if Xand Yare not independent conditional on any subset of the other measured variables of less than or equal to a specified size. In addition, there is (1) an arc X->Y   if and only if Xdirectly or indirectly causes Y, and Y   does not directly or indirectly cause X; (2) an edge X <-->Y   if and only if X   is not a direct or indirect cause of Yand Y   is not a direct or indirect cause of X(which can only occur if there are latent confounders of Xand some other variable or Yand some other variable; (3) an edge Xo->Y   only if Yis not a direct or indirect cause of X, but Xmay or may not be an indirect cause of Y; (4) an edge X oâ€“o Y   indicates that Xand Y   are dependent no matter what subset of observed variables is conditioned on, but contains no orientation information (X   may be a direct or indirect cause of Y, and Ymay be an indirect cause of X, or there may be a latent common cause of Xand Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying some various ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import configparser\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, \\\n",
    "                        Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "import keras.optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "import glob, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "# select your GPU Here\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #Comment this line out if you want all GPUS (2 hehe)\n",
    "\n",
    "# python full-display web browser\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "def get_model(dense, dropouts, inputs):\n",
    "    # dense is an ordered list of the number of dense neurons like [1024, 2048, 1024]\n",
    "    # dropouts is an ordered list of the dropout masks like [0.2, 0.3, 0.4]\n",
    "    inputs = keras.Input(shape = (inputs,))\n",
    "\n",
    "    x = keras.layers.Dense(dense[0], activation = 'relu')(inputs)\n",
    "    x = keras.layers.Dropout(dropouts[0])(x, training=True)\n",
    "    for den, drop in zip(dense[1:], dropouts[1:]):\n",
    "        x = keras.layers.Dense(den, activation = 'relu')(x)\n",
    "        x = keras.layers.Dropout(drop)(x, training=True)\n",
    "    outputs = keras.layers.Dense(1, activation = 'linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512]] ['temp/a0', 'temp/a1', 'temp/a2', 'temp/a3', 'temp/a4', 'temp/a5', 'temp/a6', 'temp/a7', 'temp/a8', 'temp/a9', 'temp/a10', 'temp/a11', 'temp/a12', 'temp/a13', 'temp/a14', 'temp/a15', 'temp/a16', 'temp/a17', 'temp/a18', 'temp/a19', 'temp/a20', 'temp/a21', 'temp/a22', 'temp/a23', 'temp/a24', 'temp/a25', 'temp/a26', 'temp/a27', 'temp/a28', 'temp/a29', 'temp/a30', 'temp/a31', 'temp/a32', 'temp/a33', 'temp/a34', 'temp/a35', 'temp/a36', 'temp/a37', 'temp/a38', 'temp/a39', 'temp/a40', 'temp/a41', 'temp/a42', 'temp/a43', 'temp/a44', 'temp/a45', 'temp/a46', 'temp/a47', 'temp/a48', 'temp/a49']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from pycausal import search as s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def discrete_gauss(low, high, samples, std = 20):\n",
    "    x = np.arange(low, high)\n",
    "    xU, xL = x + 0.5, x - 0.5 \n",
    "    prob = ss.norm.cdf(xU, scale = std) - ss.norm.cdf(xL, scale = std)\n",
    "    prob = prob / prob.sum() #normalize the probabilities so their sum is 1\n",
    "    nums = np.random.choice(x, size = samples, p = prob)\n",
    "    return nums\n",
    "\n",
    "\n",
    "\n",
    "def bar_plot(x_ax, val1, val1std, val2, val2std):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ## the data\n",
    "    N = len(x_ax)\n",
    "\n",
    "    ## necessary variables\n",
    "    ind = np.arange(N)                # the x locations for the groups\n",
    "    width = 0.35                      # the width of the bars\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    ## the bars\n",
    "    rects1 = ax.bar(ind, val1, width,\n",
    "                    color='gray',\n",
    "                    yerr=val1std,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='blue'))\n",
    "\n",
    "    rects2 = ax.bar(ind+width, val2, width,\n",
    "                        color='blue',\n",
    "                        #yerr=val2std,\n",
    "                        error_kw=dict(elinewidth=2,ecolor='gray'))\n",
    "\n",
    "    # axes and labels\n",
    "    ax.set_xlim(-width,len(ind)+width)\n",
    "    #ax.set_ylim(0,45)\n",
    "    ax.set_ylabel('Percentage')\n",
    "    ax.set_title('')\n",
    "    plt.xticks(ind + width / 2, x_ax, rotation=75, size = 14)\n",
    "    ## add a legend\n",
    "    ax.legend( (rects1[0], rects2[0]), ('Accuracy', '% Violations') )\n",
    "    fig.savefig(\"violations.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 5000):\n",
    "    a = np.random.gumbel(mean, var, SIZE)\n",
    "    b = np.random.gumbel(mean, var, SIZE)\n",
    "    c = np.random.gumbel(mean, var, SIZE)\n",
    "    d = np.random.gumbel(mean, var, SIZE)\n",
    "    e = np.random.gumbel(mean, var, SIZE)\n",
    "    f= a + b + c + d + e + np.random.gumbel(mean, var, SIZE)\n",
    "    g = f + np.random.gumbel(mean,var, SIZE)\n",
    "    g = np.rint(g)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 5000):\n",
    "    a = np.random.gumbel(mean, var, SIZE)\n",
    "    b = np.random.gumbel(mean, var, SIZE)\n",
    "    c = np.random.gumbel(mean, var, SIZE)\n",
    "    d = np.random.gumbel(mean, var, SIZE)\n",
    "\n",
    "    f= a + b + c + d + np.random.gumbel(mean, var, SIZE)\n",
    "    g = f + np.random.gumbel(mean,var, SIZE)\n",
    "    \n",
    "    \n",
    "    g = np.rint(g)\n",
    "    e = g + np.random.gumbel(mean,var,SIZE)\n",
    "    \n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 400000):\n",
    "    f = np.random.normal(mean, var, SIZE)\n",
    "    a = f + np.random.normal(mean, var, SIZE)\n",
    "    b = f + np.random.normal(mean, var, SIZE)\n",
    "    c = f + np.random.normal(mean, var, SIZE)\n",
    "    d = f + np.random.normal(mean, var, SIZE)\n",
    "    e = f + np.random.normal(mean, var, SIZE)\n",
    "    g = a + b + c + d  + e + np.random.normal(mean, var, SIZE)\n",
    "\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 20000):\n",
    "    a = np.random.normal(mean, var, SIZE)\n",
    "    b = a + np.random.normal(mean, var, SIZE)\n",
    "    c = a + np.random.normal(mean, var, SIZE)\n",
    "    e = a + np.random.normal(mean, var, SIZE)\n",
    "    d = a + np.random.normal(mean, var, SIZE)\n",
    "    \n",
    "    f= a + np.random.normal(mean, var, SIZE)\n",
    "    g = f + np.random.normal(mean,var, SIZE)\n",
    "    #g = np.rint(g)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 20000): #### THIS ONE HAS F AND G SWAPPED\n",
    "    a = np.random.normal(mean, var, SIZE)\n",
    "    b = a + np.random.normal(mean, var, SIZE)\n",
    "    c =  b + np.random.normal(mean, var, SIZE)\n",
    "    \n",
    "    e = np.random.normal(mean, var, SIZE)\n",
    "    \n",
    "    d = e + b + np.random.normal(mean, var, SIZE)\n",
    "    \n",
    "    g= b +  d + np.random.normal(mean, var, SIZE)\n",
    "    f = g + np.random.normal(mean,var, SIZE)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "\n",
    "\n",
    "def get_CG(df, tetrad):\n",
    "    tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "    #tetrad.run(algoId = 'fges-mb', targetName = 'g', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "    #       structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "    #       completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "def get_MB(graph, var, pc):\n",
    "    parents = set()\n",
    "    children = set()\n",
    "    for i in pc.extractTetradGraphEdges(graph):\n",
    "        if i[-1] == var and i[3:5] == '->':\n",
    "            parents.add(i[0])\n",
    "        if i[0] == var and i[3:5] == '->':\n",
    "            children.add(i[-1])\n",
    "    return parents, children\n",
    "\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from collections import defaultdict\n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '5000M')\n",
    "tetrad = s.tetradrunner()\n",
    "\n",
    "\n",
    "verbosity = 1\n",
    "\n",
    "\n",
    "\n",
    "models = []\n",
    "model_names = []\n",
    "\n",
    "num_models = 50\n",
    "model_layers = [2048, 2048, 512]\n",
    "for i in range(num_models):\n",
    "    models.append(model_layers)\n",
    "    model_names.append('temp/a' + str(i))\n",
    "\n",
    "print(models, model_names)\n",
    "\n",
    "\n",
    "df = gen_data()\n",
    "X = df[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "y = df['g'].values\n",
    "\n",
    "val_df = gen_data(SIZE = 2000)\n",
    "x_val = df[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "y_val = df['g'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp/a0\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 174us/step - loss: 0.7066 - mean_squared_error: 0.7066 - val_loss: 0.5624 - val_mean_squared_error: 0.5624\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56241, saving model to temp/a0\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 0.5593 - mean_squared_error: 0.5593 - val_loss: 0.5725 - val_mean_squared_error: 0.5725\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.56241\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 125us/step - loss: 0.5560 - mean_squared_error: 0.5560 - val_loss: 0.5404 - val_mean_squared_error: 0.5404\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.56241 to 0.54043, saving model to temp/a0\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5509 - mean_squared_error: 0.5509 - val_loss: 0.5358 - val_mean_squared_error: 0.5358\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.54043 to 0.53583, saving model to temp/a0\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 0.5502 - mean_squared_error: 0.5502 - val_loss: 0.5430 - val_mean_squared_error: 0.5430\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53583\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5491 - mean_squared_error: 0.5491 - val_loss: 0.5610 - val_mean_squared_error: 0.5610\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53583\n",
      "Epoch 00006: early stopping\n",
      "temp/a1\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.7153 - mean_squared_error: 0.7153 - val_loss: 0.5663 - val_mean_squared_error: 0.5663\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56627, saving model to temp/a1\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5578 - mean_squared_error: 0.5578 - val_loss: 0.5444 - val_mean_squared_error: 0.5444\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.56627 to 0.54439, saving model to temp/a1\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 0.5556 - mean_squared_error: 0.5556 - val_loss: 0.5410 - val_mean_squared_error: 0.5410\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54439 to 0.54099, saving model to temp/a1\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5505 - mean_squared_error: 0.5505 - val_loss: 0.5461 - val_mean_squared_error: 0.5461\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54099\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5499 - mean_squared_error: 0.5499 - val_loss: 0.5624 - val_mean_squared_error: 0.5624\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54099\n",
      "Epoch 00005: early stopping\n",
      "temp/a2\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 0.6916 - mean_squared_error: 0.6916 - val_loss: 0.5657 - val_mean_squared_error: 0.5657\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56574, saving model to temp/a2\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 126us/step - loss: 0.5522 - mean_squared_error: 0.5522 - val_loss: 0.5593 - val_mean_squared_error: 0.5593\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.56574 to 0.55931, saving model to temp/a2\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 126us/step - loss: 0.5578 - mean_squared_error: 0.5578 - val_loss: 0.5506 - val_mean_squared_error: 0.5506\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55931 to 0.55065, saving model to temp/a2\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 126us/step - loss: 0.5567 - mean_squared_error: 0.5567 - val_loss: 0.5712 - val_mean_squared_error: 0.5712\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.55065\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.5417 - mean_squared_error: 0.5417 - val_loss: 0.5387 - val_mean_squared_error: 0.5387\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.55065 to 0.53874, saving model to temp/a2\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5452 - mean_squared_error: 0.5452 - val_loss: 0.5433 - val_mean_squared_error: 0.5433\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53874\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5489 - mean_squared_error: 0.5489 - val_loss: 0.5370 - val_mean_squared_error: 0.5370\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.53874 to 0.53704, saving model to temp/a2\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 0.5417 - mean_squared_error: 0.5417 - val_loss: 0.5631 - val_mean_squared_error: 0.5631\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.53704\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5390 - mean_squared_error: 0.5390 - val_loss: 0.5451 - val_mean_squared_error: 0.5451\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.53704\n",
      "Epoch 00009: early stopping\n",
      "temp/a3\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.7091 - mean_squared_error: 0.7091 - val_loss: 0.5541 - val_mean_squared_error: 0.5541\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55414, saving model to temp/a3\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5643 - mean_squared_error: 0.5643 - val_loss: 0.5470 - val_mean_squared_error: 0.5470\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55414 to 0.54699, saving model to temp/a3\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5499 - mean_squared_error: 0.5499 - val_loss: 0.5502 - val_mean_squared_error: 0.5502\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54699\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 125us/step - loss: 0.5478 - mean_squared_error: 0.5478 - val_loss: 0.5478 - val_mean_squared_error: 0.5478\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54699\n",
      "Epoch 00004: early stopping\n",
      "temp/a4\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 142us/step - loss: 0.6893 - mean_squared_error: 0.6893 - val_loss: 0.5767 - val_mean_squared_error: 0.5767\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57667, saving model to temp/a4\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5565 - mean_squared_error: 0.5565 - val_loss: 0.5649 - val_mean_squared_error: 0.5649\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.57667 to 0.56488, saving model to temp/a4\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.5531 - mean_squared_error: 0.5531 - val_loss: 0.5524 - val_mean_squared_error: 0.5524\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.56488 to 0.55244, saving model to temp/a4\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5480 - mean_squared_error: 0.5480 - val_loss: 0.5609 - val_mean_squared_error: 0.5609\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.55244\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5471 - mean_squared_error: 0.5471 - val_loss: 0.5359 - val_mean_squared_error: 0.5359\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.55244 to 0.53590, saving model to temp/a4\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.5498 - mean_squared_error: 0.5498 - val_loss: 0.5324 - val_mean_squared_error: 0.5324\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.53590 to 0.53244, saving model to temp/a4\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5464 - mean_squared_error: 0.5464 - val_loss: 0.5550 - val_mean_squared_error: 0.5550\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.53244\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.5408 - mean_squared_error: 0.5408 - val_loss: 0.5454 - val_mean_squared_error: 0.5454\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.53244\n",
      "Epoch 00008: early stopping\n",
      "temp/a5\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 142us/step - loss: 0.7247 - mean_squared_error: 0.7247 - val_loss: 0.5972 - val_mean_squared_error: 0.5972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.59724, saving model to temp/a5\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 126us/step - loss: 0.5600 - mean_squared_error: 0.5600 - val_loss: 0.5438 - val_mean_squared_error: 0.5438\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.59724 to 0.54378, saving model to temp/a5\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.5526 - mean_squared_error: 0.5526 - val_loss: 0.5426 - val_mean_squared_error: 0.5426\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54378 to 0.54257, saving model to temp/a5\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5502 - mean_squared_error: 0.5502 - val_loss: 0.5640 - val_mean_squared_error: 0.5640\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54257\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5487 - mean_squared_error: 0.5487 - val_loss: 0.5744 - val_mean_squared_error: 0.5744\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54257\n",
      "Epoch 00005: early stopping\n",
      "temp/a6\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.6933 - mean_squared_error: 0.6933 - val_loss: 0.5466 - val_mean_squared_error: 0.5466\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.54663, saving model to temp/a6\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.5580 - mean_squared_error: 0.5580 - val_loss: 0.5824 - val_mean_squared_error: 0.5824\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.54663\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5511 - mean_squared_error: 0.5511 - val_loss: 0.5588 - val_mean_squared_error: 0.5588\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54663\n",
      "Epoch 00003: early stopping\n",
      "temp/a7\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.7229 - mean_squared_error: 0.7229 - val_loss: 0.5615 - val_mean_squared_error: 0.5615\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56155, saving model to temp/a7\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5563 - mean_squared_error: 0.5563 - val_loss: 0.5505 - val_mean_squared_error: 0.5505\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.56155 to 0.55045, saving model to temp/a7\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.5579 - mean_squared_error: 0.5579 - val_loss: 0.5353 - val_mean_squared_error: 0.5353\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55045 to 0.53529, saving model to temp/a7\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.5484 - mean_squared_error: 0.5484 - val_loss: 0.5415 - val_mean_squared_error: 0.5415\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53529\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5507 - mean_squared_error: 0.5507 - val_loss: 0.5353 - val_mean_squared_error: 0.5353\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53529\n",
      "Epoch 00005: early stopping\n",
      "temp/a8\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 142us/step - loss: 0.7143 - mean_squared_error: 0.7143 - val_loss: 0.5510 - val_mean_squared_error: 0.5510\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55101, saving model to temp/a8\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5622 - mean_squared_error: 0.5622 - val_loss: 0.5553 - val_mean_squared_error: 0.5553\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.55101\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.5532 - mean_squared_error: 0.5532 - val_loss: 0.5425 - val_mean_squared_error: 0.5425\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55101 to 0.54249, saving model to temp/a8\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 126us/step - loss: 0.5493 - mean_squared_error: 0.5493 - val_loss: 0.5470 - val_mean_squared_error: 0.5470\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54249\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5538 - mean_squared_error: 0.5538 - val_loss: 0.5461 - val_mean_squared_error: 0.5461\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54249\n",
      "Epoch 00005: early stopping\n",
      "temp/a9\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 0.6966 - mean_squared_error: 0.6966 - val_loss: 0.5616 - val_mean_squared_error: 0.5616\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56157, saving model to temp/a9\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5561 - mean_squared_error: 0.5561 - val_loss: 0.5468 - val_mean_squared_error: 0.5468\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.56157 to 0.54678, saving model to temp/a9\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.5546 - mean_squared_error: 0.5546 - val_loss: 0.5772 - val_mean_squared_error: 0.5772\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54678\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5501 - mean_squared_error: 0.5501 - val_loss: 0.5702 - val_mean_squared_error: 0.5702\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54678\n",
      "Epoch 00004: early stopping\n",
      "temp/a10\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 0.7358 - mean_squared_error: 0.7358 - val_loss: 0.5620 - val_mean_squared_error: 0.5620\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56200, saving model to temp/a10\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5592 - mean_squared_error: 0.5592 - val_loss: 0.5567 - val_mean_squared_error: 0.5567\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.56200 to 0.55666, saving model to temp/a10\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5527 - mean_squared_error: 0.5527 - val_loss: 0.5888 - val_mean_squared_error: 0.5888\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.55666\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.5531 - mean_squared_error: 0.5531 - val_loss: 0.5472 - val_mean_squared_error: 0.5472\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.55666 to 0.54716, saving model to temp/a10\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 0.5505 - mean_squared_error: 0.5505 - val_loss: 0.5483 - val_mean_squared_error: 0.5483\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54716\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5425 - mean_squared_error: 0.5425 - val_loss: 0.5484 - val_mean_squared_error: 0.5484\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.54716\n",
      "Epoch 00006: early stopping\n",
      "temp/a11\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 0.7126 - mean_squared_error: 0.7126 - val_loss: 0.5571 - val_mean_squared_error: 0.5571\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55706, saving model to temp/a11\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5613 - mean_squared_error: 0.5613 - val_loss: 0.5639 - val_mean_squared_error: 0.5639\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.55706\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5535 - mean_squared_error: 0.5535 - val_loss: 0.5446 - val_mean_squared_error: 0.5446\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55706 to 0.54455, saving model to temp/a11\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5513 - mean_squared_error: 0.5513 - val_loss: 0.5789 - val_mean_squared_error: 0.5789\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54455\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 0.5482 - mean_squared_error: 0.5482 - val_loss: 0.5554 - val_mean_squared_error: 0.5554\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54455\n",
      "Epoch 00005: early stopping\n",
      "temp/a12\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 147us/step - loss: 0.6983 - mean_squared_error: 0.6983 - val_loss: 0.5734 - val_mean_squared_error: 0.5734\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57343, saving model to temp/a12\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5558 - mean_squared_error: 0.5558 - val_loss: 0.5761 - val_mean_squared_error: 0.5761\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.57343\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.5516 - mean_squared_error: 0.5516 - val_loss: 0.5424 - val_mean_squared_error: 0.5424\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.57343 to 0.54245, saving model to temp/a12\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 0.5541 - mean_squared_error: 0.5541 - val_loss: 0.5444 - val_mean_squared_error: 0.5444\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54245\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5481 - mean_squared_error: 0.5481 - val_loss: 0.5340 - val_mean_squared_error: 0.5340\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.54245 to 0.53399, saving model to temp/a12\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5483 - mean_squared_error: 0.5483 - val_loss: 0.5348 - val_mean_squared_error: 0.5348\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53399\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 0.5457 - mean_squared_error: 0.5457 - val_loss: 0.5410 - val_mean_squared_error: 0.5410\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.53399\n",
      "Epoch 00007: early stopping\n",
      "temp/a13\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.7038 - mean_squared_error: 0.7038 - val_loss: 0.5745 - val_mean_squared_error: 0.5745\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57449, saving model to temp/a13\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 0.5555 - mean_squared_error: 0.5555 - val_loss: 0.5385 - val_mean_squared_error: 0.5385\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.57449 to 0.53849, saving model to temp/a13\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5528 - mean_squared_error: 0.5528 - val_loss: 0.5602 - val_mean_squared_error: 0.5602\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.53849\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.5529 - mean_squared_error: 0.5529 - val_loss: 0.5316 - val_mean_squared_error: 0.5316\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.53849 to 0.53160, saving model to temp/a13\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5492 - mean_squared_error: 0.5492 - val_loss: 0.5405 - val_mean_squared_error: 0.5405\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53160\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5467 - mean_squared_error: 0.5467 - val_loss: 0.5344 - val_mean_squared_error: 0.5344\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53160\n",
      "Epoch 00006: early stopping\n",
      "temp/a14\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 0.7195 - mean_squared_error: 0.7195 - val_loss: 0.5494 - val_mean_squared_error: 0.5494\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.54937, saving model to temp/a14\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5557 - mean_squared_error: 0.5557 - val_loss: 0.5633 - val_mean_squared_error: 0.5633\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.54937\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5554 - mean_squared_error: 0.5554 - val_loss: 0.5396 - val_mean_squared_error: 0.5396\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54937 to 0.53961, saving model to temp/a14\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5499 - mean_squared_error: 0.5499 - val_loss: 0.5398 - val_mean_squared_error: 0.5398\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53961\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 0.5552 - mean_squared_error: 0.5552 - val_loss: 0.5402 - val_mean_squared_error: 0.5402\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53961\n",
      "Epoch 00005: early stopping\n",
      "temp/a15\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.7242 - mean_squared_error: 0.7242 - val_loss: 0.5598 - val_mean_squared_error: 0.5598\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55977, saving model to temp/a15\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5552 - mean_squared_error: 0.5552 - val_loss: 0.5429 - val_mean_squared_error: 0.5429\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55977 to 0.54295, saving model to temp/a15\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5531 - mean_squared_error: 0.5531 - val_loss: 0.5573 - val_mean_squared_error: 0.5573\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54295\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.5519 - mean_squared_error: 0.5519 - val_loss: 0.5741 - val_mean_squared_error: 0.5741\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54295\n",
      "Epoch 00004: early stopping\n",
      "temp/a16\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 142us/step - loss: 0.7294 - mean_squared_error: 0.7294 - val_loss: 0.5584 - val_mean_squared_error: 0.5584\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55838, saving model to temp/a16\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5590 - mean_squared_error: 0.5590 - val_loss: 0.5618 - val_mean_squared_error: 0.5618\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.55838\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5522 - mean_squared_error: 0.5522 - val_loss: 0.5464 - val_mean_squared_error: 0.5464\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55838 to 0.54635, saving model to temp/a16\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 126us/step - loss: 0.5494 - mean_squared_error: 0.5494 - val_loss: 0.5377 - val_mean_squared_error: 0.5377\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.54635 to 0.53767, saving model to temp/a16\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.5435 - mean_squared_error: 0.5435 - val_loss: 0.5369 - val_mean_squared_error: 0.5369\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.53767 to 0.53685, saving model to temp/a16\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 125us/step - loss: 0.5415 - mean_squared_error: 0.5415 - val_loss: 0.5417 - val_mean_squared_error: 0.5417\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53685\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 126us/step - loss: 0.5440 - mean_squared_error: 0.5440 - val_loss: 0.5455 - val_mean_squared_error: 0.5455\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.53685\n",
      "Epoch 00007: early stopping\n",
      "temp/a17\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 142us/step - loss: 0.6935 - mean_squared_error: 0.6935 - val_loss: 0.5475 - val_mean_squared_error: 0.5475\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.54752, saving model to temp/a17\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5589 - mean_squared_error: 0.5589 - val_loss: 0.5558 - val_mean_squared_error: 0.5558\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.54752\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5561 - mean_squared_error: 0.5561 - val_loss: 0.5538 - val_mean_squared_error: 0.5538\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54752\n",
      "Epoch 00003: early stopping\n",
      "temp/a18\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 0.7209 - mean_squared_error: 0.7209 - val_loss: 0.5547 - val_mean_squared_error: 0.5547\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55467, saving model to temp/a18\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5601 - mean_squared_error: 0.5601 - val_loss: 0.5421 - val_mean_squared_error: 0.5421\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55467 to 0.54208, saving model to temp/a18\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.5519 - mean_squared_error: 0.5519 - val_loss: 0.5378 - val_mean_squared_error: 0.5378\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54208 to 0.53784, saving model to temp/a18\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 0.5510 - mean_squared_error: 0.5510 - val_loss: 0.5720 - val_mean_squared_error: 0.5720\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53784\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5499 - mean_squared_error: 0.5499 - val_loss: 0.5401 - val_mean_squared_error: 0.5401\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53784\n",
      "Epoch 00005: early stopping\n",
      "temp/a19\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 0.6929 - mean_squared_error: 0.6929 - val_loss: 0.5523 - val_mean_squared_error: 0.5523\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55227, saving model to temp/a19\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5606 - mean_squared_error: 0.5606 - val_loss: 0.5434 - val_mean_squared_error: 0.5434\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55227 to 0.54342, saving model to temp/a19\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5522 - mean_squared_error: 0.5522 - val_loss: 0.5386 - val_mean_squared_error: 0.5386\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54342 to 0.53860, saving model to temp/a19\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.5501 - mean_squared_error: 0.5501 - val_loss: 0.5695 - val_mean_squared_error: 0.5695\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53860\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5439 - mean_squared_error: 0.5439 - val_loss: 0.5374 - val_mean_squared_error: 0.5374\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.53860 to 0.53743, saving model to temp/a19\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 0.5474 - mean_squared_error: 0.5474 - val_loss: 0.5424 - val_mean_squared_error: 0.5424\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53743\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.5426 - mean_squared_error: 0.5426 - val_loss: 0.5369 - val_mean_squared_error: 0.5369\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.53743 to 0.53692, saving model to temp/a19\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5404 - mean_squared_error: 0.5404 - val_loss: 0.5359 - val_mean_squared_error: 0.5359\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.53692 to 0.53593, saving model to temp/a19\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5467 - mean_squared_error: 0.5467 - val_loss: 0.5408 - val_mean_squared_error: 0.5408\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.53593\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5428 - mean_squared_error: 0.5428 - val_loss: 0.5313 - val_mean_squared_error: 0.5313\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.53593 to 0.53127, saving model to temp/a19\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 125us/step - loss: 0.5423 - mean_squared_error: 0.5423 - val_loss: 0.5496 - val_mean_squared_error: 0.5496\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.53127\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.5406 - mean_squared_error: 0.5406 - val_loss: 0.5276 - val_mean_squared_error: 0.5276\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.53127 to 0.52760, saving model to temp/a19\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5333 - mean_squared_error: 0.5333 - val_loss: 0.5280 - val_mean_squared_error: 0.5280\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.52760\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 2s 123us/step - loss: 0.5422 - mean_squared_error: 0.5422 - val_loss: 0.5286 - val_mean_squared_error: 0.5286\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.52760\n",
      "Epoch 00014: early stopping\n",
      "temp/a20\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.6916 - mean_squared_error: 0.6916 - val_loss: 0.5507 - val_mean_squared_error: 0.5507\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55066, saving model to temp/a20\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5564 - mean_squared_error: 0.5564 - val_loss: 0.5448 - val_mean_squared_error: 0.5448\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55066 to 0.54482, saving model to temp/a20\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 0.5604 - mean_squared_error: 0.5604 - val_loss: 0.5381 - val_mean_squared_error: 0.5381\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54482 to 0.53810, saving model to temp/a20\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.5514 - mean_squared_error: 0.5514 - val_loss: 0.5395 - val_mean_squared_error: 0.5395\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53810\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.5483 - mean_squared_error: 0.5483 - val_loss: 0.5478 - val_mean_squared_error: 0.5478\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53810\n",
      "Epoch 00005: early stopping\n",
      "temp/a21\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 0.7107 - mean_squared_error: 0.7107 - val_loss: 0.5577 - val_mean_squared_error: 0.5577\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55770, saving model to temp/a21\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5569 - mean_squared_error: 0.5569 - val_loss: 0.5419 - val_mean_squared_error: 0.5419\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55770 to 0.54186, saving model to temp/a21\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5487 - mean_squared_error: 0.5487 - val_loss: 0.5449 - val_mean_squared_error: 0.5449\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54186\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5510 - mean_squared_error: 0.5510 - val_loss: 0.5396 - val_mean_squared_error: 0.5396\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.54186 to 0.53961, saving model to temp/a21\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5470 - mean_squared_error: 0.5470 - val_loss: 0.5346 - val_mean_squared_error: 0.5346\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.53961 to 0.53464, saving model to temp/a21\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5444 - mean_squared_error: 0.5444 - val_loss: 0.5370 - val_mean_squared_error: 0.5370\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53464\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5470 - mean_squared_error: 0.5470 - val_loss: 0.5729 - val_mean_squared_error: 0.5729\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.53464\n",
      "Epoch 00007: early stopping\n",
      "temp/a22\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.7004 - mean_squared_error: 0.7004 - val_loss: 0.5536 - val_mean_squared_error: 0.5536\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55364, saving model to temp/a22\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5597 - mean_squared_error: 0.5597 - val_loss: 0.6118 - val_mean_squared_error: 0.6118\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.55364\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5608 - mean_squared_error: 0.5608 - val_loss: 0.5427 - val_mean_squared_error: 0.5427\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55364 to 0.54274, saving model to temp/a22\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5522 - mean_squared_error: 0.5522 - val_loss: 0.5394 - val_mean_squared_error: 0.5394\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.54274 to 0.53943, saving model to temp/a22\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.5491 - mean_squared_error: 0.5491 - val_loss: 0.5432 - val_mean_squared_error: 0.5432\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53943\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5448 - mean_squared_error: 0.5448 - val_loss: 0.5509 - val_mean_squared_error: 0.5509\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53943\n",
      "Epoch 00006: early stopping\n",
      "temp/a23\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.7152 - mean_squared_error: 0.7152 - val_loss: 0.5627 - val_mean_squared_error: 0.5627\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56271, saving model to temp/a23\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5593 - mean_squared_error: 0.5593 - val_loss: 0.5788 - val_mean_squared_error: 0.5788\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.56271\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5610 - mean_squared_error: 0.5610 - val_loss: 0.5414 - val_mean_squared_error: 0.5414\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.56271 to 0.54135, saving model to temp/a23\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5521 - mean_squared_error: 0.5521 - val_loss: 0.5580 - val_mean_squared_error: 0.5580\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54135\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.5476 - mean_squared_error: 0.5476 - val_loss: 0.5436 - val_mean_squared_error: 0.5436\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54135\n",
      "Epoch 00005: early stopping\n",
      "temp/a24\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.7102 - mean_squared_error: 0.7102 - val_loss: 0.5609 - val_mean_squared_error: 0.5609\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56094, saving model to temp/a24\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5590 - mean_squared_error: 0.5590 - val_loss: 0.5468 - val_mean_squared_error: 0.5468\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.56094 to 0.54684, saving model to temp/a24\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5556 - mean_squared_error: 0.5556 - val_loss: 0.5502 - val_mean_squared_error: 0.5502\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54684\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.5497 - mean_squared_error: 0.5497 - val_loss: 0.5475 - val_mean_squared_error: 0.5475\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54684\n",
      "Epoch 00004: early stopping\n",
      "temp/a25\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.7062 - mean_squared_error: 0.7062 - val_loss: 0.5676 - val_mean_squared_error: 0.5676\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56755, saving model to temp/a25\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5621 - mean_squared_error: 0.5621 - val_loss: 0.5506 - val_mean_squared_error: 0.5506\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.56755 to 0.55057, saving model to temp/a25\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5496 - mean_squared_error: 0.5496 - val_loss: 0.5688 - val_mean_squared_error: 0.5688\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.55057\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5557 - mean_squared_error: 0.5557 - val_loss: 0.5481 - val_mean_squared_error: 0.5481\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.55057 to 0.54808, saving model to temp/a25\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.5494 - mean_squared_error: 0.5494 - val_loss: 0.5480 - val_mean_squared_error: 0.5480\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.54808 to 0.54800, saving model to temp/a25\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5483 - mean_squared_error: 0.5483 - val_loss: 0.5296 - val_mean_squared_error: 0.5296\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.54800 to 0.52958, saving model to temp/a25\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5419 - mean_squared_error: 0.5419 - val_loss: 0.5431 - val_mean_squared_error: 0.5431\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.52958\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5422 - mean_squared_error: 0.5422 - val_loss: 0.5428 - val_mean_squared_error: 0.5428\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.52958\n",
      "Epoch 00008: early stopping\n",
      "temp/a26\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.7264 - mean_squared_error: 0.7264 - val_loss: 0.5489 - val_mean_squared_error: 0.5489\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.54894, saving model to temp/a26\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5594 - mean_squared_error: 0.5594 - val_loss: 0.5423 - val_mean_squared_error: 0.5423\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.54894 to 0.54230, saving model to temp/a26\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5514 - mean_squared_error: 0.5514 - val_loss: 0.5529 - val_mean_squared_error: 0.5529\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54230\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5487 - mean_squared_error: 0.5487 - val_loss: 0.5440 - val_mean_squared_error: 0.5440\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54230\n",
      "Epoch 00004: early stopping\n",
      "temp/a27\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.7083 - mean_squared_error: 0.7083 - val_loss: 0.5536 - val_mean_squared_error: 0.5536\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55364, saving model to temp/a27\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5653 - mean_squared_error: 0.5653 - val_loss: 0.5506 - val_mean_squared_error: 0.5506\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55364 to 0.55056, saving model to temp/a27\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5501 - mean_squared_error: 0.5501 - val_loss: 0.5839 - val_mean_squared_error: 0.5839\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.55056\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5479 - mean_squared_error: 0.5479 - val_loss: 0.5362 - val_mean_squared_error: 0.5362\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.55056 to 0.53623, saving model to temp/a27\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5468 - mean_squared_error: 0.5468 - val_loss: 0.5400 - val_mean_squared_error: 0.5400\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53623\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5469 - mean_squared_error: 0.5469 - val_loss: 0.5373 - val_mean_squared_error: 0.5373\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53623\n",
      "Epoch 00006: early stopping\n",
      "temp/a28\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.7129 - mean_squared_error: 0.7129 - val_loss: 0.5566 - val_mean_squared_error: 0.5566\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55655, saving model to temp/a28\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5612 - mean_squared_error: 0.5612 - val_loss: 0.5670 - val_mean_squared_error: 0.5670\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.55655\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.5520 - mean_squared_error: 0.5520 - val_loss: 0.5407 - val_mean_squared_error: 0.5407\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55655 to 0.54067, saving model to temp/a28\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5535 - mean_squared_error: 0.5535 - val_loss: 0.5349 - val_mean_squared_error: 0.5349\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.54067 to 0.53488, saving model to temp/a28\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5517 - mean_squared_error: 0.5517 - val_loss: 0.5557 - val_mean_squared_error: 0.5557\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53488\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5440 - mean_squared_error: 0.5440 - val_loss: 0.5394 - val_mean_squared_error: 0.5394\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53488\n",
      "Epoch 00006: early stopping\n",
      "temp/a29\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.7070 - mean_squared_error: 0.7070 - val_loss: 0.5711 - val_mean_squared_error: 0.5711\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57114, saving model to temp/a29\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5640 - mean_squared_error: 0.5640 - val_loss: 0.5498 - val_mean_squared_error: 0.5498\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.57114 to 0.54978, saving model to temp/a29\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5569 - mean_squared_error: 0.5569 - val_loss: 0.5383 - val_mean_squared_error: 0.5383\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54978 to 0.53829, saving model to temp/a29\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5476 - mean_squared_error: 0.5476 - val_loss: 0.5481 - val_mean_squared_error: 0.5481\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53829\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5511 - mean_squared_error: 0.5511 - val_loss: 0.5567 - val_mean_squared_error: 0.5567\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53829\n",
      "Epoch 00005: early stopping\n",
      "temp/a30\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.6990 - mean_squared_error: 0.6990 - val_loss: 0.5592 - val_mean_squared_error: 0.5592\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55916, saving model to temp/a30\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.5576 - mean_squared_error: 0.5576 - val_loss: 0.5470 - val_mean_squared_error: 0.5470\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55916 to 0.54703, saving model to temp/a30\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5524 - mean_squared_error: 0.5524 - val_loss: 0.5384 - val_mean_squared_error: 0.5384\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54703 to 0.53841, saving model to temp/a30\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5502 - mean_squared_error: 0.5502 - val_loss: 0.5508 - val_mean_squared_error: 0.5508\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53841\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5493 - mean_squared_error: 0.5493 - val_loss: 0.5500 - val_mean_squared_error: 0.5500\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53841\n",
      "Epoch 00005: early stopping\n",
      "temp/a31\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.7199 - mean_squared_error: 0.7199 - val_loss: 0.5556 - val_mean_squared_error: 0.5556\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55563, saving model to temp/a31\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5606 - mean_squared_error: 0.5606 - val_loss: 0.5934 - val_mean_squared_error: 0.5934\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.55563\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5541 - mean_squared_error: 0.5541 - val_loss: 0.5511 - val_mean_squared_error: 0.5511\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55563 to 0.55108, saving model to temp/a31\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5511 - mean_squared_error: 0.5511 - val_loss: 0.5730 - val_mean_squared_error: 0.5730\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.55108\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5511 - mean_squared_error: 0.5511 - val_loss: 0.5448 - val_mean_squared_error: 0.5448\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.55108 to 0.54482, saving model to temp/a31\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5541 - mean_squared_error: 0.5541 - val_loss: 0.5399 - val_mean_squared_error: 0.5399\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.54482 to 0.53985, saving model to temp/a31\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5435 - mean_squared_error: 0.5435 - val_loss: 0.5394 - val_mean_squared_error: 0.5394\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.53985 to 0.53939, saving model to temp/a31\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5461 - mean_squared_error: 0.5461 - val_loss: 0.5425 - val_mean_squared_error: 0.5425\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.53939\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5398 - mean_squared_error: 0.5398 - val_loss: 0.5395 - val_mean_squared_error: 0.5395\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.53939\n",
      "Epoch 00009: early stopping\n",
      "temp/a32\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 0.6983 - mean_squared_error: 0.6983 - val_loss: 0.5824 - val_mean_squared_error: 0.5824\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.58240, saving model to temp/a32\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5619 - mean_squared_error: 0.5619 - val_loss: 0.5671 - val_mean_squared_error: 0.5671\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.58240 to 0.56710, saving model to temp/a32\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5519 - mean_squared_error: 0.5519 - val_loss: 0.5398 - val_mean_squared_error: 0.5398\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.56710 to 0.53983, saving model to temp/a32\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5504 - mean_squared_error: 0.5504 - val_loss: 0.5390 - val_mean_squared_error: 0.5390\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.53983 to 0.53899, saving model to temp/a32\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5517 - mean_squared_error: 0.5517 - val_loss: 0.5518 - val_mean_squared_error: 0.5518\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53899\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5429 - mean_squared_error: 0.5429 - val_loss: 0.5356 - val_mean_squared_error: 0.5356\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.53899 to 0.53564, saving model to temp/a32\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5446 - mean_squared_error: 0.5446 - val_loss: 0.5342 - val_mean_squared_error: 0.5342\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.53564 to 0.53417, saving model to temp/a32\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5460 - mean_squared_error: 0.5460 - val_loss: 0.5338 - val_mean_squared_error: 0.5338\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.53417 to 0.53377, saving model to temp/a32\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5437 - mean_squared_error: 0.5437 - val_loss: 0.5476 - val_mean_squared_error: 0.5476\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.53377\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5426 - mean_squared_error: 0.5426 - val_loss: 0.5399 - val_mean_squared_error: 0.5399\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.53377\n",
      "Epoch 00010: early stopping\n",
      "temp/a33\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.7183 - mean_squared_error: 0.7183 - val_loss: 0.5691 - val_mean_squared_error: 0.5691\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56906, saving model to temp/a33\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5562 - mean_squared_error: 0.5562 - val_loss: 0.5460 - val_mean_squared_error: 0.5460\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.56906 to 0.54596, saving model to temp/a33\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5477 - mean_squared_error: 0.5477 - val_loss: 0.5443 - val_mean_squared_error: 0.5443\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54596 to 0.54428, saving model to temp/a33\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5481 - mean_squared_error: 0.5481 - val_loss: 0.5454 - val_mean_squared_error: 0.5454\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54428\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5457 - mean_squared_error: 0.5457 - val_loss: 0.5418 - val_mean_squared_error: 0.5418\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.54428 to 0.54180, saving model to temp/a33\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5448 - mean_squared_error: 0.5448 - val_loss: 0.5446 - val_mean_squared_error: 0.5446\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.54180\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.5457 - mean_squared_error: 0.5457 - val_loss: 0.5389 - val_mean_squared_error: 0.5389\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.54180 to 0.53886, saving model to temp/a33\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5426 - mean_squared_error: 0.5426 - val_loss: 0.5519 - val_mean_squared_error: 0.5519\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.53886\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5427 - mean_squared_error: 0.5427 - val_loss: 0.5322 - val_mean_squared_error: 0.5322\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.53886 to 0.53217, saving model to temp/a33\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5402 - mean_squared_error: 0.5402 - val_loss: 0.5554 - val_mean_squared_error: 0.5554\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.53217\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5388 - mean_squared_error: 0.5388 - val_loss: 0.5348 - val_mean_squared_error: 0.5348\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.53217\n",
      "Epoch 00011: early stopping\n",
      "temp/a34\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.7090 - mean_squared_error: 0.7090 - val_loss: 0.5781 - val_mean_squared_error: 0.5781\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57811, saving model to temp/a34\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5581 - mean_squared_error: 0.5581 - val_loss: 0.5577 - val_mean_squared_error: 0.5577\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.57811 to 0.55773, saving model to temp/a34\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5539 - mean_squared_error: 0.5539 - val_loss: 0.5409 - val_mean_squared_error: 0.5409\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55773 to 0.54086, saving model to temp/a34\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5486 - mean_squared_error: 0.5486 - val_loss: 0.5424 - val_mean_squared_error: 0.5424\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54086\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5511 - mean_squared_error: 0.5511 - val_loss: 0.5678 - val_mean_squared_error: 0.5678\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54086\n",
      "Epoch 00005: early stopping\n",
      "temp/a35\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.6984 - mean_squared_error: 0.6984 - val_loss: 0.5528 - val_mean_squared_error: 0.5528\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55277, saving model to temp/a35\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5598 - mean_squared_error: 0.5598 - val_loss: 0.5458 - val_mean_squared_error: 0.5458\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55277 to 0.54576, saving model to temp/a35\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5534 - mean_squared_error: 0.5534 - val_loss: 0.5452 - val_mean_squared_error: 0.5452\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54576 to 0.54517, saving model to temp/a35\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5501 - mean_squared_error: 0.5501 - val_loss: 0.5598 - val_mean_squared_error: 0.5598\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54517\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5452 - mean_squared_error: 0.5452 - val_loss: 0.5388 - val_mean_squared_error: 0.5388\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.54517 to 0.53883, saving model to temp/a35\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5462 - mean_squared_error: 0.5462 - val_loss: 0.5342 - val_mean_squared_error: 0.5342\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.53883 to 0.53417, saving model to temp/a35\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5439 - mean_squared_error: 0.5439 - val_loss: 0.5362 - val_mean_squared_error: 0.5362\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.53417\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5412 - mean_squared_error: 0.5412 - val_loss: 0.5424 - val_mean_squared_error: 0.5424\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.53417\n",
      "Epoch 00008: early stopping\n",
      "temp/a36\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 0.7105 - mean_squared_error: 0.7105 - val_loss: 0.5463 - val_mean_squared_error: 0.5463\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.54631, saving model to temp/a36\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5570 - mean_squared_error: 0.5570 - val_loss: 0.5492 - val_mean_squared_error: 0.5492\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.54631\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5542 - mean_squared_error: 0.5542 - val_loss: 0.5396 - val_mean_squared_error: 0.5396\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54631 to 0.53955, saving model to temp/a36\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5465 - mean_squared_error: 0.5465 - val_loss: 0.5368 - val_mean_squared_error: 0.5368\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.53955 to 0.53676, saving model to temp/a36\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5487 - mean_squared_error: 0.5487 - val_loss: 0.5511 - val_mean_squared_error: 0.5511\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53676\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5468 - mean_squared_error: 0.5468 - val_loss: 0.5414 - val_mean_squared_error: 0.5414\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53676\n",
      "Epoch 00006: early stopping\n",
      "temp/a37\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.7009 - mean_squared_error: 0.7009 - val_loss: 0.5743 - val_mean_squared_error: 0.5743\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57435, saving model to temp/a37\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5606 - mean_squared_error: 0.5606 - val_loss: 0.5641 - val_mean_squared_error: 0.5641\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.57435 to 0.56410, saving model to temp/a37\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5550 - mean_squared_error: 0.5550 - val_loss: 0.5660 - val_mean_squared_error: 0.5660\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.56410\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5502 - mean_squared_error: 0.5502 - val_loss: 0.5371 - val_mean_squared_error: 0.5371\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.56410 to 0.53709, saving model to temp/a37\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5498 - mean_squared_error: 0.5498 - val_loss: 0.5546 - val_mean_squared_error: 0.5546\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53709\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5454 - mean_squared_error: 0.5454 - val_loss: 0.5464 - val_mean_squared_error: 0.5464\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53709\n",
      "Epoch 00006: early stopping\n",
      "temp/a38\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.7433 - mean_squared_error: 0.7433 - val_loss: 0.5571 - val_mean_squared_error: 0.5571\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55711, saving model to temp/a38\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5601 - mean_squared_error: 0.5601 - val_loss: 0.5616 - val_mean_squared_error: 0.5616\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.55711\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.5506 - mean_squared_error: 0.5506 - val_loss: 0.5656 - val_mean_squared_error: 0.5656\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.55711\n",
      "Epoch 00003: early stopping\n",
      "temp/a39\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 0.7076 - mean_squared_error: 0.7076 - val_loss: 0.5813 - val_mean_squared_error: 0.5813\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.58128, saving model to temp/a39\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5647 - mean_squared_error: 0.5647 - val_loss: 0.5564 - val_mean_squared_error: 0.5564\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.58128 to 0.55638, saving model to temp/a39\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.5494 - mean_squared_error: 0.5494 - val_loss: 0.5541 - val_mean_squared_error: 0.5541\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55638 to 0.55406, saving model to temp/a39\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.5524 - mean_squared_error: 0.5524 - val_loss: 0.5416 - val_mean_squared_error: 0.5416\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.55406 to 0.54157, saving model to temp/a39\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5495 - mean_squared_error: 0.5495 - val_loss: 0.5336 - val_mean_squared_error: 0.5336\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.54157 to 0.53360, saving model to temp/a39\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5440 - mean_squared_error: 0.5440 - val_loss: 0.5606 - val_mean_squared_error: 0.5606\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53360\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5458 - mean_squared_error: 0.5458 - val_loss: 0.5369 - val_mean_squared_error: 0.5369\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.53360\n",
      "Epoch 00007: early stopping\n",
      "temp/a40\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.7131 - mean_squared_error: 0.7131 - val_loss: 0.5685 - val_mean_squared_error: 0.5685\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56852, saving model to temp/a40\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5575 - mean_squared_error: 0.5575 - val_loss: 0.5617 - val_mean_squared_error: 0.5617\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.56852 to 0.56171, saving model to temp/a40\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5508 - mean_squared_error: 0.5508 - val_loss: 0.5633 - val_mean_squared_error: 0.5633\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.56171\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5509 - mean_squared_error: 0.5509 - val_loss: 0.5387 - val_mean_squared_error: 0.5387\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.56171 to 0.53873, saving model to temp/a40\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5475 - mean_squared_error: 0.5475 - val_loss: 0.5378 - val_mean_squared_error: 0.5378\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.53873 to 0.53784, saving model to temp/a40\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5433 - mean_squared_error: 0.5433 - val_loss: 0.5325 - val_mean_squared_error: 0.5325\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.53784 to 0.53250, saving model to temp/a40\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5466 - mean_squared_error: 0.5466 - val_loss: 0.5745 - val_mean_squared_error: 0.5745\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.53250\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5479 - mean_squared_error: 0.5479 - val_loss: 0.5630 - val_mean_squared_error: 0.5630\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.53250\n",
      "Epoch 00008: early stopping\n",
      "temp/a41\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.7088 - mean_squared_error: 0.7088 - val_loss: 0.5709 - val_mean_squared_error: 0.5709\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57087, saving model to temp/a41\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.5605 - mean_squared_error: 0.5605 - val_loss: 0.5536 - val_mean_squared_error: 0.5536\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.57087 to 0.55358, saving model to temp/a41\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5552 - mean_squared_error: 0.5552 - val_loss: 0.5474 - val_mean_squared_error: 0.5474\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55358 to 0.54735, saving model to temp/a41\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.5533 - mean_squared_error: 0.5533 - val_loss: 0.5747 - val_mean_squared_error: 0.5747\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54735\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5449 - mean_squared_error: 0.5449 - val_loss: 0.5486 - val_mean_squared_error: 0.5486\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54735\n",
      "Epoch 00005: early stopping\n",
      "temp/a42\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.7146 - mean_squared_error: 0.7146 - val_loss: 0.5665 - val_mean_squared_error: 0.5665\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56648, saving model to temp/a42\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.5610 - mean_squared_error: 0.5610 - val_loss: 0.5603 - val_mean_squared_error: 0.5603\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.56648 to 0.56025, saving model to temp/a42\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5508 - mean_squared_error: 0.5508 - val_loss: 0.5408 - val_mean_squared_error: 0.5408\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.56025 to 0.54077, saving model to temp/a42\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5507 - mean_squared_error: 0.5507 - val_loss: 0.5409 - val_mean_squared_error: 0.5409\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54077\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5512 - mean_squared_error: 0.5512 - val_loss: 0.5549 - val_mean_squared_error: 0.5549\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54077\n",
      "Epoch 00005: early stopping\n",
      "temp/a43\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.7080 - mean_squared_error: 0.7080 - val_loss: 0.5589 - val_mean_squared_error: 0.5589\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55885, saving model to temp/a43\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.5587 - mean_squared_error: 0.5587 - val_loss: 0.5415 - val_mean_squared_error: 0.5415\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55885 to 0.54154, saving model to temp/a43\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5484 - mean_squared_error: 0.5484 - val_loss: 0.5409 - val_mean_squared_error: 0.5409\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54154 to 0.54091, saving model to temp/a43\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5540 - mean_squared_error: 0.5540 - val_loss: 0.5398 - val_mean_squared_error: 0.5398\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.54091 to 0.53980, saving model to temp/a43\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5467 - mean_squared_error: 0.5467 - val_loss: 0.5423 - val_mean_squared_error: 0.5423\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53980\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5445 - mean_squared_error: 0.5445 - val_loss: 0.5479 - val_mean_squared_error: 0.5479\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53980\n",
      "Epoch 00006: early stopping\n",
      "temp/a44\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.7138 - mean_squared_error: 0.7138 - val_loss: 0.5912 - val_mean_squared_error: 0.5912\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.59115, saving model to temp/a44\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5582 - mean_squared_error: 0.5582 - val_loss: 0.5550 - val_mean_squared_error: 0.5550\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.59115 to 0.55496, saving model to temp/a44\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5522 - mean_squared_error: 0.5522 - val_loss: 0.5692 - val_mean_squared_error: 0.5692\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.55496\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.5520 - mean_squared_error: 0.5520 - val_loss: 0.5359 - val_mean_squared_error: 0.5359\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.55496 to 0.53587, saving model to temp/a44\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5458 - mean_squared_error: 0.5458 - val_loss: 0.5350 - val_mean_squared_error: 0.5350\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.53587 to 0.53499, saving model to temp/a44\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5497 - mean_squared_error: 0.5497 - val_loss: 0.5335 - val_mean_squared_error: 0.5335\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.53499 to 0.53351, saving model to temp/a44\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.5452 - mean_squared_error: 0.5452 - val_loss: 0.5380 - val_mean_squared_error: 0.5380\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.53351\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.5435 - mean_squared_error: 0.5435 - val_loss: 0.5460 - val_mean_squared_error: 0.5460\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.53351\n",
      "Epoch 00008: early stopping\n",
      "temp/a45\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.7003 - mean_squared_error: 0.7003 - val_loss: 0.5532 - val_mean_squared_error: 0.5532\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55322, saving model to temp/a45\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5575 - mean_squared_error: 0.5575 - val_loss: 0.5413 - val_mean_squared_error: 0.5413\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55322 to 0.54130, saving model to temp/a45\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5547 - mean_squared_error: 0.5547 - val_loss: 0.5593 - val_mean_squared_error: 0.5593\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54130\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5548 - mean_squared_error: 0.5548 - val_loss: 0.5576 - val_mean_squared_error: 0.5576\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54130\n",
      "Epoch 00004: early stopping\n",
      "temp/a46\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.7102 - mean_squared_error: 0.7102 - val_loss: 0.5625 - val_mean_squared_error: 0.5625\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56249, saving model to temp/a46\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5588 - mean_squared_error: 0.5588 - val_loss: 0.5523 - val_mean_squared_error: 0.5523\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.56249 to 0.55230, saving model to temp/a46\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.5513 - mean_squared_error: 0.5513 - val_loss: 0.5488 - val_mean_squared_error: 0.5488\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55230 to 0.54883, saving model to temp/a46\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5490 - mean_squared_error: 0.5490 - val_loss: 0.5642 - val_mean_squared_error: 0.5642\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54883\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5488 - mean_squared_error: 0.5488 - val_loss: 0.5505 - val_mean_squared_error: 0.5505\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54883\n",
      "Epoch 00005: early stopping\n",
      "temp/a47\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.6899 - mean_squared_error: 0.6899 - val_loss: 0.5578 - val_mean_squared_error: 0.5578\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55781, saving model to temp/a47\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.5587 - mean_squared_error: 0.5587 - val_loss: 0.5514 - val_mean_squared_error: 0.5514\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55781 to 0.55141, saving model to temp/a47\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.5530 - mean_squared_error: 0.5530 - val_loss: 0.5555 - val_mean_squared_error: 0.5555\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.55141\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5519 - mean_squared_error: 0.5519 - val_loss: 0.5445 - val_mean_squared_error: 0.5445\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.55141 to 0.54454, saving model to temp/a47\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.5449 - mean_squared_error: 0.5449 - val_loss: 0.5393 - val_mean_squared_error: 0.5393\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.54454 to 0.53930, saving model to temp/a47\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.5442 - mean_squared_error: 0.5442 - val_loss: 0.5459 - val_mean_squared_error: 0.5459\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53930\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5468 - mean_squared_error: 0.5468 - val_loss: 0.5449 - val_mean_squared_error: 0.5449\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.53930\n",
      "Epoch 00007: early stopping\n",
      "temp/a48\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.7156 - mean_squared_error: 0.7156 - val_loss: 0.5716 - val_mean_squared_error: 0.5716\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57164, saving model to temp/a48\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5592 - mean_squared_error: 0.5592 - val_loss: 0.6046 - val_mean_squared_error: 0.6046\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.57164\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5535 - mean_squared_error: 0.5535 - val_loss: 0.5401 - val_mean_squared_error: 0.5401\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.57164 to 0.54014, saving model to temp/a48\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5488 - mean_squared_error: 0.5488 - val_loss: 0.5502 - val_mean_squared_error: 0.5502\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54014\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5473 - mean_squared_error: 0.5473 - val_loss: 0.5398 - val_mean_squared_error: 0.5398\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.54014 to 0.53976, saving model to temp/a48\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5461 - mean_squared_error: 0.5461 - val_loss: 0.5392 - val_mean_squared_error: 0.5392\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.53976 to 0.53922, saving model to temp/a48\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.5430 - mean_squared_error: 0.5430 - val_loss: 0.5340 - val_mean_squared_error: 0.5340\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.53922 to 0.53397, saving model to temp/a48\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5403 - mean_squared_error: 0.5403 - val_loss: 0.5486 - val_mean_squared_error: 0.5486\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.53397\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5431 - mean_squared_error: 0.5431 - val_loss: 0.5397 - val_mean_squared_error: 0.5397\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.53397\n",
      "Epoch 00009: early stopping\n",
      "temp/a49\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.6911 - mean_squared_error: 0.6911 - val_loss: 0.5509 - val_mean_squared_error: 0.5509\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55095, saving model to temp/a49\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5595 - mean_squared_error: 0.5595 - val_loss: 0.5405 - val_mean_squared_error: 0.5405\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55095 to 0.54051, saving model to temp/a49\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.5476 - mean_squared_error: 0.5476 - val_loss: 0.5442 - val_mean_squared_error: 0.5442\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54051\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 0.5553 - mean_squared_error: 0.5553 - val_loss: 0.5405 - val_mean_squared_error: 0.5405\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54051\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "for idx, model_name in enumerate(model_names):\n",
    "    print(model_name)\n",
    "\n",
    "    if type(models[idx]) is list:\n",
    "        #clear session\n",
    "        keras.backend.clear_session() \n",
    "        #get model according to specification\n",
    "        model = get_model(models[idx], [0.2] * len(models), 6)\n",
    "        callbacks = [ModelCheckpoint(model_name, verbose= verbosity, monitor='val_loss',save_best_only=True), \n",
    "                     EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose= verbosity, mode='auto')]\n",
    "        model.compile(optimizer = optimizers.SGD(lr = 0.001, momentum = 0.9, ), loss='mean_squared_error', metrics = ['mse'])\n",
    "        #print(len(X), len(y))\n",
    "        model.fit(X, y, epochs = 20, validation_data = (x_val, y_val), callbacks = callbacks, batch_size = 32, verbose = verbosity)\n",
    "    else:\n",
    "        models[idx].fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "1 1\n",
      "1 2\n",
      "1 3\n",
      "2 1\n",
      "2 2\n",
      "2 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n            setA = get_MB(get_CG(perturbed_df, tetrad), \\'g\\', pc)\\n            if setA != {\\'f\\'}:\\n                print(\"Error in SETA markov blanket\")\\n                #setA = {\\'f\\'}\\n            setC = get_MB(get_CG(test_df2, tetrad), \\'g\\', pc)\\n\\n            if setA != setC:\\n                causal_dicts[idx][str(m) + \\'_\\' + str(v)].append(1)\\n            else:\\n                causal_dicts[idx][str(m) + \\'_\\' + str(v)].append(0)\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_test = 2000\n",
    "metrics_dicts = []\n",
    "for m in models:\n",
    "    metrics_dicts.append(defaultdict(list))\n",
    "\n",
    "\n",
    "#means = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "#variances = [1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0]\n",
    "means = [0, 1, 2]\n",
    "variances = [1,2,3]\n",
    "\n",
    "\n",
    "# ok at this point we need to check the model on various variances and means\n",
    "for m in means:\n",
    "    for v in variances:\n",
    "        print(m,v)\n",
    "        #t0 = time.time()\n",
    "        perturbed_df = gen_data(mean =m, var = v, SIZE = nb_test)\n",
    "        y_test2 = perturbed_df['g']\n",
    "        x_test2 = perturbed_df[['a', 'b', 'c', 'd', 'e', 'f']]\n",
    "        #t1 = time.time()\n",
    "        #print(\"Time for gen_data = \", t1 - t0)\n",
    "        for idx, model_name in enumerate(model_names):\n",
    "            #t0 = time.time()\n",
    "            if type(models[idx]) is list:\n",
    "                keras.backend.clear_session()\n",
    "                model = load_model(model_name)\n",
    "            else:\n",
    "                model = models[idx]\n",
    "            #t1 = time.time()\n",
    "            #print(\"Time to load model = \", t1 - t0)\n",
    "            \n",
    "            y_pred2 = model.predict(x_test2)\n",
    "            metrics_dicts[idx][str(m) + '_' + str(v)].append(mean_squared_error(y_test2, y_pred2))\n",
    "\n",
    "            test_df2 = pd.DataFrame(x_test2, columns = ['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "            test_targets2 = pd.DataFrame(model.predict(x_test2), columns = ['g'])\n",
    "            test_df2 = test_df2.join(test_targets2)\n",
    "'''\n",
    "            setA = get_MB(get_CG(perturbed_df, tetrad), 'g', pc)\n",
    "            if setA != {'f'}:\n",
    "                print(\"Error in SETA markov blanket\")\n",
    "                #setA = {'f'}\n",
    "            setC = get_MB(get_CG(test_df2, tetrad), 'g', pc)\n",
    "\n",
    "            if setA != setC:\n",
    "                causal_dicts[idx][str(m) + '_' + str(v)].append(1)\n",
    "            else:\n",
    "                causal_dicts[idx][str(m) + '_' + str(v)].append(0)\n",
    "\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times =  0\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26851.810519611947\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26852.645350422383\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26805.570341100338\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26871.90155034392\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26871.764359286914\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26910.224983308515\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26918.288609379764\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26880.842501931274\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26854.154755585027\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -27035.792609436445\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26940.51062736487\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26926.474781460005\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26882.059607480896\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26918.88205797946\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26905.967562401187\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26910.598076800656\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26947.988409183534\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26890.251573785135\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26910.292192772988\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26920.960219025066\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26857.632251778625\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26851.154113691056\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26832.286451633874\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26894.582308969297\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26917.218479183575\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26922.33069000011\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26905.471321678804\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26913.81561258139\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26887.874817858647\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26910.086261581346\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26892.965259957848\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26872.664461445507\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26841.95591049289\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26844.725530895696\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26872.23702524056\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26828.651304843348\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26909.418717366647\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26827.804717956\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26823.4377949927\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26837.66717760815\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26887.068321685387\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -27001.787969072575\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26893.936643627712\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26905.5607785794\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26891.15699754026\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26888.17537178287\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26842.95910213039\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26864.86706167288\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26943.643036987887\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26983.53420171551 -26926.226625868872\n",
      "Times =  1\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26432.106221329996\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26427.81003840689\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26398.718815943517\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26405.701447478954\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26451.999038479782\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26482.529710520674\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26499.658322550444\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26440.119308505436\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26438.180735892507\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26597.79441078688\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26514.5771365526\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26489.563744302784\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26458.255631223197\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26482.83350144591\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26468.90245569986\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26465.67151805113\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26526.02904055887\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26468.93563824077\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26504.674604328553\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26511.560113756634\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26439.85704129254\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26420.887524663238\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26413.111634941353\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26478.69020529407\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26488.649629174753\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26531.898631849643\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26484.90714061149\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26493.823261842328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26462.84297628046\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26459.742973110842\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26446.570395747447\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26441.734656715467\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26418.17403273666\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26431.746485791755\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26438.98288080902\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26420.463365803877\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26477.601650574594\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26414.534258709296\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26422.793158536217\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26412.10934658828\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26455.641261224704\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26556.105534603936\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26473.858988316293\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26472.9075103282\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26467.143081678365\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26451.905741587103\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26427.034093782942\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26435.558848795587\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26514.447562600428\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26535.878792685715 -26503.0720195064\n",
      "Times =  2\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26676.224293790816\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26701.591794354805\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26657.797273083044\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26688.590504330918\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26707.961280122498\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26753.734429535656\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26767.659805896572\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26725.8153429987\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26704.375638651443\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26869.530259382424\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26787.300603911855\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26765.727064548017\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26700.227747466808\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26750.644830206285\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26734.184054977904\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26728.91759988176\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26793.288261724298\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26738.419081645035\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26771.21987901494\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26778.931984384704\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26687.27594330209\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26680.788364644814\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26679.18897356584\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26754.14914010914\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26768.379382509924\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26784.035186648227\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26745.674331245133\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26769.51489784279\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26721.336056632314\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26731.02539982379\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26726.353778815363\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26708.660746719594\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26677.565258151306\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26689.83246900355\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26697.9542524034\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26665.86709438175\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26750.997707098337\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26681.233637015233\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26666.06588773067\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26682.398007453725\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26728.201252345763\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26825.3230097183\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26742.172784646267\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26754.138142322026\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26738.200040858785\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26720.68004776691\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26685.604189609337\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26710.824562489255\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26795.460874657158\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26842.694305267494 -26760.480347320234\n",
      "Times =  3\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26290.005713202336\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26305.91374968658\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26262.334930201665\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26288.511288719736\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26331.431161497334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26376.550611313545\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26381.431566189283\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26321.836143765784\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26316.04042482039\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26470.794029518147\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26387.89202448435\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26375.442709512135\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26323.11331456688\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26351.84557746486\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26345.89292616224\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26342.78545057972\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26406.827742990357\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26336.25670401462\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26361.33860125487\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26390.719339786905\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26300.677657539996\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26300.00926039141\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26287.37417494811\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26366.290825461485\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26386.293332560992\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26381.541573110204\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26356.573426109237\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26380.002770231877\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26366.314242193883\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26344.20623183054\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26329.36674275384\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26311.717231871204\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26289.092283180333\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26291.24553033945\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26321.05446440769\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26296.345863994266\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26341.876381176226\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26290.104724618417\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26282.789735885668\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26285.682242167073\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26340.441804944574\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26425.252620360294\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26354.507300233214\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26350.054545696203\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26342.818907864777\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26333.492212556037\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26292.225019514633\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26345.91155386617\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26410.36894069254\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26465.2015363637 -26365.152965867135\n",
      "Times =  4\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26625.48436188559\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26632.396062545675\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26594.451422217182\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26632.49836338345\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26651.006895080907\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26699.805194298464\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26693.383762514\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26650.030992180837\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26640.273457915893\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26806.49777537738\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26709.33933010007\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26687.690857132548\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26641.731184255717\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26697.842635204845\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26705.928606268208\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26683.27480784763\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26718.540290254965\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26666.79740825581\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26688.19483829833\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26698.942397898638\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26643.230801412334\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26611.743762512495\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26606.425438170823\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26673.953633155907\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26683.878000774188\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26724.94489657814\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26685.107582125846\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26681.69847206274\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26655.243286102603\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26666.600353726557\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26634.48759470018\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26627.79541594893\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26599.473513433644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26602.09003915656\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26653.05559895411\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26593.314003433647\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26678.71600543239\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26604.275750316003\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26615.256180429875\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26625.16676346184\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26647.532776773627\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26761.041977355177\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26658.997520516896\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26693.427739317867\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26663.392061715916\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26650.564243907364\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26621.68702344739\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26637.825762309967\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26727.402005532735\n",
      "['a --> f', 'b --> f', 'e --> d', 'c --> f', 'd --> f', 'f --> g']\n",
      "-26791.58288497656 -26698.047470817124\n"
     ]
    }
   ],
   "source": [
    "#the number of times to sample\n",
    "times = 5\n",
    "\n",
    "## the size of the test set\n",
    "\n",
    "\n",
    "violations = np.zeros(len(models))\n",
    "violation_mean = np.zeros((len(models), times))\n",
    "violation_mean2 = np.zeros((len(models), times))\n",
    "mean = np.zeros((len(models), times))\n",
    "\n",
    "fold = 0\n",
    "\n",
    "from pycausal import prior as p\n",
    "def get_bic(df, prior):\n",
    "\n",
    "    tetrad.run(algoId = 'gfci', dfs = df,  scoreId = 'sem-bic-deterministic', dataType = 'continuous',\n",
    "               structurePrior = 1.0, samplePrior = 1, maxDegree = -1, maxPathLength = -1, priorKnowledge = prior,\n",
    "               completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True,\n",
    "               penaltyDiscount = 200\n",
    "               )\n",
    "    BIC = tetrad.getTetradGraph().getAllAttributes().toString()\n",
    "    BIC = float(BIC.split('=')[-1].split('}')[0])\n",
    "    return BIC #/ len(df)\n",
    "import itertools\n",
    "def get_pairs(lst):\n",
    "    a = set()\n",
    "    for i in itertools.combinations(lst,2):\n",
    "        a.add(i)\n",
    "    return a\n",
    "\n",
    "full_conx = get_pairs(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n",
    "forced_conx = set({('a','f'), ('b','f'),('c','f'),('d','f'),('e','d'),('f','g')})\n",
    "restricted_conx = full_conx.difference(forced_conx)   \n",
    "prior = p.knowledge(requiredirect =  list(map(list, forced_conx)),\n",
    "                       forbiddirect = list(map(list, restricted_conx))\n",
    "                       )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for t in range(times):\n",
    "    print(\"Times = \", t)\n",
    "    df_test = gen_data(SIZE = nb_test)\n",
    "    x_test = df_test[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "    y_test = df_test['g'].values\n",
    "    bic_orig = get_bic(df_test,prior)\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        if type(models[idx]) is list:\n",
    "            keras.backend.clear_session()\n",
    "            model = load_model(model_name)\n",
    "        else:\n",
    "            model = models[idx]\n",
    "        test_df = pd.DataFrame(x_test, columns = ['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "        test_targets = pd.DataFrame(model.predict(x_test), columns = ['g'])\n",
    "        test_df = test_df.join(test_targets)\n",
    "       \n",
    "        mean[idx][t] = mean_squared_error(y_test, model.predict(x_test)) \n",
    "        bic_pred = get_bic(test_df,prior)\n",
    "        print(tetrad.getEdges())\n",
    "        print(bic_orig, bic_pred)\n",
    "        violation_mean[idx][t] = bic_pred\n",
    "        violation_mean2[idx][t] = bic_pred\n",
    "        #print(bic_orig - bic_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_name =  temp/a0 Violations =  0.0\n",
      "Average_violations =  -26575.126221964136 195.47858606315975\n",
      "MSE =  0.538216346800696 0.019925424286091658\n",
      "Model_name =  temp/a1 Violations =  0.0\n",
      "Average_violations =  -26584.071399083266 194.97054880148542\n",
      "MSE =  0.5418335930368255 0.012185610557102508\n",
      "Model_name =  temp/a2 Violations =  0.0\n",
      "Average_violations =  -26543.77455650915 192.01863897535424\n",
      "MSE =  0.5393656580900925 0.013635292191931853\n",
      "Model_name =  temp/a3 Violations =  0.0\n",
      "Average_violations =  -26577.440630851394 207.39691089528506\n",
      "MSE =  0.547464620930221 0.014845793676432484\n",
      "Model_name =  temp/a4 Violations =  0.0\n",
      "Average_violations =  -26602.83254689349 190.8437042955155\n",
      "MSE =  0.5415427056960167 0.013422314961854752\n",
      "Model_name =  temp/a5 Violations =  0.0\n",
      "Average_violations =  -26644.568985795377 191.64123650349552\n",
      "MSE =  0.5443045702833308 0.0170516280954923\n",
      "Model_name =  temp/a6 Violations =  0.0\n",
      "Average_violations =  -26652.08441330601 191.00505669754097\n",
      "MSE =  0.550197124456178 0.0176356945347932\n",
      "Model_name =  temp/a7 Violations =  0.0\n",
      "Average_violations =  -26603.728857876406 200.03627477337926\n",
      "MSE =  0.53916386600168 0.021421127353786724\n",
      "Model_name =  temp/a8 Violations =  0.0\n",
      "Average_violations =  -26590.60500257305 191.56620256824982\n",
      "MSE =  0.5387921359773615 0.016859536566288368\n",
      "Model_name =  temp/a9 Violations =  0.0\n",
      "Average_violations =  -26756.081816900252 200.04722942120446\n",
      "MSE =  0.552571824787029 0.021935649921861002\n",
      "Model_name =  temp/a10 Violations =  0.0\n",
      "Average_violations =  -26667.923944482747 196.0625616415932\n",
      "MSE =  0.5444119357135282 0.01450773052206402\n",
      "Model_name =  temp/a11 Violations =  0.0\n",
      "Average_violations =  -26648.979831391094 196.1468598504047\n",
      "MSE =  0.5419809210093588 0.016393255796294866\n",
      "Model_name =  temp/a12 Violations =  0.0\n",
      "Average_violations =  -26601.0774969987 193.9570122089405\n",
      "MSE =  0.5376707776277099 0.0179319751964035\n",
      "Model_name =  temp/a13 Violations =  0.0\n",
      "Average_violations =  -26640.409720460273 200.54759599741442\n",
      "MSE =  0.5449581514177492 0.013003595417084702\n",
      "Model_name =  temp/a14 Violations =  0.0\n",
      "Average_violations =  -26632.175121101878 199.7113709753532\n",
      "MSE =  0.5453079912372139 0.015018428618867428\n",
      "Model_name =  temp/a15 Violations =  0.0\n",
      "Average_violations =  -26626.249490632174 200.39156329422687\n",
      "MSE =  0.5464275860187945 0.013449666746438719\n",
      "Model_name =  temp/a16 Violations =  0.0\n",
      "Average_violations =  -26678.534748942406 192.0712238841759\n",
      "MSE =  0.5388807544278721 0.018873535154136867\n",
      "Model_name =  temp/a17 Violations =  0.0\n",
      "Average_violations =  -26620.13208118828 196.25473325941738\n",
      "MSE =  0.5546939551755742 0.023237381108515698\n",
      "Model_name =  temp/a18 Violations =  0.0\n",
      "Average_violations =  -26647.144023133937 194.0665973797981\n",
      "MSE =  0.5493719095295017 0.017609136955021185\n",
      "Model_name =  temp/a19 Violations =  0.0\n",
      "Average_violations =  -26660.222810970394 188.84267989333932\n",
      "MSE =  0.5330408577976183 0.015952301625970697\n",
      "Model_name =  temp/a20 Violations =  0.0\n",
      "Average_violations =  -26585.73473906512 194.97821067912622\n",
      "MSE =  0.5415829169347544 0.014484408777104755\n",
      "Model_name =  temp/a21 Violations =  0.0\n",
      "Average_violations =  -26572.916605180602 193.98517259900464\n",
      "MSE =  0.5440720166318995 0.018361161575720257\n",
      "Model_name =  temp/a22 Violations =  0.0\n",
      "Average_violations =  -26563.677334652 193.04645181743712\n",
      "MSE =  0.5473292690257578 0.009826821308345453\n",
      "Model_name =  temp/a23 Violations =  0.0\n",
      "Average_violations =  -26633.533222597976 189.59087186732825\n",
      "MSE =  0.5415771753114145 0.016128412324223098\n",
      "Model_name =  temp/a24 Violations =  0.0\n",
      "Average_violations =  -26648.883764840684 190.85912682900297\n",
      "MSE =  0.5497836289440168 0.014362228784607974\n",
      "Model_name =  temp/a25 Violations =  0.0\n",
      "Average_violations =  -26668.950195637262 190.77132072329286\n",
      "MSE =  0.536410455757901 0.01956239926609412\n",
      "Model_name =  temp/a26 Violations =  0.0\n",
      "Average_violations =  -26635.546760354096 193.88761795807324\n",
      "MSE =  0.5467112435346569 0.015671766013303622\n",
      "Model_name =  temp/a27 Violations =  0.0\n",
      "Average_violations =  -26647.771002912225 190.8675545309489\n",
      "MSE =  0.5431571342389108 0.020399424611198237\n",
      "Model_name =  temp/a28 Violations =  0.0\n",
      "Average_violations =  -26618.722275813583 185.64193198438005\n",
      "MSE =  0.5383430332430045 0.014348732222625945\n",
      "Model_name =  temp/a29 Violations =  0.0\n",
      "Average_violations =  -26622.33224401461 200.18271251507065\n",
      "MSE =  0.5460860932216296 0.020050389093627805\n",
      "Model_name =  temp/a30 Violations =  0.0\n",
      "Average_violations =  -26605.948754394933 199.7945845841675\n",
      "MSE =  0.543530536717254 0.01301894554358114\n",
      "Model_name =  temp/a31 Violations =  0.0\n",
      "Average_violations =  -26592.514502540143 197.38305980233804\n",
      "MSE =  0.541239496390957 0.018582179681363994\n",
      "Model_name =  temp/a32 Violations =  0.0\n",
      "Average_violations =  -26565.252199598966 194.03430195114623\n",
      "MSE =  0.546976123551318 0.01725700672912179\n",
      "Model_name =  temp/a33 Violations =  0.0\n",
      "Average_violations =  -26571.928011037402 193.73343076523534\n",
      "MSE =  0.5339411073003554 0.018210039386165076\n",
      "Model_name =  temp/a34 Violations =  0.0\n",
      "Average_violations =  -26596.656844362955 195.04059634214232\n",
      "MSE =  0.549372788995975 0.01488160130154096\n",
      "Model_name =  temp/a35 Violations =  0.0\n",
      "Average_violations =  -26560.928326491376 186.26371351231285\n",
      "MSE =  0.540076627888635 0.01542611874092121\n",
      "Model_name =  temp/a36 Violations =  0.0\n",
      "Average_violations =  -26631.72209232964 200.65600541469104\n",
      "MSE =  0.5400261189515332 0.01470492265732278\n",
      "Model_name =  temp/a37 Violations =  0.0\n",
      "Average_violations =  -26563.590617722988 190.95370258461662\n",
      "MSE =  0.5431810651843438 0.01959049343340946\n",
      "Model_name =  temp/a38 Violations =  0.0\n",
      "Average_violations =  -26562.068551515025 189.39511249996073\n",
      "MSE =  0.5588422374091533 0.013921047137047508\n",
      "Model_name =  temp/a39 Violations =  0.0\n",
      "Average_violations =  -26568.60470745581 196.50888706958673\n",
      "MSE =  0.542997584277337 0.015639828927006535\n",
      "Model_name =  temp/a40 Violations =  0.0\n",
      "Average_violations =  -26611.77708339481 194.22644047651252\n",
      "MSE =  0.5388700547618285 0.018406990526141878\n",
      "Model_name =  temp/a41 Violations =  0.0\n",
      "Average_violations =  -26713.902222222056 202.84641427354805\n",
      "MSE =  0.5490174842875708 0.017341627191900834\n",
      "Model_name =  temp/a42 Violations =  0.0\n",
      "Average_violations =  -26624.694647468074 191.42708846338954\n",
      "MSE =  0.5416538525383217 0.01693560144659917\n",
      "Model_name =  temp/a43 Violations =  0.0\n",
      "Average_violations =  -26635.217743248744 199.13738105088154\n",
      "MSE =  0.5423404657144957 0.015110722820839672\n",
      "Model_name =  temp/a44 Violations =  0.0\n",
      "Average_violations =  -26620.542217931616 194.71690389314415\n",
      "MSE =  0.5408704263889657 0.012635110164136789\n",
      "Model_name =  temp/a45 Violations =  0.0\n",
      "Average_violations =  -26608.96352352006 196.3273022107781\n",
      "MSE =  0.5429284092769164 0.012428943585742787\n",
      "Model_name =  temp/a46 Violations =  0.0\n",
      "Average_violations =  -26573.90188569694 193.94051410061877\n",
      "MSE =  0.5531611914533132 0.017827168654776075\n",
      "Model_name =  temp/a47 Violations =  0.0\n",
      "Average_violations =  -26598.99755782677 187.33085091413733\n",
      "MSE =  0.540302785640026 0.019784082491830977\n",
      "Model_name =  temp/a48 Violations =  0.0\n",
      "Average_violations =  -26678.264484094147 192.4470172552552\n",
      "MSE =  0.5384209512667573 0.009232516526839828\n",
      "Model_name =  temp/a49 Violations =  0.0\n",
      "Average_violations =  -26650.595885875948 196.74323588939754\n",
      "MSE =  0.5445965783671405 0.015410533953122233\n",
      "[0.53821635 0.54183359 0.53936566 0.54746462 0.54154271 0.54430457\n",
      " 0.55019712 0.53916387 0.53879214 0.55257182 0.54441194 0.54198092\n",
      " 0.53767078 0.54495815 0.54530799 0.54642759 0.53888075 0.55469396\n",
      " 0.54937191 0.53304086 0.54158292 0.54407202 0.54732927 0.54157718\n",
      " 0.54978363 0.53641046 0.54671124 0.54315713 0.53834303 0.54608609\n",
      " 0.54353054 0.5412395  0.54697612 0.53394111 0.54937279 0.54007663\n",
      " 0.54002612 0.54318107 0.55884224 0.54299758 0.53887005 0.54901748\n",
      " 0.54165385 0.54234047 0.54087043 0.54292841 0.55316119 0.54030279\n",
      " 0.53842095 0.54459658] [0.01992542 0.01218561 0.01363529 0.01484579 0.01342231 0.01705163\n",
      " 0.01763569 0.02142113 0.01685954 0.02193565 0.01450773 0.01639326\n",
      " 0.01793198 0.0130036  0.01501843 0.01344967 0.01887354 0.02323738\n",
      " 0.01760914 0.0159523  0.01448441 0.01836116 0.00982682 0.01612841\n",
      " 0.01436223 0.0195624  0.01567177 0.02039942 0.01434873 0.02005039\n",
      " 0.01301895 0.01858218 0.01725701 0.01821004 0.0148816  0.01542612\n",
      " 0.01470492 0.01959049 0.01392105 0.01563983 0.01840699 0.01734163\n",
      " 0.0169356  0.01511072 0.01263511 0.01242894 0.01782717 0.01978408\n",
      " 0.00923252 0.01541053] [-26575.12622196 -26584.07139908 -26543.77455651 -26577.44063085\n",
      " -26602.83254689 -26644.5689858  -26652.08441331 -26603.72885788\n",
      " -26590.60500257 -26756.0818169  -26667.92394448 -26648.97983139\n",
      " -26601.077497   -26640.40972046 -26632.1751211  -26626.24949063\n",
      " -26678.53474894 -26620.13208119 -26647.14402313 -26660.22281097\n",
      " -26585.73473907 -26572.91660518 -26563.67733465 -26633.5332226\n",
      " -26648.88376484 -26668.95019564 -26635.54676035 -26647.77100291\n",
      " -26618.72227581 -26622.33224401 -26605.94875439 -26592.51450254\n",
      " -26565.2521996  -26571.92801104 -26596.65684436 -26560.92832649\n",
      " -26631.72209233 -26563.59061772 -26562.06855152 -26568.60470746\n",
      " -26611.77708339 -26713.90222222 -26624.69464747 -26635.21774325\n",
      " -26620.54221793 -26608.96352352 -26573.9018857  -26598.99755783\n",
      " -26678.26448409 -26650.59588588] [195.47858606 194.9705488  192.01863898 207.3969109  190.8437043\n",
      " 191.6412365  191.0050567  200.03627477 191.56620257 200.04722942\n",
      " 196.06256164 196.14685985 193.95701221 200.547596   199.71137098\n",
      " 200.39156329 192.07122388 196.25473326 194.06659738 188.84267989\n",
      " 194.97821068 193.9851726  193.04645182 189.59087187 190.85912683\n",
      " 190.77132072 193.88761796 190.86755453 185.64193198 200.18271252\n",
      " 199.79458458 197.3830598  194.03430195 193.73343077 195.04059634\n",
      " 186.26371351 200.65600541 190.95370258 189.3951125  196.50888707\n",
      " 194.22644048 202.84641427 191.42708846 199.13738105 194.71690389\n",
      " 196.32730221 193.9405141  187.33085091 192.44701726 196.74323589]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1332x756 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp/a0\n",
      "Area under surface (rectangular approx) =  25.73210381298803\n",
      "Violations =  0.0\n",
      "Average_violations =  -26575.126221964136\n",
      "MSE =  0.538216346800696\n",
      "temp/a1\n",
      "Area under surface (rectangular approx) =  25.90259266299641\n",
      "Violations =  0.0\n",
      "Average_violations =  -26584.071399083266\n",
      "MSE =  0.5418335930368255\n",
      "temp/a2\n",
      "Area under surface (rectangular approx) =  25.910507765314954\n",
      "Violations =  0.0\n",
      "Average_violations =  -26543.77455650915\n",
      "MSE =  0.5393656580900925\n",
      "temp/a3\n",
      "Area under surface (rectangular approx) =  25.70857589737276\n",
      "Violations =  0.0\n",
      "Average_violations =  -26577.440630851394\n",
      "MSE =  0.547464620930221\n",
      "temp/a4\n",
      "Area under surface (rectangular approx) =  25.3009963772926\n",
      "Violations =  0.0\n",
      "Average_violations =  -26602.83254689349\n",
      "MSE =  0.5415427056960167\n",
      "temp/a5\n",
      "Area under surface (rectangular approx) =  25.81882777430131\n",
      "Violations =  0.0\n",
      "Average_violations =  -26644.568985795377\n",
      "MSE =  0.5443045702833308\n",
      "temp/a6\n",
      "Area under surface (rectangular approx) =  26.611488134571697\n",
      "Violations =  0.0\n",
      "Average_violations =  -26652.08441330601\n",
      "MSE =  0.550197124456178\n",
      "temp/a7\n",
      "Area under surface (rectangular approx) =  25.18935430183835\n",
      "Violations =  0.0\n",
      "Average_violations =  -26603.728857876406\n",
      "MSE =  0.53916386600168\n",
      "temp/a8\n",
      "Area under surface (rectangular approx) =  25.450219397983773\n",
      "Violations =  0.0\n",
      "Average_violations =  -26590.60500257305\n",
      "MSE =  0.5387921359773615\n",
      "temp/a9\n",
      "Area under surface (rectangular approx) =  26.279276832990384\n",
      "Violations =  0.0\n",
      "Average_violations =  -26756.081816900252\n",
      "MSE =  0.552571824787029\n",
      "temp/a10\n",
      "Area under surface (rectangular approx) =  26.325675261131938\n",
      "Violations =  0.0\n",
      "Average_violations =  -26667.923944482747\n",
      "MSE =  0.5444119357135282\n",
      "temp/a11\n",
      "Area under surface (rectangular approx) =  25.303423963474582\n",
      "Violations =  0.0\n",
      "Average_violations =  -26648.979831391094\n",
      "MSE =  0.5419809210093588\n",
      "temp/a12\n",
      "Area under surface (rectangular approx) =  25.106855014856112\n",
      "Violations =  0.0\n",
      "Average_violations =  -26601.0774969987\n",
      "MSE =  0.5376707776277099\n",
      "temp/a13\n",
      "Area under surface (rectangular approx) =  25.467118341001722\n",
      "Violations =  0.0\n",
      "Average_violations =  -26640.409720460273\n",
      "MSE =  0.5449581514177492\n",
      "temp/a14\n",
      "Area under surface (rectangular approx) =  25.415906945960074\n",
      "Violations =  0.0\n",
      "Average_violations =  -26632.175121101878\n",
      "MSE =  0.5453079912372139\n",
      "temp/a15\n",
      "Area under surface (rectangular approx) =  25.808855423246307\n",
      "Violations =  0.0\n",
      "Average_violations =  -26626.249490632174\n",
      "MSE =  0.5464275860187945\n",
      "temp/a16\n",
      "Area under surface (rectangular approx) =  24.980694948276472\n",
      "Violations =  0.0\n",
      "Average_violations =  -26678.534748942406\n",
      "MSE =  0.5388807544278721\n",
      "temp/a17\n",
      "Area under surface (rectangular approx) =  25.89178701882036\n",
      "Violations =  0.0\n",
      "Average_violations =  -26620.13208118828\n",
      "MSE =  0.5546939551755742\n",
      "temp/a18\n",
      "Area under surface (rectangular approx) =  25.326280058080783\n",
      "Violations =  0.0\n",
      "Average_violations =  -26647.144023133937\n",
      "MSE =  0.5493719095295017\n",
      "temp/a19\n",
      "Area under surface (rectangular approx) =  24.748663357997806\n",
      "Violations =  0.0\n",
      "Average_violations =  -26660.222810970394\n",
      "MSE =  0.5330408577976183\n",
      "temp/a20\n",
      "Area under surface (rectangular approx) =  25.708447913360594\n",
      "Violations =  0.0\n",
      "Average_violations =  -26585.73473906512\n",
      "MSE =  0.5415829169347544\n",
      "temp/a21\n",
      "Area under surface (rectangular approx) =  25.480853415715515\n",
      "Violations =  0.0\n",
      "Average_violations =  -26572.916605180602\n",
      "MSE =  0.5440720166318995\n",
      "temp/a22\n",
      "Area under surface (rectangular approx) =  25.301335852997695\n",
      "Violations =  0.0\n",
      "Average_violations =  -26563.677334652\n",
      "MSE =  0.5473292690257578\n",
      "temp/a23\n",
      "Area under surface (rectangular approx) =  26.340906155304147\n",
      "Violations =  0.0\n",
      "Average_violations =  -26633.533222597976\n",
      "MSE =  0.5415771753114145\n",
      "temp/a24\n",
      "Area under surface (rectangular approx) =  25.962152660673077\n",
      "Violations =  0.0\n",
      "Average_violations =  -26648.883764840684\n",
      "MSE =  0.5497836289440168\n",
      "temp/a25\n",
      "Area under surface (rectangular approx) =  25.353105124062175\n",
      "Violations =  0.0\n",
      "Average_violations =  -26668.950195637262\n",
      "MSE =  0.536410455757901\n",
      "temp/a26\n",
      "Area under surface (rectangular approx) =  26.17510233674593\n",
      "Violations =  0.0\n",
      "Average_violations =  -26635.546760354096\n",
      "MSE =  0.5467112435346569\n",
      "temp/a27\n",
      "Area under surface (rectangular approx) =  25.186099261479423\n",
      "Violations =  0.0\n",
      "Average_violations =  -26647.771002912225\n",
      "MSE =  0.5431571342389108\n",
      "temp/a28\n",
      "Area under surface (rectangular approx) =  25.56392289279944\n",
      "Violations =  0.0\n",
      "Average_violations =  -26618.722275813583\n",
      "MSE =  0.5383430332430045\n",
      "temp/a29\n",
      "Area under surface (rectangular approx) =  25.46479383720786\n",
      "Violations =  0.0\n",
      "Average_violations =  -26622.33224401461\n",
      "MSE =  0.5460860932216296\n",
      "temp/a30\n",
      "Area under surface (rectangular approx) =  25.28091714187692\n",
      "Violations =  0.0\n",
      "Average_violations =  -26605.948754394933\n",
      "MSE =  0.543530536717254\n",
      "temp/a31\n",
      "Area under surface (rectangular approx) =  24.841681883679925\n",
      "Violations =  0.0\n",
      "Average_violations =  -26592.514502540143\n",
      "MSE =  0.541239496390957\n",
      "temp/a32\n",
      "Area under surface (rectangular approx) =  26.688271822145552\n",
      "Violations =  0.0\n",
      "Average_violations =  -26565.252199598966\n",
      "MSE =  0.546976123551318\n",
      "temp/a33\n",
      "Area under surface (rectangular approx) =  24.73526713546448\n",
      "Violations =  0.0\n",
      "Average_violations =  -26571.928011037402\n",
      "MSE =  0.5339411073003554\n",
      "temp/a34\n",
      "Area under surface (rectangular approx) =  26.118683971458953\n",
      "Violations =  0.0\n",
      "Average_violations =  -26596.656844362955\n",
      "MSE =  0.549372788995975\n",
      "temp/a35\n",
      "Area under surface (rectangular approx) =  25.753437330786795\n",
      "Violations =  0.0\n",
      "Average_violations =  -26560.928326491376\n",
      "MSE =  0.540076627888635\n",
      "temp/a36\n",
      "Area under surface (rectangular approx) =  24.99987939212969\n",
      "Violations =  0.0\n",
      "Average_violations =  -26631.72209232964\n",
      "MSE =  0.5400261189515332\n",
      "temp/a37\n",
      "Area under surface (rectangular approx) =  25.25677385307612\n",
      "Violations =  0.0\n",
      "Average_violations =  -26563.590617722988\n",
      "MSE =  0.5431810651843438\n",
      "temp/a38\n",
      "Area under surface (rectangular approx) =  28.228281605957548\n",
      "Violations =  0.0\n",
      "Average_violations =  -26562.068551515025\n",
      "MSE =  0.5588422374091533\n",
      "temp/a39\n",
      "Area under surface (rectangular approx) =  24.998675412903694\n",
      "Violations =  0.0\n",
      "Average_violations =  -26568.60470745581\n",
      "MSE =  0.542997584277337\n",
      "temp/a40\n",
      "Area under surface (rectangular approx) =  25.367646602416812\n",
      "Violations =  0.0\n",
      "Average_violations =  -26611.77708339481\n",
      "MSE =  0.5388700547618285\n",
      "temp/a41\n",
      "Area under surface (rectangular approx) =  26.51434687720243\n",
      "Violations =  0.0\n",
      "Average_violations =  -26713.902222222056\n",
      "MSE =  0.5490174842875708\n",
      "temp/a42\n",
      "Area under surface (rectangular approx) =  25.385917374389884\n",
      "Violations =  0.0\n",
      "Average_violations =  -26624.694647468074\n",
      "MSE =  0.5416538525383217\n",
      "temp/a43\n",
      "Area under surface (rectangular approx) =  25.58750647314146\n",
      "Violations =  0.0\n",
      "Average_violations =  -26635.217743248744\n",
      "MSE =  0.5423404657144957\n",
      "temp/a44\n",
      "Area under surface (rectangular approx) =  24.857854709452898\n",
      "Violations =  0.0\n",
      "Average_violations =  -26620.542217931616\n",
      "MSE =  0.5408704263889657\n",
      "temp/a45\n",
      "Area under surface (rectangular approx) =  26.011174706273682\n",
      "Violations =  0.0\n",
      "Average_violations =  -26608.96352352006\n",
      "MSE =  0.5429284092769164\n",
      "temp/a46\n",
      "Area under surface (rectangular approx) =  25.441671016340443\n",
      "Violations =  0.0\n",
      "Average_violations =  -26573.90188569694\n",
      "MSE =  0.5531611914533132\n",
      "temp/a47\n",
      "Area under surface (rectangular approx) =  25.709391486188757\n",
      "Violations =  0.0\n",
      "Average_violations =  -26598.99755782677\n",
      "MSE =  0.540302785640026\n",
      "temp/a48\n",
      "Area under surface (rectangular approx) =  24.877780494445343\n",
      "Violations =  0.0\n",
      "Average_violations =  -26678.264484094147\n",
      "MSE =  0.5384209512667573\n",
      "temp/a49\n",
      "Area under surface (rectangular approx) =  25.370869021391506\n",
      "Violations =  0.0\n",
      "Average_violations =  -26650.595885875948\n",
      "MSE =  0.5445965783671405\n"
     ]
    }
   ],
   "source": [
    "metric = []\n",
    "metric_err = []\n",
    "viol = []\n",
    "viol_err = []\n",
    "\n",
    "#normalize the violations for prettier graphing.\n",
    "#also violations are always positive, so just divide by max.\n",
    "\n",
    "#TMK\n",
    "#violation_mean = violation_mean / np.max(violation_mean)\n",
    "\n",
    "for i in range(len(violations)):\n",
    "    print(\"Model_name = \", model_names[i], \"Violations = \", violations[i])\n",
    "    print(\"Average_violations = \", np.mean(violation_mean[i]), np.std(violation_mean[i]))\n",
    "    print(\"MSE = \", np.mean(mean[i]), np.std(mean[i]))\n",
    "    #print(\"mean = \", mean[i])\n",
    "    metric.append(np.mean(mean[i]))\n",
    "    metric_err.append(np.std(mean[i]))\n",
    "    viol.append(np.mean(violation_mean[i]))\n",
    "    #viol.append(violations[i]/times)\n",
    "    viol_err.append(np.std(violation_mean[i]))\n",
    "print(np.array(metric), \n",
    "         np.array(metric_err), \n",
    "         np.array(viol), \n",
    "         np.array(viol_err))    \n",
    "\n",
    "bar_plot(model_names, \n",
    "         np.array(metric), \n",
    "         np.array(metric_err), \n",
    "         np.array(viol), \n",
    "         np.array(viol_err))\n",
    "\n",
    "\n",
    "def heat_plot(x,y,z, xlab = 'Mean', ylab = 'Variance', clim_low = 0, clim_high = 1):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    cax = ax.scatter(x, y, c=z, s=450, edgecolor='')\n",
    "    cax.set_clim(clim_low, clim_high)\n",
    "    ax.set_xlabel(xlab)\n",
    "    ax.set_ylabel(ylab)\n",
    "    plt.colorbar(cax)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "MSE = []\n",
    "VIO = []\n",
    "VIO2 = []\n",
    "AUS = []\n",
    "for i, m in enumerate(models):\n",
    "    print(model_names[i])\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "    \n",
    "    \n",
    "    rectangular_approx = 0\n",
    "    for k, v in metrics_dicts[i].items():\n",
    "        x.append(float(k.split('_')[0]))\n",
    "        y.append(float(k.split('_')[-1]))\n",
    "        z.append(np.mean(v))\n",
    "        rectangular_approx += np.mean(v)\n",
    "    print(\"Area under surface (rectangular approx) = \", rectangular_approx)\n",
    "    print(\"Violations = \", violations[i])\n",
    "    print(\"Average_violations = \", np.mean(violation_mean[i]))\n",
    "    print(\"MSE = \", np.mean(mean[i]))   \n",
    "    MSE.append(np.mean(mean[i]))\n",
    "    VIO.append(np.mean(violation_mean[i]))\n",
    "    VIO2.append(np.mean(violation_mean2[i]))\n",
    "    #VIO.append(violations[i]/times)\n",
    "    AUS.append(rectangular_approx)\n",
    "    \n",
    "    #heat_plot(x,y,z, clim_low = 0, clim_high = 10)\n",
    "    \n",
    "#heat_plot(MSE,VIO,AUS, xlab = 'MSE', ylab='Violations', clim_low = np.min(AUS), clim_high = np.max(AUS))\n",
    "    \n",
    "#VIO = np.abs(VIO)\n",
    "#VIO2 = np.abs(VIO2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0029388608267132896\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6458421593582494\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6458421593582494"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.polynomial.polynomial import polyfit  \n",
    "from scipy.stats import pearsonr\n",
    "from pylab import text\n",
    "\n",
    "\n",
    "print(pearsonr(VIO,AUS)[0])\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(VIO,AUS, 1)\n",
    "ax.plot(VIO,AUS, '.')\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(VIO,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "plt.plot(VIO, b + m * np.array(VIO), '-')\n",
    "ax.set_xlabel(\"Proposed metric\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "fig.savefig('Ex4VIOVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "METRIC = -(VIO/np.max(VIO)) + np.array(MSE)\n",
    "print(pearsonr(METRIC,AUS)[0])\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(METRIC,AUS, 1)\n",
    "ax.plot(METRIC,AUS, '.')\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(METRIC,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "plt.plot(METRIC, b + m * np.array(METRIC), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"Proposed metric\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "fig.savefig('Ex4ProposedVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(MSE,AUS, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(MSE,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(MSE,AUS, '.')\n",
    "plt.plot(MSE, b + m * np.array(MSE), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "MSE = np.array(MSE)\n",
    "\n",
    "x = []\n",
    "y1 = []\n",
    "y2 = []\n",
    "y3 = []\n",
    "for split in range(10, len(AUS), 5):\n",
    "    #print(\"******\", split, \"*******\")\n",
    "    sorted_aus = [AUS for _,AUS in sorted(zip(VIO,AUS))]\n",
    "    sorted_mse = [MSE for _,MSE in sorted(zip(VIO,MSE))]\n",
    "\n",
    "    low = []\n",
    "    high = []\n",
    "    low = sorted_aus[:split]\n",
    "    high = sorted_aus[split:]\n",
    "\n",
    "    x.append(split)\n",
    "    \n",
    "    \n",
    "    #print(\"Low Violations = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High Violations = \", np.mean(high), \"for\", len(high))\n",
    "    y1.append(np.mean(low))\n",
    "    sorted_aus_by_mse = [AUS for _,AUS in sorted(zip(MSE,AUS))]\n",
    "    low = sorted_aus_by_mse[:split]\n",
    "    high = sorted_aus_by_mse[split:]\n",
    "    #print(\"Low AUS by MSE = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High AUS by MSE = \", np.mean(high), \"for\", len(high))\n",
    "    y2.append(np.mean(low))\n",
    "    sorted_aus = [AUS for _,AUS in sorted(zip(METRIC,AUS))]\n",
    "    sorted_mse = [MSE for _,MSE in sorted(zip(METRIC,MSE))]\n",
    "\n",
    "    low = []\n",
    "    high = []\n",
    "    low = sorted_aus[:split]\n",
    "    high = sorted_aus[split:]\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"Low Metric = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High Metric = \", np.mean(high), \"for\", len(high))\n",
    "    y3.append(np.mean(low))\n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x,y1, '-', label = 'BIC')\n",
    "ax.plot(x,y2, '-', label = 'MSE')\n",
    "ax.plot(x,y3, '-', label = 'METRIC')\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "plt.show()  \n",
    "pearsonr(METRIC,AUS)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
