{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes \n",
    "### Required installing Oracle JAVA 8 to get javabridge installed\n",
    "### Then, I was able to install py-causal from https://bd2kccd.github.io/docs/py-causal/\n",
    "### GFCI is slower than RFCI, but more accurate (SPIRTES), GFCI and RFCI account for unobserved variables, FGES assumes no unobserved variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure Learning Performance Guarantees If the assumptions in the previous section hold, then in the large sample limit, the CBN structure output by GFCId will contain an edge of one of four kinds between Xand Y   if and only if Xand Yare not independent conditional on any subset of the other measured variables of less than or equal to a specified size. In addition, there is (1) an arc X->Y   if and only if Xdirectly or indirectly causes Y, and Y   does not directly or indirectly cause X; (2) an edge X <-->Y   if and only if X   is not a direct or indirect cause of Yand Y   is not a direct or indirect cause of X(which can only occur if there are latent confounders of Xand some other variable or Yand some other variable; (3) an edge Xo->Y   only if Yis not a direct or indirect cause of X, but Xmay or may not be an indirect cause of Y; (4) an edge X oâ€“o Y   indicates that Xand Y   are dependent no matter what subset of observed variables is conditioned on, but contains no orientation information (X   may be a direct or indirect cause of Y, and Ymay be an indirect cause of X, or there may be a latent common cause of Xand Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying some various ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512]] ['temp/c0', 'temp/c1', 'temp/c2', 'temp/c3', 'temp/c4', 'temp/c5', 'temp/c6', 'temp/c7', 'temp/c8', 'temp/c9', 'temp/c10', 'temp/c11', 'temp/c12', 'temp/c13', 'temp/c14', 'temp/c15', 'temp/c16', 'temp/c17', 'temp/c18', 'temp/c19', 'temp/c20', 'temp/c21', 'temp/c22', 'temp/c23', 'temp/c24', 'temp/c25', 'temp/c26', 'temp/c27', 'temp/c28', 'temp/c29', 'temp/c30', 'temp/c31', 'temp/c32', 'temp/c33', 'temp/c34', 'temp/c35', 'temp/c36', 'temp/c37', 'temp/c38', 'temp/c39', 'temp/c40', 'temp/c41', 'temp/c42', 'temp/c43', 'temp/c44', 'temp/c45', 'temp/c46', 'temp/c47', 'temp/c48', 'temp/c49', 'temp/c50', 'temp/c51', 'temp/c52', 'temp/c53', 'temp/c54', 'temp/c55', 'temp/c56', 'temp/c57', 'temp/c58', 'temp/c59', 'temp/c60', 'temp/c61', 'temp/c62', 'temp/c63', 'temp/c64', 'temp/c65', 'temp/c66', 'temp/c67', 'temp/c68', 'temp/c69', 'temp/c70', 'temp/c71', 'temp/c72', 'temp/c73', 'temp/c74', 'temp/c75', 'temp/c76', 'temp/c77', 'temp/c78', 'temp/c79', 'temp/c80', 'temp/c81', 'temp/c82', 'temp/c83', 'temp/c84', 'temp/c85', 'temp/c86', 'temp/c87', 'temp/c88', 'temp/c89', 'temp/c90', 'temp/c91', 'temp/c92', 'temp/c93', 'temp/c94', 'temp/c95', 'temp/c96', 'temp/c97', 'temp/c98', 'temp/c99']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-585281.2794720768"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from pycausal import search as s\n",
    "import configparser\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, \\\n",
    "                        Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "import keras.optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "import glob, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "# select your GPU Here\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" #Comment this line out if you want all GPUS (2 hehe)\n",
    "\n",
    "# python full-display web browser\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "def get_model(dense, dropouts, inputs, target_len):\n",
    "    # dense is an ordered list of the number of dense neurons like [1024, 2048, 1024]\n",
    "    # dropouts is an ordered list of the dropout masks like [0.2, 0.3, 0.4]\n",
    "    inputs = keras.Input(shape = (inputs,))\n",
    "    x = keras.layers.BatchNormalization()(inputs)\n",
    "    x = keras.layers.Dense(dense[0], activation = 'relu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(dropouts[0])(x, training=True)\n",
    "    for den, drop in zip(dense[1:], dropouts[1:]):\n",
    "        x = keras.layers.Dense(den, activation = 'relu')(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Dropout(drop)(x, training=True)\n",
    "    outputs = keras.layers.Dense(target_len, activation = 'linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def discrete_gauss(low, high, samples, std = 20):\n",
    "    x = np.arange(low, high)\n",
    "    xU, xL = x + 0.5, x - 0.5 \n",
    "    prob = ss.norm.cdf(xU, scale = std) - ss.norm.cdf(xL, scale = std)\n",
    "    prob = prob / prob.sum() #normalize the probabilities so their sum is 1\n",
    "    nums = np.random.choice(x, size = samples, p = prob)\n",
    "    return nums\n",
    "\n",
    "\n",
    "\n",
    "def bar_plot(x_ax, val1, val1std, val2, val2std):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ## the data\n",
    "    N = len(x_ax)\n",
    "\n",
    "    ## necessary variables\n",
    "    ind = np.arange(N)                # the x locations for the groups\n",
    "    width = 0.35                      # the width of the bars\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    ## the bars\n",
    "    rects1 = ax.bar(ind, val1, width,\n",
    "                    color='gray',\n",
    "                    yerr=val1std,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='blue'))\n",
    "\n",
    "    rects2 = ax.bar(ind+width, val2, width,\n",
    "                        color='blue',\n",
    "                        #yerr=val2std,\n",
    "                        error_kw=dict(elinewidth=2,ecolor='gray'))\n",
    "\n",
    "    # axes and labels\n",
    "    ax.set_xlim(-width,len(ind)+width)\n",
    "    #ax.set_ylim(0,45)\n",
    "    ax.set_ylabel('Percentage')\n",
    "    ax.set_title('')\n",
    "    plt.xticks(ind + width / 2, x_ax, rotation=75, size = 14)\n",
    "    ## add a legend\n",
    "    ax.legend( (rects1[0], rects2[0]), ('Accuracy', '% Violations') )\n",
    "    fig.savefig(\"violations.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def normalize(x):\n",
    "    return x\n",
    "    #return (x - x.min(0)) / x.ptp(0)\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 2000):\n",
    "    # set bmi to these values real world mean and standard deviation for a certain country.\n",
    "    bmi = np.random.normal(mean, var, SIZE)\n",
    "    estrogen =  2*bmi +  np.random.normal(mean,var, SIZE)\n",
    "    age = np.random.normal(mean,var, SIZE)\n",
    "    genes = 1.1 * age + estrogen +   np.random.normal(mean,var, SIZE)\n",
    "    insomnia = estrogen +  np.random.normal(mean,var, SIZE)\n",
    "    density = estrogen + genes + np.random.normal(mean,var, SIZE)\n",
    "    cancer = density + np.random.normal(mean,var, SIZE)\n",
    "    return pd.DataFrame({'bmi' : bmi,'density' : density, 'age' : age, 'cancer' : cancer, 'estrogen': estrogen, 'genes':genes, 'insomnia': insomnia})\n",
    "\n",
    "\n",
    "def get_CG(df, tetrad):\n",
    "    tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "\n",
    "\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from collections import defaultdict\n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '5000M')\n",
    "tetrad = s.tetradrunner()\n",
    "\n",
    "\n",
    "verbosity = 1\n",
    "\n",
    "\n",
    "\n",
    "models = []\n",
    "model_names = []\n",
    "\n",
    "num_models =100\n",
    "model_layers = [1024,512]\n",
    "for i in range(num_models):\n",
    "    models.append(model_layers)\n",
    "    model_names.append('temp/c' + str(i))\n",
    "\n",
    "print(models, model_names)\n",
    "\n",
    "from pycausal import prior as p\n",
    "def get_bic(df, prior):\n",
    "\n",
    "    tetrad.run(algoId = 'fges', dfs = df,  scoreId = 'sem-bic-deterministic', dataType = 'continuous',\n",
    "               structurePrior = 1.0, samplePrior = 1, maxDegree = -1, maxPathLength = -1, priorKnowledge = prior,\n",
    "               completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True,\n",
    "               penaltyDiscount = 2\n",
    "               )\n",
    "    BIC = tetrad.getTetradGraph().getAllAttributes().toString()\n",
    "    BIC = float(BIC.split('=')[-1].split('}')[0])\n",
    "    return BIC #/ len(df)\n",
    "import itertools\n",
    "def get_pairs(lst):\n",
    "    a = set()\n",
    "    for i in itertools.permutations(lst,2):\n",
    "        a.add(i)\n",
    "    return a\n",
    "\n",
    "inputs = ['bmi', 'density', 'age', 'genes', 'insomnia', 'estrogen']\n",
    "target = ['cancer']\n",
    "full_conx = get_pairs(inputs + target)\n",
    "forced_conx = set({('age','genes'), ('bmi', 'estrogen'), ('estrogen', 'genes'),('estrogen', 'insomnia'), ('estrogen', 'density'), ('genes', 'density'), ('density', 'cancer')})\n",
    "restricted_conx = full_conx.difference(forced_conx)   \n",
    "\n",
    "prior = p.knowledge(requiredirect =  list(map(list, forced_conx)),\n",
    "                       forbiddirect = list(map(list, restricted_conx))\n",
    "                       )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = gen_data(SIZE = 50000)\n",
    "\n",
    "X = df[inputs].values\n",
    "X = normalize(X)\n",
    "y = df[target].values\n",
    "\n",
    "\n",
    "val_df = gen_data(SIZE = 5000)\n",
    "\n",
    "x_val = val_df[inputs].values\n",
    "x_val = normalize(x_val)\n",
    "y_val = val_df[target].values\n",
    "\n",
    "get_bic(df,prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp/c0\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 228us/step - loss: 2.9242 - mean_squared_error: 2.9242 - val_loss: 1.5282 - val_mean_squared_error: 1.5282\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.52821, saving model to temp/c0\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.3936 - mean_squared_error: 2.3936 - val_loss: 1.6133 - val_mean_squared_error: 1.6133\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.52821\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.3018 - mean_squared_error: 2.3018 - val_loss: 1.3641 - val_mean_squared_error: 1.3641\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.52821 to 1.36405, saving model to temp/c0\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2282 - mean_squared_error: 2.2282 - val_loss: 1.3236 - val_mean_squared_error: 1.3236\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.36405 to 1.32360, saving model to temp/c0\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1831 - mean_squared_error: 2.1831 - val_loss: 1.3477 - val_mean_squared_error: 1.3477\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.32360\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.2139 - mean_squared_error: 2.2139 - val_loss: 1.4461 - val_mean_squared_error: 1.4461\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.32360\n",
      "Epoch 00006: early stopping\n",
      "temp/c1\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 2.8562 - mean_squared_error: 2.8562 - val_loss: 1.6968 - val_mean_squared_error: 1.6968\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.69681, saving model to temp/c1\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.4479 - mean_squared_error: 2.4479 - val_loss: 1.4502 - val_mean_squared_error: 1.4502\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.69681 to 1.45025, saving model to temp/c1\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.3922 - mean_squared_error: 2.3922 - val_loss: 1.5456 - val_mean_squared_error: 1.5456\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.45025\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2405 - mean_squared_error: 2.2405 - val_loss: 1.4388 - val_mean_squared_error: 1.4388\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.45025 to 1.43884, saving model to temp/c1\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1990 - mean_squared_error: 2.1990 - val_loss: 1.4669 - val_mean_squared_error: 1.4669\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.43884\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1441 - mean_squared_error: 2.1441 - val_loss: 1.3280 - val_mean_squared_error: 1.3280\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.43884 to 1.32798, saving model to temp/c1\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1290 - mean_squared_error: 2.1290 - val_loss: 1.2348 - val_mean_squared_error: 1.2348\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.32798 to 1.23481, saving model to temp/c1\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 2.0543 - mean_squared_error: 2.0543 - val_loss: 1.3875 - val_mean_squared_error: 1.3875\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.23481\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 2.0633 - mean_squared_error: 2.0633 - val_loss: 1.1989 - val_mean_squared_error: 1.1989\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.23481 to 1.19890, saving model to temp/c1\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 2.0385 - mean_squared_error: 2.0385 - val_loss: 1.4685 - val_mean_squared_error: 1.4685\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.19890\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 2.0190 - mean_squared_error: 2.0190 - val_loss: 1.1182 - val_mean_squared_error: 1.1182\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.19890 to 1.11824, saving model to temp/c1\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 2.0575 - mean_squared_error: 2.0575 - val_loss: 1.1131 - val_mean_squared_error: 1.1131\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.11824 to 1.11311, saving model to temp/c1\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.0162 - mean_squared_error: 2.0162 - val_loss: 1.3928 - val_mean_squared_error: 1.3928\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.11311\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 1.9791 - mean_squared_error: 1.9791 - val_loss: 1.1986 - val_mean_squared_error: 1.1986\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.11311\n",
      "Epoch 00014: early stopping\n",
      "temp/c2\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.8619 - mean_squared_error: 2.8619 - val_loss: 1.5508 - val_mean_squared_error: 1.5508\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.55080, saving model to temp/c2\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 232us/step - loss: 2.4627 - mean_squared_error: 2.4627 - val_loss: 1.5121 - val_mean_squared_error: 1.5121\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.55080 to 1.51208, saving model to temp/c2\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.3753 - mean_squared_error: 2.3753 - val_loss: 1.3799 - val_mean_squared_error: 1.3799\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.51208 to 1.37989, saving model to temp/c2\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 234us/step - loss: 2.2615 - mean_squared_error: 2.2615 - val_loss: 1.7729 - val_mean_squared_error: 1.7729\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.37989\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 240us/step - loss: 2.2007 - mean_squared_error: 2.2007 - val_loss: 1.3656 - val_mean_squared_error: 1.3656\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.37989 to 1.36557, saving model to temp/c2\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 231us/step - loss: 2.1898 - mean_squared_error: 2.1898 - val_loss: 1.4772 - val_mean_squared_error: 1.4772\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.36557\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 12s 241us/step - loss: 2.0853 - mean_squared_error: 2.0853 - val_loss: 1.3963 - val_mean_squared_error: 1.3963\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.36557\n",
      "Epoch 00007: early stopping\n",
      "temp/c3\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.8646 - mean_squared_error: 2.8646 - val_loss: 1.7680 - val_mean_squared_error: 1.7680\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.76796, saving model to temp/c3\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.4491 - mean_squared_error: 2.4491 - val_loss: 1.7643 - val_mean_squared_error: 1.7643\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.76796 to 1.76430, saving model to temp/c3\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.3965 - mean_squared_error: 2.3965 - val_loss: 1.3561 - val_mean_squared_error: 1.3561\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.76430 to 1.35613, saving model to temp/c3\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.3067 - mean_squared_error: 2.3067 - val_loss: 1.6948 - val_mean_squared_error: 1.6948\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.35613\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1863 - mean_squared_error: 2.1863 - val_loss: 1.2415 - val_mean_squared_error: 1.2415\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.35613 to 1.24148, saving model to temp/c3\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1723 - mean_squared_error: 2.1723 - val_loss: 1.4937 - val_mean_squared_error: 1.4937\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.24148\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0846 - mean_squared_error: 2.0846 - val_loss: 1.4053 - val_mean_squared_error: 1.4053\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.24148\n",
      "Epoch 00007: early stopping\n",
      "temp/c4\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.8993 - mean_squared_error: 2.8993 - val_loss: 1.6501 - val_mean_squared_error: 1.6501\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.65009, saving model to temp/c4\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.4708 - mean_squared_error: 2.4708 - val_loss: 1.4718 - val_mean_squared_error: 1.4718\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.65009 to 1.47183, saving model to temp/c4\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.3170 - mean_squared_error: 2.3170 - val_loss: 1.3377 - val_mean_squared_error: 1.3377\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.47183 to 1.33773, saving model to temp/c4\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2981 - mean_squared_error: 2.2981 - val_loss: 1.3780 - val_mean_squared_error: 1.3780\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.33773\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2208 - mean_squared_error: 2.2208 - val_loss: 1.2456 - val_mean_squared_error: 1.2456\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.33773 to 1.24560, saving model to temp/c4\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1359 - mean_squared_error: 2.1359 - val_loss: 1.3420 - val_mean_squared_error: 1.3420\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.24560\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1178 - mean_squared_error: 2.1178 - val_loss: 1.4062 - val_mean_squared_error: 1.4062\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.24560\n",
      "Epoch 00007: early stopping\n",
      "temp/c5\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 220us/step - loss: 2.9405 - mean_squared_error: 2.9405 - val_loss: 1.5826 - val_mean_squared_error: 1.5826\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.58261, saving model to temp/c5\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.5356 - mean_squared_error: 2.5356 - val_loss: 1.4949 - val_mean_squared_error: 1.4949\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.58261 to 1.49488, saving model to temp/c5\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.3706 - mean_squared_error: 2.3706 - val_loss: 1.4553 - val_mean_squared_error: 1.4553\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.49488 to 1.45527, saving model to temp/c5\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2974 - mean_squared_error: 2.2974 - val_loss: 1.4958 - val_mean_squared_error: 1.4958\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.45527\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2081 - mean_squared_error: 2.2081 - val_loss: 1.3433 - val_mean_squared_error: 1.3433\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.45527 to 1.34329, saving model to temp/c5\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1568 - mean_squared_error: 2.1568 - val_loss: 1.5849 - val_mean_squared_error: 1.5849\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.34329\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1721 - mean_squared_error: 2.1721 - val_loss: 1.3541 - val_mean_squared_error: 1.3541\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.34329\n",
      "Epoch 00007: early stopping\n",
      "temp/c6\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.8522 - mean_squared_error: 2.8522 - val_loss: 1.6956 - val_mean_squared_error: 1.6956\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.69561, saving model to temp/c6\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.5158 - mean_squared_error: 2.5158 - val_loss: 1.8943 - val_mean_squared_error: 1.8943\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.69561\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.3665 - mean_squared_error: 2.3665 - val_loss: 1.3299 - val_mean_squared_error: 1.3299\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.69561 to 1.32989, saving model to temp/c6\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2698 - mean_squared_error: 2.2698 - val_loss: 1.3337 - val_mean_squared_error: 1.3337\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.32989\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1792 - mean_squared_error: 2.1792 - val_loss: 1.2861 - val_mean_squared_error: 1.2861\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.32989 to 1.28607, saving model to temp/c6\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1709 - mean_squared_error: 2.1709 - val_loss: 1.2694 - val_mean_squared_error: 1.2694\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.28607 to 1.26940, saving model to temp/c6\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1092 - mean_squared_error: 2.1092 - val_loss: 1.3888 - val_mean_squared_error: 1.3888\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.26940\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1005 - mean_squared_error: 2.1005 - val_loss: 1.3659 - val_mean_squared_error: 1.3659\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.26940\n",
      "Epoch 00008: early stopping\n",
      "temp/c7\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.9166 - mean_squared_error: 2.9166 - val_loss: 1.6705 - val_mean_squared_error: 1.6705\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.67049, saving model to temp/c7\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.5070 - mean_squared_error: 2.5070 - val_loss: 1.9851 - val_mean_squared_error: 1.9851\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.67049\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.3021 - mean_squared_error: 2.3021 - val_loss: 1.5391 - val_mean_squared_error: 1.5391\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.67049 to 1.53907, saving model to temp/c7\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2716 - mean_squared_error: 2.2716 - val_loss: 1.9934 - val_mean_squared_error: 1.9934\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.53907\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2248 - mean_squared_error: 2.2248 - val_loss: 1.3621 - val_mean_squared_error: 1.3621\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.53907 to 1.36213, saving model to temp/c7\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1624 - mean_squared_error: 2.1624 - val_loss: 1.4024 - val_mean_squared_error: 1.4024\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.36213\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1151 - mean_squared_error: 2.1151 - val_loss: 1.3382 - val_mean_squared_error: 1.3382\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.36213 to 1.33825, saving model to temp/c7\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0900 - mean_squared_error: 2.0900 - val_loss: 1.2804 - val_mean_squared_error: 1.2804\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.33825 to 1.28042, saving model to temp/c7\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 200us/step - loss: 2.0540 - mean_squared_error: 2.0540 - val_loss: 1.1481 - val_mean_squared_error: 1.1481\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.28042 to 1.14811, saving model to temp/c7\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.0593 - mean_squared_error: 2.0593 - val_loss: 1.4133 - val_mean_squared_error: 1.4133\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.14811\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0653 - mean_squared_error: 2.0653 - val_loss: 1.1728 - val_mean_squared_error: 1.1728\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.14811\n",
      "Epoch 00011: early stopping\n",
      "temp/c8\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.8294 - mean_squared_error: 2.8294 - val_loss: 1.5927 - val_mean_squared_error: 1.5927\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.59271, saving model to temp/c8\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.5244 - mean_squared_error: 2.5244 - val_loss: 1.4876 - val_mean_squared_error: 1.4876\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.59271 to 1.48761, saving model to temp/c8\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.3551 - mean_squared_error: 2.3551 - val_loss: 1.3539 - val_mean_squared_error: 1.3539\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.48761 to 1.35392, saving model to temp/c8\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2542 - mean_squared_error: 2.2542 - val_loss: 1.3168 - val_mean_squared_error: 1.3168\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.35392 to 1.31685, saving model to temp/c8\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2258 - mean_squared_error: 2.2258 - val_loss: 1.2860 - val_mean_squared_error: 1.2860\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.31685 to 1.28604, saving model to temp/c8\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2269 - mean_squared_error: 2.2269 - val_loss: 1.5185 - val_mean_squared_error: 1.5185\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.28604\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1222 - mean_squared_error: 2.1222 - val_loss: 1.2905 - val_mean_squared_error: 1.2905\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.28604\n",
      "Epoch 00007: early stopping\n",
      "temp/c9\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.8269 - mean_squared_error: 2.8269 - val_loss: 2.0087 - val_mean_squared_error: 2.0087\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.00870, saving model to temp/c9\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.4501 - mean_squared_error: 2.4501 - val_loss: 1.6874 - val_mean_squared_error: 1.6874\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.00870 to 1.68741, saving model to temp/c9\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.3228 - mean_squared_error: 2.3228 - val_loss: 1.3910 - val_mean_squared_error: 1.3910\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.68741 to 1.39096, saving model to temp/c9\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1763 - mean_squared_error: 2.1763 - val_loss: 1.2864 - val_mean_squared_error: 1.2864\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.39096 to 1.28642, saving model to temp/c9\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1537 - mean_squared_error: 2.1537 - val_loss: 1.2255 - val_mean_squared_error: 1.2255\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.28642 to 1.22553, saving model to temp/c9\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1324 - mean_squared_error: 2.1324 - val_loss: 1.2872 - val_mean_squared_error: 1.2872\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.22553\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0761 - mean_squared_error: 2.0761 - val_loss: 1.2904 - val_mean_squared_error: 1.2904\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.22553\n",
      "Epoch 00007: early stopping\n",
      "temp/c10\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.7722 - mean_squared_error: 2.7722 - val_loss: 1.4992 - val_mean_squared_error: 1.4992\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.49921, saving model to temp/c10\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.4535 - mean_squared_error: 2.4535 - val_loss: 1.5446 - val_mean_squared_error: 1.5446\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.49921\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.3576 - mean_squared_error: 2.3576 - val_loss: 1.2865 - val_mean_squared_error: 1.2865\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.49921 to 1.28647, saving model to temp/c10\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1545 - mean_squared_error: 2.1545 - val_loss: 1.3215 - val_mean_squared_error: 1.3215\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.28647\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1040 - mean_squared_error: 2.1040 - val_loss: 1.2607 - val_mean_squared_error: 1.2607\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.28647 to 1.26075, saving model to temp/c10\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1398 - mean_squared_error: 2.1398 - val_loss: 1.2544 - val_mean_squared_error: 1.2544\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.26075 to 1.25439, saving model to temp/c10\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1058 - mean_squared_error: 2.1058 - val_loss: 1.1850 - val_mean_squared_error: 1.1850\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.25439 to 1.18497, saving model to temp/c10\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1079 - mean_squared_error: 2.1079 - val_loss: 1.3256 - val_mean_squared_error: 1.3256\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.18497\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.0400 - mean_squared_error: 2.0400 - val_loss: 1.2406 - val_mean_squared_error: 1.2406\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.18497\n",
      "Epoch 00009: early stopping\n",
      "temp/c11\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.8886 - mean_squared_error: 2.8886 - val_loss: 1.5983 - val_mean_squared_error: 1.5983\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.59832, saving model to temp/c11\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.5151 - mean_squared_error: 2.5151 - val_loss: 1.3960 - val_mean_squared_error: 1.3960\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.59832 to 1.39597, saving model to temp/c11\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 200us/step - loss: 2.3775 - mean_squared_error: 2.3775 - val_loss: 1.5360 - val_mean_squared_error: 1.5360\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.39597\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.3308 - mean_squared_error: 2.3308 - val_loss: 1.2984 - val_mean_squared_error: 1.2984\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.39597 to 1.29844, saving model to temp/c11\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2145 - mean_squared_error: 2.2145 - val_loss: 1.2300 - val_mean_squared_error: 1.2300\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.29844 to 1.23001, saving model to temp/c11\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1885 - mean_squared_error: 2.1885 - val_loss: 1.3093 - val_mean_squared_error: 1.3093\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.23001\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1377 - mean_squared_error: 2.1377 - val_loss: 1.3415 - val_mean_squared_error: 1.3415\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.23001\n",
      "Epoch 00007: early stopping\n",
      "temp/c12\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.8617 - mean_squared_error: 2.8617 - val_loss: 1.6783 - val_mean_squared_error: 1.6783\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.67833, saving model to temp/c12\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.4173 - mean_squared_error: 2.4173 - val_loss: 1.6838 - val_mean_squared_error: 1.6838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_loss did not improve from 1.67833\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2912 - mean_squared_error: 2.2912 - val_loss: 1.3894 - val_mean_squared_error: 1.3894\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.67833 to 1.38942, saving model to temp/c12\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2819 - mean_squared_error: 2.2819 - val_loss: 1.3216 - val_mean_squared_error: 1.3216\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.38942 to 1.32161, saving model to temp/c12\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2132 - mean_squared_error: 2.2132 - val_loss: 1.2856 - val_mean_squared_error: 1.2856\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.32161 to 1.28564, saving model to temp/c12\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1355 - mean_squared_error: 2.1355 - val_loss: 1.1743 - val_mean_squared_error: 1.1743\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.28564 to 1.17427, saving model to temp/c12\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0479 - mean_squared_error: 2.0479 - val_loss: 1.1897 - val_mean_squared_error: 1.1897\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.17427\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0986 - mean_squared_error: 2.0986 - val_loss: 1.6585 - val_mean_squared_error: 1.6585\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.17427\n",
      "Epoch 00008: early stopping\n",
      "temp/c13\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.8967 - mean_squared_error: 2.8967 - val_loss: 1.5585 - val_mean_squared_error: 1.5585\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.55853, saving model to temp/c13\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.5170 - mean_squared_error: 2.5170 - val_loss: 1.5271 - val_mean_squared_error: 1.5271\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.55853 to 1.52707, saving model to temp/c13\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.3183 - mean_squared_error: 2.3183 - val_loss: 1.3497 - val_mean_squared_error: 1.3497\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.52707 to 1.34970, saving model to temp/c13\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2344 - mean_squared_error: 2.2344 - val_loss: 1.6537 - val_mean_squared_error: 1.6537\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.34970\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2056 - mean_squared_error: 2.2056 - val_loss: 1.3143 - val_mean_squared_error: 1.3143\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.34970 to 1.31431, saving model to temp/c13\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1386 - mean_squared_error: 2.1386 - val_loss: 1.3450 - val_mean_squared_error: 1.3450\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.31431\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.0844 - mean_squared_error: 2.0844 - val_loss: 1.4053 - val_mean_squared_error: 1.4053\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.31431\n",
      "Epoch 00007: early stopping\n",
      "temp/c14\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.8477 - mean_squared_error: 2.8477 - val_loss: 1.6235 - val_mean_squared_error: 1.6235\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.62351, saving model to temp/c14\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.4529 - mean_squared_error: 2.4529 - val_loss: 1.5098 - val_mean_squared_error: 1.5098\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.62351 to 1.50979, saving model to temp/c14\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.3576 - mean_squared_error: 2.3576 - val_loss: 1.5252 - val_mean_squared_error: 1.5252\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.50979\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1702 - mean_squared_error: 2.1702 - val_loss: 1.2781 - val_mean_squared_error: 1.2781\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.50979 to 1.27810, saving model to temp/c14\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2019 - mean_squared_error: 2.2019 - val_loss: 1.3959 - val_mean_squared_error: 1.3959\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.27810\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1317 - mean_squared_error: 2.1317 - val_loss: 1.2712 - val_mean_squared_error: 1.2712\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.27810 to 1.27124, saving model to temp/c14\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0702 - mean_squared_error: 2.0702 - val_loss: 1.3498 - val_mean_squared_error: 1.3498\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.27124\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0753 - mean_squared_error: 2.0753 - val_loss: 1.1868 - val_mean_squared_error: 1.1868\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.27124 to 1.18681, saving model to temp/c14\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.0484 - mean_squared_error: 2.0484 - val_loss: 1.3689 - val_mean_squared_error: 1.3689\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.18681\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.0445 - mean_squared_error: 2.0445 - val_loss: 1.2239 - val_mean_squared_error: 1.2239\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.18681\n",
      "Epoch 00010: early stopping\n",
      "temp/c15\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 2.8916 - mean_squared_error: 2.8916 - val_loss: 1.6319 - val_mean_squared_error: 1.6319\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.63190, saving model to temp/c15\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.4448 - mean_squared_error: 2.4448 - val_loss: 1.6564 - val_mean_squared_error: 1.6564\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.63190\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 200us/step - loss: 2.3361 - mean_squared_error: 2.3361 - val_loss: 1.5561 - val_mean_squared_error: 1.5561\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.63190 to 1.55614, saving model to temp/c15\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.2534 - mean_squared_error: 2.2534 - val_loss: 1.3452 - val_mean_squared_error: 1.3452\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.55614 to 1.34524, saving model to temp/c15\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2253 - mean_squared_error: 2.2253 - val_loss: 1.3084 - val_mean_squared_error: 1.3084\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.34524 to 1.30842, saving model to temp/c15\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1731 - mean_squared_error: 2.1731 - val_loss: 1.2255 - val_mean_squared_error: 1.2255\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.30842 to 1.22552, saving model to temp/c15\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1604 - mean_squared_error: 2.1604 - val_loss: 1.6243 - val_mean_squared_error: 1.6243\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.22552\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1041 - mean_squared_error: 2.1041 - val_loss: 1.1611 - val_mean_squared_error: 1.1611\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.22552 to 1.16109, saving model to temp/c15\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0762 - mean_squared_error: 2.0762 - val_loss: 1.1753 - val_mean_squared_error: 1.1753\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.16109\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0760 - mean_squared_error: 2.0760 - val_loss: 1.1768 - val_mean_squared_error: 1.1768\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.16109\n",
      "Epoch 00010: early stopping\n",
      "temp/c16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 2.9336 - mean_squared_error: 2.9336 - val_loss: 1.7104 - val_mean_squared_error: 1.7104\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.71040, saving model to temp/c16\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.4586 - mean_squared_error: 2.4586 - val_loss: 1.4659 - val_mean_squared_error: 1.4659\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.71040 to 1.46587, saving model to temp/c16\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.3755 - mean_squared_error: 2.3755 - val_loss: 1.4593 - val_mean_squared_error: 1.4593\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.46587 to 1.45933, saving model to temp/c16\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.3284 - mean_squared_error: 2.3284 - val_loss: 1.2985 - val_mean_squared_error: 1.2985\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.45933 to 1.29851, saving model to temp/c16\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2572 - mean_squared_error: 2.2572 - val_loss: 1.5048 - val_mean_squared_error: 1.5048\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.29851\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1728 - mean_squared_error: 2.1728 - val_loss: 1.4511 - val_mean_squared_error: 1.4511\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.29851\n",
      "Epoch 00006: early stopping\n",
      "temp/c17\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.8095 - mean_squared_error: 2.8095 - val_loss: 1.5665 - val_mean_squared_error: 1.5665\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.56652, saving model to temp/c17\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.4236 - mean_squared_error: 2.4236 - val_loss: 1.4492 - val_mean_squared_error: 1.4492\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.56652 to 1.44919, saving model to temp/c17\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.3544 - mean_squared_error: 2.3544 - val_loss: 1.6426 - val_mean_squared_error: 1.6426\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.44919\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2375 - mean_squared_error: 2.2375 - val_loss: 1.4240 - val_mean_squared_error: 1.4240\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.44919 to 1.42398, saving model to temp/c17\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1509 - mean_squared_error: 2.1509 - val_loss: 1.3088 - val_mean_squared_error: 1.3088\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.42398 to 1.30880, saving model to temp/c17\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1508 - mean_squared_error: 2.1508 - val_loss: 1.2563 - val_mean_squared_error: 1.2563\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.30880 to 1.25630, saving model to temp/c17\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1174 - mean_squared_error: 2.1174 - val_loss: 1.2897 - val_mean_squared_error: 1.2897\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.25630\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0467 - mean_squared_error: 2.0467 - val_loss: 1.1587 - val_mean_squared_error: 1.1587\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.25630 to 1.15867, saving model to temp/c17\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0735 - mean_squared_error: 2.0735 - val_loss: 1.2082 - val_mean_squared_error: 1.2082\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.15867\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0260 - mean_squared_error: 2.0260 - val_loss: 1.2652 - val_mean_squared_error: 1.2652\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.15867\n",
      "Epoch 00010: early stopping\n",
      "temp/c18\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 213us/step - loss: 2.9445 - mean_squared_error: 2.9445 - val_loss: 1.5876 - val_mean_squared_error: 1.5876\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.58764, saving model to temp/c18\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.5138 - mean_squared_error: 2.5138 - val_loss: 2.0718 - val_mean_squared_error: 2.0718\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.58764\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.4014 - mean_squared_error: 2.4014 - val_loss: 1.5664 - val_mean_squared_error: 1.5664\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.58764 to 1.56636, saving model to temp/c18\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.2469 - mean_squared_error: 2.2469 - val_loss: 1.3625 - val_mean_squared_error: 1.3625\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.56636 to 1.36255, saving model to temp/c18\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1612 - mean_squared_error: 2.1612 - val_loss: 1.2598 - val_mean_squared_error: 1.2598\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.36255 to 1.25982, saving model to temp/c18\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1570 - mean_squared_error: 2.1570 - val_loss: 1.3628 - val_mean_squared_error: 1.3628\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.25982\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1445 - mean_squared_error: 2.1445 - val_loss: 1.2846 - val_mean_squared_error: 1.2846\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.25982\n",
      "Epoch 00007: early stopping\n",
      "temp/c19\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.8892 - mean_squared_error: 2.8892 - val_loss: 1.7423 - val_mean_squared_error: 1.7423\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.74230, saving model to temp/c19\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.5028 - mean_squared_error: 2.5028 - val_loss: 1.3764 - val_mean_squared_error: 1.3764\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.74230 to 1.37641, saving model to temp/c19\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.3130 - mean_squared_error: 2.3130 - val_loss: 1.3665 - val_mean_squared_error: 1.3665\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.37641 to 1.36653, saving model to temp/c19\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.3206 - mean_squared_error: 2.3206 - val_loss: 1.2686 - val_mean_squared_error: 1.2686\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.36653 to 1.26856, saving model to temp/c19\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1699 - mean_squared_error: 2.1699 - val_loss: 1.2513 - val_mean_squared_error: 1.2513\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.26856 to 1.25129, saving model to temp/c19\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1215 - mean_squared_error: 2.1215 - val_loss: 1.4929 - val_mean_squared_error: 1.4929\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.25129\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1275 - mean_squared_error: 2.1275 - val_loss: 1.3390 - val_mean_squared_error: 1.3390\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.25129\n",
      "Epoch 00007: early stopping\n",
      "temp/c20\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 2.8146 - mean_squared_error: 2.8146 - val_loss: 1.5823 - val_mean_squared_error: 1.5823\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.58231, saving model to temp/c20\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.4272 - mean_squared_error: 2.4272 - val_loss: 1.4416 - val_mean_squared_error: 1.4416\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.58231 to 1.44162, saving model to temp/c20\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.3383 - mean_squared_error: 2.3383 - val_loss: 1.5111 - val_mean_squared_error: 1.5111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss did not improve from 1.44162\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2353 - mean_squared_error: 2.2353 - val_loss: 1.2824 - val_mean_squared_error: 1.2824\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.44162 to 1.28238, saving model to temp/c20\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2023 - mean_squared_error: 2.2023 - val_loss: 1.3802 - val_mean_squared_error: 1.3802\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.28238\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1163 - mean_squared_error: 2.1163 - val_loss: 1.2822 - val_mean_squared_error: 1.2822\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.28238 to 1.28222, saving model to temp/c20\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0594 - mean_squared_error: 2.0594 - val_loss: 1.2646 - val_mean_squared_error: 1.2646\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.28222 to 1.26459, saving model to temp/c20\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1047 - mean_squared_error: 2.1047 - val_loss: 1.4250 - val_mean_squared_error: 1.4250\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.26459\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0678 - mean_squared_error: 2.0678 - val_loss: 1.5304 - val_mean_squared_error: 1.5304\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.26459\n",
      "Epoch 00009: early stopping\n",
      "temp/c21\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 2.9143 - mean_squared_error: 2.9143 - val_loss: 1.6104 - val_mean_squared_error: 1.6104\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.61038, saving model to temp/c21\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.4527 - mean_squared_error: 2.4527 - val_loss: 1.4443 - val_mean_squared_error: 1.4443\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.61038 to 1.44430, saving model to temp/c21\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.3396 - mean_squared_error: 2.3396 - val_loss: 1.6473 - val_mean_squared_error: 1.6473\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.44430\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2824 - mean_squared_error: 2.2824 - val_loss: 1.3279 - val_mean_squared_error: 1.3279\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.44430 to 1.32794, saving model to temp/c21\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2458 - mean_squared_error: 2.2458 - val_loss: 1.2469 - val_mean_squared_error: 1.2469\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.32794 to 1.24693, saving model to temp/c21\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1584 - mean_squared_error: 2.1584 - val_loss: 1.3244 - val_mean_squared_error: 1.3244\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.24693\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0831 - mean_squared_error: 2.0831 - val_loss: 1.2358 - val_mean_squared_error: 1.2358\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.24693 to 1.23582, saving model to temp/c21\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1192 - mean_squared_error: 2.1192 - val_loss: 1.3374 - val_mean_squared_error: 1.3374\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.23582\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0387 - mean_squared_error: 2.0387 - val_loss: 1.1792 - val_mean_squared_error: 1.1792\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.23582 to 1.17923, saving model to temp/c21\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0808 - mean_squared_error: 2.0808 - val_loss: 1.2449 - val_mean_squared_error: 1.2449\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.17923\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0533 - mean_squared_error: 2.0533 - val_loss: 1.1964 - val_mean_squared_error: 1.1964\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.17923\n",
      "Epoch 00011: early stopping\n",
      "temp/c22\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 2.9243 - mean_squared_error: 2.9243 - val_loss: 1.6146 - val_mean_squared_error: 1.6146\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.61461, saving model to temp/c22\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.4854 - mean_squared_error: 2.4854 - val_loss: 1.5211 - val_mean_squared_error: 1.5211\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.61461 to 1.52108, saving model to temp/c22\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.3916 - mean_squared_error: 2.3916 - val_loss: 1.3605 - val_mean_squared_error: 1.3605\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.52108 to 1.36047, saving model to temp/c22\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.2403 - mean_squared_error: 2.2403 - val_loss: 1.2528 - val_mean_squared_error: 1.2528\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.36047 to 1.25277, saving model to temp/c22\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2005 - mean_squared_error: 2.2005 - val_loss: 1.3145 - val_mean_squared_error: 1.3145\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.25277\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1134 - mean_squared_error: 2.1134 - val_loss: 1.2383 - val_mean_squared_error: 1.2383\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.25277 to 1.23829, saving model to temp/c22\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1277 - mean_squared_error: 2.1277 - val_loss: 1.2870 - val_mean_squared_error: 1.2870\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.23829\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.0629 - mean_squared_error: 2.0629 - val_loss: 1.1596 - val_mean_squared_error: 1.1596\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.23829 to 1.15963, saving model to temp/c22\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.0650 - mean_squared_error: 2.0650 - val_loss: 1.3791 - val_mean_squared_error: 1.3791\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.15963\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0322 - mean_squared_error: 2.0322 - val_loss: 1.1236 - val_mean_squared_error: 1.1236\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.15963 to 1.12356, saving model to temp/c22\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0203 - mean_squared_error: 2.0203 - val_loss: 1.2870 - val_mean_squared_error: 1.2870\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.12356\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0017 - mean_squared_error: 2.0017 - val_loss: 1.1123 - val_mean_squared_error: 1.1123\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.12356 to 1.11234, saving model to temp/c22\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 1.9661 - mean_squared_error: 1.9661 - val_loss: 1.2567 - val_mean_squared_error: 1.2567\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.11234\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.0293 - mean_squared_error: 2.0293 - val_loss: 1.1065 - val_mean_squared_error: 1.1065\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.11234 to 1.10648, saving model to temp/c22\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 1.9987 - mean_squared_error: 1.9987 - val_loss: 1.2555 - val_mean_squared_error: 1.2555\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.10648\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 1.9795 - mean_squared_error: 1.9795 - val_loss: 1.3280 - val_mean_squared_error: 1.3280\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.10648\n",
      "Epoch 00016: early stopping\n",
      "temp/c23\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.8497 - mean_squared_error: 2.8497 - val_loss: 1.6082 - val_mean_squared_error: 1.6082\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.60819, saving model to temp/c23\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.3901 - mean_squared_error: 2.3901 - val_loss: 1.3912 - val_mean_squared_error: 1.3912\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.60819 to 1.39119, saving model to temp/c23\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2967 - mean_squared_error: 2.2967 - val_loss: 1.4181 - val_mean_squared_error: 1.4181\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.39119\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.2713 - mean_squared_error: 2.2713 - val_loss: 1.5228 - val_mean_squared_error: 1.5228\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.39119\n",
      "Epoch 00004: early stopping\n",
      "temp/c24\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 2.8544 - mean_squared_error: 2.8544 - val_loss: 2.1706 - val_mean_squared_error: 2.1706\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.17058, saving model to temp/c24\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.4490 - mean_squared_error: 2.4490 - val_loss: 1.5323 - val_mean_squared_error: 1.5323\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.17058 to 1.53230, saving model to temp/c24\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.3205 - mean_squared_error: 2.3205 - val_loss: 1.3710 - val_mean_squared_error: 1.3710\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.53230 to 1.37098, saving model to temp/c24\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2510 - mean_squared_error: 2.2510 - val_loss: 1.3207 - val_mean_squared_error: 1.3207\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.37098 to 1.32074, saving model to temp/c24\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1383 - mean_squared_error: 2.1383 - val_loss: 1.2420 - val_mean_squared_error: 1.2420\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.32074 to 1.24203, saving model to temp/c24\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1612 - mean_squared_error: 2.1612 - val_loss: 1.2222 - val_mean_squared_error: 1.2222\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.24203 to 1.22222, saving model to temp/c24\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1159 - mean_squared_error: 2.1159 - val_loss: 1.3142 - val_mean_squared_error: 1.3142\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.22222\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0485 - mean_squared_error: 2.0485 - val_loss: 1.2818 - val_mean_squared_error: 1.2818\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.22222\n",
      "Epoch 00008: early stopping\n",
      "temp/c25\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.8857 - mean_squared_error: 2.8857 - val_loss: 1.6522 - val_mean_squared_error: 1.6522\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.65217, saving model to temp/c25\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.4701 - mean_squared_error: 2.4701 - val_loss: 1.5615 - val_mean_squared_error: 1.5615\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.65217 to 1.56145, saving model to temp/c25\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.3491 - mean_squared_error: 2.3491 - val_loss: 1.5984 - val_mean_squared_error: 1.5984\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.56145\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2913 - mean_squared_error: 2.2913 - val_loss: 1.3226 - val_mean_squared_error: 1.3226\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.56145 to 1.32262, saving model to temp/c25\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1680 - mean_squared_error: 2.1680 - val_loss: 1.3609 - val_mean_squared_error: 1.3609\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.32262\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1491 - mean_squared_error: 2.1491 - val_loss: 1.2462 - val_mean_squared_error: 1.2462\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.32262 to 1.24620, saving model to temp/c25\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1265 - mean_squared_error: 2.1265 - val_loss: 1.3827 - val_mean_squared_error: 1.3827\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.24620\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1382 - mean_squared_error: 2.1382 - val_loss: 1.2838 - val_mean_squared_error: 1.2838\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.24620\n",
      "Epoch 00008: early stopping\n",
      "temp/c26\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.8372 - mean_squared_error: 2.8372 - val_loss: 1.7006 - val_mean_squared_error: 1.7006\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.70056, saving model to temp/c26\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.4489 - mean_squared_error: 2.4489 - val_loss: 1.6268 - val_mean_squared_error: 1.6268\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.70056 to 1.62685, saving model to temp/c26\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.3819 - mean_squared_error: 2.3819 - val_loss: 1.5023 - val_mean_squared_error: 1.5023\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.62685 to 1.50235, saving model to temp/c26\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2727 - mean_squared_error: 2.2727 - val_loss: 1.3337 - val_mean_squared_error: 1.3337\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.50235 to 1.33365, saving model to temp/c26\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2210 - mean_squared_error: 2.2210 - val_loss: 1.3246 - val_mean_squared_error: 1.3246\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.33365 to 1.32458, saving model to temp/c26\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1673 - mean_squared_error: 2.1673 - val_loss: 1.2246 - val_mean_squared_error: 1.2246\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.32458 to 1.22458, saving model to temp/c26\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1397 - mean_squared_error: 2.1397 - val_loss: 1.3592 - val_mean_squared_error: 1.3592\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.22458\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0971 - mean_squared_error: 2.0971 - val_loss: 1.2082 - val_mean_squared_error: 1.2082\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.22458 to 1.20819, saving model to temp/c26\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0311 - mean_squared_error: 2.0311 - val_loss: 1.1339 - val_mean_squared_error: 1.1339\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.20819 to 1.13393, saving model to temp/c26\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.0251 - mean_squared_error: 2.0251 - val_loss: 1.5053 - val_mean_squared_error: 1.5053\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.13393\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0260 - mean_squared_error: 2.0260 - val_loss: 1.0997 - val_mean_squared_error: 1.0997\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.13393 to 1.09969, saving model to temp/c26\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0568 - mean_squared_error: 2.0568 - val_loss: 1.3530 - val_mean_squared_error: 1.3530\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.09969\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.0092 - mean_squared_error: 2.0092 - val_loss: 1.3555 - val_mean_squared_error: 1.3555\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.09969\n",
      "Epoch 00013: early stopping\n",
      "temp/c27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.8463 - mean_squared_error: 2.8463 - val_loss: 1.5175 - val_mean_squared_error: 1.5175\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.51750, saving model to temp/c27\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.4507 - mean_squared_error: 2.4507 - val_loss: 1.6533 - val_mean_squared_error: 1.6533\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.51750\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2679 - mean_squared_error: 2.2679 - val_loss: 1.8718 - val_mean_squared_error: 1.8718\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.51750\n",
      "Epoch 00003: early stopping\n",
      "temp/c28\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.8534 - mean_squared_error: 2.8534 - val_loss: 1.6419 - val_mean_squared_error: 1.6419\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.64192, saving model to temp/c28\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.4604 - mean_squared_error: 2.4604 - val_loss: 1.4174 - val_mean_squared_error: 1.4174\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.64192 to 1.41737, saving model to temp/c28\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.3159 - mean_squared_error: 2.3159 - val_loss: 1.5330 - val_mean_squared_error: 1.5330\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.41737\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2399 - mean_squared_error: 2.2399 - val_loss: 1.3629 - val_mean_squared_error: 1.3629\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.41737 to 1.36294, saving model to temp/c28\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1965 - mean_squared_error: 2.1965 - val_loss: 1.4019 - val_mean_squared_error: 1.4019\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.36294\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1660 - mean_squared_error: 2.1660 - val_loss: 1.4865 - val_mean_squared_error: 1.4865\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.36294\n",
      "Epoch 00006: early stopping\n",
      "temp/c29\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 2.8993 - mean_squared_error: 2.8993 - val_loss: 1.5191 - val_mean_squared_error: 1.5191\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.51907, saving model to temp/c29\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.4549 - mean_squared_error: 2.4549 - val_loss: 1.5005 - val_mean_squared_error: 1.5005\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.51907 to 1.50048, saving model to temp/c29\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.3124 - mean_squared_error: 2.3124 - val_loss: 1.3963 - val_mean_squared_error: 1.3963\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.50048 to 1.39629, saving model to temp/c29\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2643 - mean_squared_error: 2.2643 - val_loss: 1.5903 - val_mean_squared_error: 1.5903\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.39629\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1692 - mean_squared_error: 2.1692 - val_loss: 1.5356 - val_mean_squared_error: 1.5356\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.39629\n",
      "Epoch 00005: early stopping\n",
      "temp/c30\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 2.8266 - mean_squared_error: 2.8266 - val_loss: 1.8345 - val_mean_squared_error: 1.8345\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.83450, saving model to temp/c30\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.5035 - mean_squared_error: 2.5035 - val_loss: 1.9207 - val_mean_squared_error: 1.9207\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.83450\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2589 - mean_squared_error: 2.2589 - val_loss: 1.3601 - val_mean_squared_error: 1.3601\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.83450 to 1.36014, saving model to temp/c30\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2475 - mean_squared_error: 2.2475 - val_loss: 1.2950 - val_mean_squared_error: 1.2950\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.36014 to 1.29497, saving model to temp/c30\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1443 - mean_squared_error: 2.1443 - val_loss: 1.2240 - val_mean_squared_error: 1.2240\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.29497 to 1.22401, saving model to temp/c30\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1667 - mean_squared_error: 2.1667 - val_loss: 1.4907 - val_mean_squared_error: 1.4907\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.22401\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1273 - mean_squared_error: 2.1273 - val_loss: 1.1696 - val_mean_squared_error: 1.1696\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.22401 to 1.16963, saving model to temp/c30\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.0328 - mean_squared_error: 2.0328 - val_loss: 1.4451 - val_mean_squared_error: 1.4451\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.16963\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0782 - mean_squared_error: 2.0782 - val_loss: 1.1946 - val_mean_squared_error: 1.1946\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.16963\n",
      "Epoch 00009: early stopping\n",
      "temp/c31\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 2.8655 - mean_squared_error: 2.8655 - val_loss: 1.8391 - val_mean_squared_error: 1.8391\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.83908, saving model to temp/c31\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.4668 - mean_squared_error: 2.4668 - val_loss: 1.4406 - val_mean_squared_error: 1.4406\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.83908 to 1.44058, saving model to temp/c31\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.3572 - mean_squared_error: 2.3572 - val_loss: 1.3436 - val_mean_squared_error: 1.3436\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.44058 to 1.34363, saving model to temp/c31\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.2860 - mean_squared_error: 2.2860 - val_loss: 1.4592 - val_mean_squared_error: 1.4592\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.34363\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2117 - mean_squared_error: 2.2117 - val_loss: 1.3211 - val_mean_squared_error: 1.3211\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.34363 to 1.32109, saving model to temp/c31\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1875 - mean_squared_error: 2.1875 - val_loss: 1.2624 - val_mean_squared_error: 1.2624\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.32109 to 1.26238, saving model to temp/c31\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1070 - mean_squared_error: 2.1070 - val_loss: 1.2628 - val_mean_squared_error: 1.2628\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.26238\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0599 - mean_squared_error: 2.0599 - val_loss: 1.4550 - val_mean_squared_error: 1.4550\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.26238\n",
      "Epoch 00008: early stopping\n",
      "temp/c32\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.8029 - mean_squared_error: 2.8029 - val_loss: 1.6374 - val_mean_squared_error: 1.6374\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.63743, saving model to temp/c32\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.4691 - mean_squared_error: 2.4691 - val_loss: 1.5940 - val_mean_squared_error: 1.5940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_loss improved from 1.63743 to 1.59404, saving model to temp/c32\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.3616 - mean_squared_error: 2.3616 - val_loss: 1.3594 - val_mean_squared_error: 1.3594\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.59404 to 1.35940, saving model to temp/c32\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1989 - mean_squared_error: 2.1989 - val_loss: 1.2748 - val_mean_squared_error: 1.2748\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.35940 to 1.27479, saving model to temp/c32\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2687 - mean_squared_error: 2.2687 - val_loss: 1.2523 - val_mean_squared_error: 1.2523\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.27479 to 1.25226, saving model to temp/c32\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2029 - mean_squared_error: 2.2029 - val_loss: 1.3079 - val_mean_squared_error: 1.3079\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.25226\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.0846 - mean_squared_error: 2.0846 - val_loss: 1.1981 - val_mean_squared_error: 1.1981\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.25226 to 1.19809, saving model to temp/c32\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1067 - mean_squared_error: 2.1067 - val_loss: 1.3113 - val_mean_squared_error: 1.3113\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.19809\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1009 - mean_squared_error: 2.1009 - val_loss: 1.1230 - val_mean_squared_error: 1.1230\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.19809 to 1.12304, saving model to temp/c32\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0610 - mean_squared_error: 2.0610 - val_loss: 1.1806 - val_mean_squared_error: 1.1806\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.12304\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0798 - mean_squared_error: 2.0798 - val_loss: 1.6744 - val_mean_squared_error: 1.6744\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.12304\n",
      "Epoch 00011: early stopping\n",
      "temp/c33\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 221us/step - loss: 2.8517 - mean_squared_error: 2.8517 - val_loss: 1.8177 - val_mean_squared_error: 1.8177\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.81775, saving model to temp/c33\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.4860 - mean_squared_error: 2.4860 - val_loss: 1.6293 - val_mean_squared_error: 1.6293\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.81775 to 1.62925, saving model to temp/c33\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.3785 - mean_squared_error: 2.3785 - val_loss: 1.5844 - val_mean_squared_error: 1.5844\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.62925 to 1.58440, saving model to temp/c33\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2590 - mean_squared_error: 2.2590 - val_loss: 1.3625 - val_mean_squared_error: 1.3625\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.58440 to 1.36246, saving model to temp/c33\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.2172 - mean_squared_error: 2.2172 - val_loss: 1.2492 - val_mean_squared_error: 1.2492\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.36246 to 1.24920, saving model to temp/c33\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1397 - mean_squared_error: 2.1397 - val_loss: 1.5943 - val_mean_squared_error: 1.5943\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.24920\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1400 - mean_squared_error: 2.1400 - val_loss: 1.1843 - val_mean_squared_error: 1.1843\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.24920 to 1.18426, saving model to temp/c33\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.0604 - mean_squared_error: 2.0604 - val_loss: 1.1316 - val_mean_squared_error: 1.1316\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.18426 to 1.13157, saving model to temp/c33\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0955 - mean_squared_error: 2.0955 - val_loss: 1.3746 - val_mean_squared_error: 1.3746\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.13157\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0287 - mean_squared_error: 2.0287 - val_loss: 1.1847 - val_mean_squared_error: 1.1847\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.13157\n",
      "Epoch 00010: early stopping\n",
      "temp/c34\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 2.9261 - mean_squared_error: 2.9261 - val_loss: 2.0449 - val_mean_squared_error: 2.0449\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.04493, saving model to temp/c34\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.4827 - mean_squared_error: 2.4827 - val_loss: 1.4897 - val_mean_squared_error: 1.4897\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.04493 to 1.48972, saving model to temp/c34\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.3452 - mean_squared_error: 2.3452 - val_loss: 1.3871 - val_mean_squared_error: 1.3871\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.48972 to 1.38711, saving model to temp/c34\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.2364 - mean_squared_error: 2.2364 - val_loss: 1.6932 - val_mean_squared_error: 1.6932\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.38711\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1462 - mean_squared_error: 2.1462 - val_loss: 1.4028 - val_mean_squared_error: 1.4028\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.38711\n",
      "Epoch 00005: early stopping\n",
      "temp/c35\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 213us/step - loss: 2.8963 - mean_squared_error: 2.8963 - val_loss: 1.7229 - val_mean_squared_error: 1.7229\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.72293, saving model to temp/c35\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.5040 - mean_squared_error: 2.5040 - val_loss: 1.7619 - val_mean_squared_error: 1.7619\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.72293\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.3874 - mean_squared_error: 2.3874 - val_loss: 1.6296 - val_mean_squared_error: 1.6296\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.72293 to 1.62960, saving model to temp/c35\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2760 - mean_squared_error: 2.2760 - val_loss: 1.6491 - val_mean_squared_error: 1.6491\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.62960\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2358 - mean_squared_error: 2.2358 - val_loss: 1.2232 - val_mean_squared_error: 1.2232\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.62960 to 1.22320, saving model to temp/c35\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1882 - mean_squared_error: 2.1882 - val_loss: 1.3197 - val_mean_squared_error: 1.3197\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.22320\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1014 - mean_squared_error: 2.1014 - val_loss: 1.2530 - val_mean_squared_error: 1.2530\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.22320\n",
      "Epoch 00007: early stopping\n",
      "temp/c36\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.9300 - mean_squared_error: 2.9300 - val_loss: 1.5660 - val_mean_squared_error: 1.5660\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.56599, saving model to temp/c36\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.5125 - mean_squared_error: 2.5125 - val_loss: 1.8720 - val_mean_squared_error: 1.8720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_loss did not improve from 1.56599\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.3348 - mean_squared_error: 2.3348 - val_loss: 1.4963 - val_mean_squared_error: 1.4963\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.56599 to 1.49634, saving model to temp/c36\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2772 - mean_squared_error: 2.2772 - val_loss: 1.3574 - val_mean_squared_error: 1.3574\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.49634 to 1.35738, saving model to temp/c36\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2487 - mean_squared_error: 2.2487 - val_loss: 1.3418 - val_mean_squared_error: 1.3418\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.35738 to 1.34184, saving model to temp/c36\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1701 - mean_squared_error: 2.1701 - val_loss: 1.4285 - val_mean_squared_error: 1.4285\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.34184\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1386 - mean_squared_error: 2.1386 - val_loss: 1.3204 - val_mean_squared_error: 1.3204\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.34184 to 1.32041, saving model to temp/c36\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0991 - mean_squared_error: 2.0991 - val_loss: 1.5432 - val_mean_squared_error: 1.5432\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.32041\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1061 - mean_squared_error: 2.1061 - val_loss: 1.2182 - val_mean_squared_error: 1.2182\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.32041 to 1.21823, saving model to temp/c36\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0680 - mean_squared_error: 2.0680 - val_loss: 1.1397 - val_mean_squared_error: 1.1397\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.21823 to 1.13972, saving model to temp/c36\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0166 - mean_squared_error: 2.0166 - val_loss: 1.1319 - val_mean_squared_error: 1.1319\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.13972 to 1.13185, saving model to temp/c36\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0201 - mean_squared_error: 2.0201 - val_loss: 1.2339 - val_mean_squared_error: 1.2339\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.13185\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0004 - mean_squared_error: 2.0004 - val_loss: 1.2303 - val_mean_squared_error: 1.2303\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.13185\n",
      "Epoch 00013: early stopping\n",
      "temp/c37\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.8247 - mean_squared_error: 2.8247 - val_loss: 1.6934 - val_mean_squared_error: 1.6934\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.69337, saving model to temp/c37\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.4598 - mean_squared_error: 2.4598 - val_loss: 1.4390 - val_mean_squared_error: 1.4390\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.69337 to 1.43902, saving model to temp/c37\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2955 - mean_squared_error: 2.2955 - val_loss: 1.3889 - val_mean_squared_error: 1.3889\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.43902 to 1.38893, saving model to temp/c37\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2060 - mean_squared_error: 2.2060 - val_loss: 1.4418 - val_mean_squared_error: 1.4418\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.38893\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 200us/step - loss: 2.1958 - mean_squared_error: 2.1958 - val_loss: 1.4027 - val_mean_squared_error: 1.4027\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.38893\n",
      "Epoch 00005: early stopping\n",
      "temp/c38\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.8437 - mean_squared_error: 2.8437 - val_loss: 1.7459 - val_mean_squared_error: 1.7459\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.74592, saving model to temp/c38\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.4745 - mean_squared_error: 2.4745 - val_loss: 1.4851 - val_mean_squared_error: 1.4851\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.74592 to 1.48513, saving model to temp/c38\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.3629 - mean_squared_error: 2.3629 - val_loss: 1.5696 - val_mean_squared_error: 1.5696\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.48513\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2703 - mean_squared_error: 2.2703 - val_loss: 1.2798 - val_mean_squared_error: 1.2798\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.48513 to 1.27984, saving model to temp/c38\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1936 - mean_squared_error: 2.1936 - val_loss: 1.3170 - val_mean_squared_error: 1.3170\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.27984\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1593 - mean_squared_error: 2.1593 - val_loss: 1.2082 - val_mean_squared_error: 1.2082\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.27984 to 1.20823, saving model to temp/c38\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0629 - mean_squared_error: 2.0629 - val_loss: 1.1954 - val_mean_squared_error: 1.1954\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.20823 to 1.19541, saving model to temp/c38\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0894 - mean_squared_error: 2.0894 - val_loss: 1.1655 - val_mean_squared_error: 1.1655\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.19541 to 1.16553, saving model to temp/c38\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0880 - mean_squared_error: 2.0880 - val_loss: 1.3950 - val_mean_squared_error: 1.3950\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.16553\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0711 - mean_squared_error: 2.0711 - val_loss: 1.4496 - val_mean_squared_error: 1.4496\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.16553\n",
      "Epoch 00010: early stopping\n",
      "temp/c39\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.9023 - mean_squared_error: 2.9023 - val_loss: 1.9892 - val_mean_squared_error: 1.9892\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.98917, saving model to temp/c39\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.4997 - mean_squared_error: 2.4997 - val_loss: 1.4292 - val_mean_squared_error: 1.4292\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.98917 to 1.42919, saving model to temp/c39\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.3440 - mean_squared_error: 2.3440 - val_loss: 1.3601 - val_mean_squared_error: 1.3601\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.42919 to 1.36015, saving model to temp/c39\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2584 - mean_squared_error: 2.2584 - val_loss: 1.3110 - val_mean_squared_error: 1.3110\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.36015 to 1.31099, saving model to temp/c39\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1838 - mean_squared_error: 2.1838 - val_loss: 1.2494 - val_mean_squared_error: 1.2494\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.31099 to 1.24938, saving model to temp/c39\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1882 - mean_squared_error: 2.1882 - val_loss: 1.5743 - val_mean_squared_error: 1.5743\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.24938\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1364 - mean_squared_error: 2.1364 - val_loss: 1.1918 - val_mean_squared_error: 1.1918\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.24938 to 1.19176, saving model to temp/c39\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1089 - mean_squared_error: 2.1089 - val_loss: 1.2143 - val_mean_squared_error: 1.2143\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.19176\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.0916 - mean_squared_error: 2.0916 - val_loss: 1.2229 - val_mean_squared_error: 1.2229\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.19176\n",
      "Epoch 00009: early stopping\n",
      "temp/c40\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.8748 - mean_squared_error: 2.8748 - val_loss: 2.0580 - val_mean_squared_error: 2.0580\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.05795, saving model to temp/c40\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.5035 - mean_squared_error: 2.5035 - val_loss: 1.6523 - val_mean_squared_error: 1.6523\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.05795 to 1.65228, saving model to temp/c40\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.3321 - mean_squared_error: 2.3321 - val_loss: 1.3697 - val_mean_squared_error: 1.3697\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.65228 to 1.36973, saving model to temp/c40\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2582 - mean_squared_error: 2.2582 - val_loss: 1.4803 - val_mean_squared_error: 1.4803\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.36973\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1875 - mean_squared_error: 2.1875 - val_loss: 1.2777 - val_mean_squared_error: 1.2777\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.36973 to 1.27775, saving model to temp/c40\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1547 - mean_squared_error: 2.1547 - val_loss: 1.2183 - val_mean_squared_error: 1.2183\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.27775 to 1.21826, saving model to temp/c40\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1203 - mean_squared_error: 2.1203 - val_loss: 1.3245 - val_mean_squared_error: 1.3245\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.21826\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0486 - mean_squared_error: 2.0486 - val_loss: 1.4226 - val_mean_squared_error: 1.4226\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.21826\n",
      "Epoch 00008: early stopping\n",
      "temp/c41\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 2.8974 - mean_squared_error: 2.8974 - val_loss: 1.6815 - val_mean_squared_error: 1.6815\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.68146, saving model to temp/c41\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.4345 - mean_squared_error: 2.4345 - val_loss: 1.4813 - val_mean_squared_error: 1.4813\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.68146 to 1.48131, saving model to temp/c41\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2970 - mean_squared_error: 2.2970 - val_loss: 1.3890 - val_mean_squared_error: 1.3890\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.48131 to 1.38900, saving model to temp/c41\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2844 - mean_squared_error: 2.2844 - val_loss: 1.3099 - val_mean_squared_error: 1.3099\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.38900 to 1.30990, saving model to temp/c41\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1477 - mean_squared_error: 2.1477 - val_loss: 1.4267 - val_mean_squared_error: 1.4267\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.30990\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1843 - mean_squared_error: 2.1843 - val_loss: 1.3601 - val_mean_squared_error: 1.3601\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.30990\n",
      "Epoch 00006: early stopping\n",
      "temp/c42\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.9316 - mean_squared_error: 2.9316 - val_loss: 1.6312 - val_mean_squared_error: 1.6312\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.63115, saving model to temp/c42\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.4661 - mean_squared_error: 2.4661 - val_loss: 1.4533 - val_mean_squared_error: 1.4533\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.63115 to 1.45326, saving model to temp/c42\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.3645 - mean_squared_error: 2.3645 - val_loss: 1.4306 - val_mean_squared_error: 1.4306\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.45326 to 1.43062, saving model to temp/c42\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.3218 - mean_squared_error: 2.3218 - val_loss: 1.3860 - val_mean_squared_error: 1.3860\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.43062 to 1.38600, saving model to temp/c42\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2045 - mean_squared_error: 2.2045 - val_loss: 1.3967 - val_mean_squared_error: 1.3967\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.38600\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1549 - mean_squared_error: 2.1549 - val_loss: 1.2813 - val_mean_squared_error: 1.2813\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.38600 to 1.28126, saving model to temp/c42\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1012 - mean_squared_error: 2.1012 - val_loss: 1.4076 - val_mean_squared_error: 1.4076\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.28126\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1187 - mean_squared_error: 2.1187 - val_loss: 1.3058 - val_mean_squared_error: 1.3058\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.28126\n",
      "Epoch 00008: early stopping\n",
      "temp/c43\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 2.8562 - mean_squared_error: 2.8562 - val_loss: 1.6811 - val_mean_squared_error: 1.6811\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.68105, saving model to temp/c43\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.4707 - mean_squared_error: 2.4707 - val_loss: 1.4533 - val_mean_squared_error: 1.4533\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.68105 to 1.45334, saving model to temp/c43\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.3170 - mean_squared_error: 2.3170 - val_loss: 1.4288 - val_mean_squared_error: 1.4288\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.45334 to 1.42877, saving model to temp/c43\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2797 - mean_squared_error: 2.2797 - val_loss: 1.3004 - val_mean_squared_error: 1.3004\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.42877 to 1.30042, saving model to temp/c43\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1981 - mean_squared_error: 2.1981 - val_loss: 1.4780 - val_mean_squared_error: 1.4780\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.30042\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1728 - mean_squared_error: 2.1728 - val_loss: 1.4413 - val_mean_squared_error: 1.4413\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.30042\n",
      "Epoch 00006: early stopping\n",
      "temp/c44\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 2.8382 - mean_squared_error: 2.8382 - val_loss: 1.5436 - val_mean_squared_error: 1.5436\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.54365, saving model to temp/c44\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.4411 - mean_squared_error: 2.4411 - val_loss: 1.4062 - val_mean_squared_error: 1.4062\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.54365 to 1.40623, saving model to temp/c44\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.3109 - mean_squared_error: 2.3109 - val_loss: 1.5575 - val_mean_squared_error: 1.5575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss did not improve from 1.40623\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2511 - mean_squared_error: 2.2511 - val_loss: 1.4566 - val_mean_squared_error: 1.4566\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.40623\n",
      "Epoch 00004: early stopping\n",
      "temp/c45\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.8294 - mean_squared_error: 2.8294 - val_loss: 1.5539 - val_mean_squared_error: 1.5539\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.55392, saving model to temp/c45\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.4675 - mean_squared_error: 2.4675 - val_loss: 1.5313 - val_mean_squared_error: 1.5313\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.55392 to 1.53126, saving model to temp/c45\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.3137 - mean_squared_error: 2.3137 - val_loss: 1.4490 - val_mean_squared_error: 1.4490\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.53126 to 1.44898, saving model to temp/c45\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2608 - mean_squared_error: 2.2608 - val_loss: 1.2937 - val_mean_squared_error: 1.2937\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.44898 to 1.29368, saving model to temp/c45\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1478 - mean_squared_error: 2.1478 - val_loss: 1.2252 - val_mean_squared_error: 1.2252\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.29368 to 1.22525, saving model to temp/c45\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1253 - mean_squared_error: 2.1253 - val_loss: 1.2314 - val_mean_squared_error: 1.2314\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.22525\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0680 - mean_squared_error: 2.0680 - val_loss: 1.3247 - val_mean_squared_error: 1.3247\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.22525\n",
      "Epoch 00007: early stopping\n",
      "temp/c46\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.8143 - mean_squared_error: 2.8143 - val_loss: 1.5778 - val_mean_squared_error: 1.5778\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.57783, saving model to temp/c46\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.4347 - mean_squared_error: 2.4347 - val_loss: 1.7140 - val_mean_squared_error: 1.7140\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.57783\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.3350 - mean_squared_error: 2.3350 - val_loss: 1.4218 - val_mean_squared_error: 1.4218\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.57783 to 1.42176, saving model to temp/c46\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.2023 - mean_squared_error: 2.2023 - val_loss: 1.2549 - val_mean_squared_error: 1.2549\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.42176 to 1.25488, saving model to temp/c46\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1890 - mean_squared_error: 2.1890 - val_loss: 1.4203 - val_mean_squared_error: 1.4203\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.25488\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1997 - mean_squared_error: 2.1997 - val_loss: 1.2145 - val_mean_squared_error: 1.2145\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.25488 to 1.21449, saving model to temp/c46\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0965 - mean_squared_error: 2.0965 - val_loss: 1.5204 - val_mean_squared_error: 1.5204\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.21449\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0408 - mean_squared_error: 2.0408 - val_loss: 1.4398 - val_mean_squared_error: 1.4398\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.21449\n",
      "Epoch 00008: early stopping\n",
      "temp/c47\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.8293 - mean_squared_error: 2.8293 - val_loss: 1.5999 - val_mean_squared_error: 1.5999\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.59985, saving model to temp/c47\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.5007 - mean_squared_error: 2.5007 - val_loss: 1.4907 - val_mean_squared_error: 1.4907\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.59985 to 1.49065, saving model to temp/c47\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.3044 - mean_squared_error: 2.3044 - val_loss: 1.4201 - val_mean_squared_error: 1.4201\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.49065 to 1.42008, saving model to temp/c47\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2236 - mean_squared_error: 2.2236 - val_loss: 1.2666 - val_mean_squared_error: 1.2666\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.42008 to 1.26664, saving model to temp/c47\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2071 - mean_squared_error: 2.2071 - val_loss: 1.2741 - val_mean_squared_error: 1.2741\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.26664\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1515 - mean_squared_error: 2.1515 - val_loss: 1.3933 - val_mean_squared_error: 1.3933\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.26664\n",
      "Epoch 00006: early stopping\n",
      "temp/c48\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.9404 - mean_squared_error: 2.9404 - val_loss: 1.5468 - val_mean_squared_error: 1.5468\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.54677, saving model to temp/c48\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 200us/step - loss: 2.4856 - mean_squared_error: 2.4856 - val_loss: 1.6295 - val_mean_squared_error: 1.6295\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.54677\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.3371 - mean_squared_error: 2.3371 - val_loss: 1.4861 - val_mean_squared_error: 1.4861\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.54677 to 1.48608, saving model to temp/c48\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2209 - mean_squared_error: 2.2209 - val_loss: 1.4384 - val_mean_squared_error: 1.4384\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.48608 to 1.43836, saving model to temp/c48\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1447 - mean_squared_error: 2.1447 - val_loss: 1.4036 - val_mean_squared_error: 1.4036\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.43836 to 1.40361, saving model to temp/c48\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2077 - mean_squared_error: 2.2077 - val_loss: 1.2094 - val_mean_squared_error: 1.2094\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.40361 to 1.20940, saving model to temp/c48\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1179 - mean_squared_error: 2.1179 - val_loss: 1.3765 - val_mean_squared_error: 1.3765\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.20940\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0592 - mean_squared_error: 2.0592 - val_loss: 1.1191 - val_mean_squared_error: 1.1191\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.20940 to 1.11913, saving model to temp/c48\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1301 - mean_squared_error: 2.1301 - val_loss: 1.4897 - val_mean_squared_error: 1.4897\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.11913\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0255 - mean_squared_error: 2.0255 - val_loss: 1.1129 - val_mean_squared_error: 1.1129\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.11913 to 1.11290, saving model to temp/c48\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.0155 - mean_squared_error: 2.0155 - val_loss: 1.2351 - val_mean_squared_error: 1.2351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00011: val_loss did not improve from 1.11290\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 1.9857 - mean_squared_error: 1.9857 - val_loss: 1.4113 - val_mean_squared_error: 1.4113\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.11290\n",
      "Epoch 00012: early stopping\n",
      "temp/c49\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.7796 - mean_squared_error: 2.7796 - val_loss: 1.6120 - val_mean_squared_error: 1.6120\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.61201, saving model to temp/c49\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.4248 - mean_squared_error: 2.4248 - val_loss: 1.3892 - val_mean_squared_error: 1.3892\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.61201 to 1.38920, saving model to temp/c49\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.3573 - mean_squared_error: 2.3573 - val_loss: 1.4553 - val_mean_squared_error: 1.4553\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.38920\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2695 - mean_squared_error: 2.2695 - val_loss: 1.5087 - val_mean_squared_error: 1.5087\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.38920\n",
      "Epoch 00004: early stopping\n",
      "temp/c50\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.8034 - mean_squared_error: 2.8034 - val_loss: 1.4830 - val_mean_squared_error: 1.4830\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.48299, saving model to temp/c50\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.4215 - mean_squared_error: 2.4215 - val_loss: 1.3749 - val_mean_squared_error: 1.3749\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.48299 to 1.37489, saving model to temp/c50\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.3139 - mean_squared_error: 2.3139 - val_loss: 1.3205 - val_mean_squared_error: 1.3205\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.37489 to 1.32049, saving model to temp/c50\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1674 - mean_squared_error: 2.1674 - val_loss: 1.2977 - val_mean_squared_error: 1.2977\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.32049 to 1.29768, saving model to temp/c50\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.2095 - mean_squared_error: 2.2095 - val_loss: 1.3735 - val_mean_squared_error: 1.3735\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.29768\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1261 - mean_squared_error: 2.1261 - val_loss: 1.5814 - val_mean_squared_error: 1.5814\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.29768\n",
      "Epoch 00006: early stopping\n",
      "temp/c51\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 2.9150 - mean_squared_error: 2.9150 - val_loss: 1.7942 - val_mean_squared_error: 1.7942\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.79421, saving model to temp/c51\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.4501 - mean_squared_error: 2.4501 - val_loss: 1.4971 - val_mean_squared_error: 1.4971\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.79421 to 1.49708, saving model to temp/c51\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2876 - mean_squared_error: 2.2876 - val_loss: 1.4829 - val_mean_squared_error: 1.4829\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.49708 to 1.48288, saving model to temp/c51\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2924 - mean_squared_error: 2.2924 - val_loss: 1.3098 - val_mean_squared_error: 1.3098\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.48288 to 1.30979, saving model to temp/c51\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.2423 - mean_squared_error: 2.2423 - val_loss: 1.2654 - val_mean_squared_error: 1.2654\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.30979 to 1.26537, saving model to temp/c51\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 199us/step - loss: 2.1007 - mean_squared_error: 2.1007 - val_loss: 1.2892 - val_mean_squared_error: 1.2892\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.26537\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.0773 - mean_squared_error: 2.0773 - val_loss: 1.4546 - val_mean_squared_error: 1.4546\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.26537\n",
      "Epoch 00007: early stopping\n",
      "temp/c52\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.8148 - mean_squared_error: 2.8148 - val_loss: 1.7425 - val_mean_squared_error: 1.7425\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.74249, saving model to temp/c52\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.4224 - mean_squared_error: 2.4224 - val_loss: 1.4691 - val_mean_squared_error: 1.4691\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.74249 to 1.46911, saving model to temp/c52\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.3392 - mean_squared_error: 2.3392 - val_loss: 1.5269 - val_mean_squared_error: 1.5269\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.46911\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1723 - mean_squared_error: 2.1723 - val_loss: 1.2688 - val_mean_squared_error: 1.2688\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.46911 to 1.26881, saving model to temp/c52\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1543 - mean_squared_error: 2.1543 - val_loss: 1.3093 - val_mean_squared_error: 1.3093\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.26881\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1268 - mean_squared_error: 2.1268 - val_loss: 1.2561 - val_mean_squared_error: 1.2561\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.26881 to 1.25612, saving model to temp/c52\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1041 - mean_squared_error: 2.1041 - val_loss: 1.1713 - val_mean_squared_error: 1.1713\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.25612 to 1.17131, saving model to temp/c52\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0695 - mean_squared_error: 2.0695 - val_loss: 1.1132 - val_mean_squared_error: 1.1132\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.17131 to 1.11325, saving model to temp/c52\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0760 - mean_squared_error: 2.0760 - val_loss: 1.1823 - val_mean_squared_error: 1.1823\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.11325\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.0675 - mean_squared_error: 2.0675 - val_loss: 1.1532 - val_mean_squared_error: 1.1532\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.11325\n",
      "Epoch 00010: early stopping\n",
      "temp/c53\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.8720 - mean_squared_error: 2.8720 - val_loss: 1.6070 - val_mean_squared_error: 1.6070\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.60699, saving model to temp/c53\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.5041 - mean_squared_error: 2.5041 - val_loss: 1.4175 - val_mean_squared_error: 1.4175\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.60699 to 1.41752, saving model to temp/c53\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.3581 - mean_squared_error: 2.3581 - val_loss: 1.4252 - val_mean_squared_error: 1.4252\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.41752\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2677 - mean_squared_error: 2.2677 - val_loss: 1.3906 - val_mean_squared_error: 1.3906\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.41752 to 1.39060, saving model to temp/c53\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.1438 - mean_squared_error: 2.1438 - val_loss: 1.3903 - val_mean_squared_error: 1.3903\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.39060 to 1.39034, saving model to temp/c53\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1742 - mean_squared_error: 2.1742 - val_loss: 1.4392 - val_mean_squared_error: 1.4392\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.39034\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 2.1122 - mean_squared_error: 2.1122 - val_loss: 1.3797 - val_mean_squared_error: 1.3797\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.39034 to 1.37974, saving model to temp/c53\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0868 - mean_squared_error: 2.0868 - val_loss: 1.1731 - val_mean_squared_error: 1.1731\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.37974 to 1.17312, saving model to temp/c53\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0898 - mean_squared_error: 2.0898 - val_loss: 1.4571 - val_mean_squared_error: 1.4571\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.17312\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0371 - mean_squared_error: 2.0371 - val_loss: 1.2705 - val_mean_squared_error: 1.2705\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.17312\n",
      "Epoch 00010: early stopping\n",
      "temp/c54\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.8505 - mean_squared_error: 2.8505 - val_loss: 1.5737 - val_mean_squared_error: 1.5737\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.57369, saving model to temp/c54\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.4332 - mean_squared_error: 2.4332 - val_loss: 1.4530 - val_mean_squared_error: 1.4530\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.57369 to 1.45298, saving model to temp/c54\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2931 - mean_squared_error: 2.2931 - val_loss: 1.3402 - val_mean_squared_error: 1.3402\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.45298 to 1.34022, saving model to temp/c54\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2697 - mean_squared_error: 2.2697 - val_loss: 1.2619 - val_mean_squared_error: 1.2619\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.34022 to 1.26186, saving model to temp/c54\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1256 - mean_squared_error: 2.1256 - val_loss: 1.3619 - val_mean_squared_error: 1.3619\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.26186\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1717 - mean_squared_error: 2.1717 - val_loss: 1.2181 - val_mean_squared_error: 1.2181\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.26186 to 1.21808, saving model to temp/c54\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1262 - mean_squared_error: 2.1262 - val_loss: 1.3609 - val_mean_squared_error: 1.3609\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.21808\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0265 - mean_squared_error: 2.0265 - val_loss: 1.1535 - val_mean_squared_error: 1.1535\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.21808 to 1.15349, saving model to temp/c54\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0744 - mean_squared_error: 2.0744 - val_loss: 1.1672 - val_mean_squared_error: 1.1672\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.15349\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0337 - mean_squared_error: 2.0337 - val_loss: 1.2350 - val_mean_squared_error: 1.2350\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.15349\n",
      "Epoch 00010: early stopping\n",
      "temp/c55\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 2.8906 - mean_squared_error: 2.8906 - val_loss: 1.5549 - val_mean_squared_error: 1.5549\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.55493, saving model to temp/c55\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.4777 - mean_squared_error: 2.4777 - val_loss: 1.5503 - val_mean_squared_error: 1.5503\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.55493 to 1.55030, saving model to temp/c55\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2737 - mean_squared_error: 2.2737 - val_loss: 1.3954 - val_mean_squared_error: 1.3954\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.55030 to 1.39535, saving model to temp/c55\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2260 - mean_squared_error: 2.2260 - val_loss: 1.9710 - val_mean_squared_error: 1.9710\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.39535\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1732 - mean_squared_error: 2.1732 - val_loss: 1.2425 - val_mean_squared_error: 1.2425\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.39535 to 1.24246, saving model to temp/c55\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1408 - mean_squared_error: 2.1408 - val_loss: 1.2857 - val_mean_squared_error: 1.2857\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.24246\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1439 - mean_squared_error: 2.1439 - val_loss: 1.4208 - val_mean_squared_error: 1.4208\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.24246\n",
      "Epoch 00007: early stopping\n",
      "temp/c56\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.8754 - mean_squared_error: 2.8754 - val_loss: 1.6093 - val_mean_squared_error: 1.6093\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.60932, saving model to temp/c56\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.4881 - mean_squared_error: 2.4881 - val_loss: 1.5803 - val_mean_squared_error: 1.5803\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.60932 to 1.58033, saving model to temp/c56\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.3124 - mean_squared_error: 2.3124 - val_loss: 1.5259 - val_mean_squared_error: 1.5259\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.58033 to 1.52592, saving model to temp/c56\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2794 - mean_squared_error: 2.2794 - val_loss: 1.2710 - val_mean_squared_error: 1.2710\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.52592 to 1.27100, saving model to temp/c56\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2013 - mean_squared_error: 2.2013 - val_loss: 1.5214 - val_mean_squared_error: 1.5214\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.27100\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1563 - mean_squared_error: 2.1563 - val_loss: 1.3802 - val_mean_squared_error: 1.3802\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.27100\n",
      "Epoch 00006: early stopping\n",
      "temp/c57\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.8866 - mean_squared_error: 2.8866 - val_loss: 1.5423 - val_mean_squared_error: 1.5423\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.54233, saving model to temp/c57\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.4468 - mean_squared_error: 2.4468 - val_loss: 1.4574 - val_mean_squared_error: 1.4574\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.54233 to 1.45742, saving model to temp/c57\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.3953 - mean_squared_error: 2.3953 - val_loss: 1.4179 - val_mean_squared_error: 1.4179\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.45742 to 1.41788, saving model to temp/c57\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2308 - mean_squared_error: 2.2308 - val_loss: 1.4231 - val_mean_squared_error: 1.4231\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.41788\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2256 - mean_squared_error: 2.2256 - val_loss: 1.2276 - val_mean_squared_error: 1.2276\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.41788 to 1.22756, saving model to temp/c57\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1568 - mean_squared_error: 2.1568 - val_loss: 1.2060 - val_mean_squared_error: 1.2060\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.22756 to 1.20604, saving model to temp/c57\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1189 - mean_squared_error: 2.1189 - val_loss: 1.2870 - val_mean_squared_error: 1.2870\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.20604\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0981 - mean_squared_error: 2.0981 - val_loss: 1.2559 - val_mean_squared_error: 1.2559\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.20604\n",
      "Epoch 00008: early stopping\n",
      "temp/c58\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.8883 - mean_squared_error: 2.8883 - val_loss: 1.7845 - val_mean_squared_error: 1.7845\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.78454, saving model to temp/c58\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.4648 - mean_squared_error: 2.4648 - val_loss: 1.4804 - val_mean_squared_error: 1.4804\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.78454 to 1.48039, saving model to temp/c58\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.3645 - mean_squared_error: 2.3645 - val_loss: 1.4152 - val_mean_squared_error: 1.4152\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.48039 to 1.41520, saving model to temp/c58\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2504 - mean_squared_error: 2.2504 - val_loss: 1.4772 - val_mean_squared_error: 1.4772\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.41520\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.2082 - mean_squared_error: 2.2082 - val_loss: 1.3973 - val_mean_squared_error: 1.3973\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.41520 to 1.39728, saving model to temp/c58\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1362 - mean_squared_error: 2.1362 - val_loss: 1.3059 - val_mean_squared_error: 1.3059\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.39728 to 1.30587, saving model to temp/c58\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1527 - mean_squared_error: 2.1527 - val_loss: 1.5493 - val_mean_squared_error: 1.5493\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.30587\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1449 - mean_squared_error: 2.1449 - val_loss: 1.1720 - val_mean_squared_error: 1.1720\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.30587 to 1.17197, saving model to temp/c58\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0369 - mean_squared_error: 2.0369 - val_loss: 1.2846 - val_mean_squared_error: 1.2846\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.17197\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0545 - mean_squared_error: 2.0545 - val_loss: 1.2159 - val_mean_squared_error: 1.2159\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.17197\n",
      "Epoch 00010: early stopping\n",
      "temp/c59\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 2.8595 - mean_squared_error: 2.8595 - val_loss: 2.1629 - val_mean_squared_error: 2.1629\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.16287, saving model to temp/c59\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.5174 - mean_squared_error: 2.5174 - val_loss: 1.7771 - val_mean_squared_error: 1.7771\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.16287 to 1.77713, saving model to temp/c59\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.3494 - mean_squared_error: 2.3494 - val_loss: 1.4059 - val_mean_squared_error: 1.4059\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.77713 to 1.40587, saving model to temp/c59\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2953 - mean_squared_error: 2.2953 - val_loss: 1.2942 - val_mean_squared_error: 1.2942\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.40587 to 1.29424, saving model to temp/c59\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1885 - mean_squared_error: 2.1885 - val_loss: 1.3526 - val_mean_squared_error: 1.3526\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.29424\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1391 - mean_squared_error: 2.1391 - val_loss: 1.1923 - val_mean_squared_error: 1.1923\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.29424 to 1.19228, saving model to temp/c59\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1174 - mean_squared_error: 2.1174 - val_loss: 1.2432 - val_mean_squared_error: 1.2432\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.19228\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0924 - mean_squared_error: 2.0924 - val_loss: 1.2253 - val_mean_squared_error: 1.2253\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.19228\n",
      "Epoch 00008: early stopping\n",
      "temp/c60\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 2.8049 - mean_squared_error: 2.8049 - val_loss: 1.7521 - val_mean_squared_error: 1.7521\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.75209, saving model to temp/c60\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.4472 - mean_squared_error: 2.4472 - val_loss: 1.3547 - val_mean_squared_error: 1.3547\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.75209 to 1.35472, saving model to temp/c60\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.3089 - mean_squared_error: 2.3089 - val_loss: 1.3200 - val_mean_squared_error: 1.3200\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.35472 to 1.31998, saving model to temp/c60\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2281 - mean_squared_error: 2.2281 - val_loss: 1.2568 - val_mean_squared_error: 1.2568\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.31998 to 1.25679, saving model to temp/c60\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.2165 - mean_squared_error: 2.2165 - val_loss: 1.3933 - val_mean_squared_error: 1.3933\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.25679\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1179 - mean_squared_error: 2.1179 - val_loss: 1.2711 - val_mean_squared_error: 1.2711\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.25679\n",
      "Epoch 00006: early stopping\n",
      "temp/c61\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.8920 - mean_squared_error: 2.8920 - val_loss: 1.7710 - val_mean_squared_error: 1.7710\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.77103, saving model to temp/c61\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.4906 - mean_squared_error: 2.4906 - val_loss: 1.4585 - val_mean_squared_error: 1.4585\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.77103 to 1.45852, saving model to temp/c61\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.3553 - mean_squared_error: 2.3553 - val_loss: 1.5568 - val_mean_squared_error: 1.5568\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.45852\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2272 - mean_squared_error: 2.2272 - val_loss: 1.3529 - val_mean_squared_error: 1.3529\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.45852 to 1.35289, saving model to temp/c61\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2272 - mean_squared_error: 2.2272 - val_loss: 1.3210 - val_mean_squared_error: 1.3210\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.35289 to 1.32101, saving model to temp/c61\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1354 - mean_squared_error: 2.1354 - val_loss: 1.4135 - val_mean_squared_error: 1.4135\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.32101\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.1246 - mean_squared_error: 2.1246 - val_loss: 1.2645 - val_mean_squared_error: 1.2645\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.32101 to 1.26455, saving model to temp/c61\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0774 - mean_squared_error: 2.0774 - val_loss: 1.1771 - val_mean_squared_error: 1.1771\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.26455 to 1.17710, saving model to temp/c61\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1023 - mean_squared_error: 2.1023 - val_loss: 1.3533 - val_mean_squared_error: 1.3533\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.17710\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.0215 - mean_squared_error: 2.0215 - val_loss: 1.1599 - val_mean_squared_error: 1.1599\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.17710 to 1.15994, saving model to temp/c61\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 1.9815 - mean_squared_error: 1.9815 - val_loss: 1.2095 - val_mean_squared_error: 1.2095\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.15994\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 1.9821 - mean_squared_error: 1.9821 - val_loss: 1.3647 - val_mean_squared_error: 1.3647\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.15994\n",
      "Epoch 00012: early stopping\n",
      "temp/c62\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 2.8390 - mean_squared_error: 2.8390 - val_loss: 1.9195 - val_mean_squared_error: 1.9195\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.91951, saving model to temp/c62\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.4881 - mean_squared_error: 2.4881 - val_loss: 1.6404 - val_mean_squared_error: 1.6404\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.91951 to 1.64035, saving model to temp/c62\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.3613 - mean_squared_error: 2.3613 - val_loss: 1.3478 - val_mean_squared_error: 1.3478\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.64035 to 1.34779, saving model to temp/c62\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2555 - mean_squared_error: 2.2555 - val_loss: 1.2781 - val_mean_squared_error: 1.2781\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.34779 to 1.27813, saving model to temp/c62\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1491 - mean_squared_error: 2.1491 - val_loss: 1.2588 - val_mean_squared_error: 1.2588\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.27813 to 1.25884, saving model to temp/c62\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1227 - mean_squared_error: 2.1227 - val_loss: 1.3427 - val_mean_squared_error: 1.3427\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.25884\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1064 - mean_squared_error: 2.1064 - val_loss: 1.2301 - val_mean_squared_error: 1.2301\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.25884 to 1.23010, saving model to temp/c62\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0929 - mean_squared_error: 2.0929 - val_loss: 1.2030 - val_mean_squared_error: 1.2030\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.23010 to 1.20296, saving model to temp/c62\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0462 - mean_squared_error: 2.0462 - val_loss: 1.1356 - val_mean_squared_error: 1.1356\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.20296 to 1.13557, saving model to temp/c62\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0104 - mean_squared_error: 2.0104 - val_loss: 1.3004 - val_mean_squared_error: 1.3004\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.13557\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 1.9929 - mean_squared_error: 1.9929 - val_loss: 1.1468 - val_mean_squared_error: 1.1468\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.13557\n",
      "Epoch 00011: early stopping\n",
      "temp/c63\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 2.9364 - mean_squared_error: 2.9364 - val_loss: 1.6711 - val_mean_squared_error: 1.6711\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.67108, saving model to temp/c63\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.4928 - mean_squared_error: 2.4928 - val_loss: 1.6458 - val_mean_squared_error: 1.6458\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.67108 to 1.64577, saving model to temp/c63\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.3306 - mean_squared_error: 2.3306 - val_loss: 1.3736 - val_mean_squared_error: 1.3736\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.64577 to 1.37362, saving model to temp/c63\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2054 - mean_squared_error: 2.2054 - val_loss: 1.8358 - val_mean_squared_error: 1.8358\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.37362\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2039 - mean_squared_error: 2.2039 - val_loss: 1.3555 - val_mean_squared_error: 1.3555\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.37362 to 1.35546, saving model to temp/c63\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1299 - mean_squared_error: 2.1299 - val_loss: 1.2351 - val_mean_squared_error: 1.2351\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.35546 to 1.23515, saving model to temp/c63\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1466 - mean_squared_error: 2.1466 - val_loss: 1.3527 - val_mean_squared_error: 1.3527\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.23515\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0739 - mean_squared_error: 2.0739 - val_loss: 1.3591 - val_mean_squared_error: 1.3591\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.23515\n",
      "Epoch 00008: early stopping\n",
      "temp/c64\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 2.9618 - mean_squared_error: 2.9618 - val_loss: 1.6291 - val_mean_squared_error: 1.6291\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.62915, saving model to temp/c64\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.4397 - mean_squared_error: 2.4397 - val_loss: 1.9169 - val_mean_squared_error: 1.9169\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.62915\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.3504 - mean_squared_error: 2.3504 - val_loss: 1.4547 - val_mean_squared_error: 1.4547\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.62915 to 1.45470, saving model to temp/c64\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.2717 - mean_squared_error: 2.2717 - val_loss: 1.3702 - val_mean_squared_error: 1.3702\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.45470 to 1.37017, saving model to temp/c64\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.2137 - mean_squared_error: 2.2137 - val_loss: 1.5538 - val_mean_squared_error: 1.5538\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.37017\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1955 - mean_squared_error: 2.1955 - val_loss: 1.2294 - val_mean_squared_error: 1.2294\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.37017 to 1.22939, saving model to temp/c64\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1063 - mean_squared_error: 2.1063 - val_loss: 1.2937 - val_mean_squared_error: 1.2937\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.22939\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0468 - mean_squared_error: 2.0468 - val_loss: 1.2205 - val_mean_squared_error: 1.2205\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.22939 to 1.22049, saving model to temp/c64\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0433 - mean_squared_error: 2.0433 - val_loss: 1.1327 - val_mean_squared_error: 1.1327\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.22049 to 1.13273, saving model to temp/c64\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.0650 - mean_squared_error: 2.0650 - val_loss: 1.1509 - val_mean_squared_error: 1.1509\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.13273\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 1.9930 - mean_squared_error: 1.9930 - val_loss: 1.1593 - val_mean_squared_error: 1.1593\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.13273\n",
      "Epoch 00011: early stopping\n",
      "temp/c65\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.8358 - mean_squared_error: 2.8358 - val_loss: 1.5976 - val_mean_squared_error: 1.5976\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.59756, saving model to temp/c65\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.4913 - mean_squared_error: 2.4913 - val_loss: 1.4759 - val_mean_squared_error: 1.4759\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.59756 to 1.47591, saving model to temp/c65\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.3153 - mean_squared_error: 2.3153 - val_loss: 1.5334 - val_mean_squared_error: 1.5334\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.47591\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.2775 - mean_squared_error: 2.2775 - val_loss: 1.3397 - val_mean_squared_error: 1.3397\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.47591 to 1.33966, saving model to temp/c65\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.1854 - mean_squared_error: 2.1854 - val_loss: 1.3587 - val_mean_squared_error: 1.3587\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.33966\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1545 - mean_squared_error: 2.1545 - val_loss: 1.2094 - val_mean_squared_error: 1.2094\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.33966 to 1.20942, saving model to temp/c65\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0857 - mean_squared_error: 2.0857 - val_loss: 1.2768 - val_mean_squared_error: 1.2768\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.20942\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1282 - mean_squared_error: 2.1282 - val_loss: 1.2980 - val_mean_squared_error: 1.2980\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.20942\n",
      "Epoch 00008: early stopping\n",
      "temp/c66\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.8281 - mean_squared_error: 2.8281 - val_loss: 1.6750 - val_mean_squared_error: 1.6750\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.67495, saving model to temp/c66\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.4448 - mean_squared_error: 2.4448 - val_loss: 1.4397 - val_mean_squared_error: 1.4397\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.67495 to 1.43969, saving model to temp/c66\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.3458 - mean_squared_error: 2.3458 - val_loss: 1.3540 - val_mean_squared_error: 1.3540\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.43969 to 1.35404, saving model to temp/c66\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2419 - mean_squared_error: 2.2419 - val_loss: 1.4623 - val_mean_squared_error: 1.4623\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.35404\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1881 - mean_squared_error: 2.1881 - val_loss: 1.2663 - val_mean_squared_error: 1.2663\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.35404 to 1.26630, saving model to temp/c66\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1675 - mean_squared_error: 2.1675 - val_loss: 1.2319 - val_mean_squared_error: 1.2319\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.26630 to 1.23192, saving model to temp/c66\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.1011 - mean_squared_error: 2.1011 - val_loss: 1.3466 - val_mean_squared_error: 1.3466\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.23192\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0570 - mean_squared_error: 2.0570 - val_loss: 1.1604 - val_mean_squared_error: 1.1604\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.23192 to 1.16044, saving model to temp/c66\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 2.0660 - mean_squared_error: 2.0660 - val_loss: 1.2635 - val_mean_squared_error: 1.2635\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.16044\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.0499 - mean_squared_error: 2.0499 - val_loss: 1.2953 - val_mean_squared_error: 1.2953\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.16044\n",
      "Epoch 00010: early stopping\n",
      "temp/c67\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 2.8423 - mean_squared_error: 2.8423 - val_loss: 2.0627 - val_mean_squared_error: 2.0627\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.06265, saving model to temp/c67\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.4816 - mean_squared_error: 2.4816 - val_loss: 1.5395 - val_mean_squared_error: 1.5395\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.06265 to 1.53947, saving model to temp/c67\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.3357 - mean_squared_error: 2.3357 - val_loss: 1.3994 - val_mean_squared_error: 1.3994\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.53947 to 1.39940, saving model to temp/c67\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 2.2970 - mean_squared_error: 2.2970 - val_loss: 1.3966 - val_mean_squared_error: 1.3966\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.39940 to 1.39657, saving model to temp/c67\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 200us/step - loss: 2.2314 - mean_squared_error: 2.2314 - val_loss: 1.3083 - val_mean_squared_error: 1.3083\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.39657 to 1.30829, saving model to temp/c67\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1970 - mean_squared_error: 2.1970 - val_loss: 1.4276 - val_mean_squared_error: 1.4276\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.30829\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 2.1080 - mean_squared_error: 2.1080 - val_loss: 1.2064 - val_mean_squared_error: 1.2064\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.30829 to 1.20635, saving model to temp/c67\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 2.0733 - mean_squared_error: 2.0733 - val_loss: 1.4792 - val_mean_squared_error: 1.4792\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.20635\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.0456 - mean_squared_error: 2.0456 - val_loss: 1.4002 - val_mean_squared_error: 1.4002\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.20635\n",
      "Epoch 00009: early stopping\n",
      "temp/c68\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 2.9052 - mean_squared_error: 2.9052 - val_loss: 1.5591 - val_mean_squared_error: 1.5591\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.55914, saving model to temp/c68\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 2.4585 - mean_squared_error: 2.4585 - val_loss: 1.5066 - val_mean_squared_error: 1.5066\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.55914 to 1.50658, saving model to temp/c68\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 10s 208us/step - loss: 2.3455 - mean_squared_error: 2.3455 - val_loss: 1.3674 - val_mean_squared_error: 1.3674\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.50658 to 1.36736, saving model to temp/c68\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 2.2327 - mean_squared_error: 2.2327 - val_loss: 1.3979 - val_mean_squared_error: 1.3979\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.36736\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 2.1809 - mean_squared_error: 2.1809 - val_loss: 1.2480 - val_mean_squared_error: 1.2480\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.36736 to 1.24796, saving model to temp/c68\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 2.1464 - mean_squared_error: 2.1464 - val_loss: 1.1986 - val_mean_squared_error: 1.1986\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.24796 to 1.19862, saving model to temp/c68\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 2.1335 - mean_squared_error: 2.1335 - val_loss: 1.2363 - val_mean_squared_error: 1.2363\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.19862\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 2.0808 - mean_squared_error: 2.0808 - val_loss: 1.1625 - val_mean_squared_error: 1.1625\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.19862 to 1.16250, saving model to temp/c68\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 2.0695 - mean_squared_error: 2.0695 - val_loss: 1.1503 - val_mean_squared_error: 1.1503\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.16250 to 1.15033, saving model to temp/c68\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 2.0707 - mean_squared_error: 2.0707 - val_loss: 1.2382 - val_mean_squared_error: 1.2382\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.15033\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 10s 209us/step - loss: 2.0695 - mean_squared_error: 2.0695 - val_loss: 1.5092 - val_mean_squared_error: 1.5092\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.15033\n",
      "Epoch 00011: early stopping\n",
      "temp/c69\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 2.8635 - mean_squared_error: 2.8635 - val_loss: 1.6000 - val_mean_squared_error: 1.6000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.59997, saving model to temp/c69\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 2.4357 - mean_squared_error: 2.4357 - val_loss: 1.4312 - val_mean_squared_error: 1.4312\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.59997 to 1.43119, saving model to temp/c69\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 209us/step - loss: 2.3252 - mean_squared_error: 2.3252 - val_loss: 1.3259 - val_mean_squared_error: 1.3259\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.43119 to 1.32587, saving model to temp/c69\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 2.3360 - mean_squared_error: 2.3360 - val_loss: 1.3140 - val_mean_squared_error: 1.3140\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.32587 to 1.31400, saving model to temp/c69\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 2.1664 - mean_squared_error: 2.1664 - val_loss: 1.2640 - val_mean_squared_error: 1.2640\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.31400 to 1.26399, saving model to temp/c69\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 11s 213us/step - loss: 2.1689 - mean_squared_error: 2.1689 - val_loss: 1.4042 - val_mean_squared_error: 1.4042\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.26399\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 209us/step - loss: 2.1257 - mean_squared_error: 2.1257 - val_loss: 1.1801 - val_mean_squared_error: 1.1801\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.26399 to 1.18012, saving model to temp/c69\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 2.1099 - mean_squared_error: 2.1099 - val_loss: 1.1718 - val_mean_squared_error: 1.1718\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.18012 to 1.17179, saving model to temp/c69\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 2.0520 - mean_squared_error: 2.0520 - val_loss: 1.5372 - val_mean_squared_error: 1.5372\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.17179\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 2.0422 - mean_squared_error: 2.0422 - val_loss: 1.1664 - val_mean_squared_error: 1.1664\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.17179 to 1.16645, saving model to temp/c69\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 2.0040 - mean_squared_error: 2.0040 - val_loss: 1.1726 - val_mean_squared_error: 1.1726\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.16645\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 1.9874 - mean_squared_error: 1.9874 - val_loss: 1.2220 - val_mean_squared_error: 1.2220\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.16645\n",
      "Epoch 00012: early stopping\n",
      "temp/c70\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 222us/step - loss: 2.9224 - mean_squared_error: 2.9224 - val_loss: 1.7271 - val_mean_squared_error: 1.7271\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.72714, saving model to temp/c70\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 2.5079 - mean_squared_error: 2.5079 - val_loss: 1.6561 - val_mean_squared_error: 1.6561\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.72714 to 1.65614, saving model to temp/c70\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 2.3676 - mean_squared_error: 2.3676 - val_loss: 1.3806 - val_mean_squared_error: 1.3806\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.65614 to 1.38059, saving model to temp/c70\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 209us/step - loss: 2.3301 - mean_squared_error: 2.3301 - val_loss: 1.3332 - val_mean_squared_error: 1.3332\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.38059 to 1.33321, saving model to temp/c70\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 2.1822 - mean_squared_error: 2.1822 - val_loss: 1.2986 - val_mean_squared_error: 1.2986\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.33321 to 1.29858, saving model to temp/c70\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 2.1819 - mean_squared_error: 2.1819 - val_loss: 1.3137 - val_mean_squared_error: 1.3137\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.29858\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 2.0907 - mean_squared_error: 2.0907 - val_loss: 1.3540 - val_mean_squared_error: 1.3540\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.29858\n",
      "Epoch 00007: early stopping\n",
      "temp/c71\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 227us/step - loss: 2.9821 - mean_squared_error: 2.9821 - val_loss: 1.7351 - val_mean_squared_error: 1.7351\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.73512, saving model to temp/c71\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.5293 - mean_squared_error: 2.5293 - val_loss: 1.5510 - val_mean_squared_error: 1.5510\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.73512 to 1.55096, saving model to temp/c71\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.3338 - mean_squared_error: 2.3338 - val_loss: 1.4781 - val_mean_squared_error: 1.4781\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.55096 to 1.47815, saving model to temp/c71\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.2743 - mean_squared_error: 2.2743 - val_loss: 1.3628 - val_mean_squared_error: 1.3628\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.47815 to 1.36284, saving model to temp/c71\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.1934 - mean_squared_error: 2.1934 - val_loss: 1.4147 - val_mean_squared_error: 1.4147\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.36284\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.1471 - mean_squared_error: 2.1471 - val_loss: 1.4880 - val_mean_squared_error: 1.4880\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.36284\n",
      "Epoch 00006: early stopping\n",
      "temp/c72\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 260us/step - loss: 2.8410 - mean_squared_error: 2.8410 - val_loss: 1.5603 - val_mean_squared_error: 1.5603\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.56031, saving model to temp/c72\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 2.4245 - mean_squared_error: 2.4245 - val_loss: 1.5018 - val_mean_squared_error: 1.5018\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.56031 to 1.50179, saving model to temp/c72\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.3130 - mean_squared_error: 2.3130 - val_loss: 1.5650 - val_mean_squared_error: 1.5650\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.50179\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 2.2733 - mean_squared_error: 2.2733 - val_loss: 1.3842 - val_mean_squared_error: 1.3842\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.50179 to 1.38421, saving model to temp/c72\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.2152 - mean_squared_error: 2.2152 - val_loss: 1.3140 - val_mean_squared_error: 1.3140\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.38421 to 1.31405, saving model to temp/c72\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.1232 - mean_squared_error: 2.1232 - val_loss: 1.2359 - val_mean_squared_error: 1.2359\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.31405 to 1.23592, saving model to temp/c72\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.1461 - mean_squared_error: 2.1461 - val_loss: 1.5064 - val_mean_squared_error: 1.5064\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.23592\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.0661 - mean_squared_error: 2.0661 - val_loss: 1.1448 - val_mean_squared_error: 1.1448\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.23592 to 1.14482, saving model to temp/c72\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 2.0582 - mean_squared_error: 2.0582 - val_loss: 1.1984 - val_mean_squared_error: 1.1984\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.14482\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 1.9840 - mean_squared_error: 1.9840 - val_loss: 1.2552 - val_mean_squared_error: 1.2552\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.14482\n",
      "Epoch 00010: early stopping\n",
      "temp/c73\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 257us/step - loss: 2.8871 - mean_squared_error: 2.8871 - val_loss: 1.5487 - val_mean_squared_error: 1.5487\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.54869, saving model to temp/c73\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.5092 - mean_squared_error: 2.5092 - val_loss: 1.8431 - val_mean_squared_error: 1.8431\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.54869\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.3845 - mean_squared_error: 2.3845 - val_loss: 1.4080 - val_mean_squared_error: 1.4080\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.54869 to 1.40802, saving model to temp/c73\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.2408 - mean_squared_error: 2.2408 - val_loss: 1.4095 - val_mean_squared_error: 1.4095\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.40802\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 250us/step - loss: 2.1953 - mean_squared_error: 2.1953 - val_loss: 1.4109 - val_mean_squared_error: 1.4109\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.40802\n",
      "Epoch 00005: early stopping\n",
      "temp/c74\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 258us/step - loss: 2.9753 - mean_squared_error: 2.9753 - val_loss: 1.6480 - val_mean_squared_error: 1.6480\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.64800, saving model to temp/c74\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.4562 - mean_squared_error: 2.4562 - val_loss: 1.6116 - val_mean_squared_error: 1.6116\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.64800 to 1.61163, saving model to temp/c74\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 2.3681 - mean_squared_error: 2.3681 - val_loss: 1.6047 - val_mean_squared_error: 1.6047\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.61163 to 1.60475, saving model to temp/c74\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.3142 - mean_squared_error: 2.3142 - val_loss: 1.3148 - val_mean_squared_error: 1.3148\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.60475 to 1.31475, saving model to temp/c74\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.2396 - mean_squared_error: 2.2396 - val_loss: 1.2962 - val_mean_squared_error: 1.2962\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.31475 to 1.29622, saving model to temp/c74\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.1713 - mean_squared_error: 2.1713 - val_loss: 1.2697 - val_mean_squared_error: 1.2697\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.29622 to 1.26973, saving model to temp/c74\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 2.1396 - mean_squared_error: 2.1396 - val_loss: 1.1943 - val_mean_squared_error: 1.1943\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.26973 to 1.19428, saving model to temp/c74\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.0925 - mean_squared_error: 2.0925 - val_loss: 1.3639 - val_mean_squared_error: 1.3639\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.19428\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 12s 244us/step - loss: 2.0526 - mean_squared_error: 2.0526 - val_loss: 1.2587 - val_mean_squared_error: 1.2587\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.19428\n",
      "Epoch 00009: early stopping\n",
      "temp/c75\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 258us/step - loss: 2.8327 - mean_squared_error: 2.8327 - val_loss: 2.2310 - val_mean_squared_error: 2.2310\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.23102, saving model to temp/c75\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 2.4067 - mean_squared_error: 2.4067 - val_loss: 1.3431 - val_mean_squared_error: 1.3431\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.23102 to 1.34306, saving model to temp/c75\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 243us/step - loss: 2.3310 - mean_squared_error: 2.3310 - val_loss: 1.7395 - val_mean_squared_error: 1.7395\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.34306\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.2712 - mean_squared_error: 2.2712 - val_loss: 1.4038 - val_mean_squared_error: 1.4038\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.34306\n",
      "Epoch 00004: early stopping\n",
      "temp/c76\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 257us/step - loss: 2.9295 - mean_squared_error: 2.9295 - val_loss: 1.8259 - val_mean_squared_error: 1.8259\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.82587, saving model to temp/c76\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.5092 - mean_squared_error: 2.5092 - val_loss: 1.4649 - val_mean_squared_error: 1.4649\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.82587 to 1.46486, saving model to temp/c76\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.3350 - mean_squared_error: 2.3350 - val_loss: 1.4346 - val_mean_squared_error: 1.4346\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.46486 to 1.43463, saving model to temp/c76\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 2.2968 - mean_squared_error: 2.2968 - val_loss: 1.3400 - val_mean_squared_error: 1.3400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss improved from 1.43463 to 1.34004, saving model to temp/c76\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.2240 - mean_squared_error: 2.2240 - val_loss: 1.5855 - val_mean_squared_error: 1.5855\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.34004\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.1730 - mean_squared_error: 2.1730 - val_loss: 1.4074 - val_mean_squared_error: 1.4074\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.34004\n",
      "Epoch 00006: early stopping\n",
      "temp/c77\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 258us/step - loss: 2.9026 - mean_squared_error: 2.9026 - val_loss: 1.5466 - val_mean_squared_error: 1.5466\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.54664, saving model to temp/c77\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 2.4747 - mean_squared_error: 2.4747 - val_loss: 1.6217 - val_mean_squared_error: 1.6217\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.54664\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.3482 - mean_squared_error: 2.3482 - val_loss: 1.6033 - val_mean_squared_error: 1.6033\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.54664\n",
      "Epoch 00003: early stopping\n",
      "temp/c78\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 262us/step - loss: 2.8589 - mean_squared_error: 2.8589 - val_loss: 1.7079 - val_mean_squared_error: 1.7079\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.70791, saving model to temp/c78\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 2.4902 - mean_squared_error: 2.4902 - val_loss: 1.6985 - val_mean_squared_error: 1.6985\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.70791 to 1.69846, saving model to temp/c78\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 2.3203 - mean_squared_error: 2.3203 - val_loss: 1.3120 - val_mean_squared_error: 1.3120\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.69846 to 1.31205, saving model to temp/c78\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.2359 - mean_squared_error: 2.2359 - val_loss: 1.3133 - val_mean_squared_error: 1.3133\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.31205\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.2070 - mean_squared_error: 2.2070 - val_loss: 1.5247 - val_mean_squared_error: 1.5247\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.31205\n",
      "Epoch 00005: early stopping\n",
      "temp/c79\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 2.8950 - mean_squared_error: 2.8950 - val_loss: 1.6347 - val_mean_squared_error: 1.6347\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.63466, saving model to temp/c79\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.4923 - mean_squared_error: 2.4923 - val_loss: 1.7646 - val_mean_squared_error: 1.7646\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.63466\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 2.3413 - mean_squared_error: 2.3413 - val_loss: 1.6326 - val_mean_squared_error: 1.6326\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.63466 to 1.63258, saving model to temp/c79\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.2365 - mean_squared_error: 2.2365 - val_loss: 1.2989 - val_mean_squared_error: 1.2989\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.63258 to 1.29891, saving model to temp/c79\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.1529 - mean_squared_error: 2.1529 - val_loss: 1.3117 - val_mean_squared_error: 1.3117\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.29891\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.1616 - mean_squared_error: 2.1616 - val_loss: 1.2693 - val_mean_squared_error: 1.2693\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.29891 to 1.26934, saving model to temp/c79\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.1128 - mean_squared_error: 2.1128 - val_loss: 1.1856 - val_mean_squared_error: 1.1856\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.26934 to 1.18564, saving model to temp/c79\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.1009 - mean_squared_error: 2.1009 - val_loss: 1.2204 - val_mean_squared_error: 1.2204\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.18564\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 2.0821 - mean_squared_error: 2.0821 - val_loss: 1.2292 - val_mean_squared_error: 1.2292\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.18564\n",
      "Epoch 00009: early stopping\n",
      "temp/c80\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 258us/step - loss: 2.9203 - mean_squared_error: 2.9203 - val_loss: 1.6133 - val_mean_squared_error: 1.6133\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.61335, saving model to temp/c80\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.4863 - mean_squared_error: 2.4863 - val_loss: 1.5431 - val_mean_squared_error: 1.5431\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.61335 to 1.54306, saving model to temp/c80\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 2.3067 - mean_squared_error: 2.3067 - val_loss: 1.4661 - val_mean_squared_error: 1.4661\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.54306 to 1.46609, saving model to temp/c80\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.2872 - mean_squared_error: 2.2872 - val_loss: 1.4249 - val_mean_squared_error: 1.4249\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.46609 to 1.42491, saving model to temp/c80\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.2174 - mean_squared_error: 2.2174 - val_loss: 1.3420 - val_mean_squared_error: 1.3420\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.42491 to 1.34201, saving model to temp/c80\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.1241 - mean_squared_error: 2.1241 - val_loss: 1.2533 - val_mean_squared_error: 1.2533\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.34201 to 1.25331, saving model to temp/c80\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.1429 - mean_squared_error: 2.1429 - val_loss: 1.2185 - val_mean_squared_error: 1.2185\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.25331 to 1.21846, saving model to temp/c80\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.0801 - mean_squared_error: 2.0801 - val_loss: 1.1593 - val_mean_squared_error: 1.1593\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.21846 to 1.15928, saving model to temp/c80\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 2.1068 - mean_squared_error: 2.1068 - val_loss: 1.2344 - val_mean_squared_error: 1.2344\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.15928\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.0351 - mean_squared_error: 2.0351 - val_loss: 1.1171 - val_mean_squared_error: 1.1171\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.15928 to 1.11710, saving model to temp/c80\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.0684 - mean_squared_error: 2.0684 - val_loss: 1.1406 - val_mean_squared_error: 1.1406\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.11710\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.0349 - mean_squared_error: 2.0349 - val_loss: 1.2900 - val_mean_squared_error: 1.2900\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.11710\n",
      "Epoch 00012: early stopping\n",
      "temp/c81\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 257us/step - loss: 2.8851 - mean_squared_error: 2.8851 - val_loss: 1.5436 - val_mean_squared_error: 1.5436\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.54358, saving model to temp/c81\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 12s 249us/step - loss: 2.5162 - mean_squared_error: 2.5162 - val_loss: 1.5561 - val_mean_squared_error: 1.5561\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.54358\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 2.3362 - mean_squared_error: 2.3362 - val_loss: 1.3299 - val_mean_squared_error: 1.3299\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.54358 to 1.32989, saving model to temp/c81\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.2826 - mean_squared_error: 2.2826 - val_loss: 1.3084 - val_mean_squared_error: 1.3084\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.32989 to 1.30837, saving model to temp/c81\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.1953 - mean_squared_error: 2.1953 - val_loss: 1.3065 - val_mean_squared_error: 1.3065\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.30837 to 1.30653, saving model to temp/c81\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.1464 - mean_squared_error: 2.1464 - val_loss: 1.5495 - val_mean_squared_error: 1.5495\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.30653\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.1256 - mean_squared_error: 2.1256 - val_loss: 1.2606 - val_mean_squared_error: 1.2606\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.30653 to 1.26063, saving model to temp/c81\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 2.0430 - mean_squared_error: 2.0430 - val_loss: 1.3700 - val_mean_squared_error: 1.3700\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.26063\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.0573 - mean_squared_error: 2.0573 - val_loss: 1.1631 - val_mean_squared_error: 1.1631\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.26063 to 1.16310, saving model to temp/c81\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.0473 - mean_squared_error: 2.0473 - val_loss: 1.2211 - val_mean_squared_error: 1.2211\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.16310\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.0449 - mean_squared_error: 2.0449 - val_loss: 1.1274 - val_mean_squared_error: 1.1274\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.16310 to 1.12743, saving model to temp/c81\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.0467 - mean_squared_error: 2.0467 - val_loss: 1.3555 - val_mean_squared_error: 1.3555\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.12743\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.0024 - mean_squared_error: 2.0024 - val_loss: 1.0951 - val_mean_squared_error: 1.0951\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.12743 to 1.09514, saving model to temp/c81\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 1.9750 - mean_squared_error: 1.9750 - val_loss: 1.1596 - val_mean_squared_error: 1.1596\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.09514\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 1.9934 - mean_squared_error: 1.9934 - val_loss: 1.1179 - val_mean_squared_error: 1.1179\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.09514\n",
      "Epoch 00015: early stopping\n",
      "temp/c82\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 257us/step - loss: 2.9308 - mean_squared_error: 2.9308 - val_loss: 1.6792 - val_mean_squared_error: 1.6792\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.67921, saving model to temp/c82\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.4624 - mean_squared_error: 2.4624 - val_loss: 1.4369 - val_mean_squared_error: 1.4369\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.67921 to 1.43688, saving model to temp/c82\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.3517 - mean_squared_error: 2.3517 - val_loss: 1.4149 - val_mean_squared_error: 1.4149\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.43688 to 1.41486, saving model to temp/c82\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.2890 - mean_squared_error: 2.2890 - val_loss: 1.3011 - val_mean_squared_error: 1.3011\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.41486 to 1.30113, saving model to temp/c82\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 244us/step - loss: 2.2359 - mean_squared_error: 2.2359 - val_loss: 1.4167 - val_mean_squared_error: 1.4167\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.30113\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.1878 - mean_squared_error: 2.1878 - val_loss: 1.3623 - val_mean_squared_error: 1.3623\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.30113\n",
      "Epoch 00006: early stopping\n",
      "temp/c83\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 258us/step - loss: 2.8662 - mean_squared_error: 2.8662 - val_loss: 1.6623 - val_mean_squared_error: 1.6623\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.66229, saving model to temp/c83\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 2.4523 - mean_squared_error: 2.4523 - val_loss: 1.4701 - val_mean_squared_error: 1.4701\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.66229 to 1.47008, saving model to temp/c83\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.3372 - mean_squared_error: 2.3372 - val_loss: 1.3902 - val_mean_squared_error: 1.3902\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.47008 to 1.39025, saving model to temp/c83\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.2099 - mean_squared_error: 2.2099 - val_loss: 1.3254 - val_mean_squared_error: 1.3254\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.39025 to 1.32535, saving model to temp/c83\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.2007 - mean_squared_error: 2.2007 - val_loss: 1.2709 - val_mean_squared_error: 1.2709\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.32535 to 1.27086, saving model to temp/c83\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 244us/step - loss: 2.1284 - mean_squared_error: 2.1284 - val_loss: 1.3064 - val_mean_squared_error: 1.3064\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.27086\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 2.1679 - mean_squared_error: 2.1679 - val_loss: 1.2437 - val_mean_squared_error: 1.2437\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.27086 to 1.24373, saving model to temp/c83\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.1031 - mean_squared_error: 2.1031 - val_loss: 1.1971 - val_mean_squared_error: 1.1971\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.24373 to 1.19710, saving model to temp/c83\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.1291 - mean_squared_error: 2.1291 - val_loss: 1.3773 - val_mean_squared_error: 1.3773\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.19710\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.0724 - mean_squared_error: 2.0724 - val_loss: 1.2481 - val_mean_squared_error: 1.2481\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.19710\n",
      "Epoch 00010: early stopping\n",
      "temp/c84\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 257us/step - loss: 2.8573 - mean_squared_error: 2.8573 - val_loss: 1.6681 - val_mean_squared_error: 1.6681\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.66810, saving model to temp/c84\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.4370 - mean_squared_error: 2.4370 - val_loss: 1.4413 - val_mean_squared_error: 1.4413\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.66810 to 1.44134, saving model to temp/c84\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.3611 - mean_squared_error: 2.3611 - val_loss: 1.3360 - val_mean_squared_error: 1.3360\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.44134 to 1.33597, saving model to temp/c84\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.2986 - mean_squared_error: 2.2986 - val_loss: 1.2486 - val_mean_squared_error: 1.2486\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.33597 to 1.24864, saving model to temp/c84\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.2017 - mean_squared_error: 2.2017 - val_loss: 1.2599 - val_mean_squared_error: 1.2599\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.24864\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.1396 - mean_squared_error: 2.1396 - val_loss: 1.5571 - val_mean_squared_error: 1.5571\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.24864\n",
      "Epoch 00006: early stopping\n",
      "temp/c85\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 2.9164 - mean_squared_error: 2.9164 - val_loss: 1.7073 - val_mean_squared_error: 1.7073\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.70727, saving model to temp/c85\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.4932 - mean_squared_error: 2.4932 - val_loss: 1.5911 - val_mean_squared_error: 1.5911\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.70727 to 1.59113, saving model to temp/c85\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.3022 - mean_squared_error: 2.3022 - val_loss: 1.3186 - val_mean_squared_error: 1.3186\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.59113 to 1.31861, saving model to temp/c85\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.1558 - mean_squared_error: 2.1558 - val_loss: 1.4834 - val_mean_squared_error: 1.4834\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.31861\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.1990 - mean_squared_error: 2.1990 - val_loss: 1.2747 - val_mean_squared_error: 1.2747\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.31861 to 1.27471, saving model to temp/c85\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 2.1501 - mean_squared_error: 2.1501 - val_loss: 1.3791 - val_mean_squared_error: 1.3791\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.27471\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 2.1196 - mean_squared_error: 2.1196 - val_loss: 1.3097 - val_mean_squared_error: 1.3097\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.27471\n",
      "Epoch 00007: early stopping\n",
      "temp/c86\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 259us/step - loss: 2.9064 - mean_squared_error: 2.9064 - val_loss: 1.8297 - val_mean_squared_error: 1.8297\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.82971, saving model to temp/c86\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.5058 - mean_squared_error: 2.5058 - val_loss: 1.4707 - val_mean_squared_error: 1.4707\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.82971 to 1.47067, saving model to temp/c86\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 2.3566 - mean_squared_error: 2.3566 - val_loss: 1.3612 - val_mean_squared_error: 1.3612\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.47067 to 1.36124, saving model to temp/c86\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.2531 - mean_squared_error: 2.2531 - val_loss: 1.4713 - val_mean_squared_error: 1.4713\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.36124\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 250us/step - loss: 2.1861 - mean_squared_error: 2.1861 - val_loss: 1.3875 - val_mean_squared_error: 1.3875\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.36124\n",
      "Epoch 00005: early stopping\n",
      "temp/c87\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 258us/step - loss: 2.8037 - mean_squared_error: 2.8037 - val_loss: 1.6392 - val_mean_squared_error: 1.6392\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.63922, saving model to temp/c87\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.4108 - mean_squared_error: 2.4108 - val_loss: 1.3916 - val_mean_squared_error: 1.3916\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.63922 to 1.39159, saving model to temp/c87\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 2.3035 - mean_squared_error: 2.3035 - val_loss: 1.4354 - val_mean_squared_error: 1.4354\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.39159\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 2.2429 - mean_squared_error: 2.2429 - val_loss: 1.3403 - val_mean_squared_error: 1.3403\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.39159 to 1.34032, saving model to temp/c87\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.2341 - mean_squared_error: 2.2341 - val_loss: 1.2158 - val_mean_squared_error: 1.2158\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.34032 to 1.21583, saving model to temp/c87\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.2150 - mean_squared_error: 2.2150 - val_loss: 1.2762 - val_mean_squared_error: 1.2762\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.21583\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.0860 - mean_squared_error: 2.0860 - val_loss: 1.3504 - val_mean_squared_error: 1.3504\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.21583\n",
      "Epoch 00007: early stopping\n",
      "temp/c88\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 255us/step - loss: 2.9197 - mean_squared_error: 2.9197 - val_loss: 2.2428 - val_mean_squared_error: 2.2428\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.24278, saving model to temp/c88\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 244us/step - loss: 2.4965 - mean_squared_error: 2.4965 - val_loss: 1.4347 - val_mean_squared_error: 1.4347\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.24278 to 1.43469, saving model to temp/c88\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 2.3179 - mean_squared_error: 2.3179 - val_loss: 1.6277 - val_mean_squared_error: 1.6277\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.43469\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 2.2644 - mean_squared_error: 2.2644 - val_loss: 1.5964 - val_mean_squared_error: 1.5964\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.43469\n",
      "Epoch 00004: early stopping\n",
      "temp/c89\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 257us/step - loss: 2.9105 - mean_squared_error: 2.9105 - val_loss: 1.5438 - val_mean_squared_error: 1.5438\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.54384, saving model to temp/c89\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.4418 - mean_squared_error: 2.4418 - val_loss: 1.5168 - val_mean_squared_error: 1.5168\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.54384 to 1.51680, saving model to temp/c89\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.3284 - mean_squared_error: 2.3284 - val_loss: 1.4828 - val_mean_squared_error: 1.4828\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.51680 to 1.48281, saving model to temp/c89\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.2759 - mean_squared_error: 2.2759 - val_loss: 1.5552 - val_mean_squared_error: 1.5552\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.48281\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.2221 - mean_squared_error: 2.2221 - val_loss: 1.2607 - val_mean_squared_error: 1.2607\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.48281 to 1.26074, saving model to temp/c89\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.1930 - mean_squared_error: 2.1930 - val_loss: 1.1818 - val_mean_squared_error: 1.1818\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.26074 to 1.18175, saving model to temp/c89\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.0717 - mean_squared_error: 2.0717 - val_loss: 1.1965 - val_mean_squared_error: 1.1965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: val_loss did not improve from 1.18175\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.0998 - mean_squared_error: 2.0998 - val_loss: 1.2996 - val_mean_squared_error: 1.2996\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.18175\n",
      "Epoch 00008: early stopping\n",
      "temp/c90\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 260us/step - loss: 2.8341 - mean_squared_error: 2.8341 - val_loss: 1.5348 - val_mean_squared_error: 1.5348\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.53481, saving model to temp/c90\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 242us/step - loss: 2.3968 - mean_squared_error: 2.3968 - val_loss: 1.4463 - val_mean_squared_error: 1.4463\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.53481 to 1.44635, saving model to temp/c90\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.3110 - mean_squared_error: 2.3110 - val_loss: 1.3357 - val_mean_squared_error: 1.3357\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.44635 to 1.33567, saving model to temp/c90\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 2.2449 - mean_squared_error: 2.2449 - val_loss: 1.4435 - val_mean_squared_error: 1.4435\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.33567\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.1759 - mean_squared_error: 2.1759 - val_loss: 1.2677 - val_mean_squared_error: 1.2677\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.33567 to 1.26766, saving model to temp/c90\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.1444 - mean_squared_error: 2.1444 - val_loss: 1.3322 - val_mean_squared_error: 1.3322\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.26766\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.1212 - mean_squared_error: 2.1212 - val_loss: 1.2566 - val_mean_squared_error: 1.2566\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.26766 to 1.25663, saving model to temp/c90\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.1284 - mean_squared_error: 2.1284 - val_loss: 1.5106 - val_mean_squared_error: 1.5106\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.25663\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.0444 - mean_squared_error: 2.0444 - val_loss: 1.2592 - val_mean_squared_error: 1.2592\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.25663\n",
      "Epoch 00009: early stopping\n",
      "temp/c91\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 2.8795 - mean_squared_error: 2.8795 - val_loss: 1.6250 - val_mean_squared_error: 1.6250\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.62496, saving model to temp/c91\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 13s 250us/step - loss: 2.4318 - mean_squared_error: 2.4318 - val_loss: 1.4722 - val_mean_squared_error: 1.4722\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.62496 to 1.47222, saving model to temp/c91\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.3637 - mean_squared_error: 2.3637 - val_loss: 1.5155 - val_mean_squared_error: 1.5155\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.47222\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.2687 - mean_squared_error: 2.2687 - val_loss: 1.2851 - val_mean_squared_error: 1.2851\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.47222 to 1.28514, saving model to temp/c91\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.2404 - mean_squared_error: 2.2404 - val_loss: 1.3583 - val_mean_squared_error: 1.3583\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.28514\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.2078 - mean_squared_error: 2.2078 - val_loss: 1.5546 - val_mean_squared_error: 1.5546\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.28514\n",
      "Epoch 00006: early stopping\n",
      "temp/c92\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 260us/step - loss: 2.8927 - mean_squared_error: 2.8927 - val_loss: 1.5771 - val_mean_squared_error: 1.5771\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.57713, saving model to temp/c92\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.4208 - mean_squared_error: 2.4208 - val_loss: 1.3744 - val_mean_squared_error: 1.3744\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.57713 to 1.37439, saving model to temp/c92\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 2.3326 - mean_squared_error: 2.3326 - val_loss: 1.5628 - val_mean_squared_error: 1.5628\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.37439\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.2371 - mean_squared_error: 2.2371 - val_loss: 1.5253 - val_mean_squared_error: 1.5253\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.37439\n",
      "Epoch 00004: early stopping\n",
      "temp/c93\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 2.8310 - mean_squared_error: 2.8310 - val_loss: 1.6528 - val_mean_squared_error: 1.6528\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.65276, saving model to temp/c93\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.4543 - mean_squared_error: 2.4543 - val_loss: 1.4701 - val_mean_squared_error: 1.4701\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.65276 to 1.47006, saving model to temp/c93\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.2926 - mean_squared_error: 2.2926 - val_loss: 1.4416 - val_mean_squared_error: 1.4416\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.47006 to 1.44164, saving model to temp/c93\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.2657 - mean_squared_error: 2.2657 - val_loss: 1.4339 - val_mean_squared_error: 1.4339\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.44164 to 1.43391, saving model to temp/c93\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.1351 - mean_squared_error: 2.1351 - val_loss: 1.5768 - val_mean_squared_error: 1.5768\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.43391\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 244us/step - loss: 2.1304 - mean_squared_error: 2.1304 - val_loss: 1.3013 - val_mean_squared_error: 1.3013\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.43391 to 1.30127, saving model to temp/c93\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.0968 - mean_squared_error: 2.0968 - val_loss: 1.2958 - val_mean_squared_error: 1.2958\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.30127 to 1.29578, saving model to temp/c93\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.1122 - mean_squared_error: 2.1122 - val_loss: 1.1579 - val_mean_squared_error: 1.1579\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.29578 to 1.15792, saving model to temp/c93\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.0871 - mean_squared_error: 2.0871 - val_loss: 1.1254 - val_mean_squared_error: 1.1254\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.15792 to 1.12541, saving model to temp/c93\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.0674 - mean_squared_error: 2.0674 - val_loss: 1.2619 - val_mean_squared_error: 1.2619\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.12541\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 1.9748 - mean_squared_error: 1.9748 - val_loss: 1.0761 - val_mean_squared_error: 1.0761\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.12541 to 1.07612, saving model to temp/c93\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 13s 250us/step - loss: 2.0021 - mean_squared_error: 2.0021 - val_loss: 1.2355 - val_mean_squared_error: 1.2355\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.07612\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.0093 - mean_squared_error: 2.0093 - val_loss: 1.3341 - val_mean_squared_error: 1.3341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00013: val_loss did not improve from 1.07612\n",
      "Epoch 00013: early stopping\n",
      "temp/c94\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 259us/step - loss: 2.8859 - mean_squared_error: 2.8859 - val_loss: 1.5457 - val_mean_squared_error: 1.5457\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.54574, saving model to temp/c94\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.4064 - mean_squared_error: 2.4064 - val_loss: 1.5484 - val_mean_squared_error: 1.5484\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.54574\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 2.3404 - mean_squared_error: 2.3404 - val_loss: 1.3669 - val_mean_squared_error: 1.3669\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.54574 to 1.36690, saving model to temp/c94\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.2380 - mean_squared_error: 2.2380 - val_loss: 1.5571 - val_mean_squared_error: 1.5571\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.36690\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.2116 - mean_squared_error: 2.2116 - val_loss: 1.3461 - val_mean_squared_error: 1.3461\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.36690 to 1.34613, saving model to temp/c94\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.1590 - mean_squared_error: 2.1590 - val_loss: 1.1960 - val_mean_squared_error: 1.1960\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.34613 to 1.19598, saving model to temp/c94\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.0869 - mean_squared_error: 2.0869 - val_loss: 1.2625 - val_mean_squared_error: 1.2625\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.19598\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.0899 - mean_squared_error: 2.0899 - val_loss: 1.2811 - val_mean_squared_error: 1.2811\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.19598\n",
      "Epoch 00008: early stopping\n",
      "temp/c95\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 260us/step - loss: 2.8796 - mean_squared_error: 2.8796 - val_loss: 1.9487 - val_mean_squared_error: 1.9487\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.94868, saving model to temp/c95\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.4519 - mean_squared_error: 2.4519 - val_loss: 1.5539 - val_mean_squared_error: 1.5539\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.94868 to 1.55392, saving model to temp/c95\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.3050 - mean_squared_error: 2.3050 - val_loss: 1.6086 - val_mean_squared_error: 1.6086\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.55392\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.2519 - mean_squared_error: 2.2519 - val_loss: 1.2911 - val_mean_squared_error: 1.2911\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.55392 to 1.29108, saving model to temp/c95\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 2.1988 - mean_squared_error: 2.1988 - val_loss: 1.3791 - val_mean_squared_error: 1.3791\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.29108\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.1108 - mean_squared_error: 2.1108 - val_loss: 1.3520 - val_mean_squared_error: 1.3520\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.29108\n",
      "Epoch 00006: early stopping\n",
      "temp/c96\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 255us/step - loss: 2.8628 - mean_squared_error: 2.8628 - val_loss: 1.8833 - val_mean_squared_error: 1.8833\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.88328, saving model to temp/c96\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 2.4883 - mean_squared_error: 2.4883 - val_loss: 1.4130 - val_mean_squared_error: 1.4130\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.88328 to 1.41302, saving model to temp/c96\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.3710 - mean_squared_error: 2.3710 - val_loss: 1.5507 - val_mean_squared_error: 1.5507\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.41302\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.2062 - mean_squared_error: 2.2062 - val_loss: 1.2835 - val_mean_squared_error: 1.2835\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.41302 to 1.28354, saving model to temp/c96\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.1497 - mean_squared_error: 2.1497 - val_loss: 1.5053 - val_mean_squared_error: 1.5053\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.28354\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.1445 - mean_squared_error: 2.1445 - val_loss: 1.2684 - val_mean_squared_error: 1.2684\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.28354 to 1.26845, saving model to temp/c96\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 2.1505 - mean_squared_error: 2.1505 - val_loss: 1.2294 - val_mean_squared_error: 1.2294\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.26845 to 1.22941, saving model to temp/c96\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.1208 - mean_squared_error: 2.1208 - val_loss: 1.2281 - val_mean_squared_error: 1.2281\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.22941 to 1.22813, saving model to temp/c96\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.1025 - mean_squared_error: 2.1025 - val_loss: 1.2723 - val_mean_squared_error: 1.2723\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.22813\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.0514 - mean_squared_error: 2.0514 - val_loss: 1.2335 - val_mean_squared_error: 1.2335\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.22813\n",
      "Epoch 00010: early stopping\n",
      "temp/c97\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 260us/step - loss: 2.8442 - mean_squared_error: 2.8442 - val_loss: 1.5465 - val_mean_squared_error: 1.5465\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.54651, saving model to temp/c97\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.4487 - mean_squared_error: 2.4487 - val_loss: 1.7823 - val_mean_squared_error: 1.7823\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.54651\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.3451 - mean_squared_error: 2.3451 - val_loss: 1.3122 - val_mean_squared_error: 1.3122\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.54651 to 1.31217, saving model to temp/c97\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.2416 - mean_squared_error: 2.2416 - val_loss: 1.3745 - val_mean_squared_error: 1.3745\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.31217\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 2.2086 - mean_squared_error: 2.2086 - val_loss: 1.3477 - val_mean_squared_error: 1.3477\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.31217\n",
      "Epoch 00005: early stopping\n",
      "temp/c98\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 259us/step - loss: 2.8144 - mean_squared_error: 2.8144 - val_loss: 1.6480 - val_mean_squared_error: 1.6480\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.64801, saving model to temp/c98\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 243us/step - loss: 2.4329 - mean_squared_error: 2.4329 - val_loss: 1.7454 - val_mean_squared_error: 1.7454\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.64801\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 243us/step - loss: 2.3450 - mean_squared_error: 2.3450 - val_loss: 1.3245 - val_mean_squared_error: 1.3245\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.64801 to 1.32448, saving model to temp/c98\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 2.2826 - mean_squared_error: 2.2826 - val_loss: 1.2489 - val_mean_squared_error: 1.2489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss improved from 1.32448 to 1.24891, saving model to temp/c98\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.2283 - mean_squared_error: 2.2283 - val_loss: 1.2423 - val_mean_squared_error: 1.2423\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.24891 to 1.24234, saving model to temp/c98\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.1413 - mean_squared_error: 2.1413 - val_loss: 1.3656 - val_mean_squared_error: 1.3656\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.24234\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.1246 - mean_squared_error: 2.1246 - val_loss: 1.3343 - val_mean_squared_error: 1.3343\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.24234\n",
      "Epoch 00007: early stopping\n",
      "temp/c99\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 262us/step - loss: 2.8584 - mean_squared_error: 2.8584 - val_loss: 2.0605 - val_mean_squared_error: 2.0605\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.06054, saving model to temp/c99\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 2.4683 - mean_squared_error: 2.4683 - val_loss: 1.4275 - val_mean_squared_error: 1.4275\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.06054 to 1.42747, saving model to temp/c99\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 2.3110 - mean_squared_error: 2.3110 - val_loss: 1.3511 - val_mean_squared_error: 1.3511\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.42747 to 1.35111, saving model to temp/c99\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 2.2622 - mean_squared_error: 2.2622 - val_loss: 1.4844 - val_mean_squared_error: 1.4844\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.35111\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 2.1670 - mean_squared_error: 2.1670 - val_loss: 1.4225 - val_mean_squared_error: 1.4225\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.35111\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "for idx, model_name in enumerate(model_names):\n",
    "    print(model_name)\n",
    "\n",
    "    if type(models[idx]) is list:\n",
    "        #clear session\n",
    "        keras.backend.clear_session() \n",
    "        #get model according to specification\n",
    "        model = get_model(models[idx], [0.2] * len(models), len(inputs), 1)\n",
    "        callbacks = [ModelCheckpoint(model_name, verbose= verbosity, monitor='val_loss',save_best_only=True), \n",
    "                     EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose= verbosity, mode='auto')]\n",
    "        model.compile(optimizer = optimizers.Adam(lr = 0.0001), loss='mse', metrics = ['mse'])\n",
    "        #print(len(X), len(y))\n",
    "        model.fit(X, y, epochs = 20, validation_data = (x_val, y_val), callbacks = callbacks, batch_size = 32, verbose = verbosity)\n",
    "    else:\n",
    "        models[idx].fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1045444005545215\n",
      "0.9891069723311482\n",
      "1.0945540088202792\n",
      "1.0609516205368303\n",
      "1.0581161922581865\n",
      "1.102875837830242\n",
      "1.0487987287915532\n",
      "1.003691142858016\n",
      "1.065068927325321\n",
      "1.046184404815653\n",
      "1.0154391881418752\n",
      "1.022903504873902\n",
      "1.0007123527348485\n",
      "1.0737535204524633\n",
      "1.005022967988206\n",
      "1.0344379600075813\n",
      "1.0436955367725762\n",
      "0.9958771196747628\n",
      "1.0590358732695146\n",
      "1.0388864145562966\n",
      "1.0738344323618803\n",
      "1.0009646347530086\n",
      "1.0107601086413593\n",
      "1.1041318206345057\n",
      "1.0400415839922448\n",
      "1.0403867635719861\n",
      "0.9838801531341833\n",
      "1.1850067738100807\n",
      "1.046271735933317\n",
      "1.0796876767954109\n",
      "1.0163212513072974\n",
      "1.0641597471411393\n",
      "1.014441191400064\n",
      "1.0070149650795865\n",
      "1.0703393305560582\n",
      "1.0345782205347287\n",
      "0.9870115959412039\n",
      "1.0983869641335604\n",
      "1.002648901448822\n",
      "1.0227589441755929\n",
      "1.030958560570318\n",
      "1.078658404963646\n",
      "1.1187298640758307\n",
      "1.0706627059387719\n",
      "1.0861730908257554\n",
      "1.0115916293178646\n",
      "1.031214315249626\n",
      "1.076786243442735\n",
      "0.9962832669880929\n",
      "1.0860991505512498\n",
      "1.085054338655672\n",
      "1.062441610834301\n",
      "0.9958708596740498\n",
      "1.0232497791683421\n",
      "1.006707372746031\n",
      "1.0503111783741819\n",
      "1.0662791292787999\n",
      "1.0277954716946178\n",
      "1.0209808499617448\n",
      "1.039206087896145\n",
      "1.0742163796449349\n",
      "0.9986995621305199\n",
      "1.0006785524235307\n",
      "1.0487600975164095\n",
      "0.9919529142643108\n",
      "1.0346961355014845\n",
      "1.0191860329240192\n",
      "1.0143898173135872\n",
      "1.0073664226655925\n",
      "1.0080623407745832\n",
      "1.075906921914578\n",
      "1.1116878370011136\n",
      "1.0073559821964486\n",
      "1.1060203665527562\n",
      "1.0081197444907948\n",
      "1.0946389262557714\n",
      "1.0643592797750616\n",
      "1.1610297281596704\n",
      "1.072516407186381\n",
      "1.0261302853413672\n",
      "0.9952645418435953\n",
      "0.9993173332373249\n",
      "1.065147766481737\n",
      "1.027001417605405\n",
      "1.04873935402756\n",
      "1.071787664124195\n",
      "1.1049561504918768\n",
      "1.053087622012832\n",
      "1.0947580585222914\n",
      "1.0154097721124233\n",
      "1.0656053113538342\n",
      "1.0866416625044262\n",
      "1.1305725809870038\n",
      "1.0036489892603087\n",
      "1.0219222076781744\n",
      "1.0745388959930482\n",
      "1.0586793580414164\n",
      "1.0798317717198664\n",
      "1.0963348748167137\n",
      "1.0976000623548745\n"
     ]
    }
   ],
   "source": [
    "def gen_data(mean = 0.1, var = 1.2, SIZE = 2000):\n",
    "    # set bmi to these values real world mean and standard deviation for a certain country.\n",
    "    bmi = np.random.normal(mean, var, SIZE)\n",
    "    estrogen =  2*bmi +  np.random.normal(mean,var, SIZE)\n",
    "    age = np.random.normal(mean,var, SIZE)\n",
    "    genes = 1.1 * age + estrogen +  np.random.normal(mean,var, SIZE)\n",
    "    insomnia = estrogen +  np.random.normal(mean,var, SIZE)\n",
    "    density = estrogen + genes + np.random.normal(mean,var, SIZE)\n",
    "    cancer = density + np.random.normal(mean,var, SIZE)\n",
    "    return pd.DataFrame({'bmi' : bmi,'density' : density, 'age' : age, 'cancer' : cancer, 'estrogen': estrogen, 'genes':genes, 'insomnia': insomnia})\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "nb_test = 6000\n",
    "metrics_dicts = []\n",
    "\n",
    "perturbed_df = gen_data()\n",
    "y_test2 = perturbed_df[target]\n",
    "x_test2 = normalize(perturbed_df[inputs].values)\n",
    "for idx, model_name in enumerate(model_names):\n",
    "\n",
    "    if type(models[idx]) is list:\n",
    "        keras.backend.clear_session()\n",
    "        model = load_model(model_name)\n",
    "    else:\n",
    "        model = models[idx]\n",
    "    #y_pred2 = model.predict(x_test2)[:,1]\n",
    "    y_pred2 = model.predict(x_test2)\n",
    "    print(mean_absolute_error(y_test2, y_pred2))\n",
    "    metrics_dicts.append(mean_absolute_error(y_test2, y_pred2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-18.778410976678792, -25.404208526529104)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(perturbed_df['cancer']), np.min(df['cancer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times =  0\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58219.74141897515\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58183.249501656755\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58284.53182140237\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58152.07403603442\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58121.13182128183\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58187.13578570564\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58288.31966171716\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58184.535536586256\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58166.2307847263\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58191.10662059022\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58161.70608870465\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58222.81192869746\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58222.81192869752\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58204.95369321195\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58221.27196512473\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58173.29926060847\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58192.44902541979\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58160.59836033844\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58209.29119844936\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58210.755851007234\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58218.220289226796\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58175.730492853414\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58189.77361295556\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58162.82319457434\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58225.92011228061\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58163.94967869802\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58183.24950165671\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58191.10662059022\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58181.97285864911\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58162.823194574354\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58203.52667237733\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58165.08554183069\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58209.29119844935\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58169.72279972749\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58155.200355389716\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58213.71338822068\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58180.70560671978\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58133.38639737528\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58192.44902541979\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58165.0855418307\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58175.73049285343\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58247.042192115376\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58139.50101187643\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58181.9728586491\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58153.10677049886\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58203.52667237737\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58252.14286312622\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58189.773612955556\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58166.23078472629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58207.835955281385\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58175.73049285343\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58219.74141897515\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58222.81192869747\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58139.50101187643\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58259.07589767777\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58160.59836033844\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58181.9728586492\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58210.755851007234\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58175.73049285342\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58196.53263211529\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58221.27196512473\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58173.29926060848\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58203.52667237737\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58156.26120721713\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58148.03680198409\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58173.29926060849\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58166.23078472628\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58215.20627486541\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58252.14286312617\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58213.71338822068\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58131.72360040762\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58139.50101187643\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58163.94967869802\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58133.38639737532\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58167.3854081579\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58233.85544939157\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58179.44774501427\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58158.41103317238\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58174.51018369346\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58135.95078354108\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58123.970977985504\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58144.149461865796\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58243.688920389985\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58146.07439782068\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58159.50000874417\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58202.10905706481\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58159.50000874417\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58135.086628634075\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58142.26198897943\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58212.22991393273\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58179.44774501427\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58174.51018369348\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58204.953693211944\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58152.07403603442\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58260.832760477875\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58161.70608870468\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58119.10063450419\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58212.22991393273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58275.22774807701\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58191.106620590246\n",
      "Times =  1\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58294.072394354866\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58232.2495345104\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58365.23322488264\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58224.36131073634\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58159.50000874421\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58225.92011228042\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58385.9086028457\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58280.78182097906\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58233.85544939147\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58284.53182140237\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58197.91263478283\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58313.86368614975\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58319.985867408184\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58266.160017717506\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58277.06965034733\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58260.83276047789\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58218.22028922693\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58202.109057064896\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58255.59049942062\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58280.78182097906\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58275.22774807693\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58273.39529951424\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58233.8554493915\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58233.855449391456\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58334.602958265474\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58245.36083944507\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58294.07239435484\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58313.86368614975\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58232.24953451041\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58225.9201122805\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58282.652091831726\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58232.24953451041\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58255.59049942062\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58280.781820979086\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58180.7056067197\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58322.04556041473\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58282.65209183173\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58191.106620590275\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58288.31966171716\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58237.09555788465\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58229.065978062295\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58309.829633018926\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58183.2495016567\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58227.48833436647\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58267.95466373624\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58290.22777502355\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58330.37919914956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58240.37337861698\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58267.954663736244\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58250.433202817694\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58227.48833436647\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58297.95487929776\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58273.3952995142\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58197.91263478282\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58315.894931655734\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58235.47079015988\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58250.43320281769\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58286.421010933576\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58259.07589767774\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58282.652091831726\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58242.02643381495\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58252.14286312622\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58307.8268226752\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58222.811928697556\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58207.83595528145\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58247.04219211546\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58232.2495345104\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58286.42101093361\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58385.9086028457\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58259.07589767777\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58160.59836033838\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58174.510183693375\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58260.832760477875\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58245.36083944508\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58218.22028922688\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58280.78182097909\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58243.688920389905\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58216.708574860924\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58196.53263211523\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58185.830964317036\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58166.2307847263\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58213.713388220625\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58296.00890296889\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58191.106620590275\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58224.36131073639\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58255.59049942062\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58230.65304442429\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58195.162030274005\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58196.53263211523\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58328.28155664743\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58247.04219211545\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58202.10905706495\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58262.599067286676\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58235.47079015992\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58372.03933187545\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58233.8554493915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58180.7056067197\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58297.95487929777\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58332.48633254259\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58305.83348800239\n",
      "Times =  2\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58191.106620590275\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58140.41197410697\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58221.271965124724\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58132.550319887225\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58111.270978466564\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58150.036679605044\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58245.36083944507\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58168.54941290476\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58142.26198897944\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58168.549412904795\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58132.550319887225\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58206.390120529395\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58193.800828346866\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58172.09772278101\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58181.97285864904\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58141.33229959935\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58150.036679605044\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58130.09823329585\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58172.097722781014\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58175.73049285337\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58168.549412904795\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58166.230784726344\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58151.05067262858\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58136.8242987228\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58196.532632115326\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58151.05067262858\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58212.22991393269\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58204.95369321204\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58150.036679605044\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58133.386397375216\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58159.50000874416\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58123.24716158598\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58180.705606719726\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58153.10677049886\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58120.44540923972\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58180.705606719726\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58169.72279972744\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58106.32275364897\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58165.08554183078\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58129.29958457258\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58151.05067262856\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58210.755851007234\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58119.76834713467\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58156.26120721728\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58150.036679605044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58172.09772278097\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58188.45000161968\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58158.41103317237\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58150.036679605044\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58172.09772278098\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58124.7041466693\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58176.960188914585\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58187.13578570561\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58143.20104285737\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58200.700846322085\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58148.03680198417\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58147.05091604738\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58161.7060887046\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58148.03680198417\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58167.3854081579\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58158.41103317233\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58135.08662863413\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58170.90556942141\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58119.1006345042\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58126.95977134621\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58155.20035538972\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58143.20104285736\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58158.41103317237\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58219.74141897509\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58159.50000874415\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58100.05428327367\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58111.81718617813\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58159.50000874416\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58125.44666812673\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58133.38639737513\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58199.302039206785\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58143.20104285738\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58126.19854284813\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58124.70414666931\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58125.44666812673\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58112.37273806122\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58132.55031988723\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58209.29119844934\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58114.68839480013\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58126.19854284812\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58163.94967869817\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58169.7227997274\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58105.87930913608\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58136.8242987228\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58209.29119844934\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58148.03680198414\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58143.20104285737\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58160.59836033844\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58144.14946186582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58206.3901205294\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58147.05091604744\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58114.09546246639\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58174.51018369333\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58262.599067286574\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58162.82319457423\n",
      "Times =  3\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58196.53263211525\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58145.10724663547\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58232.249534510454\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58135.08662863413\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58109.68841674563\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58160.59836033841\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58229.06597806234\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58156.26120721727\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58143.201042857356\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58160.5983603384\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58137.707174782176\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58173.29926060853\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58166.2307847263\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58170.90556942141\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58175.73049285343\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58166.2307847263\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58163.949678698125\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58135.08662863413\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58161.7060887046\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58159.50000874417\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58169.72279972744\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58166.2307847263\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58154.14887672252\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58149.03205627716\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58187.13578570565\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58152.07403603446\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58169.722799727395\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58169.72279972743\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58146.074397820674\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58150.036679605\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58191.10662059026\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58139.50101187643\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58175.73049285342\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58163.94967869817\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58135.950783541084\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58180.70560671973\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58157.331432906634\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58128.51029168662\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58191.10662059026\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58139.50101187644\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58137.70717478217\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58212.229913932686\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58126.19854284812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58142.26198897944\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58163.94967869812\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58169.72279972743\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58185.830964317036\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58173.299260608524\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58148.036801984126\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58156.26120721728\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58133.386397375216\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58165.08554183074\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58200.700846322085\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58119.768347134705\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58227.48833436644\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58138.59941230156\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58184.53553658624\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58196.53263211525\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58152.07403603446\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58158.41103317243\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58173.29926060853\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58149.03205627716\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58179.44774501427\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58147.05091604739\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58136.82429872277\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58176.960188914585\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58151.05067262856\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58175.73049285346\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58196.532632115246\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58165.08554183075\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58117.79325589565\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58117.153589045374\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58153.10677049882\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58166.230784726344\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58139.50101187644\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58195.16203027405\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58136.82429872277\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58121.131821281895\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58149.03205627715\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58111.817186178145\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58114.09546246644\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58138.59941230156\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58184.53553658624\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58127.73035412216\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58125.44666812674\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58151.05067262856\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58163.94967869813\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58131.72360040761\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58131.72360040761\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58216.708574860924\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58150.03667960501\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58123.97097798549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58160.5983603384\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58135.08662863413\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58229.06597806234\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58152.07403603446\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58124.70414666929\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58175.73049285342\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58222.81192869757\n",
      "['estrogen --> genes', 'age --> genes', 'genes --> density', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "58165.08554183074\n",
      "Model_name =  temp/c0 Violations =  0.0\n",
      "Average_violations =  58225.36326650888 41.10131594055143\n",
      "MSE =  1.9541439889007552 0.009498470924722816\n",
      "Model_name =  temp/c1 Violations =  0.0\n",
      "Average_violations =  58175.254564227405 36.86194686376678\n",
      "MSE =  1.6302579402179105 0.012020585286997445\n",
      "Model_name =  temp/c2 Violations =  0.0\n",
      "Average_violations =  58275.82163648005 56.886936499249344\n",
      "MSE =  1.95047038208758 0.026653743726064303\n",
      "Model_name =  temp/c3 Violations =  0.0\n",
      "Average_violations =  58161.018073823034 37.33367895866381\n",
      "MSE =  1.805665586206258 0.024103457047729527\n",
      "Model_name =  temp/c4 Violations =  0.0\n",
      "Average_violations =  58125.39780630956 20.171211072513657\n",
      "MSE =  1.8378380557152398 0.019134864825133716\n",
      "Model_name =  temp/c5 Violations =  0.0\n",
      "Average_violations =  58180.922734482374 29.284785232406144\n",
      "MSE =  1.9564477325928626 0.033940639208899795\n",
      "Model_name =  temp/c6 Violations =  0.0\n",
      "Average_violations =  58287.16377051757 60.98097352945306\n",
      "MSE =  1.8217146718366592 0.023194709729301505\n",
      "Model_name =  temp/c7 Violations =  0.0\n",
      "Average_violations =  58197.53199442183 49.09864928375805\n",
      "MSE =  1.6888927302661207 0.027804103184159205\n",
      "Model_name =  temp/c8 Violations =  0.0\n",
      "Average_violations =  58171.387316488646 37.32160289938563\n",
      "MSE =  1.832347677561768 0.017829217718928266\n",
      "Model_name =  temp/c9 Violations =  0.0\n",
      "Average_violations =  58201.19655380894 49.39793188867729\n",
      "MSE =  1.799591045486219 0.009553420067607305\n",
      "Model_name =  temp/c10 Violations =  0.0\n",
      "Average_violations =  58157.46905453922 25.812355905789406\n",
      "MSE =  1.7291013705350378 0.02701099065290932\n",
      "Model_name =  temp/c11 Violations =  0.0\n",
      "Average_violations =  58229.09124899629 52.09098570358154\n",
      "MSE =  1.7950792906181408 0.03485978454950388\n",
      "Model_name =  temp/c12 Violations =  0.0\n",
      "Average_violations =  58225.70735229472 57.99204740730131\n",
      "MSE =  1.6974951098903366 0.01032512808524624\n",
      "Model_name =  temp/c13 Violations =  0.0\n",
      "Average_violations =  58203.52925078297 38.655166346687544\n",
      "MSE =  1.9296593001070144 0.015055340352730904\n",
      "Model_name =  temp/c14 Violations =  0.0\n",
      "Average_violations =  58214.01124174363 40.376214305450134\n",
      "MSE =  1.716834041789451 0.010454321303081002\n",
      "Model_name =  temp/c15 Violations =  0.0\n",
      "Average_violations =  58185.423776353 45.127452893685614\n",
      "MSE =  1.7187568674012632 0.01992277231783009\n",
      "Model_name =  temp/c16 Violations =  0.0\n",
      "Average_violations =  58181.16391823747 26.29526241941033\n",
      "MSE =  1.8760416884745579 0.017649006166071443\n",
      "Model_name =  temp/c17 Violations =  0.0\n",
      "Average_violations =  58156.97306983333 28.51172224672049\n",
      "MSE =  1.651150190497573 0.010508444320439481\n",
      "Model_name =  temp/c18 Violations =  0.0\n",
      "Average_violations =  58199.6713773389 36.81425925966335\n",
      "MSE =  1.8924992380588066 0.020778088013485296\n",
      "Model_name =  temp/c19 Violations =  0.0\n",
      "Average_violations =  58206.69204339596 46.61414866001246\n",
      "MSE =  1.8075082278860455 0.022385061238141282\n",
      "Model_name =  temp/c20 Violations =  0.0\n",
      "Average_violations =  58207.93006248399 43.71926846715465\n",
      "MSE =  1.8530847502617278 0.01267464642610067\n",
      "Model_name =  temp/c21 Violations =  0.0\n",
      "Average_violations =  58195.396840455076 45.19912183090206\n",
      "MSE =  1.686749632383542 0.0156515842306742\n",
      "Model_name =  temp/c22 Violations =  0.0\n",
      "Average_violations =  58182.20715292454 33.47682397271845\n",
      "MSE =  1.6615218588109912 0.015043610027343363\n",
      "Model_name =  temp/c23 Violations =  0.0\n",
      "Average_violations =  58170.633749741435 37.64206495850657\n",
      "MSE =  1.9984219361977469 0.02180910222885003\n",
      "Model_name =  temp/c24 Violations =  0.0\n",
      "Average_violations =  58236.04787209176 58.67180522733617\n",
      "MSE =  1.7870475291757628 0.021473108136071936\n",
      "Model_name =  temp/c25 Violations =  0.0\n",
      "Average_violations =  58178.10880670154 39.15759414581902\n",
      "MSE =  1.7637219210618915 0.01485483287954537\n",
      "Model_name =  temp/c26 Violations =  0.0\n",
      "Average_violations =  58214.81865241791 48.265164424296934\n",
      "MSE =  1.6009281918441343 0.0167251636936886\n",
      "Model_name =  temp/c27 Violations =  0.0\n",
      "Average_violations =  58219.91169991986 55.676242642004375\n",
      "MSE =  2.2080116710530273 0.025202873188078203\n",
      "Model_name =  temp/c28 Violations =  0.0\n",
      "Average_violations =  58177.58336764631 34.49381333077377\n",
      "MSE =  1.8886399536786738 0.02874147043114696\n",
      "Model_name =  temp/c29 Violations =  0.0\n",
      "Average_violations =  58168.04159595876 35.008263904347466\n",
      "MSE =  1.9583010657489348 0.027262877371403402\n",
      "Model_name =  temp/c30 Violations =  0.0\n",
      "Average_violations =  58209.19634838587 45.34549563277683\n",
      "MSE =  1.6920692760545735 0.013973901325220982\n",
      "Model_name =  temp/c31 Violations =  0.0\n",
      "Average_violations =  58165.02081245088 41.58125595623613\n",
      "MSE =  1.8810093830866708 0.025077332173682507\n",
      "Model_name =  temp/c32 Violations =  0.0\n",
      "Average_violations =  58205.329449360775 31.718687164251108\n",
      "MSE =  1.6603498388353914 0.008236934932031295\n",
      "Model_name =  temp/c33 Violations =  0.0\n",
      "Average_violations =  58191.8902674759 51.66706167763195\n",
      "MSE =  1.6804859498695939 0.008575643710582037\n",
      "Model_name =  temp/c34 Violations =  0.0\n",
      "Average_violations =  58148.07553872255 22.50509994138862\n",
      "MSE =  1.9431490428627285 0.01689514277694035\n",
      "Model_name =  temp/c35 Violations =  0.0\n",
      "Average_violations =  58224.29254051872 58.024160825748936\n",
      "MSE =  1.7983108665641447 0.027280290091236212\n",
      "Model_name =  temp/c36 Violations =  0.0\n",
      "Average_violations =  58197.602982796394 49.79451426729015\n",
      "MSE =  1.6200018240159952 0.01826812654542554\n",
      "Model_name =  temp/c37 Violations =  0.0\n",
      "Average_violations =  58139.83151582529 31.311671861630973\n",
      "MSE =  1.976064797746151 0.013565351909784222\n",
      "Model_name =  temp/c38 Violations =  0.0\n",
      "Average_violations =  58209.2402123895 46.94136062554309\n",
      "MSE =  1.6898185269020758 0.005373595339687206\n",
      "Model_name =  temp/c39 Violations =  0.0\n",
      "Average_violations =  58167.745424041095 42.10803197864236\n",
      "MSE =  1.7451367783800913 0.014766421342729381\n",
      "Model_name =  temp/c40 Violations =  0.0\n",
      "Average_violations =  58173.38857958162 34.91991550267012\n",
      "MSE =  1.76670481663352 0.006602990855527334\n",
      "Model_name =  temp/c41 Violations =  0.0\n",
      "Average_violations =  58244.96439751855 40.16711054132441\n",
      "MSE =  1.9371521313692148 0.007560797677160007\n",
      "Model_name =  temp/c42 Violations =  0.0\n",
      "Average_violations =  58142.17935087898 24.75666626821826\n",
      "MSE =  1.9717230803802093 0.023175596120142056\n",
      "Model_name =  temp/c43 Violations =  0.0\n",
      "Average_violations =  58176.996097303076 32.444676393322275\n",
      "MSE =  1.9077492911839053 0.02200657886844651\n",
      "Model_name =  temp/c44 Violations =  0.0\n",
      "Average_violations =  58183.76194813457 48.88269896873337\n",
      "MSE =  2.0485845972055237 0.03737237003688909\n",
      "Model_name =  temp/c45 Violations =  0.0\n",
      "Average_violations =  58208.89374247733 48.81685188820162\n",
      "MSE =  1.758427810984286 0.021489880411201014\n",
      "Model_name =  temp/c46 Violations =  0.0\n",
      "Average_violations =  58239.20075705313 58.95969039627458\n",
      "MSE =  1.770206261465383 0.02796231787933451\n",
      "Model_name =  temp/c47 Violations =  0.0\n",
      "Average_violations =  58190.46432133836 30.876543154236916\n",
      "MSE =  1.86025566594221 0.03087671724115308\n",
      "Model_name =  temp/c48 Violations =  0.0\n",
      "Average_violations =  58183.064732512925 49.51638722988326\n",
      "MSE =  1.643019756901992 0.01995422592927739\n",
      "Model_name =  temp/c49 Violations =  0.0\n",
      "Average_violations =  58196.65702202433 36.23474576541294\n",
      "MSE =  2.020495024017584 0.019192599117931183\n",
      "Model_name =  temp/c50 Violations =  0.0\n",
      "Average_violations =  58165.327342816105 40.75132654915495\n",
      "MSE =  1.92794083838904 0.025514474628079282\n",
      "Model_name =  temp/c51 Violations =  0.0\n",
      "Average_violations =  58214.93550725456 52.063559179361675\n",
      "MSE =  1.8323567384201342 0.022343327004749246\n",
      "Model_name =  temp/c52 Violations =  0.0\n",
      "Average_violations =  58221.01096505984 32.8153571734097\n",
      "MSE =  1.662906348347621 0.0052490824464262245\n",
      "Model_name =  temp/c53 Violations =  0.0\n",
      "Average_violations =  58150.09575916283 29.008581908534936\n",
      "MSE =  1.69751489621691 0.00924021281014572\n",
      "Model_name =  temp/c54 Violations =  0.0\n",
      "Average_violations =  58250.790002505506 42.892887416482814\n",
      "MSE =  1.6812444704461376 0.02205004587226023\n",
      "Model_name =  temp/c55 Violations =  0.0\n",
      "Average_violations =  58170.67634119601 38.21440964388042\n",
      "MSE =  1.7764242875200242 0.017905459646489937\n",
      "Model_name =  temp/c56 Violations =  0.0\n",
      "Average_violations =  58190.99812852513 37.37347365320802\n",
      "MSE =  1.854748539044904 0.03150417947016011\n",
      "Model_name =  temp/c57 Violations =  0.0\n",
      "Average_violations =  58213.85389569016 45.538456388098496\n",
      "MSE =  1.7192273836660827 0.02164653736036586\n",
      "Model_name =  temp/c58 Violations =  0.0\n",
      "Average_violations =  58183.729307137444 44.76913235902029\n",
      "MSE =  1.6880614089193766 0.025005942090163068\n",
      "Model_name =  temp/c59 Violations =  0.0\n",
      "Average_violations =  58201.245291319334 49.067661665142445\n",
      "MSE =  1.7846012784578262 0.027510983537477853\n",
      "Model_name =  temp/c60 Violations =  0.0\n",
      "Average_violations =  58198.75217318014 34.114003716307636\n",
      "MSE =  1.8437957953459996 0.01625197725505934\n",
      "Model_name =  temp/c61 Violations =  0.0\n",
      "Average_violations =  58177.39020216149 45.27270948175898\n",
      "MSE =  1.6468176744575271 0.01922611469020509\n",
      "Model_name =  temp/c62 Violations =  0.0\n",
      "Average_violations =  58215.42670237206 54.67176967519794\n",
      "MSE =  1.6881538176166717 0.0159884417206599\n",
      "Model_name =  temp/c63 Violations =  0.0\n",
      "Average_violations =  58161.30617161657 38.05564837899708\n",
      "MSE =  1.8255430536934578 0.006837773643032404\n",
      "Model_name =  temp/c64 Violations =  0.0\n",
      "Average_violations =  58154.91420683364 31.451174044534287\n",
      "MSE =  1.6383009514766793 0.026199725771033244\n",
      "Model_name =  temp/c65 Violations =  0.0\n",
      "Average_violations =  58188.12549925707 34.99901178449334\n",
      "MSE =  1.7531651279350906 0.010101281609846885\n",
      "Model_name =  temp/c66 Violations =  0.0\n",
      "Average_violations =  58173.18300868065 35.092543599958844\n",
      "MSE =  1.6888349290491538 0.016235681474457413\n",
      "Model_name =  temp/c67 Violations =  0.0\n",
      "Average_violations =  58208.94220295621 49.24078870752198\n",
      "MSE =  1.7310235239068819 0.025648686987749046\n",
      "Model_name =  temp/c68 Violations =  0.0\n",
      "Average_violations =  58263.58137926555 73.33530168512573\n",
      "MSE =  1.6581443916752376 0.012951165613244008\n",
      "Model_name =  temp/c69 Violations =  0.0\n",
      "Average_violations =  58199.34370911834 40.42141917664671\n",
      "MSE =  1.6850230142052371 0.03209992958153187\n",
      "Model_name =  temp/c70 Violations =  0.0\n",
      "Average_violations =  58127.54237497883 22.14058108240634\n",
      "MSE =  1.8945637523450718 0.009537578578203892\n",
      "Model_name =  temp/c71 Violations =  0.0\n",
      "Average_violations =  58135.74549269833 24.673001066975427\n",
      "MSE =  1.9869334616677767 0.022673684496661236\n",
      "Model_name =  temp/c72 Violations =  0.0\n",
      "Average_violations =  58184.34730460472 44.326761627593115\n",
      "MSE =  1.645007504273012 0.02175132814058942\n",
      "Model_name =  temp/c73 Violations =  0.0\n",
      "Average_violations =  58167.60617241837 47.423877798667085\n",
      "MSE =  2.0223215508489436 0.018695159804217546\n",
      "Model_name =  temp/c74 Violations =  0.0\n",
      "Average_violations =  58164.623276659084 33.49305876528463\n",
      "MSE =  1.7550289750681545 0.005845162068121058\n",
      "Model_name =  temp/c75 Violations =  0.0\n",
      "Average_violations =  58227.27533496288 34.35117015908482\n",
      "MSE =  2.051997195420756 0.026011808596078684\n",
      "Model_name =  temp/c76 Violations =  0.0\n",
      "Average_violations =  58175.79050174609 42.438215358821736\n",
      "MSE =  1.9153038768236006 0.025596684360233657\n",
      "Model_name =  temp/c77 Violations =  0.0\n",
      "Average_violations =  58155.61249304083 38.0613328442454\n",
      "MSE =  2.296734126944593 0.04028329527167378\n",
      "Model_name =  temp/c78 Violations =  0.0\n",
      "Average_violations =  58161.19475468879 26.95163231080984\n",
      "MSE =  1.9574471795187398 0.009801343793110892\n",
      "Model_name =  temp/c79 Violations =  0.0\n",
      "Average_violations =  58139.761400540745 27.940634338198738\n",
      "MSE =  1.7525861821206932 0.017141105318511207\n",
      "Model_name =  temp/c80 Violations =  0.0\n",
      "Average_violations =  58129.167490809865 21.85132295619118\n",
      "MSE =  1.6662797705753505 0.027481718443871546\n",
      "Model_name =  temp/c81 Violations =  0.0\n",
      "Average_violations =  58157.2531455688 32.85443991438045\n",
      "MSE =  1.625023965651905 0.029342927498246747\n",
      "Model_name =  temp/c82 Violations =  0.0\n",
      "Average_violations =  58233.38113959861 41.81717735519688\n",
      "MSE =  1.9007112759918297 0.030022312989758597\n",
      "Model_name =  temp/c83 Violations =  0.0\n",
      "Average_violations =  58144.89994183331 28.91352981272209\n",
      "MSE =  1.688214732869105 0.024557472314701914\n",
      "Model_name =  temp/c84 Violations =  0.0\n",
      "Average_violations =  58158.87663261386 40.230746149430324\n",
      "MSE =  1.851737965596094 0.008290036787712283\n",
      "Model_name =  temp/c85 Violations =  0.0\n",
      "Average_violations =  58193.17497695304 40.63277711790131\n",
      "MSE =  1.8557743275161314 0.016559954946441174\n",
      "Model_name =  temp/c86 Violations =  0.0\n",
      "Average_violations =  58180.9563828985 28.92038873044442\n",
      "MSE =  2.029046307396393 0.03703240397223934\n",
      "Model_name =  temp/c87 Violations =  0.0\n",
      "Average_violations =  58141.96289211295 32.727279735664304\n",
      "MSE =  1.8122351136674293 0.014890371769274675\n",
      "Model_name =  temp/c88 Violations =  0.0\n",
      "Average_violations =  58151.83563005626 26.0735037994451\n",
      "MSE =  2.0934122386342846 0.010905681914201222\n",
      "Model_name =  temp/c89 Violations =  0.0\n",
      "Average_violations =  58241.62781097261 50.09923359621878\n",
      "MSE =  1.7135063382746833 0.014437521406516492\n",
      "Model_name =  temp/c90 Violations =  0.0\n",
      "Average_violations =  58181.14085467972 40.02874251118507\n",
      "MSE =  1.7857402589631612 0.024968502010612174\n",
      "Model_name =  temp/c91 Violations =  0.0\n",
      "Average_violations =  58160.947815400315 29.834627909473976\n",
      "MSE =  1.9093664583001524 0.020349322456689405\n",
      "Model_name =  temp/c92 Violations =  0.0\n",
      "Average_violations =  58197.18737029387 41.882326744662116\n",
      "MSE =  2.0186802102911483 0.02575947099036737\n",
      "Model_name =  temp/c93 Violations =  0.0\n",
      "Average_violations =  58166.69522917357 40.15990640351426\n",
      "MSE =  1.6252564984797377 0.020398034766650072\n",
      "Model_name =  temp/c94 Violations =  0.0\n",
      "Average_violations =  58267.082047736265 63.60781529546406\n",
      "MSE =  1.7477683453094595 0.026227270701844312\n",
      "Model_name =  temp/c95 Violations =  0.0\n",
      "Average_violations =  58173.67162254452 35.143935364024415\n",
      "MSE =  1.9158243584519261 0.02587621349806027\n",
      "Model_name =  temp/c96 Violations =  0.0\n",
      "Average_violations =  58134.651462589885 26.852889419863974\n",
      "MSE =  1.8027603894742779 0.02699022028283322\n",
      "Model_name =  temp/c97 Violations =  0.0\n",
      "Average_violations =  58215.10636744431 50.17633788968229\n",
      "MSE =  1.889034126244486 0.01999981558206597\n",
      "Model_name =  temp/c98 Violations =  0.0\n",
      "Average_violations =  58273.28126915093 39.27560334956091\n",
      "MSE =  1.8797667044898145 0.029894208186214907\n",
      "Model_name =  temp/c99 Violations =  0.0\n",
      "Average_violations =  58206.2122112494 58.58025980773671\n",
      "MSE =  1.9408417145456105 0.012595375589405789\n"
     ]
    }
   ],
   "source": [
    "#the number of times to sample\n",
    "times = 4\n",
    "## the size of the test set\n",
    "\n",
    "\n",
    "violations = np.zeros(len(models))\n",
    "violation_mean = np.zeros((len(models), times))\n",
    "violation_mean2 = np.zeros((len(models), times))\n",
    "mean = np.zeros((len(models), times))\n",
    "\n",
    "fold = 0\n",
    "\n",
    "\n",
    "\n",
    "for t in range(times):\n",
    "    print(\"Times = \", t)\n",
    "    df_test = gen_data(SIZE = nb_test)\n",
    "    x_test = df_test[inputs].values\n",
    "    x_test_norm = normalize(df_test[inputs].values)\n",
    "    y_test = df_test[target].values\n",
    "    #bic_orig = get_bic(df_test,prior)\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        if type(models[idx]) is list:\n",
    "            keras.backend.clear_session()\n",
    "            model = load_model(model_name)\n",
    "        else:\n",
    "            model = models[idx]\n",
    "            \n",
    "        predicted = model.predict(x_test_norm)\n",
    "        test_df = pd.DataFrame(x_test, columns = inputs)\n",
    "        test_targets = pd.DataFrame(predicted,columns = target)\n",
    "        test_df = test_df.join(test_targets)\n",
    "       \n",
    "        \n",
    "    \n",
    "        mean[idx][t] = mean_squared_error(y_test, predicted) \n",
    "        test_df[test_df['cancer'] > 0.5] = 1\n",
    "        test_df[test_df['cancer'] <= 0.5] = 0\n",
    "        bic_pred = get_bic(test_df,prior)\n",
    "        \n",
    "        #bic_pred = get_bic(df_test.join(pd.DataFrame(model.predict(x_test), columns = ['target'])), prior)\n",
    "        \n",
    "        print(tetrad.getEdges())\n",
    "        print(bic_pred)\n",
    "        violation_mean[idx][t] = bic_pred\n",
    "        violation_mean2[idx][t] = bic_pred\n",
    "        #print(bic_orig - bic_pred)\n",
    "metric = []\n",
    "metric_err = []\n",
    "viol = []\n",
    "viol_err = []\n",
    "\n",
    "#normalize the violations for prettier graphing.\n",
    "#also violations are always positive, so just divide by max.\n",
    "\n",
    "#TMK\n",
    "#violation_mean = violation_mean / np.max(violation_mean)\n",
    "\n",
    "for i in range(len(violations)):\n",
    "    print(\"Model_name = \", model_names[i], \"Violations = \", violations[i])\n",
    "    print(\"Average_violations = \", np.mean(violation_mean[i]), np.std(violation_mean[i]))\n",
    "    print(\"MSE = \", np.mean(mean[i]), np.std(mean[i]))\n",
    "    #print(\"mean = \", mean[i])\n",
    "    metric.append(np.mean(mean[i]))\n",
    "    metric_err.append(np.std(mean[i]))\n",
    "    viol.append(np.mean(violation_mean[i]))\n",
    "    #viol.append(violations[i]/times)\n",
    "    viol_err.append(np.std(violation_mean[i]))\n",
    "   \n",
    "    \n",
    "MSE = []\n",
    "VIO = []\n",
    "VIO2 = []\n",
    "AUS = []\n",
    "for i, m in enumerate(models):\n",
    "    MSE.append(np.mean(mean[i]))\n",
    "    VIO.append(np.mean(violation_mean[i]))\n",
    "    VIO2.append(np.mean(violation_mean2[i]))\n",
    "    AUS.append(metrics_dicts[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best by BIC =  1.0622991004227884\n",
      "Best by AUC =  0.9953133889158303\n",
      "Best by MET =  1.017923793069502\n",
      "Random =  1.0573892236121751\n",
      "-0.08568540229987733\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4607467154439681\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.4607467154439681"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.polynomial.polynomial import polyfit  \n",
    "from scipy.stats import pearsonr\n",
    "from pylab import text\n",
    "\n",
    "def norm(a):\n",
    "    return (a - np.min(a)) / a.ptp()\n",
    "METRIC = norm(np.array(VIO)) + np.array(MSE)\n",
    "n_low = 10\n",
    "sorted_aus = [AUS for _,AUS in sorted(zip(VIO,AUS))]\n",
    "print(\"Best by BIC = \", np.mean(sorted_aus[:n_low]))\n",
    "\n",
    "sorted_aus = [AUS for _,AUS in sorted(zip(MSE,AUS))]\n",
    "print(\"Best by AUC = \", np.mean(sorted_aus[:n_low]))\n",
    "\n",
    "\n",
    "sorted_aus = [AUS for _,AUS in sorted(zip(METRIC,AUS))]\n",
    "print(\"Best by MET = \", np.mean(sorted_aus[:n_low]))\n",
    "print(\"Random = \", np.mean(AUS[:n_low]))\n",
    "print(pearsonr(VIO,AUS)[0])\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(VIO,AUS, 1)\n",
    "ax.plot(VIO,AUS, '.')\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(VIO,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "plt.plot(VIO, b + m * np.array(VIO), '-')\n",
    "ax.set_xlabel(\"BIC\")\n",
    "ax.set_ylabel(\"OoS MSE\")\n",
    "fig.savefig('Ex4VIOVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(pearsonr(METRIC,AUS)[0])\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(METRIC,AUS, 1)\n",
    "ax.plot(METRIC,AUS, '.')\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(METRIC,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "plt.plot(METRIC, b + m * np.array(METRIC), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"Combined\")\n",
    "ax.set_ylabel(\"OoS AUCROC\")\n",
    "fig.savefig('Ex4ProposedVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(MSE,AUS, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(MSE,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(MSE,AUS, '.')\n",
    "plt.plot(MSE, b + m * np.array(MSE), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"AUC\")\n",
    "ax.set_ylabel(\"OoS AUCROC\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "MSE = np.array(MSE)\n",
    "\n",
    "x = []\n",
    "y1 = []\n",
    "y2 = []\n",
    "y3 = []\n",
    "for split in range(10, len(AUS), 5):\n",
    "    #print(\"******\", split, \"*******\")\n",
    "    sorted_aus = [AUS for _,AUS in sorted(zip(VIO,AUS))]\n",
    "    sorted_mse = [MSE for _,MSE in sorted(zip(VIO,MSE))]\n",
    "\n",
    "    low = []\n",
    "    high = []\n",
    "    low = sorted_aus[:split]\n",
    "    high = sorted_aus[split:]\n",
    "\n",
    "    x.append(split)\n",
    "    \n",
    "    \n",
    "    #print(\"Low Violations = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High Violations = \", np.mean(high), \"for\", len(high))\n",
    "    y1.append(np.mean(low)) \n",
    "    sorted_aus_by_mse = [AUS for _,AUS in sorted(zip(MSE,AUS))]\n",
    "    low = sorted_aus_by_mse[:split]\n",
    "    high = sorted_aus_by_mse[split:]\n",
    "    #print(\"Low AUS by MSE = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High AUS by MSE = \", np.mean(high), \"for\", len(high))\n",
    "    y2.append(np.mean(low))\n",
    "    sorted_aus = [AUS for _,AUS in sorted(zip(METRIC,AUS))]\n",
    "    sorted_mse = [MSE for _,MSE in sorted(zip(METRIC,MSE))]\n",
    "\n",
    "    low = []\n",
    "    high = []\n",
    "    low = sorted_aus[:split]\n",
    "    high = sorted_aus[split:]\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"Low Metric = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High Metric = \", np.mean(high), \"for\", len(high))\n",
    "    y3.append(np.mean(low))\n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x,y1, '-', label = 'BIC')\n",
    "ax.plot(x,y2, '-', label = 'MSE')\n",
    "ax.plot(x,y3, '-', label = 'METRIC')\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"Out of Sample AUCROC\")\n",
    "plt.show()  \n",
    "pearsonr(METRIC,AUS)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
