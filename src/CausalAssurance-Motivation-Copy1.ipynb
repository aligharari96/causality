{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes \n",
    "### Required installing Oracle JAVA 8 to get javabridge installed\n",
    "### Then, I was able to install py-causal from https://bd2kccd.github.io/docs/py-causal/\n",
    "### GFCI is slower than RFCI, but more accurate (SPIRTES), GFCI and RFCI account for unobserved variables, FGES assumes no unobserved variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure Learning Performance Guarantees If the assumptions in the previous section hold, then in the large sample limit, the CBN structure output by GFCId will contain an edge of one of four kinds between Xand Y   if and only if Xand Yare not independent conditional on any subset of the other measured variables of less than or equal to a specified size. In addition, there is (1) an arc X->Y   if and only if Xdirectly or indirectly causes Y, and Y   does not directly or indirectly cause X; (2) an edge X <-->Y   if and only if X   is not a direct or indirect cause of Yand Y   is not a direct or indirect cause of X(which can only occur if there are latent confounders of Xand some other variable or Yand some other variable; (3) an edge Xo->Y   only if Yis not a direct or indirect cause of X, but Xmay or may not be an indirect cause of Y; (4) an edge X oâ€“o Y   indicates that Xand Y   are dependent no matter what subset of observed variables is conditioned on, but contains no orientation information (X   may be a direct or indirect cause of Y, and Ymay be an indirect cause of X, or there may be a latent common cause of Xand Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying some various ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512]] ['temp/a0', 'temp/a1', 'temp/a2', 'temp/a3', 'temp/a4', 'temp/a5', 'temp/a6', 'temp/a7', 'temp/a8', 'temp/a9', 'temp/a10', 'temp/a11', 'temp/a12', 'temp/a13', 'temp/a14', 'temp/a15', 'temp/a16', 'temp/a17', 'temp/a18', 'temp/a19', 'temp/a20', 'temp/a21', 'temp/a22', 'temp/a23', 'temp/a24', 'temp/a25', 'temp/a26', 'temp/a27', 'temp/a28', 'temp/a29', 'temp/a30', 'temp/a31', 'temp/a32', 'temp/a33', 'temp/a34', 'temp/a35', 'temp/a36', 'temp/a37', 'temp/a38', 'temp/a39', 'temp/a40', 'temp/a41', 'temp/a42', 'temp/a43', 'temp/a44', 'temp/a45', 'temp/a46', 'temp/a47', 'temp/a48', 'temp/a49', 'temp/a50', 'temp/a51', 'temp/a52', 'temp/a53', 'temp/a54', 'temp/a55', 'temp/a56', 'temp/a57', 'temp/a58', 'temp/a59', 'temp/a60', 'temp/a61', 'temp/a62', 'temp/a63', 'temp/a64', 'temp/a65', 'temp/a66', 'temp/a67', 'temp/a68', 'temp/a69', 'temp/a70', 'temp/a71', 'temp/a72', 'temp/a73', 'temp/a74', 'temp/a75', 'temp/a76', 'temp/a77', 'temp/a78', 'temp/a79', 'temp/a80', 'temp/a81', 'temp/a82', 'temp/a83', 'temp/a84', 'temp/a85', 'temp/a86', 'temp/a87', 'temp/a88', 'temp/a89', 'temp/a90', 'temp/a91', 'temp/a92', 'temp/a93', 'temp/a94', 'temp/a95', 'temp/a96', 'temp/a97', 'temp/a98', 'temp/a99']\n",
      "9042 9042 18084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-507936.57097312226"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from pycausal import search as s\n",
    "import configparser\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, \\\n",
    "                        Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "import keras.optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "import glob, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "# select your GPU Here\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" #Comment this line out if you want all GPUS (2 hehe)\n",
    "\n",
    "# python full-display web browser\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "def get_model(dense, dropouts, inputs, target_len):\n",
    "    # dense is an ordered list of the number of dense neurons like [1024, 2048, 1024]\n",
    "    # dropouts is an ordered list of the dropout masks like [0.2, 0.3, 0.4]\n",
    "    inputs = keras.Input(shape = (inputs,))\n",
    "\n",
    "    x = keras.layers.Dense(dense[0], activation = 'relu')(inputs)\n",
    "    x = keras.layers.Dropout(dropouts[0])(x, training=True)\n",
    "    for den, drop in zip(dense[1:], dropouts[1:]):\n",
    "        x = keras.layers.Dense(den, activation = 'relu')(x)\n",
    "        x = keras.layers.Dropout(drop)(x, training=True)\n",
    "    outputs = keras.layers.Dense(target_len, activation = 'softmax')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def discrete_gauss(low, high, samples, std = 20):\n",
    "    x = np.arange(low, high)\n",
    "    xU, xL = x + 0.5, x - 0.5 \n",
    "    prob = ss.norm.cdf(xU, scale = std) - ss.norm.cdf(xL, scale = std)\n",
    "    prob = prob / prob.sum() #normalize the probabilities so their sum is 1\n",
    "    nums = np.random.choice(x, size = samples, p = prob)\n",
    "    return nums\n",
    "\n",
    "\n",
    "\n",
    "def bar_plot(x_ax, val1, val1std, val2, val2std):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ## the data\n",
    "    N = len(x_ax)\n",
    "\n",
    "    ## necessary variables\n",
    "    ind = np.arange(N)                # the x locations for the groups\n",
    "    width = 0.35                      # the width of the bars\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    ## the bars\n",
    "    rects1 = ax.bar(ind, val1, width,\n",
    "                    color='gray',\n",
    "                    yerr=val1std,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='blue'))\n",
    "\n",
    "    rects2 = ax.bar(ind+width, val2, width,\n",
    "                        color='blue',\n",
    "                        #yerr=val2std,\n",
    "                        error_kw=dict(elinewidth=2,ecolor='gray'))\n",
    "\n",
    "    # axes and labels\n",
    "    ax.set_xlim(-width,len(ind)+width)\n",
    "    #ax.set_ylim(0,45)\n",
    "    ax.set_ylabel('Percentage')\n",
    "    ax.set_title('')\n",
    "    plt.xticks(ind + width / 2, x_ax, rotation=75, size = 14)\n",
    "    ## add a legend\n",
    "    ax.legend( (rects1[0], rects2[0]), ('Accuracy', '% Violations') )\n",
    "    fig.savefig(\"violations.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def normalize(x):\n",
    "    return (x - x.min(0)) / x.ptp(0)\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 20000):\n",
    "    # set bmi to these values real world mean and standard deviation for a certain country.\n",
    "    bmi = np.random.normal(25,5, SIZE)\n",
    "    estrogen =  np.random.normal(bmi, 10) +  np.random.normal(mean,var, SIZE)\n",
    "    \n",
    "    age = np.random.normal(55,10, SIZE)\n",
    "    genes = np.random.normal(age, 10,SIZE) +   np.random.normal(mean,var, SIZE)\n",
    "    \n",
    "    insomnia = np.random.normal(estrogen, 8,SIZE) +   np.random.normal(mean,var, SIZE)\n",
    "    density = np.random.normal(estrogen, 4, SIZE) + np.random.normal(genes,12, SIZE) + np.random.normal(mean,var + 10, SIZE)\n",
    "\n",
    "    cancer = np.zeros_like(density)\n",
    "    m = np.mean(density)\n",
    "\n",
    "    cancer[density > m] = np.random.binomial(n=1, p=0.08, size=len(density[density > m]))\n",
    "    cancer[density <= m] = np.random.binomial(n=1, p=0.01, size=len(density[density <= m]))\n",
    "    \n",
    "    return pd.DataFrame({'bmi' : bmi,'density' : density, 'age' : age, 'cancer' : cancer, 'estrogen': estrogen, 'genes':genes, 'insomnia': insomnia})\n",
    "\n",
    "def gen_data_perturbed(mean = 0, var = 1, SIZE = 20000):\n",
    "    bmi = np.random.normal(30,3, SIZE)\n",
    "    \n",
    "    age = np.random.normal(60,14, SIZE) + np.random.normal(-bmi,var, SIZE) \n",
    "    income = np.random.normal(age, var,SIZE) + np.random.normal(10,12, SIZE)\n",
    "    density = np.random.normal(-bmi,var, SIZE) + np.random.normal(-age,var, SIZE) + np.random.normal(mean,var, SIZE)\n",
    "    cancer = np.zeros_like(density)\n",
    "    m = np.mean(density)\n",
    "    print(m)\n",
    "    cancer[density > m] = np.random.binomial(n=1, p=0.08, size=len(density[density > m]))\n",
    "    cancer[density <= m] = np.random.binomial(n=1, p=0.01, size=len(density[density <= m]))\n",
    "    \n",
    "    return pd.DataFrame({'bmi' : bmi,'density' : density, 'age' : age, 'cancer' : cancer, 'income':income})\n",
    "def gen_data_perturbed(mean = 2, var = 5, SIZE = 20000):\n",
    "    # set bmi to these values real world mean and standard deviation for a certain country.\n",
    "    bmi = np.random.normal(25,5, SIZE)\n",
    "    estrogen =  np.random.normal(bmi, 10) +  np.random.normal(mean,var, SIZE)\n",
    "    \n",
    "    age = np.random.normal(55,10, SIZE)\n",
    "    genes = np.random.normal(age, 10,SIZE) +   np.random.normal(mean,var, SIZE)\n",
    "    \n",
    "    insomnia = np.random.normal(estrogen, 8,SIZE) +   np.random.normal(mean,var, SIZE)\n",
    "    density = np.random.normal(estrogen, 4, SIZE) + np.random.normal(genes,12, SIZE) + np.random.normal(mean,var + 10, SIZE)\n",
    "\n",
    "    cancer = np.zeros_like(density)\n",
    "    m = np.mean(density)\n",
    "\n",
    "    cancer[density > m] = np.random.binomial(n=1, p=0.08, size=len(density[density > m]))\n",
    "    cancer[density <= m] = np.random.binomial(n=1, p=0.01, size=len(density[density <= m]))\n",
    "    \n",
    "    return pd.DataFrame({'bmi' : bmi,'density' : density, 'age' : age, 'cancer' : cancer, 'estrogen': estrogen, 'genes':genes, 'insomnia': insomnia})\n",
    "\n",
    "def get_CG(df, tetrad):\n",
    "    tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "    #tetrad.run(algoId = 'fges-mb', targetName = 'g', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "    #       structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "    #       completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "\n",
    "\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from collections import defaultdict\n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '5000M')\n",
    "tetrad = s.tetradrunner()\n",
    "\n",
    "\n",
    "verbosity = 1\n",
    "\n",
    "\n",
    "\n",
    "models = []\n",
    "model_names = []\n",
    "\n",
    "num_models =100\n",
    "model_layers = [1024,512]\n",
    "for i in range(num_models):\n",
    "    models.append(model_layers)\n",
    "    model_names.append('temp/a' + str(i))\n",
    "\n",
    "print(models, model_names)\n",
    "\n",
    "from pycausal import prior as p\n",
    "def get_bic(df, prior):\n",
    "\n",
    "    tetrad.run(algoId = 'gfci', dfs = df,  scoreId = 'sem-bic-deterministic', dataType = 'continuous',\n",
    "               structurePrior = 1.0, samplePrior = 1, maxDegree = -1, maxPathLength = -1, priorKnowledge = prior,\n",
    "               completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True,\n",
    "               penaltyDiscount = 2\n",
    "               )\n",
    "    BIC = tetrad.getTetradGraph().getAllAttributes().toString()\n",
    "    BIC = float(BIC.split('=')[-1].split('}')[0])\n",
    "    return BIC #/ len(df)\n",
    "import itertools\n",
    "def get_pairs(lst):\n",
    "    a = set()\n",
    "    for i in itertools.permutations(lst,2):\n",
    "        a.add(i)\n",
    "    return a\n",
    "\n",
    "inputs = ['bmi', 'density', 'age', 'genes', 'insomnia', 'estrogen']\n",
    "target = ['cancer']\n",
    "full_conx = get_pairs(inputs + target)\n",
    "forced_conx = set({('age','genes'), ('bmi', 'estrogen'), ('estrogen', 'genes'),('estrogen', 'insomnia'), ('estrogen', 'density'), ('genes', 'density'), ('density', 'cancer')})\n",
    "restricted_conx = full_conx.difference(forced_conx)   \n",
    "\n",
    "prior = p.knowledge(requiredirect =  list(map(list, forced_conx)),\n",
    "                       forbiddirect = list(map(list, restricted_conx))\n",
    "                       )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = gen_data(SIZE = 200000)\n",
    "cancer_df = df[df['cancer'] == 1]\n",
    "\n",
    "ben_df = df[df['cancer'] == 0][:len(cancer_df)]\n",
    "\n",
    "df = cancer_df.append(ben_df, ignore_index=True)\n",
    "print(len(cancer_df), len(ben_df), len(df))\n",
    "\n",
    "\n",
    "X = df[inputs].values\n",
    "X = normalize(X)\n",
    "y = df[target].values\n",
    "y = to_categorical(y)\n",
    "\n",
    "val_df = gen_data(SIZE = 2000)\n",
    "\n",
    "x_val = val_df[inputs].values\n",
    "x_val = normalize(x_val)\n",
    "y_val = val_df[target].values\n",
    "y_val = to_categorical(y_val)\n",
    "\n",
    "get_bic(df,prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp/a0\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 3s 142us/step - loss: 0.6570 - acc: 0.6190 - val_loss: 0.5432 - val_acc: 0.7285\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.54322, saving model to temp/a0\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 1s 82us/step - loss: 0.6321 - acc: 0.6510 - val_loss: 0.5785 - val_acc: 0.6585\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.54322\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 1s 82us/step - loss: 0.6235 - acc: 0.6645 - val_loss: 0.6622 - val_acc: 0.5420\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54322\n",
      "Epoch 00003: early stopping\n",
      "temp/a1\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 93us/step - loss: 0.6573 - acc: 0.6188 - val_loss: 0.7007 - val_acc: 0.5670\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70069, saving model to temp/a1\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 83us/step - loss: 0.6345 - acc: 0.6543 - val_loss: 0.5126 - val_acc: 0.7355\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.70069 to 0.51265, saving model to temp/a1\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 85us/step - loss: 0.6275 - acc: 0.6590 - val_loss: 0.7125 - val_acc: 0.5135\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51265\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6222 - acc: 0.6672 - val_loss: 0.5556 - val_acc: 0.6455\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51265\n",
      "Epoch 00004: early stopping\n",
      "temp/a2\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 95us/step - loss: 0.6582 - acc: 0.6145 - val_loss: 0.6604 - val_acc: 0.6080\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66040, saving model to temp/a2\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 89us/step - loss: 0.6321 - acc: 0.6504 - val_loss: 0.7118 - val_acc: 0.5265\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.66040\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 92us/step - loss: 0.6245 - acc: 0.6652 - val_loss: 0.7783 - val_acc: 0.4765\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.66040\n",
      "Epoch 00003: early stopping\n",
      "temp/a3\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6561 - acc: 0.6225 - val_loss: 0.5715 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57149, saving model to temp/a3\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 86us/step - loss: 0.6309 - acc: 0.6541 - val_loss: 0.6103 - val_acc: 0.6205\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.57149\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 1s 83us/step - loss: 0.6253 - acc: 0.6624 - val_loss: 0.5420 - val_acc: 0.6655\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.57149 to 0.54201, saving model to temp/a3\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6222 - acc: 0.6695 - val_loss: 0.7483 - val_acc: 0.4470\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54201\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 1s 83us/step - loss: 0.6192 - acc: 0.6718 - val_loss: 0.5779 - val_acc: 0.5990\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54201\n",
      "Epoch 00005: early stopping\n",
      "temp/a4\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 95us/step - loss: 0.6604 - acc: 0.6114 - val_loss: 0.8985 - val_acc: 0.3675\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.89846, saving model to temp/a4\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6320 - acc: 0.6570 - val_loss: 0.4895 - val_acc: 0.7510\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.89846 to 0.48954, saving model to temp/a4\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 83us/step - loss: 0.6269 - acc: 0.6616 - val_loss: 0.6249 - val_acc: 0.5960\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.48954\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 83us/step - loss: 0.6209 - acc: 0.6708 - val_loss: 0.6271 - val_acc: 0.5985\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.48954\n",
      "Epoch 00004: early stopping\n",
      "temp/a5\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6571 - acc: 0.6190 - val_loss: 0.7340 - val_acc: 0.5165\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.73396, saving model to temp/a5\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 93us/step - loss: 0.6305 - acc: 0.6547 - val_loss: 0.4467 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.73396 to 0.44667, saving model to temp/a5\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 89us/step - loss: 0.6277 - acc: 0.6577 - val_loss: 0.6589 - val_acc: 0.5635\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.44667\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 1s 82us/step - loss: 0.6208 - acc: 0.6695 - val_loss: 0.7179 - val_acc: 0.4955\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.44667\n",
      "Epoch 00004: early stopping\n",
      "temp/a6\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 94us/step - loss: 0.6595 - acc: 0.6134 - val_loss: 0.7038 - val_acc: 0.5575\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70383, saving model to temp/a6\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 83us/step - loss: 0.6315 - acc: 0.6580 - val_loss: 0.5261 - val_acc: 0.7145\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.70383 to 0.52605, saving model to temp/a6\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 83us/step - loss: 0.6261 - acc: 0.6633 - val_loss: 0.6341 - val_acc: 0.5805\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.52605\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6227 - acc: 0.6661 - val_loss: 0.5097 - val_acc: 0.6945\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52605 to 0.50966, saving model to temp/a6\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 1s 82us/step - loss: 0.6183 - acc: 0.6755 - val_loss: 0.6085 - val_acc: 0.5890\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.50966\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 1s 83us/step - loss: 0.6162 - acc: 0.6777 - val_loss: 0.6031 - val_acc: 0.5755\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.50966\n",
      "Epoch 00006: early stopping\n",
      "temp/a7\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 101us/step - loss: 0.6600 - acc: 0.6087 - val_loss: 0.5031 - val_acc: 0.7730\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.50305, saving model to temp/a7\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 94us/step - loss: 0.6329 - acc: 0.6532 - val_loss: 0.7276 - val_acc: 0.5240\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.50305\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 91us/step - loss: 0.6258 - acc: 0.6590 - val_loss: 0.8223 - val_acc: 0.4375\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.50305\n",
      "Epoch 00003: early stopping\n",
      "temp/a8\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 112us/step - loss: 0.6588 - acc: 0.6202 - val_loss: 0.6524 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65244, saving model to temp/a8\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6345 - acc: 0.6468 - val_loss: 0.6736 - val_acc: 0.5815\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.65244\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 110us/step - loss: 0.6253 - acc: 0.6623 - val_loss: 0.5348 - val_acc: 0.6880\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.65244 to 0.53482, saving model to temp/a8\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6229 - acc: 0.6677 - val_loss: 0.4971 - val_acc: 0.6960\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.53482 to 0.49711, saving model to temp/a8\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6191 - acc: 0.6751 - val_loss: 0.6680 - val_acc: 0.5520\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.49711\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6164 - acc: 0.6776 - val_loss: 0.5680 - val_acc: 0.5905\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.49711\n",
      "Epoch 00006: early stopping\n",
      "temp/a9\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 113us/step - loss: 0.6598 - acc: 0.6092 - val_loss: 0.7090 - val_acc: 0.5660\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70904, saving model to temp/a9\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6338 - acc: 0.6509 - val_loss: 0.5290 - val_acc: 0.7180\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.70904 to 0.52904, saving model to temp/a9\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6280 - acc: 0.6595 - val_loss: 0.6480 - val_acc: 0.5650\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.52904\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6216 - acc: 0.6671 - val_loss: 0.6302 - val_acc: 0.5850\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.52904\n",
      "Epoch 00004: early stopping\n",
      "temp/a10\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 116us/step - loss: 0.6611 - acc: 0.6063 - val_loss: 0.6059 - val_acc: 0.6585\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.60594, saving model to temp/a10\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6323 - acc: 0.6537 - val_loss: 0.6596 - val_acc: 0.5860\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.60594\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6275 - acc: 0.6569 - val_loss: 0.7643 - val_acc: 0.4690\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.60594\n",
      "Epoch 00003: early stopping\n",
      "temp/a11\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 96us/step - loss: 0.6593 - acc: 0.6072 - val_loss: 0.5402 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.54022, saving model to temp/a11\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 86us/step - loss: 0.6334 - acc: 0.6502 - val_loss: 0.5818 - val_acc: 0.6530\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.54022\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 99us/step - loss: 0.6251 - acc: 0.6651 - val_loss: 0.4797 - val_acc: 0.7265\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54022 to 0.47969, saving model to temp/a11\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 101us/step - loss: 0.6228 - acc: 0.6667 - val_loss: 0.5632 - val_acc: 0.6340\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.47969\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6185 - acc: 0.6757 - val_loss: 0.5402 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.47969\n",
      "Epoch 00005: early stopping\n",
      "temp/a12\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 111us/step - loss: 0.6575 - acc: 0.6144 - val_loss: 0.4529 - val_acc: 0.8225\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.45293, saving model to temp/a12\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6336 - acc: 0.6515 - val_loss: 0.5140 - val_acc: 0.7195\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.45293\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6259 - acc: 0.6633 - val_loss: 0.5568 - val_acc: 0.6625\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.45293\n",
      "Epoch 00003: early stopping\n",
      "temp/a13\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 111us/step - loss: 0.6572 - acc: 0.6145 - val_loss: 0.7888 - val_acc: 0.4840\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.78875, saving model to temp/a13\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 98us/step - loss: 0.6308 - acc: 0.6560 - val_loss: 0.7909 - val_acc: 0.4700\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.78875\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6271 - acc: 0.6640 - val_loss: 0.5930 - val_acc: 0.6140\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.78875 to 0.59295, saving model to temp/a13\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6206 - acc: 0.6731 - val_loss: 0.6377 - val_acc: 0.5840\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.59295\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6195 - acc: 0.6740 - val_loss: 0.4683 - val_acc: 0.7180\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.59295 to 0.46831, saving model to temp/a13\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 99us/step - loss: 0.6163 - acc: 0.6769 - val_loss: 0.6270 - val_acc: 0.5400\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.46831\n",
      "Epoch 7/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6147 - acc: 0.6770 - val_loss: 0.5814 - val_acc: 0.5915\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.46831\n",
      "Epoch 00007: early stopping\n",
      "temp/a14\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 115us/step - loss: 0.6583 - acc: 0.6163 - val_loss: 0.7531 - val_acc: 0.5015\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.75312, saving model to temp/a14\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6309 - acc: 0.6569 - val_loss: 0.7819 - val_acc: 0.4725\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.75312\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6261 - acc: 0.6654 - val_loss: 0.7032 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.75312 to 0.70322, saving model to temp/a14\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6209 - acc: 0.6715 - val_loss: 0.7479 - val_acc: 0.4675\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.70322\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6177 - acc: 0.6764 - val_loss: 0.5843 - val_acc: 0.6140\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.70322 to 0.58425, saving model to temp/a14\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6168 - acc: 0.6742 - val_loss: 0.5131 - val_acc: 0.6695\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.58425 to 0.51310, saving model to temp/a14\n",
      "Epoch 7/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6125 - acc: 0.6799 - val_loss: 0.6716 - val_acc: 0.4910\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51310\n",
      "Epoch 8/20\n",
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6103 - acc: 0.6815 - val_loss: 0.5352 - val_acc: 0.6360\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51310\n",
      "Epoch 00008: early stopping\n",
      "temp/a15\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 117us/step - loss: 0.6572 - acc: 0.6177 - val_loss: 0.5516 - val_acc: 0.7240\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55158, saving model to temp/a15\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6350 - acc: 0.6483 - val_loss: 0.8148 - val_acc: 0.4365\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.55158\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6273 - acc: 0.6621 - val_loss: 0.6038 - val_acc: 0.6195\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.55158\n",
      "Epoch 00003: early stopping\n",
      "temp/a16\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 115us/step - loss: 0.6592 - acc: 0.6167 - val_loss: 0.5700 - val_acc: 0.7140\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56999, saving model to temp/a16\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 109us/step - loss: 0.6335 - acc: 0.6532 - val_loss: 0.5708 - val_acc: 0.6715\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.56999\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6271 - acc: 0.6579 - val_loss: 0.5806 - val_acc: 0.6500\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.56999\n",
      "Epoch 00003: early stopping\n",
      "temp/a17\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18084/18084 [==============================] - 2s 116us/step - loss: 0.6612 - acc: 0.6047 - val_loss: 0.7368 - val_acc: 0.5155\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.73676, saving model to temp/a17\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6350 - acc: 0.6506 - val_loss: 0.7672 - val_acc: 0.4645\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.73676\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 108us/step - loss: 0.6245 - acc: 0.6666 - val_loss: 0.6113 - val_acc: 0.6075\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.73676 to 0.61127, saving model to temp/a17\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 108us/step - loss: 0.6211 - acc: 0.6730 - val_loss: 0.6234 - val_acc: 0.5730\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.61127\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6175 - acc: 0.6737 - val_loss: 0.6599 - val_acc: 0.5235\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.61127\n",
      "Epoch 00005: early stopping\n",
      "temp/a18\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 115us/step - loss: 0.6599 - acc: 0.6143 - val_loss: 0.6514 - val_acc: 0.6180\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65141, saving model to temp/a18\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6343 - acc: 0.6509 - val_loss: 0.6103 - val_acc: 0.6440\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65141 to 0.61031, saving model to temp/a18\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6277 - acc: 0.6569 - val_loss: 0.6154 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.61031\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 108us/step - loss: 0.6214 - acc: 0.6722 - val_loss: 0.7914 - val_acc: 0.4160\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.61031\n",
      "Epoch 00004: early stopping\n",
      "temp/a19\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 115us/step - loss: 0.6604 - acc: 0.6123 - val_loss: 0.6761 - val_acc: 0.5815\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.67608, saving model to temp/a19\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6331 - acc: 0.6538 - val_loss: 0.6984 - val_acc: 0.5515\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.67608\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6275 - acc: 0.6636 - val_loss: 0.5882 - val_acc: 0.6265\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.67608 to 0.58825, saving model to temp/a19\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6214 - acc: 0.6706 - val_loss: 0.7419 - val_acc: 0.4775\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.58825\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6191 - acc: 0.6716 - val_loss: 0.5633 - val_acc: 0.6325\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.58825 to 0.56325, saving model to temp/a19\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6166 - acc: 0.6771 - val_loss: 0.6031 - val_acc: 0.5880\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.56325\n",
      "Epoch 7/20\n",
      "18084/18084 [==============================] - 2s 108us/step - loss: 0.6131 - acc: 0.6803 - val_loss: 0.6038 - val_acc: 0.5670\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.56325\n",
      "Epoch 00007: early stopping\n",
      "temp/a20\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 115us/step - loss: 0.6592 - acc: 0.6129 - val_loss: 0.6169 - val_acc: 0.6610\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61691, saving model to temp/a20\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6322 - acc: 0.6502 - val_loss: 0.5585 - val_acc: 0.6800\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.61691 to 0.55847, saving model to temp/a20\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6255 - acc: 0.6642 - val_loss: 0.5777 - val_acc: 0.6420\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.55847\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6212 - acc: 0.6700 - val_loss: 0.6022 - val_acc: 0.6285\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.55847\n",
      "Epoch 00004: early stopping\n",
      "temp/a21\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 116us/step - loss: 0.6619 - acc: 0.6074 - val_loss: 0.7810 - val_acc: 0.4695\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.78096, saving model to temp/a21\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6338 - acc: 0.6506 - val_loss: 0.4937 - val_acc: 0.7425\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.78096 to 0.49370, saving model to temp/a21\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6265 - acc: 0.6611 - val_loss: 0.5662 - val_acc: 0.6465\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.49370\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 108us/step - loss: 0.6205 - acc: 0.6695 - val_loss: 0.6351 - val_acc: 0.5520\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.49370\n",
      "Epoch 00004: early stopping\n",
      "temp/a22\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 117us/step - loss: 0.6612 - acc: 0.6082 - val_loss: 0.5468 - val_acc: 0.7330\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.54682, saving model to temp/a22\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6328 - acc: 0.6544 - val_loss: 0.7031 - val_acc: 0.5505\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.54682\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 108us/step - loss: 0.6269 - acc: 0.6625 - val_loss: 0.7433 - val_acc: 0.4945\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54682\n",
      "Epoch 00003: early stopping\n",
      "temp/a23\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 116us/step - loss: 0.6568 - acc: 0.6202 - val_loss: 0.5862 - val_acc: 0.6930\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.58616, saving model to temp/a23\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6337 - acc: 0.6479 - val_loss: 0.5784 - val_acc: 0.6590\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.58616 to 0.57841, saving model to temp/a23\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6252 - acc: 0.6634 - val_loss: 0.6419 - val_acc: 0.5825\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.57841\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6220 - acc: 0.6711 - val_loss: 0.5791 - val_acc: 0.6185\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.57841\n",
      "Epoch 00004: early stopping\n",
      "temp/a24\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 113us/step - loss: 0.6581 - acc: 0.6172 - val_loss: 0.4886 - val_acc: 0.7935\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48858, saving model to temp/a24\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6320 - acc: 0.6541 - val_loss: 0.6269 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.48858\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6295 - acc: 0.6587 - val_loss: 0.7559 - val_acc: 0.4810\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.48858\n",
      "Epoch 00003: early stopping\n",
      "temp/a25\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 117us/step - loss: 0.6594 - acc: 0.6123 - val_loss: 0.9366 - val_acc: 0.3340\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.93662, saving model to temp/a25\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6337 - acc: 0.6501 - val_loss: 0.6808 - val_acc: 0.5595\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.93662 to 0.68077, saving model to temp/a25\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6273 - acc: 0.6598 - val_loss: 0.6284 - val_acc: 0.5815\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.68077 to 0.62844, saving model to temp/a25\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6211 - acc: 0.6677 - val_loss: 0.6205 - val_acc: 0.5990\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62844 to 0.62050, saving model to temp/a25\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6168 - acc: 0.6758 - val_loss: 0.6845 - val_acc: 0.5050\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62050\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 108us/step - loss: 0.6157 - acc: 0.6814 - val_loss: 0.5509 - val_acc: 0.6095\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62050 to 0.55086, saving model to temp/a25\n",
      "Epoch 7/20\n",
      "18084/18084 [==============================] - 2s 108us/step - loss: 0.6134 - acc: 0.6767 - val_loss: 0.4807 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.55086 to 0.48074, saving model to temp/a25\n",
      "Epoch 8/20\n",
      "18084/18084 [==============================] - 2s 108us/step - loss: 0.6117 - acc: 0.6837 - val_loss: 0.6612 - val_acc: 0.5245\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48074\n",
      "Epoch 9/20\n",
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6112 - acc: 0.6804 - val_loss: 0.6131 - val_acc: 0.5635\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48074\n",
      "Epoch 00009: early stopping\n",
      "temp/a26\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 116us/step - loss: 0.6603 - acc: 0.6125 - val_loss: 0.7214 - val_acc: 0.5405\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.72138, saving model to temp/a26\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6342 - acc: 0.6511 - val_loss: 0.6281 - val_acc: 0.6190\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.72138 to 0.62808, saving model to temp/a26\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 108us/step - loss: 0.6258 - acc: 0.6614 - val_loss: 0.6443 - val_acc: 0.5675\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.62808\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 108us/step - loss: 0.6198 - acc: 0.6737 - val_loss: 0.7020 - val_acc: 0.5175\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62808\n",
      "Epoch 00004: early stopping\n",
      "temp/a27\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 117us/step - loss: 0.6593 - acc: 0.6187 - val_loss: 0.6038 - val_acc: 0.6670\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.60378, saving model to temp/a27\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 108us/step - loss: 0.6350 - acc: 0.6492 - val_loss: 0.5118 - val_acc: 0.7215\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.60378 to 0.51181, saving model to temp/a27\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6250 - acc: 0.6629 - val_loss: 0.6775 - val_acc: 0.5360\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51181\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6212 - acc: 0.6714 - val_loss: 0.5020 - val_acc: 0.6860\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.51181 to 0.50199, saving model to temp/a27\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 99us/step - loss: 0.6185 - acc: 0.6730 - val_loss: 0.5967 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.50199\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6160 - acc: 0.6760 - val_loss: 0.5703 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.50199\n",
      "Epoch 00006: early stopping\n",
      "temp/a28\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 113us/step - loss: 0.6577 - acc: 0.6170 - val_loss: 0.4914 - val_acc: 0.7955\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49139, saving model to temp/a28\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 95us/step - loss: 0.6342 - acc: 0.6514 - val_loss: 0.8030 - val_acc: 0.4435\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.49139\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6243 - acc: 0.6663 - val_loss: 0.8584 - val_acc: 0.3510\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.49139\n",
      "Epoch 00003: early stopping\n",
      "temp/a29\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 116us/step - loss: 0.6601 - acc: 0.6085 - val_loss: 0.9086 - val_acc: 0.3400\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.90858, saving model to temp/a29\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6324 - acc: 0.6498 - val_loss: 0.5988 - val_acc: 0.6510\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.90858 to 0.59876, saving model to temp/a29\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6261 - acc: 0.6655 - val_loss: 0.5302 - val_acc: 0.6940\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.59876 to 0.53016, saving model to temp/a29\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6221 - acc: 0.6680 - val_loss: 0.5830 - val_acc: 0.6155\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53016\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 97us/step - loss: 0.6180 - acc: 0.6768 - val_loss: 0.5366 - val_acc: 0.6440\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53016\n",
      "Epoch 00005: early stopping\n",
      "temp/a30\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 114us/step - loss: 0.6602 - acc: 0.6071 - val_loss: 0.6082 - val_acc: 0.6560\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.60817, saving model to temp/a30\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6327 - acc: 0.6508 - val_loss: 0.6700 - val_acc: 0.5785\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.60817\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6238 - acc: 0.6625 - val_loss: 0.7882 - val_acc: 0.4395\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.60817\n",
      "Epoch 00003: early stopping\n",
      "temp/a31\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 113us/step - loss: 0.6602 - acc: 0.6136 - val_loss: 0.6464 - val_acc: 0.6230\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64636, saving model to temp/a31\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6317 - acc: 0.6565 - val_loss: 0.5995 - val_acc: 0.6465\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64636 to 0.59948, saving model to temp/a31\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6267 - acc: 0.6601 - val_loss: 0.5821 - val_acc: 0.6335\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.59948 to 0.58212, saving model to temp/a31\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6226 - acc: 0.6687 - val_loss: 0.5893 - val_acc: 0.6190\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.58212\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6175 - acc: 0.6741 - val_loss: 0.5325 - val_acc: 0.6420\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.58212 to 0.53251, saving model to temp/a31\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6156 - acc: 0.6781 - val_loss: 0.6028 - val_acc: 0.5860\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53251\n",
      "Epoch 7/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6134 - acc: 0.6797 - val_loss: 0.6328 - val_acc: 0.5595\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.53251\n",
      "Epoch 00007: early stopping\n",
      "temp/a32\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 115us/step - loss: 0.6575 - acc: 0.6184 - val_loss: 0.6536 - val_acc: 0.6140\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65360, saving model to temp/a32\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6350 - acc: 0.6473 - val_loss: 0.5423 - val_acc: 0.7035\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65360 to 0.54235, saving model to temp/a32\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6261 - acc: 0.6635 - val_loss: 0.6147 - val_acc: 0.6010\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54235\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6223 - acc: 0.6663 - val_loss: 0.7302 - val_acc: 0.4635\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54235\n",
      "Epoch 00004: early stopping\n",
      "temp/a33\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 112us/step - loss: 0.6566 - acc: 0.6137 - val_loss: 0.8513 - val_acc: 0.4260\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.85128, saving model to temp/a33\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6342 - acc: 0.6497 - val_loss: 0.5654 - val_acc: 0.6805\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.85128 to 0.56545, saving model to temp/a33\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6251 - acc: 0.6638 - val_loss: 0.5950 - val_acc: 0.6145\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.56545\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6222 - acc: 0.6697 - val_loss: 0.6957 - val_acc: 0.5360\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.56545\n",
      "Epoch 00004: early stopping\n",
      "temp/a34\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 114us/step - loss: 0.6580 - acc: 0.6141 - val_loss: 0.4870 - val_acc: 0.7915\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48696, saving model to temp/a34\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6334 - acc: 0.6529 - val_loss: 0.6873 - val_acc: 0.5610\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.48696\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6260 - acc: 0.6625 - val_loss: 0.7325 - val_acc: 0.5240\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.48696\n",
      "Epoch 00003: early stopping\n",
      "temp/a35\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 113us/step - loss: 0.6604 - acc: 0.6042 - val_loss: 0.6911 - val_acc: 0.5615\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69111, saving model to temp/a35\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 101us/step - loss: 0.6323 - acc: 0.6505 - val_loss: 0.6956 - val_acc: 0.5700\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.69111\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6267 - acc: 0.6601 - val_loss: 0.5727 - val_acc: 0.6460\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.69111 to 0.57268, saving model to temp/a35\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6224 - acc: 0.6676 - val_loss: 0.6682 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.57268\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6176 - acc: 0.6750 - val_loss: 0.8273 - val_acc: 0.3840\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.57268\n",
      "Epoch 00005: early stopping\n",
      "temp/a36\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 115us/step - loss: 0.6602 - acc: 0.6100 - val_loss: 0.5583 - val_acc: 0.7250\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55833, saving model to temp/a36\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6328 - acc: 0.6502 - val_loss: 0.5684 - val_acc: 0.6710\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.55833\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 101us/step - loss: 0.6260 - acc: 0.6612 - val_loss: 0.5514 - val_acc: 0.6570\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55833 to 0.55139, saving model to temp/a36\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6206 - acc: 0.6700 - val_loss: 0.5391 - val_acc: 0.6740\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.55139 to 0.53914, saving model to temp/a36\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6190 - acc: 0.6726 - val_loss: 0.5997 - val_acc: 0.5860\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53914\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6163 - acc: 0.6773 - val_loss: 0.6762 - val_acc: 0.5135\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53914\n",
      "Epoch 00006: early stopping\n",
      "temp/a37\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 114us/step - loss: 0.6584 - acc: 0.6130 - val_loss: 0.6502 - val_acc: 0.6180\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65021, saving model to temp/a37\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6342 - acc: 0.6452 - val_loss: 0.5643 - val_acc: 0.6725\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65021 to 0.56430, saving model to temp/a37\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6272 - acc: 0.6595 - val_loss: 0.5945 - val_acc: 0.6235\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.56430\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 108us/step - loss: 0.6201 - acc: 0.6718 - val_loss: 0.6254 - val_acc: 0.5970\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.56430\n",
      "Epoch 00004: early stopping\n",
      "temp/a38\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 115us/step - loss: 0.6593 - acc: 0.6139 - val_loss: 0.8083 - val_acc: 0.4635\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.80826, saving model to temp/a38\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6349 - acc: 0.6479 - val_loss: 0.8319 - val_acc: 0.4120\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.80826\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6277 - acc: 0.6590 - val_loss: 0.6262 - val_acc: 0.6160\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.80826 to 0.62620, saving model to temp/a38\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6219 - acc: 0.6701 - val_loss: 0.6018 - val_acc: 0.5970\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62620 to 0.60179, saving model to temp/a38\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6186 - acc: 0.6735 - val_loss: 0.5908 - val_acc: 0.5915\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.60179 to 0.59083, saving model to temp/a38\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 99us/step - loss: 0.6161 - acc: 0.6752 - val_loss: 0.5931 - val_acc: 0.5855\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.59083\n",
      "Epoch 7/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6139 - acc: 0.6786 - val_loss: 0.6337 - val_acc: 0.5410\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.59083\n",
      "Epoch 00007: early stopping\n",
      "temp/a39\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 108us/step - loss: 0.6592 - acc: 0.6108 - val_loss: 0.5586 - val_acc: 0.7300\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55860, saving model to temp/a39\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6315 - acc: 0.6540 - val_loss: 0.6042 - val_acc: 0.6390\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.55860\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6252 - acc: 0.6620 - val_loss: 0.6921 - val_acc: 0.5565\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.55860\n",
      "Epoch 00003: early stopping\n",
      "temp/a40\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6612 - acc: 0.6083 - val_loss: 0.5189 - val_acc: 0.7645\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51892, saving model to temp/a40\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 110us/step - loss: 0.6333 - acc: 0.6475 - val_loss: 0.5289 - val_acc: 0.7195\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51892\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6272 - acc: 0.6639 - val_loss: 0.5050 - val_acc: 0.7050\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.51892 to 0.50503, saving model to temp/a40\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 99us/step - loss: 0.6229 - acc: 0.6659 - val_loss: 0.5860 - val_acc: 0.6215\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.50503\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6182 - acc: 0.6763 - val_loss: 0.5276 - val_acc: 0.6735\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.50503\n",
      "Epoch 00005: early stopping\n",
      "temp/a41\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 115us/step - loss: 0.6590 - acc: 0.6146 - val_loss: 0.6877 - val_acc: 0.5825\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68766, saving model to temp/a41\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 99us/step - loss: 0.6304 - acc: 0.6543 - val_loss: 0.7843 - val_acc: 0.4440\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.68766\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6238 - acc: 0.6667 - val_loss: 0.6493 - val_acc: 0.5785\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.68766 to 0.64930, saving model to temp/a41\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 100us/step - loss: 0.6218 - acc: 0.6689 - val_loss: 0.5581 - val_acc: 0.6490\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.64930 to 0.55805, saving model to temp/a41\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6184 - acc: 0.6761 - val_loss: 0.5713 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.55805\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6140 - acc: 0.6782 - val_loss: 0.5690 - val_acc: 0.6225\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.55805\n",
      "Epoch 00006: early stopping\n",
      "temp/a42\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 110us/step - loss: 0.6590 - acc: 0.6095 - val_loss: 0.6398 - val_acc: 0.6235\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63975, saving model to temp/a42\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 99us/step - loss: 0.6314 - acc: 0.6505 - val_loss: 0.6161 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63975 to 0.61610, saving model to temp/a42\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 101us/step - loss: 0.6251 - acc: 0.6643 - val_loss: 0.5941 - val_acc: 0.6140\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.61610 to 0.59412, saving model to temp/a42\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6217 - acc: 0.6698 - val_loss: 0.5303 - val_acc: 0.6665\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.59412 to 0.53026, saving model to temp/a42\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6177 - acc: 0.6752 - val_loss: 0.6006 - val_acc: 0.5845\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53026\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6154 - acc: 0.6793 - val_loss: 0.5329 - val_acc: 0.6470\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53026\n",
      "Epoch 00006: early stopping\n",
      "temp/a43\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 116us/step - loss: 0.6586 - acc: 0.6135 - val_loss: 0.7540 - val_acc: 0.5125\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.75398, saving model to temp/a43\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 101us/step - loss: 0.6319 - acc: 0.6551 - val_loss: 0.5946 - val_acc: 0.6465\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.75398 to 0.59460, saving model to temp/a43\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 92us/step - loss: 0.6270 - acc: 0.6615 - val_loss: 0.5141 - val_acc: 0.6945\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.59460 to 0.51410, saving model to temp/a43\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 86us/step - loss: 0.6211 - acc: 0.6723 - val_loss: 0.6464 - val_acc: 0.5740\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51410\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 95us/step - loss: 0.6166 - acc: 0.6766 - val_loss: 0.6311 - val_acc: 0.5820\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51410\n",
      "Epoch 00005: early stopping\n",
      "temp/a44\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 108us/step - loss: 0.6563 - acc: 0.6201 - val_loss: 0.8161 - val_acc: 0.4545\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.81613, saving model to temp/a44\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 100us/step - loss: 0.6344 - acc: 0.6486 - val_loss: 0.6782 - val_acc: 0.5610\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.81613 to 0.67823, saving model to temp/a44\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6268 - acc: 0.6593 - val_loss: 0.6771 - val_acc: 0.5310\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.67823 to 0.67712, saving model to temp/a44\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6203 - acc: 0.6707 - val_loss: 0.6815 - val_acc: 0.5240\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.67712\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6171 - acc: 0.6770 - val_loss: 0.6423 - val_acc: 0.5465\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.67712 to 0.64226, saving model to temp/a44\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6159 - acc: 0.6757 - val_loss: 0.5777 - val_acc: 0.6095\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.64226 to 0.57768, saving model to temp/a44\n",
      "Epoch 7/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6140 - acc: 0.6769 - val_loss: 0.6138 - val_acc: 0.5700\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.57768\n",
      "Epoch 8/20\n",
      "18084/18084 [==============================] - 2s 98us/step - loss: 0.6136 - acc: 0.6794 - val_loss: 0.6306 - val_acc: 0.5350\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.57768\n",
      "Epoch 00008: early stopping\n",
      "temp/a45\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 94us/step - loss: 0.6613 - acc: 0.6113 - val_loss: 0.6438 - val_acc: 0.6215\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64377, saving model to temp/a45\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 1s 81us/step - loss: 0.6313 - acc: 0.6552 - val_loss: 0.6931 - val_acc: 0.5640\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.64377\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 1s 82us/step - loss: 0.6248 - acc: 0.6653 - val_loss: 0.6338 - val_acc: 0.5920\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.64377 to 0.63384, saving model to temp/a45\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 1s 82us/step - loss: 0.6224 - acc: 0.6663 - val_loss: 0.5191 - val_acc: 0.6735\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.63384 to 0.51913, saving model to temp/a45\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 1s 82us/step - loss: 0.6168 - acc: 0.6740 - val_loss: 0.7032 - val_acc: 0.5105\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51913\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 1s 81us/step - loss: 0.6153 - acc: 0.6763 - val_loss: 0.6028 - val_acc: 0.5960\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51913\n",
      "Epoch 00006: early stopping\n",
      "temp/a46\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 111us/step - loss: 0.6617 - acc: 0.6064 - val_loss: 0.7494 - val_acc: 0.4985\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.74942, saving model to temp/a46\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 101us/step - loss: 0.6310 - acc: 0.6548 - val_loss: 0.6371 - val_acc: 0.5945\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.74942 to 0.63705, saving model to temp/a46\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6239 - acc: 0.6640 - val_loss: 0.6557 - val_acc: 0.5525\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.63705\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 100us/step - loss: 0.6202 - acc: 0.6702 - val_loss: 0.6216 - val_acc: 0.5825\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.63705 to 0.62156, saving model to temp/a46\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 100us/step - loss: 0.6170 - acc: 0.6766 - val_loss: 0.5904 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62156 to 0.59041, saving model to temp/a46\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6174 - acc: 0.6726 - val_loss: 0.6252 - val_acc: 0.5435\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.59041\n",
      "Epoch 7/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6129 - acc: 0.6819 - val_loss: 0.5618 - val_acc: 0.6070\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.59041 to 0.56177, saving model to temp/a46\n",
      "Epoch 8/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6122 - acc: 0.6789 - val_loss: 0.5524 - val_acc: 0.6115\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.56177 to 0.55236, saving model to temp/a46\n",
      "Epoch 9/20\n",
      "18084/18084 [==============================] - 2s 99us/step - loss: 0.6121 - acc: 0.6806 - val_loss: 0.6441 - val_acc: 0.5385\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.55236\n",
      "Epoch 10/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6097 - acc: 0.6822 - val_loss: 0.6047 - val_acc: 0.5920\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.55236\n",
      "Epoch 00010: early stopping\n",
      "temp/a47\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 112us/step - loss: 0.6602 - acc: 0.6046 - val_loss: 0.8404 - val_acc: 0.4415\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.84045, saving model to temp/a47\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6351 - acc: 0.6494 - val_loss: 0.5679 - val_acc: 0.6735\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.84045 to 0.56789, saving model to temp/a47\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 96us/step - loss: 0.6262 - acc: 0.6630 - val_loss: 0.6817 - val_acc: 0.5595\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.56789\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 83us/step - loss: 0.6218 - acc: 0.6702 - val_loss: 0.7056 - val_acc: 0.5230\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.56789\n",
      "Epoch 00004: early stopping\n",
      "temp/a48\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 92us/step - loss: 0.6580 - acc: 0.6166 - val_loss: 0.6430 - val_acc: 0.6235\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64298, saving model to temp/a48\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 83us/step - loss: 0.6329 - acc: 0.6548 - val_loss: 0.5177 - val_acc: 0.7235\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64298 to 0.51768, saving model to temp/a48\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 1s 83us/step - loss: 0.6249 - acc: 0.6626 - val_loss: 0.6242 - val_acc: 0.5925\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51768\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 1s 82us/step - loss: 0.6193 - acc: 0.6719 - val_loss: 0.7661 - val_acc: 0.3975\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51768\n",
      "Epoch 00004: early stopping\n",
      "temp/a49\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 95us/step - loss: 0.6596 - acc: 0.6174 - val_loss: 0.6682 - val_acc: 0.5940\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66819, saving model to temp/a49\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 1s 83us/step - loss: 0.6320 - acc: 0.6539 - val_loss: 0.5947 - val_acc: 0.6530\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.66819 to 0.59475, saving model to temp/a49\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6275 - acc: 0.6606 - val_loss: 0.6553 - val_acc: 0.5720\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.59475\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 1s 83us/step - loss: 0.6201 - acc: 0.6687 - val_loss: 0.5614 - val_acc: 0.6350\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.59475 to 0.56139, saving model to temp/a49\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 1s 83us/step - loss: 0.6188 - acc: 0.6709 - val_loss: 0.6590 - val_acc: 0.5275\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.56139\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 83us/step - loss: 0.6166 - acc: 0.6756 - val_loss: 0.5352 - val_acc: 0.6345\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.56139 to 0.53515, saving model to temp/a49\n",
      "Epoch 7/20\n",
      "18084/18084 [==============================] - 2s 83us/step - loss: 0.6146 - acc: 0.6812 - val_loss: 0.6030 - val_acc: 0.5685\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.53515\n",
      "Epoch 8/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6126 - acc: 0.6809 - val_loss: 0.5320 - val_acc: 0.6400\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.53515 to 0.53198, saving model to temp/a49\n",
      "Epoch 9/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6117 - acc: 0.6800 - val_loss: 0.6634 - val_acc: 0.5370\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.53198\n",
      "Epoch 10/20\n",
      "18084/18084 [==============================] - 2s 83us/step - loss: 0.6115 - acc: 0.6807 - val_loss: 0.6332 - val_acc: 0.5370\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.53198\n",
      "Epoch 00010: early stopping\n",
      "temp/a50\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 94us/step - loss: 0.6583 - acc: 0.6198 - val_loss: 0.5519 - val_acc: 0.7215\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55188, saving model to temp/a50\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 83us/step - loss: 0.6312 - acc: 0.6545 - val_loss: 0.5320 - val_acc: 0.7060\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55188 to 0.53198, saving model to temp/a50\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6268 - acc: 0.6610 - val_loss: 0.7583 - val_acc: 0.4450\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.53198\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 1s 83us/step - loss: 0.6232 - acc: 0.6646 - val_loss: 0.5922 - val_acc: 0.6010\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53198\n",
      "Epoch 00004: early stopping\n",
      "temp/a51\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 94us/step - loss: 0.6605 - acc: 0.6077 - val_loss: 0.4253 - val_acc: 0.8520\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.42531, saving model to temp/a51\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 1s 83us/step - loss: 0.6316 - acc: 0.6511 - val_loss: 0.6874 - val_acc: 0.5595\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.42531\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 1s 83us/step - loss: 0.6246 - acc: 0.6643 - val_loss: 0.5889 - val_acc: 0.6170\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.42531\n",
      "Epoch 00003: early stopping\n",
      "temp/a52\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 93us/step - loss: 0.6583 - acc: 0.6124 - val_loss: 0.5715 - val_acc: 0.7080\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57153, saving model to temp/a52\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 83us/step - loss: 0.6321 - acc: 0.6527 - val_loss: 0.6001 - val_acc: 0.6465\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.57153\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6279 - acc: 0.6604 - val_loss: 0.6456 - val_acc: 0.5705\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.57153\n",
      "Epoch 00003: early stopping\n",
      "temp/a53\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 95us/step - loss: 0.6591 - acc: 0.6118 - val_loss: 0.7003 - val_acc: 0.5605\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70026, saving model to temp/a53\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 83us/step - loss: 0.6324 - acc: 0.6564 - val_loss: 0.7130 - val_acc: 0.5250\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.70026\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 1s 82us/step - loss: 0.6264 - acc: 0.6626 - val_loss: 0.4844 - val_acc: 0.7260\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.70026 to 0.48445, saving model to temp/a53\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6218 - acc: 0.6704 - val_loss: 0.5899 - val_acc: 0.6085\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.48445\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6186 - acc: 0.6718 - val_loss: 0.5568 - val_acc: 0.6305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: val_loss did not improve from 0.48445\n",
      "Epoch 00005: early stopping\n",
      "temp/a54\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 113us/step - loss: 0.6604 - acc: 0.6130 - val_loss: 0.6854 - val_acc: 0.5645\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68542, saving model to temp/a54\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6345 - acc: 0.6523 - val_loss: 0.7420 - val_acc: 0.4870\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.68542\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6237 - acc: 0.6641 - val_loss: 0.5311 - val_acc: 0.6895\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.68542 to 0.53111, saving model to temp/a54\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6210 - acc: 0.6678 - val_loss: 0.6402 - val_acc: 0.5425\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53111\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6192 - acc: 0.6726 - val_loss: 0.5657 - val_acc: 0.6195\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53111\n",
      "Epoch 00005: early stopping\n",
      "temp/a55\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 114us/step - loss: 0.6583 - acc: 0.6155 - val_loss: 0.7323 - val_acc: 0.5290\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.73233, saving model to temp/a55\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6327 - acc: 0.6546 - val_loss: 0.7041 - val_acc: 0.5400\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.73233 to 0.70411, saving model to temp/a55\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6270 - acc: 0.6583 - val_loss: 0.6418 - val_acc: 0.5820\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.70411 to 0.64177, saving model to temp/a55\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6213 - acc: 0.6681 - val_loss: 0.5850 - val_acc: 0.6120\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.64177 to 0.58502, saving model to temp/a55\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6194 - acc: 0.6724 - val_loss: 0.5632 - val_acc: 0.6310\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.58502 to 0.56324, saving model to temp/a55\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 99us/step - loss: 0.6145 - acc: 0.6814 - val_loss: 0.5011 - val_acc: 0.6820\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.56324 to 0.50111, saving model to temp/a55\n",
      "Epoch 7/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6137 - acc: 0.6810 - val_loss: 0.5688 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.50111\n",
      "Epoch 8/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6109 - acc: 0.6815 - val_loss: 0.6477 - val_acc: 0.5375\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.50111\n",
      "Epoch 00008: early stopping\n",
      "temp/a56\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 115us/step - loss: 0.6598 - acc: 0.6105 - val_loss: 0.7118 - val_acc: 0.5485\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.71181, saving model to temp/a56\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6325 - acc: 0.6525 - val_loss: 0.6785 - val_acc: 0.5710\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.71181 to 0.67846, saving model to temp/a56\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6268 - acc: 0.6623 - val_loss: 0.6476 - val_acc: 0.5745\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.67846 to 0.64764, saving model to temp/a56\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 95us/step - loss: 0.6202 - acc: 0.6715 - val_loss: 0.6246 - val_acc: 0.5970\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.64764 to 0.62464, saving model to temp/a56\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 93us/step - loss: 0.6183 - acc: 0.6736 - val_loss: 0.6336 - val_acc: 0.5540\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62464\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6146 - acc: 0.6805 - val_loss: 0.5582 - val_acc: 0.6215\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62464 to 0.55821, saving model to temp/a56\n",
      "Epoch 7/20\n",
      "18084/18084 [==============================] - 2s 98us/step - loss: 0.6142 - acc: 0.6801 - val_loss: 0.6256 - val_acc: 0.5475\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.55821\n",
      "Epoch 8/20\n",
      "18084/18084 [==============================] - 2s 86us/step - loss: 0.6129 - acc: 0.6797 - val_loss: 0.6097 - val_acc: 0.5785\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.55821\n",
      "Epoch 00008: early stopping\n",
      "temp/a57\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6592 - acc: 0.6139 - val_loss: 0.6318 - val_acc: 0.6410\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63179, saving model to temp/a57\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 99us/step - loss: 0.6327 - acc: 0.6534 - val_loss: 0.5338 - val_acc: 0.7050\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63179 to 0.53377, saving model to temp/a57\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 96us/step - loss: 0.6252 - acc: 0.6626 - val_loss: 0.6560 - val_acc: 0.5570\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.53377\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6206 - acc: 0.6714 - val_loss: 0.6333 - val_acc: 0.5740\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53377\n",
      "Epoch 00004: early stopping\n",
      "temp/a58\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 110us/step - loss: 0.6598 - acc: 0.6068 - val_loss: 0.6146 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61460, saving model to temp/a58\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6330 - acc: 0.6498 - val_loss: 0.7202 - val_acc: 0.5175\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.61460\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 101us/step - loss: 0.6278 - acc: 0.6620 - val_loss: 0.7193 - val_acc: 0.5190\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.61460\n",
      "Epoch 00003: early stopping\n",
      "temp/a59\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 113us/step - loss: 0.6584 - acc: 0.6111 - val_loss: 0.7450 - val_acc: 0.5250\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.74499, saving model to temp/a59\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 101us/step - loss: 0.6327 - acc: 0.6521 - val_loss: 0.6608 - val_acc: 0.6020\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.74499 to 0.66080, saving model to temp/a59\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6254 - acc: 0.6641 - val_loss: 0.7168 - val_acc: 0.5150\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.66080\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6240 - acc: 0.6662 - val_loss: 0.5907 - val_acc: 0.6255\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.66080 to 0.59074, saving model to temp/a59\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6185 - acc: 0.6735 - val_loss: 0.6687 - val_acc: 0.5490\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.59074\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 100us/step - loss: 0.6159 - acc: 0.6767 - val_loss: 0.5321 - val_acc: 0.6475\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.59074 to 0.53209, saving model to temp/a59\n",
      "Epoch 7/20\n",
      "18084/18084 [==============================] - 2s 100us/step - loss: 0.6177 - acc: 0.6742 - val_loss: 0.5597 - val_acc: 0.6210\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.53209\n",
      "Epoch 8/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6117 - acc: 0.6813 - val_loss: 0.6400 - val_acc: 0.5590\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.53209\n",
      "Epoch 00008: early stopping\n",
      "temp/a60\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 115us/step - loss: 0.6590 - acc: 0.6185 - val_loss: 0.5317 - val_acc: 0.7375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53170, saving model to temp/a60\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 99us/step - loss: 0.6341 - acc: 0.6510 - val_loss: 0.5750 - val_acc: 0.6655\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.53170\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6269 - acc: 0.6626 - val_loss: 0.6061 - val_acc: 0.6130\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.53170\n",
      "Epoch 00003: early stopping\n",
      "temp/a61\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 114us/step - loss: 0.6590 - acc: 0.6105 - val_loss: 0.6697 - val_acc: 0.5980\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66974, saving model to temp/a61\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6348 - acc: 0.6497 - val_loss: 0.6250 - val_acc: 0.6195\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.66974 to 0.62501, saving model to temp/a61\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6274 - acc: 0.6605 - val_loss: 0.5467 - val_acc: 0.6730\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62501 to 0.54668, saving model to temp/a61\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6224 - acc: 0.6685 - val_loss: 0.6800 - val_acc: 0.5475\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54668\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6189 - acc: 0.6752 - val_loss: 0.6492 - val_acc: 0.5560\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54668\n",
      "Epoch 00005: early stopping\n",
      "temp/a62\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 113us/step - loss: 0.6583 - acc: 0.6085 - val_loss: 0.6528 - val_acc: 0.6050\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65285, saving model to temp/a62\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 101us/step - loss: 0.6308 - acc: 0.6530 - val_loss: 0.7647 - val_acc: 0.4910\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.65285\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6255 - acc: 0.6627 - val_loss: 0.5666 - val_acc: 0.6430\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.65285 to 0.56658, saving model to temp/a62\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6229 - acc: 0.6680 - val_loss: 0.5528 - val_acc: 0.6410\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.56658 to 0.55282, saving model to temp/a62\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6175 - acc: 0.6768 - val_loss: 0.5551 - val_acc: 0.6295\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.55282\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6160 - acc: 0.6763 - val_loss: 0.5888 - val_acc: 0.5865\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.55282\n",
      "Epoch 00006: early stopping\n",
      "temp/a63\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 116us/step - loss: 0.6586 - acc: 0.6147 - val_loss: 0.6388 - val_acc: 0.6360\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63881, saving model to temp/a63\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6319 - acc: 0.6527 - val_loss: 0.6066 - val_acc: 0.6320\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63881 to 0.60657, saving model to temp/a63\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6264 - acc: 0.6636 - val_loss: 0.6404 - val_acc: 0.5715\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.60657\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 98us/step - loss: 0.6198 - acc: 0.6730 - val_loss: 0.4906 - val_acc: 0.7005\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.60657 to 0.49058, saving model to temp/a63\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 99us/step - loss: 0.6206 - acc: 0.6702 - val_loss: 0.5616 - val_acc: 0.6120\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.49058\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6158 - acc: 0.6782 - val_loss: 0.5389 - val_acc: 0.6440\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.49058\n",
      "Epoch 00006: early stopping\n",
      "temp/a64\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 109us/step - loss: 0.6563 - acc: 0.6198 - val_loss: 0.8173 - val_acc: 0.4615\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.81732, saving model to temp/a64\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 93us/step - loss: 0.6333 - acc: 0.6478 - val_loss: 0.5649 - val_acc: 0.6735\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.81732 to 0.56491, saving model to temp/a64\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 86us/step - loss: 0.6264 - acc: 0.6619 - val_loss: 0.7304 - val_acc: 0.5220\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.56491\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 85us/step - loss: 0.6236 - acc: 0.6653 - val_loss: 0.5969 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.56491\n",
      "Epoch 00004: early stopping\n",
      "temp/a65\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 97us/step - loss: 0.6576 - acc: 0.6212 - val_loss: 0.6386 - val_acc: 0.6330\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63859, saving model to temp/a65\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 96us/step - loss: 0.6314 - acc: 0.6595 - val_loss: 0.6421 - val_acc: 0.6040\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.63859\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 94us/step - loss: 0.6258 - acc: 0.6639 - val_loss: 0.6125 - val_acc: 0.6150\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63859 to 0.61246, saving model to temp/a65\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 83us/step - loss: 0.6198 - acc: 0.6715 - val_loss: 0.6136 - val_acc: 0.6015\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.61246\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6199 - acc: 0.6710 - val_loss: 0.7197 - val_acc: 0.4735\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.61246\n",
      "Epoch 00005: early stopping\n",
      "temp/a66\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 95us/step - loss: 0.6582 - acc: 0.6194 - val_loss: 0.5844 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.58443, saving model to temp/a66\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 83us/step - loss: 0.6334 - acc: 0.6515 - val_loss: 0.7037 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.58443\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6250 - acc: 0.6632 - val_loss: 0.5936 - val_acc: 0.6305\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.58443\n",
      "Epoch 00003: early stopping\n",
      "temp/a67\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 94us/step - loss: 0.6581 - acc: 0.6177 - val_loss: 0.7094 - val_acc: 0.5560\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70940, saving model to temp/a67\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6327 - acc: 0.6518 - val_loss: 0.5445 - val_acc: 0.6925\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.70940 to 0.54449, saving model to temp/a67\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 1s 82us/step - loss: 0.6252 - acc: 0.6618 - val_loss: 0.5438 - val_acc: 0.6640\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54449 to 0.54383, saving model to temp/a67\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 83us/step - loss: 0.6204 - acc: 0.6715 - val_loss: 0.6074 - val_acc: 0.5950\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54383\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6191 - acc: 0.6741 - val_loss: 0.5728 - val_acc: 0.6070\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54383\n",
      "Epoch 00005: early stopping\n",
      "temp/a68\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 94us/step - loss: 0.6580 - acc: 0.6111 - val_loss: 0.6852 - val_acc: 0.5835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68524, saving model to temp/a68\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6320 - acc: 0.6536 - val_loss: 0.5910 - val_acc: 0.6605\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.68524 to 0.59099, saving model to temp/a68\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 83us/step - loss: 0.6259 - acc: 0.6612 - val_loss: 0.6307 - val_acc: 0.5980\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.59099\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 85us/step - loss: 0.6212 - acc: 0.6708 - val_loss: 0.6269 - val_acc: 0.5865\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.59099\n",
      "Epoch 00004: early stopping\n",
      "temp/a69\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 95us/step - loss: 0.6571 - acc: 0.6224 - val_loss: 0.7213 - val_acc: 0.5445\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.72130, saving model to temp/a69\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 83us/step - loss: 0.6339 - acc: 0.6502 - val_loss: 0.4404 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.72130 to 0.44042, saving model to temp/a69\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6264 - acc: 0.6605 - val_loss: 0.6028 - val_acc: 0.6225\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.44042\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6221 - acc: 0.6689 - val_loss: 0.5855 - val_acc: 0.6120\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.44042\n",
      "Epoch 00004: early stopping\n",
      "temp/a70\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 94us/step - loss: 0.6609 - acc: 0.6105 - val_loss: 0.7314 - val_acc: 0.5330\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.73138, saving model to temp/a70\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6351 - acc: 0.6492 - val_loss: 0.7179 - val_acc: 0.5460\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.73138 to 0.71788, saving model to temp/a70\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 83us/step - loss: 0.6268 - acc: 0.6612 - val_loss: 0.6348 - val_acc: 0.5945\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.71788 to 0.63483, saving model to temp/a70\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 1s 82us/step - loss: 0.6216 - acc: 0.6686 - val_loss: 0.5822 - val_acc: 0.6235\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.63483 to 0.58220, saving model to temp/a70\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 92us/step - loss: 0.6177 - acc: 0.6758 - val_loss: 0.6212 - val_acc: 0.5540\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.58220\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 98us/step - loss: 0.6157 - acc: 0.6778 - val_loss: 0.5833 - val_acc: 0.6025\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.58220\n",
      "Epoch 00006: early stopping\n",
      "temp/a71\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 114us/step - loss: 0.6577 - acc: 0.6188 - val_loss: 0.7852 - val_acc: 0.4790\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.78525, saving model to temp/a71\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6324 - acc: 0.6541 - val_loss: 0.4829 - val_acc: 0.7595\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.78525 to 0.48287, saving model to temp/a71\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6269 - acc: 0.6616 - val_loss: 0.6366 - val_acc: 0.5920\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.48287\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6229 - acc: 0.6666 - val_loss: 0.5336 - val_acc: 0.6565\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.48287\n",
      "Epoch 00004: early stopping\n",
      "temp/a72\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6554 - acc: 0.6225 - val_loss: 0.5655 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56546, saving model to temp/a72\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 86us/step - loss: 0.6315 - acc: 0.6578 - val_loss: 0.6169 - val_acc: 0.6245\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.56546\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 90us/step - loss: 0.6267 - acc: 0.6609 - val_loss: 0.7180 - val_acc: 0.5300\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.56546\n",
      "Epoch 00003: early stopping\n",
      "temp/a73\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 94us/step - loss: 0.6604 - acc: 0.6091 - val_loss: 0.6974 - val_acc: 0.5650\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69741, saving model to temp/a73\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 83us/step - loss: 0.6329 - acc: 0.6538 - val_loss: 0.5622 - val_acc: 0.6780\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.69741 to 0.56217, saving model to temp/a73\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6255 - acc: 0.6624 - val_loss: 0.5918 - val_acc: 0.6295\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.56217\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 1s 82us/step - loss: 0.6206 - acc: 0.6708 - val_loss: 0.6767 - val_acc: 0.5335\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.56217\n",
      "Epoch 00004: early stopping\n",
      "temp/a74\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 94us/step - loss: 0.6566 - acc: 0.6137 - val_loss: 0.7670 - val_acc: 0.4985\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.76699, saving model to temp/a74\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 83us/step - loss: 0.6316 - acc: 0.6528 - val_loss: 0.6055 - val_acc: 0.6380\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.76699 to 0.60548, saving model to temp/a74\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 1s 82us/step - loss: 0.6263 - acc: 0.6650 - val_loss: 0.6558 - val_acc: 0.5670\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.60548\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 100us/step - loss: 0.6210 - acc: 0.6712 - val_loss: 0.5425 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.60548 to 0.54249, saving model to temp/a74\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 100us/step - loss: 0.6178 - acc: 0.6752 - val_loss: 0.6192 - val_acc: 0.5825\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54249\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6150 - acc: 0.6791 - val_loss: 0.6732 - val_acc: 0.5350\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.54249\n",
      "Epoch 00006: early stopping\n",
      "temp/a75\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 112us/step - loss: 0.6576 - acc: 0.6134 - val_loss: 0.6587 - val_acc: 0.6105\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65875, saving model to temp/a75\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6343 - acc: 0.6510 - val_loss: 0.7321 - val_acc: 0.5235\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.65875\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 99us/step - loss: 0.6242 - acc: 0.6651 - val_loss: 0.6655 - val_acc: 0.5610\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.65875\n",
      "Epoch 00003: early stopping\n",
      "temp/a76\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 115us/step - loss: 0.6566 - acc: 0.6233 - val_loss: 0.6684 - val_acc: 0.6045\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66843, saving model to temp/a76\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 93us/step - loss: 0.6339 - acc: 0.6488 - val_loss: 0.7366 - val_acc: 0.5400\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.66843\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 99us/step - loss: 0.6273 - acc: 0.6580 - val_loss: 0.5959 - val_acc: 0.6195\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.66843 to 0.59586, saving model to temp/a76\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6210 - acc: 0.6728 - val_loss: 0.5324 - val_acc: 0.6785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss improved from 0.59586 to 0.53237, saving model to temp/a76\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6187 - acc: 0.6702 - val_loss: 0.5471 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53237\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6161 - acc: 0.6757 - val_loss: 0.6254 - val_acc: 0.5530\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53237\n",
      "Epoch 00006: early stopping\n",
      "temp/a77\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 109us/step - loss: 0.6604 - acc: 0.6092 - val_loss: 0.6505 - val_acc: 0.6155\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65049, saving model to temp/a77\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 92us/step - loss: 0.6322 - acc: 0.6530 - val_loss: 0.7626 - val_acc: 0.4670\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.65049\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6253 - acc: 0.6635 - val_loss: 0.5158 - val_acc: 0.6995\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.65049 to 0.51580, saving model to temp/a77\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 108us/step - loss: 0.6225 - acc: 0.6652 - val_loss: 0.5403 - val_acc: 0.6610\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51580\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6180 - acc: 0.6728 - val_loss: 0.6578 - val_acc: 0.5350\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51580\n",
      "Epoch 00005: early stopping\n",
      "temp/a78\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 115us/step - loss: 0.6593 - acc: 0.6102 - val_loss: 0.7089 - val_acc: 0.5540\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70888, saving model to temp/a78\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6312 - acc: 0.6539 - val_loss: 0.6050 - val_acc: 0.6460\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.70888 to 0.60499, saving model to temp/a78\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6280 - acc: 0.6588 - val_loss: 0.6359 - val_acc: 0.5850\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.60499\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 100us/step - loss: 0.6213 - acc: 0.6695 - val_loss: 0.5605 - val_acc: 0.6295\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.60499 to 0.56055, saving model to temp/a78\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6179 - acc: 0.6746 - val_loss: 0.5713 - val_acc: 0.6070\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.56055\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6153 - acc: 0.6804 - val_loss: 0.6295 - val_acc: 0.5605\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.56055\n",
      "Epoch 00006: early stopping\n",
      "temp/a79\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 114us/step - loss: 0.6622 - acc: 0.6058 - val_loss: 0.6073 - val_acc: 0.6645\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.60729, saving model to temp/a79\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6315 - acc: 0.6549 - val_loss: 0.6625 - val_acc: 0.5850\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.60729\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 108us/step - loss: 0.6247 - acc: 0.6646 - val_loss: 0.6453 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.60729\n",
      "Epoch 00003: early stopping\n",
      "temp/a80\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 111us/step - loss: 0.6596 - acc: 0.6111 - val_loss: 0.7075 - val_acc: 0.5550\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70751, saving model to temp/a80\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 101us/step - loss: 0.6327 - acc: 0.6572 - val_loss: 0.6100 - val_acc: 0.6375\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.70751 to 0.60997, saving model to temp/a80\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6259 - acc: 0.6643 - val_loss: 0.5921 - val_acc: 0.6270\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.60997 to 0.59214, saving model to temp/a80\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6233 - acc: 0.6669 - val_loss: 0.5262 - val_acc: 0.6690\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.59214 to 0.52617, saving model to temp/a80\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6180 - acc: 0.6732 - val_loss: 0.6372 - val_acc: 0.5725\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.52617\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 100us/step - loss: 0.6165 - acc: 0.6761 - val_loss: 0.5679 - val_acc: 0.6145\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.52617\n",
      "Epoch 00006: early stopping\n",
      "temp/a81\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 113us/step - loss: 0.6591 - acc: 0.6165 - val_loss: 0.8502 - val_acc: 0.4145\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.85020, saving model to temp/a81\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 101us/step - loss: 0.6337 - acc: 0.6521 - val_loss: 0.7490 - val_acc: 0.4905\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.85020 to 0.74899, saving model to temp/a81\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6266 - acc: 0.6621 - val_loss: 0.5545 - val_acc: 0.6565\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.74899 to 0.55452, saving model to temp/a81\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6209 - acc: 0.6715 - val_loss: 0.5793 - val_acc: 0.6230\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.55452\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6192 - acc: 0.6724 - val_loss: 0.6525 - val_acc: 0.5460\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.55452\n",
      "Epoch 00005: early stopping\n",
      "temp/a82\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 116us/step - loss: 0.6585 - acc: 0.6158 - val_loss: 0.7300 - val_acc: 0.5410\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.73002, saving model to temp/a82\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6324 - acc: 0.6524 - val_loss: 0.6443 - val_acc: 0.6130\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.73002 to 0.64427, saving model to temp/a82\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 96us/step - loss: 0.6288 - acc: 0.6572 - val_loss: 0.7883 - val_acc: 0.4080\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.64427\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 98us/step - loss: 0.6221 - acc: 0.6674 - val_loss: 0.5963 - val_acc: 0.5970\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.64427 to 0.59633, saving model to temp/a82\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 100us/step - loss: 0.6180 - acc: 0.6752 - val_loss: 0.5854 - val_acc: 0.6015\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.59633 to 0.58543, saving model to temp/a82\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 101us/step - loss: 0.6161 - acc: 0.6753 - val_loss: 0.5538 - val_acc: 0.6240\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.58543 to 0.55382, saving model to temp/a82\n",
      "Epoch 7/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6129 - acc: 0.6814 - val_loss: 0.6133 - val_acc: 0.5660\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.55382\n",
      "Epoch 8/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6117 - acc: 0.6826 - val_loss: 0.5633 - val_acc: 0.6095\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.55382\n",
      "Epoch 00008: early stopping\n",
      "temp/a83\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6615 - acc: 0.6096 - val_loss: 0.5984 - val_acc: 0.6740\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.59844, saving model to temp/a83\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 97us/step - loss: 0.6327 - acc: 0.6530 - val_loss: 0.6621 - val_acc: 0.5845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_loss did not improve from 0.59844\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 97us/step - loss: 0.6273 - acc: 0.6578 - val_loss: 0.6106 - val_acc: 0.6005\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.59844\n",
      "Epoch 00003: early stopping\n",
      "temp/a84\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 95us/step - loss: 0.6594 - acc: 0.6147 - val_loss: 0.6700 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66999, saving model to temp/a84\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 86us/step - loss: 0.6334 - acc: 0.6508 - val_loss: 0.7932 - val_acc: 0.4590\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.66999\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 100us/step - loss: 0.6264 - acc: 0.6600 - val_loss: 0.7759 - val_acc: 0.4310\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.66999\n",
      "Epoch 00003: early stopping\n",
      "temp/a85\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 114us/step - loss: 0.6600 - acc: 0.6107 - val_loss: 0.6565 - val_acc: 0.6180\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65646, saving model to temp/a85\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6334 - acc: 0.6509 - val_loss: 0.7080 - val_acc: 0.5500\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.65646\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 99us/step - loss: 0.6277 - acc: 0.6554 - val_loss: 0.5999 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.65646 to 0.59986, saving model to temp/a85\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6229 - acc: 0.6687 - val_loss: 0.7208 - val_acc: 0.4990\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.59986\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 108us/step - loss: 0.6178 - acc: 0.6761 - val_loss: 0.6143 - val_acc: 0.5785\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.59986\n",
      "Epoch 00005: early stopping\n",
      "temp/a86\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 111us/step - loss: 0.6574 - acc: 0.6122 - val_loss: 0.6785 - val_acc: 0.5890\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.67851, saving model to temp/a86\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 101us/step - loss: 0.6326 - acc: 0.6518 - val_loss: 0.7565 - val_acc: 0.5135\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.67851\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6279 - acc: 0.6567 - val_loss: 0.5587 - val_acc: 0.6665\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.67851 to 0.55874, saving model to temp/a86\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 100us/step - loss: 0.6214 - acc: 0.6696 - val_loss: 0.6673 - val_acc: 0.5370\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.55874\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6175 - acc: 0.6758 - val_loss: 0.6413 - val_acc: 0.5680\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.55874\n",
      "Epoch 00005: early stopping\n",
      "temp/a87\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 110us/step - loss: 0.6590 - acc: 0.6119 - val_loss: 0.6820 - val_acc: 0.5800\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68202, saving model to temp/a87\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6333 - acc: 0.6522 - val_loss: 0.5756 - val_acc: 0.6675\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.68202 to 0.57556, saving model to temp/a87\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6251 - acc: 0.6642 - val_loss: 0.6641 - val_acc: 0.5670\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.57556\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6200 - acc: 0.6719 - val_loss: 0.6754 - val_acc: 0.5465\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.57556\n",
      "Epoch 00004: early stopping\n",
      "temp/a88\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 113us/step - loss: 0.6581 - acc: 0.6146 - val_loss: 0.6707 - val_acc: 0.5985\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.67069, saving model to temp/a88\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 101us/step - loss: 0.6333 - acc: 0.6511 - val_loss: 0.6170 - val_acc: 0.6180\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.67069 to 0.61700, saving model to temp/a88\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6255 - acc: 0.6652 - val_loss: 0.6317 - val_acc: 0.5870\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.61700\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 99us/step - loss: 0.6216 - acc: 0.6692 - val_loss: 0.6114 - val_acc: 0.5965\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.61700 to 0.61139, saving model to temp/a88\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6181 - acc: 0.6734 - val_loss: 0.5740 - val_acc: 0.6160\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.61139 to 0.57396, saving model to temp/a88\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6160 - acc: 0.6771 - val_loss: 0.5811 - val_acc: 0.5785\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.57396\n",
      "Epoch 7/20\n",
      "18084/18084 [==============================] - 2s 95us/step - loss: 0.6135 - acc: 0.6817 - val_loss: 0.6181 - val_acc: 0.5655\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.57396\n",
      "Epoch 00007: early stopping\n",
      "temp/a89\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 101us/step - loss: 0.6597 - acc: 0.6103 - val_loss: 0.6841 - val_acc: 0.5765\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68407, saving model to temp/a89\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 83us/step - loss: 0.6348 - acc: 0.6531 - val_loss: 0.7233 - val_acc: 0.5305\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.68407\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6252 - acc: 0.6631 - val_loss: 0.6556 - val_acc: 0.5470\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.68407 to 0.65558, saving model to temp/a89\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6205 - acc: 0.6701 - val_loss: 0.7071 - val_acc: 0.4970\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.65558\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 88us/step - loss: 0.6180 - acc: 0.6729 - val_loss: 0.6768 - val_acc: 0.5100\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.65558\n",
      "Epoch 00005: early stopping\n",
      "temp/a90\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 116us/step - loss: 0.6620 - acc: 0.6073 - val_loss: 0.6930 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69299, saving model to temp/a90\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6335 - acc: 0.6522 - val_loss: 0.5645 - val_acc: 0.6880\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.69299 to 0.56452, saving model to temp/a90\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6274 - acc: 0.6599 - val_loss: 0.4660 - val_acc: 0.7400\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.56452 to 0.46598, saving model to temp/a90\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6220 - acc: 0.6695 - val_loss: 0.6263 - val_acc: 0.5750\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.46598\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 101us/step - loss: 0.6188 - acc: 0.6732 - val_loss: 0.6178 - val_acc: 0.5790\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.46598\n",
      "Epoch 00005: early stopping\n",
      "temp/a91\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 110us/step - loss: 0.6598 - acc: 0.6094 - val_loss: 0.6933 - val_acc: 0.5845\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69332, saving model to temp/a91\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6342 - acc: 0.6491 - val_loss: 0.6933 - val_acc: 0.5465\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.69332 to 0.69328, saving model to temp/a91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6263 - acc: 0.6611 - val_loss: 0.6023 - val_acc: 0.6075\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.69328 to 0.60230, saving model to temp/a91\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 100us/step - loss: 0.6217 - acc: 0.6672 - val_loss: 0.5893 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.60230 to 0.58932, saving model to temp/a91\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6181 - acc: 0.6765 - val_loss: 0.5878 - val_acc: 0.6140\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.58932 to 0.58776, saving model to temp/a91\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6142 - acc: 0.6797 - val_loss: 0.5792 - val_acc: 0.6110\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.58776 to 0.57918, saving model to temp/a91\n",
      "Epoch 7/20\n",
      "18084/18084 [==============================] - 2s 98us/step - loss: 0.6121 - acc: 0.6823 - val_loss: 0.6535 - val_acc: 0.5370\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.57918\n",
      "Epoch 8/20\n",
      "18084/18084 [==============================] - 2s 101us/step - loss: 0.6137 - acc: 0.6784 - val_loss: 0.5678 - val_acc: 0.6110\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.57918 to 0.56784, saving model to temp/a91\n",
      "Epoch 9/20\n",
      "18084/18084 [==============================] - 2s 85us/step - loss: 0.6109 - acc: 0.6812 - val_loss: 0.5715 - val_acc: 0.5980\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.56784\n",
      "Epoch 10/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6093 - acc: 0.6847 - val_loss: 0.6190 - val_acc: 0.5440\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.56784\n",
      "Epoch 00010: early stopping\n",
      "temp/a92\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 94us/step - loss: 0.6610 - acc: 0.6176 - val_loss: 0.6129 - val_acc: 0.6620\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61289, saving model to temp/a92\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6337 - acc: 0.6520 - val_loss: 0.5972 - val_acc: 0.6425\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.61289 to 0.59722, saving model to temp/a92\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6256 - acc: 0.6628 - val_loss: 0.6817 - val_acc: 0.5715\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.59722\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6221 - acc: 0.6714 - val_loss: 0.6939 - val_acc: 0.5390\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.59722\n",
      "Epoch 00004: early stopping\n",
      "temp/a93\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 92us/step - loss: 0.6591 - acc: 0.6113 - val_loss: 0.7761 - val_acc: 0.4955\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.77614, saving model to temp/a93\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 83us/step - loss: 0.6332 - acc: 0.6501 - val_loss: 0.7496 - val_acc: 0.5155\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.77614 to 0.74959, saving model to temp/a93\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 84us/step - loss: 0.6267 - acc: 0.6608 - val_loss: 0.8503 - val_acc: 0.3995\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.74959\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 1s 82us/step - loss: 0.6234 - acc: 0.6687 - val_loss: 0.6336 - val_acc: 0.5810\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.74959 to 0.63363, saving model to temp/a93\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 1s 83us/step - loss: 0.6187 - acc: 0.6756 - val_loss: 0.6274 - val_acc: 0.5720\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.63363 to 0.62741, saving model to temp/a93\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6164 - acc: 0.6753 - val_loss: 0.6472 - val_acc: 0.5380\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.62741\n",
      "Epoch 7/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6136 - acc: 0.6793 - val_loss: 0.6611 - val_acc: 0.5290\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.62741\n",
      "Epoch 00007: early stopping\n",
      "temp/a94\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 115us/step - loss: 0.6589 - acc: 0.6184 - val_loss: 0.5151 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51507, saving model to temp/a94\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6333 - acc: 0.6509 - val_loss: 0.5861 - val_acc: 0.6485\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51507\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 106us/step - loss: 0.6250 - acc: 0.6624 - val_loss: 0.6200 - val_acc: 0.6035\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51507\n",
      "Epoch 00003: early stopping\n",
      "temp/a95\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 114us/step - loss: 0.6600 - acc: 0.6099 - val_loss: 0.7566 - val_acc: 0.4910\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.75663, saving model to temp/a95\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6324 - acc: 0.6502 - val_loss: 0.6115 - val_acc: 0.6245\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.75663 to 0.61146, saving model to temp/a95\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6250 - acc: 0.6622 - val_loss: 0.5597 - val_acc: 0.6545\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.61146 to 0.55971, saving model to temp/a95\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6221 - acc: 0.6706 - val_loss: 0.5600 - val_acc: 0.6555\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.55971\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6177 - acc: 0.6755 - val_loss: 0.6992 - val_acc: 0.5060\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.55971\n",
      "Epoch 00005: early stopping\n",
      "temp/a96\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 114us/step - loss: 0.6588 - acc: 0.6181 - val_loss: 0.6669 - val_acc: 0.6020\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66685, saving model to temp/a96\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 97us/step - loss: 0.6337 - acc: 0.6516 - val_loss: 0.6468 - val_acc: 0.6020\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.66685 to 0.64677, saving model to temp/a96\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 108us/step - loss: 0.6269 - acc: 0.6616 - val_loss: 0.5655 - val_acc: 0.6445\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.64677 to 0.56553, saving model to temp/a96\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6230 - acc: 0.6687 - val_loss: 0.6547 - val_acc: 0.5695\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.56553\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6185 - acc: 0.6751 - val_loss: 0.5627 - val_acc: 0.6275\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.56553 to 0.56269, saving model to temp/a96\n",
      "Epoch 6/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6160 - acc: 0.6732 - val_loss: 0.5766 - val_acc: 0.6150\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.56269\n",
      "Epoch 7/20\n",
      "18084/18084 [==============================] - 2s 104us/step - loss: 0.6139 - acc: 0.6787 - val_loss: 0.6663 - val_acc: 0.5240\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.56269\n",
      "Epoch 00007: early stopping\n",
      "temp/a97\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 112us/step - loss: 0.6565 - acc: 0.6178 - val_loss: 0.5866 - val_acc: 0.6910\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.58661, saving model to temp/a97\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 100us/step - loss: 0.6341 - acc: 0.6480 - val_loss: 0.5987 - val_acc: 0.6490\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.58661\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 105us/step - loss: 0.6270 - acc: 0.6611 - val_loss: 0.6017 - val_acc: 0.6225\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.58661\n",
      "Epoch 00003: early stopping\n",
      "temp/a98\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18084/18084 [==============================] - 2s 116us/step - loss: 0.6575 - acc: 0.6130 - val_loss: 0.7318 - val_acc: 0.5500\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.73176, saving model to temp/a98\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 107us/step - loss: 0.6314 - acc: 0.6545 - val_loss: 0.6569 - val_acc: 0.5800\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.73176 to 0.65690, saving model to temp/a98\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 103us/step - loss: 0.6271 - acc: 0.6609 - val_loss: 0.5114 - val_acc: 0.7100\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.65690 to 0.51139, saving model to temp/a98\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 101us/step - loss: 0.6229 - acc: 0.6693 - val_loss: 0.5294 - val_acc: 0.6785\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51139\n",
      "Epoch 5/20\n",
      "18084/18084 [==============================] - 2s 102us/step - loss: 0.6192 - acc: 0.6746 - val_loss: 0.5584 - val_acc: 0.6380\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51139\n",
      "Epoch 00005: early stopping\n",
      "temp/a99\n",
      "Train on 18084 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18084/18084 [==============================] - 2s 108us/step - loss: 0.6592 - acc: 0.6152 - val_loss: 0.6690 - val_acc: 0.6055\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66897, saving model to temp/a99\n",
      "Epoch 2/20\n",
      "18084/18084 [==============================] - 2s 94us/step - loss: 0.6327 - acc: 0.6518 - val_loss: 0.5765 - val_acc: 0.6645\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.66897 to 0.57649, saving model to temp/a99\n",
      "Epoch 3/20\n",
      "18084/18084 [==============================] - 2s 93us/step - loss: 0.6258 - acc: 0.6616 - val_loss: 0.6046 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.57649\n",
      "Epoch 4/20\n",
      "18084/18084 [==============================] - 2s 100us/step - loss: 0.6228 - acc: 0.6654 - val_loss: 0.6678 - val_acc: 0.5560\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.57649\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "for idx, model_name in enumerate(model_names):\n",
    "    print(model_name)\n",
    "\n",
    "    if type(models[idx]) is list:\n",
    "        #clear session\n",
    "        keras.backend.clear_session() \n",
    "        #get model according to specification\n",
    "        model = get_model(models[idx], [0.2] * len(models), len(inputs), 2)\n",
    "        callbacks = [ModelCheckpoint(model_name, verbose= verbosity, monitor='val_loss',save_best_only=True), \n",
    "                     EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose= verbosity, mode='auto')]\n",
    "        model.compile(optimizer = optimizers.SGD(lr = 0.01, momentum = 0.9, ), loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "        #print(len(X), len(y))\n",
    "        model.fit(X, y, epochs = 20, validation_data = (x_val, y_val), callbacks = callbacks, batch_size = 32, verbose = verbosity)\n",
    "    else:\n",
    "        models[idx].fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c09bcac55d46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0my_pred2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmetrics_dicts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "nb_test = 2000\n",
    "metrics_dicts = []\n",
    "\n",
    "perturbed_df = gen_data_perturbed(SIZE = nb_test)\n",
    "y_test2 = perturbed_df[target]\n",
    "x_test2 = normalize(perturbed_df[inputs].values)\n",
    "for idx, model_name in enumerate(model_names):\n",
    "\n",
    "    if type(models[idx]) is list:\n",
    "        keras.backend.clear_session()\n",
    "        model = load_model(model_name)\n",
    "    else:\n",
    "        model = models[idx]\n",
    "    y_pred2 = model.predict(x_test2)[:,1]\n",
    "    print(roc_auc_score(y_test2, y_pred2))\n",
    "    metrics_dicts.append(roc_auc_score(y_test2, y_pred2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the number of times to sample\n",
    "times = 3\n",
    "## the size of the test set\n",
    "\n",
    "\n",
    "violations = np.zeros(len(models))\n",
    "violation_mean = np.zeros((len(models), times))\n",
    "violation_mean2 = np.zeros((len(models), times))\n",
    "mean = np.zeros((len(models), times))\n",
    "\n",
    "fold = 0\n",
    "\n",
    "\n",
    "\n",
    "for t in range(times):\n",
    "    print(\"Times = \", t)\n",
    "    df_test = gen_data(SIZE = nb_test)\n",
    "    x_test = df_test[inputs].values\n",
    "    x_test_norm = normalize(df_test[inputs].values)\n",
    "    y_test = df_test[target].values\n",
    "    #bic_orig = get_bic(df_test,prior)\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        if type(models[idx]) is list:\n",
    "            keras.backend.clear_session()\n",
    "            model = load_model(model_name)\n",
    "        else:\n",
    "            model = models[idx]\n",
    "            \n",
    "        predicted = model.predict(x_test_norm)[:,1]\n",
    "        test_df = pd.DataFrame(x_test, columns = inputs)\n",
    "        test_targets = pd.DataFrame(predicted,columns = target)\n",
    "        test_df = test_df.join(test_targets)\n",
    "       \n",
    "        \n",
    "    \n",
    "        mean[idx][t] = roc_auc_score(y_test, predicted) \n",
    "        test_df[test_df['cancer'] > 0.5] = 1\n",
    "        test_df[test_df['cancer'] <= 0.5] = 0\n",
    "        bic_pred = get_bic(test_df,prior)\n",
    "        \n",
    "        #bic_pred = get_bic(df_test.join(pd.DataFrame(model.predict(x_test), columns = ['target'])), prior)\n",
    "        \n",
    "        print(tetrad.getEdges())\n",
    "        print(bic_pred)\n",
    "        violation_mean[idx][t] = bic_pred\n",
    "        violation_mean2[idx][t] = bic_pred\n",
    "        #print(bic_orig - bic_pred)\n",
    "metric = []\n",
    "metric_err = []\n",
    "viol = []\n",
    "viol_err = []\n",
    "\n",
    "#normalize the violations for prettier graphing.\n",
    "#also violations are always positive, so just divide by max.\n",
    "\n",
    "#TMK\n",
    "#violation_mean = violation_mean / np.max(violation_mean)\n",
    "\n",
    "for i in range(len(violations)):\n",
    "    print(\"Model_name = \", model_names[i], \"Violations = \", violations[i])\n",
    "    print(\"Average_violations = \", np.mean(violation_mean[i]), np.std(violation_mean[i]))\n",
    "    print(\"MSE = \", np.mean(mean[i]), np.std(mean[i]))\n",
    "    #print(\"mean = \", mean[i])\n",
    "    metric.append(np.mean(mean[i]))\n",
    "    metric_err.append(np.std(mean[i]))\n",
    "    viol.append(np.mean(violation_mean[i]))\n",
    "    #viol.append(violations[i]/times)\n",
    "    viol_err.append(np.std(violation_mean[i]))\n",
    "print(np.array(metric), \n",
    "         np.array(metric_err), \n",
    "         np.array(viol), \n",
    "         np.array(viol_err))    \n",
    "\n",
    "bar_plot(model_names, \n",
    "         np.array(metric), \n",
    "         np.array(metric_err), \n",
    "         np.array(viol), \n",
    "         np.array(viol_err))\n",
    "\n",
    "\n",
    "    \n",
    "MSE = []\n",
    "VIO = []\n",
    "VIO2 = []\n",
    "AUS = []\n",
    "for i, m in enumerate(models):\n",
    "    MSE.append(np.mean(mean[i]))\n",
    "    VIO.append(np.mean(violation_mean[i]))\n",
    "    VIO2.append(np.mean(violation_mean2[i]))\n",
    "    AUS.append(metrics_dicts[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from numpy.polynomial.polynomial import polyfit  \n",
    "from scipy.stats import pearsonr\n",
    "from pylab import text\n",
    "\n",
    "def norm(a):\n",
    "    return (a - np.min(a)) / a.ptp()\n",
    "METRIC = norm(-np.array(VIO)) + np.array(MSE)\n",
    "n_low = int(num_models * 0.2)\n",
    "sorted_aus = [AUS for _,AUS in sorted(zip(VIO,AUS))]\n",
    "\n",
    "print(\"Best by BIC = \", np.mean(sorted_aus[:n_low]))\n",
    "\n",
    "sorted_aus = [AUS for _,AUS in sorted(zip(MSE,AUS))]\n",
    "print(\"Best by AUC = \", np.mean(sorted_aus[:n_low]))\n",
    "\n",
    "\n",
    "sorted_aus = [AUS for _,AUS in sorted(zip(METRIC,AUS))]\n",
    "print(\"Best by MET = \", np.mean(sorted_aus[:n_low]))\n",
    "\n",
    "#sorted_aus = [AUS for _,AUS in sorted(zip(METRIC,AUS))]\n",
    "print(\"Random = \", np.mean(AUS[:n_low]))\n",
    "\n",
    "print(pearsonr(VIO,AUS)[0])\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(VIO,AUS, 1)\n",
    "ax.plot(VIO,AUS, '.')\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(VIO,AUS)[0])[0:6], ha='left', va='center', transform=ax.transAxes)\n",
    "plt.plot(VIO, b + m * np.array(VIO), '-')\n",
    "ax.set_xlabel(\"BIC\")\n",
    "ax.set_ylabel(\"OoS AUCROC\")\n",
    "fig.savefig('Ex4VIOVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(pearsonr(METRIC,AUS)[0])\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(METRIC,AUS, 1)\n",
    "ax.plot(METRIC,AUS, '.')\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(METRIC,AUS)[0])[0:6], ha='left', va='center', transform=ax.transAxes)\n",
    "plt.plot(METRIC, b + m * np.array(METRIC), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"Combined\")\n",
    "ax.set_ylabel(\"OoS AUCROC\")\n",
    "fig.savefig('Ex4ProposedVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(MSE,AUS, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(MSE,AUS)[0])[0:6], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(MSE,AUS, '.')\n",
    "plt.plot(MSE, b + m * np.array(MSE), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"AUC\")\n",
    "ax.set_ylabel(\"OoS AUCROC\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "MSE = np.array(MSE)\n",
    "\n",
    "x = []\n",
    "y1 = []\n",
    "y2 = []\n",
    "y3 = []\n",
    "for split in range(10, len(AUS), 5):\n",
    "    #print(\"******\", split, \"*******\")\n",
    "    sorted_aus = [AUS for _,AUS in sorted(zip(VIO,AUS))]\n",
    "    sorted_mse = [MSE for _,MSE in sorted(zip(VIO,MSE))]\n",
    "\n",
    "    low = []\n",
    "    high = []\n",
    "    low = sorted_aus[:split]\n",
    "    high = sorted_aus[split:]\n",
    "\n",
    "    x.append(split)\n",
    "    \n",
    "    \n",
    "    #print(\"Low Violations = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High Violations = \", np.mean(high), \"for\", len(high))\n",
    "    y1.append(np.mean(low)) \n",
    "    sorted_aus_by_mse = [AUS for _,AUS in sorted(zip(MSE,AUS))]\n",
    "    low = sorted_aus_by_mse[:split]\n",
    "    high = sorted_aus_by_mse[split:]\n",
    "    #print(\"Low AUS by MSE = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High AUS by MSE = \", np.mean(high), \"for\", len(high))\n",
    "    y2.append(np.mean(low))\n",
    "    sorted_aus = [AUS for _,AUS in sorted(zip(METRIC,AUS))]\n",
    "    sorted_mse = [MSE for _,MSE in sorted(zip(METRIC,MSE))]\n",
    "\n",
    "    low = []\n",
    "    high = []\n",
    "    low = sorted_aus[:split]\n",
    "    high = sorted_aus[split:]\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"Low Metric = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High Metric = \", np.mean(high), \"for\", len(high))\n",
    "    y3.append(np.mean(low))\n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x,y1, '-', label = 'BIC')\n",
    "ax.plot(x,y2, '-', label = 'MSE')\n",
    "ax.plot(x,y3, '-', label = 'METRIC')\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"Out of Sample AUCROC\")\n",
    "plt.show()  \n",
    "pearsonr(METRIC,AUS)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfm = SelectFromModel(model, threshold=0.25)\n",
    "sfm.fit(X, y)\n",
    "n_features = sfm.transform(X).shape[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
