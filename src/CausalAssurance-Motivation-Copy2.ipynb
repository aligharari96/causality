{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes \n",
    "### Required installing Oracle JAVA 8 to get javabridge installed\n",
    "### Then, I was able to install py-causal from https://bd2kccd.github.io/docs/py-causal/\n",
    "### GFCI is slower than RFCI, but more accurate (SPIRTES), GFCI and RFCI account for unobserved variables, FGES assumes no unobserved variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure Learning Performance Guarantees If the assumptions in the previous section hold, then in the large sample limit, the CBN structure output by GFCId will contain an edge of one of four kinds between Xand Y   if and only if Xand Yare not independent conditional on any subset of the other measured variables of less than or equal to a specified size. In addition, there is (1) an arc X->Y   if and only if Xdirectly or indirectly causes Y, and Y   does not directly or indirectly cause X; (2) an edge X <-->Y   if and only if X   is not a direct or indirect cause of Yand Y   is not a direct or indirect cause of X(which can only occur if there are latent confounders of Xand some other variable or Yand some other variable; (3) an edge Xo->Y   only if Yis not a direct or indirect cause of X, but Xmay or may not be an indirect cause of Y; (4) an edge X oâ€“o Y   indicates that Xand Y   are dependent no matter what subset of observed variables is conditioned on, but contains no orientation information (X   may be a direct or indirect cause of Y, and Ymay be an indirect cause of X, or there may be a latent common cause of Xand Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying some various ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512], [1024, 512]] ['temp/a0', 'temp/a1', 'temp/a2', 'temp/a3', 'temp/a4', 'temp/a5', 'temp/a6', 'temp/a7', 'temp/a8', 'temp/a9', 'temp/a10', 'temp/a11', 'temp/a12', 'temp/a13', 'temp/a14', 'temp/a15', 'temp/a16', 'temp/a17', 'temp/a18', 'temp/a19', 'temp/a20', 'temp/a21', 'temp/a22', 'temp/a23', 'temp/a24', 'temp/a25', 'temp/a26', 'temp/a27', 'temp/a28', 'temp/a29', 'temp/a30', 'temp/a31', 'temp/a32', 'temp/a33', 'temp/a34', 'temp/a35', 'temp/a36', 'temp/a37', 'temp/a38', 'temp/a39', 'temp/a40', 'temp/a41', 'temp/a42', 'temp/a43', 'temp/a44', 'temp/a45', 'temp/a46', 'temp/a47', 'temp/a48', 'temp/a49', 'temp/a50', 'temp/a51', 'temp/a52', 'temp/a53', 'temp/a54', 'temp/a55', 'temp/a56', 'temp/a57', 'temp/a58', 'temp/a59', 'temp/a60', 'temp/a61', 'temp/a62', 'temp/a63', 'temp/a64', 'temp/a65', 'temp/a66', 'temp/a67', 'temp/a68', 'temp/a69', 'temp/a70', 'temp/a71', 'temp/a72', 'temp/a73', 'temp/a74', 'temp/a75', 'temp/a76', 'temp/a77', 'temp/a78', 'temp/a79', 'temp/a80', 'temp/a81', 'temp/a82', 'temp/a83', 'temp/a84', 'temp/a85', 'temp/a86', 'temp/a87', 'temp/a88', 'temp/a89', 'temp/a90', 'temp/a91', 'temp/a92', 'temp/a93', 'temp/a94', 'temp/a95', 'temp/a96', 'temp/a97', 'temp/a98', 'temp/a99']\n",
      "8874 8874 17748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-498436.0349539574"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from pycausal import search as s\n",
    "import configparser\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, \\\n",
    "                        Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "import keras.optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "import glob, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "# select your GPU Here\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" #Comment this line out if you want all GPUS (2 hehe)\n",
    "\n",
    "# python full-display web browser\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "def get_model(dense, dropouts, inputs, target_len):\n",
    "    # dense is an ordered list of the number of dense neurons like [1024, 2048, 1024]\n",
    "    # dropouts is an ordered list of the dropout masks like [0.2, 0.3, 0.4]\n",
    "    inputs = keras.Input(shape = (inputs,))\n",
    "\n",
    "    x = keras.layers.Dense(dense[0], activation = 'relu')(inputs)\n",
    "    x = keras.layers.Dropout(dropouts[0])(x, training=True)\n",
    "    for den, drop in zip(dense[1:], dropouts[1:]):\n",
    "        x = keras.layers.Dense(den, activation = 'relu')(x)\n",
    "        x = keras.layers.Dropout(drop)(x, training=True)\n",
    "    outputs = keras.layers.Dense(target_len, activation = 'softmax')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def discrete_gauss(low, high, samples, std = 20):\n",
    "    x = np.arange(low, high)\n",
    "    xU, xL = x + 0.5, x - 0.5 \n",
    "    prob = ss.norm.cdf(xU, scale = std) - ss.norm.cdf(xL, scale = std)\n",
    "    prob = prob / prob.sum() #normalize the probabilities so their sum is 1\n",
    "    nums = np.random.choice(x, size = samples, p = prob)\n",
    "    return nums\n",
    "\n",
    "\n",
    "\n",
    "def bar_plot(x_ax, val1, val1std, val2, val2std):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ## the data\n",
    "    N = len(x_ax)\n",
    "\n",
    "    ## necessary variables\n",
    "    ind = np.arange(N)                # the x locations for the groups\n",
    "    width = 0.35                      # the width of the bars\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    ## the bars\n",
    "    rects1 = ax.bar(ind, val1, width,\n",
    "                    color='gray',\n",
    "                    yerr=val1std,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='blue'))\n",
    "\n",
    "    rects2 = ax.bar(ind+width, val2, width,\n",
    "                        color='blue',\n",
    "                        #yerr=val2std,\n",
    "                        error_kw=dict(elinewidth=2,ecolor='gray'))\n",
    "\n",
    "    # axes and labels\n",
    "    ax.set_xlim(-width,len(ind)+width)\n",
    "    #ax.set_ylim(0,45)\n",
    "    ax.set_ylabel('Percentage')\n",
    "    ax.set_title('')\n",
    "    plt.xticks(ind + width / 2, x_ax, rotation=75, size = 14)\n",
    "    ## add a legend\n",
    "    ax.legend( (rects1[0], rects2[0]), ('Accuracy', '% Violations') )\n",
    "    fig.savefig(\"violations.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def normalize(x):\n",
    "    return (x - x.min(0)) / x.ptp(0)\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 20000):\n",
    "    # set bmi to these values real world mean and standard deviation for a certain country.\n",
    "    bmi = np.random.normal(25,5, SIZE)\n",
    "    estrogen =  np.random.normal(bmi, 10) +  np.random.normal(mean,var, SIZE)\n",
    "    \n",
    "    age = np.random.normal(55,10, SIZE)\n",
    "    genes = np.random.normal(age, 10,SIZE) +   np.random.normal(mean,var, SIZE)\n",
    "    \n",
    "    insomnia = np.random.normal(estrogen, 8,SIZE) +   np.random.normal(mean,var, SIZE)\n",
    "    density = np.random.normal(estrogen, 4, SIZE) + np.random.normal(genes,12, SIZE) + np.random.normal(mean,var + 10, SIZE)\n",
    "\n",
    "    cancer = np.zeros_like(density)\n",
    "    m = np.mean(density)\n",
    "\n",
    "    cancer[density > m] = np.random.binomial(n=1, p=0.08, size=len(density[density > m]))\n",
    "    cancer[density <= m] = np.random.binomial(n=1, p=0.01, size=len(density[density <= m]))\n",
    "    \n",
    "    return pd.DataFrame({'bmi' : bmi,'density' : density, 'age' : age, 'cancer' : cancer, 'estrogen': estrogen, 'genes':genes, 'insomnia': insomnia})\n",
    "\n",
    "def gen_data_perturbed(mean = 0, var = 1, SIZE = 20000):\n",
    "    bmi = np.random.normal(30,3, SIZE)\n",
    "    \n",
    "    age = np.random.normal(60,14, SIZE) + np.random.normal(-bmi,var, SIZE) \n",
    "    income = np.random.normal(age, var,SIZE) + np.random.normal(10,12, SIZE)\n",
    "    density = np.random.normal(-bmi,var, SIZE) + np.random.normal(-age,var, SIZE) + np.random.normal(mean,var, SIZE)\n",
    "    cancer = np.zeros_like(density)\n",
    "    m = np.mean(density)\n",
    "    print(m)\n",
    "    cancer[density > m] = np.random.binomial(n=1, p=0.08, size=len(density[density > m]))\n",
    "    cancer[density <= m] = np.random.binomial(n=1, p=0.01, size=len(density[density <= m]))\n",
    "    \n",
    "    return pd.DataFrame({'bmi' : bmi,'density' : density, 'age' : age, 'cancer' : cancer, 'income':income})\n",
    "def gen_data_perturbed(mean = 2, var = 5, SIZE = 20000):\n",
    "    # set bmi to these values real world mean and standard deviation for a certain country.\n",
    "    bmi = np.random.normal(25,5, SIZE)\n",
    "    estrogen =  np.random.normal(bmi, 10) +  np.random.normal(mean,var, SIZE)\n",
    "    \n",
    "    age = np.random.normal(55,10, SIZE)\n",
    "    genes = np.random.normal(age, 10,SIZE) +   np.random.normal(mean,var, SIZE)\n",
    "    \n",
    "    insomnia = np.random.normal(estrogen, 8,SIZE) +   np.random.normal(mean,var, SIZE)\n",
    "    density = np.random.normal(estrogen, 4, SIZE) + np.random.normal(genes,12, SIZE) + np.random.normal(mean,var + 10, SIZE)\n",
    "\n",
    "    cancer = np.zeros_like(density)\n",
    "    m = np.mean(density)\n",
    "\n",
    "    cancer[density > m] = np.random.binomial(n=1, p=0.08, size=len(density[density > m]))\n",
    "    cancer[density <= m] = np.random.binomial(n=1, p=0.01, size=len(density[density <= m]))\n",
    "    \n",
    "    return pd.DataFrame({'bmi' : bmi,'density' : density, 'age' : age, 'cancer' : cancer, 'estrogen': estrogen, 'genes':genes, 'insomnia': insomnia})\n",
    "\n",
    "def get_CG(df, tetrad):\n",
    "    tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "    #tetrad.run(algoId = 'fges-mb', targetName = 'g', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "    #       structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "    #       completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "\n",
    "\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from collections import defaultdict\n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '5000M')\n",
    "tetrad = s.tetradrunner()\n",
    "\n",
    "\n",
    "verbosity = 1\n",
    "\n",
    "\n",
    "\n",
    "models = []\n",
    "model_names = []\n",
    "\n",
    "num_models =100\n",
    "model_layers = [1024,512]\n",
    "for i in range(num_models):\n",
    "    models.append(model_layers)\n",
    "    model_names.append('temp/a' + str(i))\n",
    "\n",
    "print(models, model_names)\n",
    "\n",
    "from pycausal import prior as p\n",
    "def get_bic(df, prior):\n",
    "\n",
    "    tetrad.run(algoId = 'gfci', dfs = df,  scoreId = 'sem-bic-deterministic', dataType = 'continuous',\n",
    "               structurePrior = 1.0, samplePrior = 1, maxDegree = -1, maxPathLength = -1, priorKnowledge = prior,\n",
    "               completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True,\n",
    "               penaltyDiscount = 2\n",
    "               )\n",
    "    BIC = tetrad.getTetradGraph().getAllAttributes().toString()\n",
    "    BIC = float(BIC.split('=')[-1].split('}')[0])\n",
    "    return BIC #/ len(df)\n",
    "import itertools\n",
    "def get_pairs(lst):\n",
    "    a = set()\n",
    "    for i in itertools.permutations(lst,2):\n",
    "        a.add(i)\n",
    "    return a\n",
    "\n",
    "inputs = ['bmi', 'density', 'age', 'genes', 'insomnia', 'estrogen']\n",
    "target = ['cancer']\n",
    "full_conx = get_pairs(inputs + target)\n",
    "forced_conx = set({('age','genes'), ('bmi', 'estrogen'), ('estrogen', 'genes'),('estrogen', 'insomnia'), ('estrogen', 'density'), ('genes', 'density'), ('density', 'cancer')})\n",
    "restricted_conx = full_conx.difference(forced_conx)   \n",
    "\n",
    "prior = p.knowledge(requiredirect =  list(map(list, forced_conx)),\n",
    "                       forbiddirect = list(map(list, restricted_conx))\n",
    "                       )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = gen_data(SIZE = 200000)\n",
    "cancer_df = df[df['cancer'] == 1]\n",
    "\n",
    "ben_df = df[df['cancer'] == 0][:len(cancer_df)]\n",
    "\n",
    "df = cancer_df.append(ben_df, ignore_index=True)\n",
    "print(len(cancer_df), len(ben_df), len(df))\n",
    "\n",
    "\n",
    "X = df[inputs].values\n",
    "X = normalize(X)\n",
    "y = df[target].values\n",
    "y = to_categorical(y)\n",
    "\n",
    "val_df = gen_data(SIZE = 2000)\n",
    "\n",
    "x_val = df[inputs].values\n",
    "x_val = normalize(x_val)\n",
    "y_val = df[target].values\n",
    "y_val = to_categorical(y_val)\n",
    "\n",
    "get_bic(df,prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp/a0\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 188us/step - loss: 0.6619 - acc: 0.6131 - val_loss: 0.6383 - val_acc: 0.6263\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63830, saving model to temp/a0\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6369 - acc: 0.6507 - val_loss: 0.6278 - val_acc: 0.6722\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63830 to 0.62780, saving model to temp/a0\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6313 - acc: 0.6573 - val_loss: 0.6234 - val_acc: 0.6825\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62780 to 0.62340, saving model to temp/a0\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6251 - acc: 0.6673 - val_loss: 0.6203 - val_acc: 0.6778\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62340 to 0.62028, saving model to temp/a0\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6223 - acc: 0.6725 - val_loss: 0.6265 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62028\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6197 - acc: 0.6719 - val_loss: 0.6199 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62028 to 0.61986, saving model to temp/a0\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6175 - acc: 0.6760 - val_loss: 0.6132 - val_acc: 0.6805\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61986 to 0.61320, saving model to temp/a0\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6167 - acc: 0.6769 - val_loss: 0.6179 - val_acc: 0.6868\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61320\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6161 - acc: 0.6779 - val_loss: 0.6108 - val_acc: 0.6915\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61320 to 0.61082, saving model to temp/a0\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6149 - acc: 0.6779 - val_loss: 0.6110 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61082\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6140 - acc: 0.6781 - val_loss: 0.6097 - val_acc: 0.6890\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.61082 to 0.60975, saving model to temp/a0\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6138 - acc: 0.6798 - val_loss: 0.6127 - val_acc: 0.6695\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.60975\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6133 - acc: 0.6801 - val_loss: 0.6086 - val_acc: 0.6953\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.60975 to 0.60861, saving model to temp/a0\n",
      "Epoch 14/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6135 - acc: 0.6807 - val_loss: 0.6077 - val_acc: 0.6817\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.60861 to 0.60768, saving model to temp/a0\n",
      "Epoch 15/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6121 - acc: 0.6810 - val_loss: 0.6084 - val_acc: 0.6941\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.60768\n",
      "Epoch 16/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6111 - acc: 0.6823 - val_loss: 0.6230 - val_acc: 0.6654\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.60768\n",
      "Epoch 00016: early stopping\n",
      "temp/a1\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6620 - acc: 0.6116 - val_loss: 0.6337 - val_acc: 0.6599\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63368, saving model to temp/a1\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6374 - acc: 0.6438 - val_loss: 0.6543 - val_acc: 0.5712\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.63368\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6286 - acc: 0.6592 - val_loss: 0.6270 - val_acc: 0.6820\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63368 to 0.62697, saving model to temp/a1\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6259 - acc: 0.6662 - val_loss: 0.6244 - val_acc: 0.6472\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62697 to 0.62442, saving model to temp/a1\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6230 - acc: 0.6677 - val_loss: 0.6164 - val_acc: 0.6895\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62442 to 0.61636, saving model to temp/a1\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6188 - acc: 0.6770 - val_loss: 0.6132 - val_acc: 0.6779\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61636 to 0.61323, saving model to temp/a1\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6163 - acc: 0.6789 - val_loss: 0.6120 - val_acc: 0.6887\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61323 to 0.61198, saving model to temp/a1\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6174 - acc: 0.6766 - val_loss: 0.6332 - val_acc: 0.6328\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61198\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6148 - acc: 0.6792 - val_loss: 0.6223 - val_acc: 0.6683\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61198\n",
      "Epoch 00009: early stopping\n",
      "temp/a2\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6638 - acc: 0.6077 - val_loss: 0.6386 - val_acc: 0.6227\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63856, saving model to temp/a2\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6365 - acc: 0.6530 - val_loss: 0.6959 - val_acc: 0.5506\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.63856\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6312 - acc: 0.6541 - val_loss: 0.6249 - val_acc: 0.6847\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63856 to 0.62493, saving model to temp/a2\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6283 - acc: 0.6591 - val_loss: 0.6204 - val_acc: 0.6872\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62493 to 0.62042, saving model to temp/a2\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6213 - acc: 0.6765 - val_loss: 0.6241 - val_acc: 0.6795\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62042\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6203 - acc: 0.6750 - val_loss: 0.6174 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62042 to 0.61739, saving model to temp/a2\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6184 - acc: 0.6771 - val_loss: 0.6136 - val_acc: 0.6769\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61739 to 0.61361, saving model to temp/a2\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6183 - acc: 0.6740 - val_loss: 0.6220 - val_acc: 0.6735\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61361\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6155 - acc: 0.6786 - val_loss: 0.6129 - val_acc: 0.6755\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61361 to 0.61295, saving model to temp/a2\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6155 - acc: 0.6775 - val_loss: 0.6109 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61295 to 0.61085, saving model to temp/a2\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6133 - acc: 0.6805 - val_loss: 0.6088 - val_acc: 0.6854\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.61085 to 0.60880, saving model to temp/a2\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6148 - acc: 0.6758 - val_loss: 0.6132 - val_acc: 0.6648\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.60880\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6143 - acc: 0.6792 - val_loss: 0.6106 - val_acc: 0.6728\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.60880\n",
      "Epoch 00013: early stopping\n",
      "temp/a3\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6618 - acc: 0.6121 - val_loss: 0.6520 - val_acc: 0.5796\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65196, saving model to temp/a3\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6348 - acc: 0.6489 - val_loss: 0.6284 - val_acc: 0.6755\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65196 to 0.62837, saving model to temp/a3\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6295 - acc: 0.6576 - val_loss: 0.6273 - val_acc: 0.6868\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62837 to 0.62732, saving model to temp/a3\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6254 - acc: 0.6670 - val_loss: 0.6205 - val_acc: 0.6601\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62732 to 0.62049, saving model to temp/a3\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6227 - acc: 0.6662 - val_loss: 0.6170 - val_acc: 0.6893\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62049 to 0.61701, saving model to temp/a3\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6190 - acc: 0.6760 - val_loss: 0.6178 - val_acc: 0.6885\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61701\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6186 - acc: 0.6777 - val_loss: 0.6151 - val_acc: 0.6647\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61701 to 0.61508, saving model to temp/a3\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6156 - acc: 0.6785 - val_loss: 0.6110 - val_acc: 0.6868\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61508 to 0.61102, saving model to temp/a3\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6151 - acc: 0.6786 - val_loss: 0.6169 - val_acc: 0.6836\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61102\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6143 - acc: 0.6786 - val_loss: 0.6090 - val_acc: 0.6884\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61102 to 0.60899, saving model to temp/a3\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6125 - acc: 0.6809 - val_loss: 0.6079 - val_acc: 0.6925\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.60899 to 0.60794, saving model to temp/a3\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6129 - acc: 0.6811 - val_loss: 0.6075 - val_acc: 0.6910\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.60794 to 0.60749, saving model to temp/a3\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6141 - acc: 0.6788 - val_loss: 0.6080 - val_acc: 0.6947\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.60749\n",
      "Epoch 14/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6118 - acc: 0.6829 - val_loss: 0.6221 - val_acc: 0.6500\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.60749\n",
      "Epoch 00014: early stopping\n",
      "temp/a4\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6655 - acc: 0.6049 - val_loss: 0.6403 - val_acc: 0.6715\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64034, saving model to temp/a4\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6374 - acc: 0.6454 - val_loss: 0.6367 - val_acc: 0.6730\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64034 to 0.63668, saving model to temp/a4\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6307 - acc: 0.6582 - val_loss: 0.6239 - val_acc: 0.6538\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63668 to 0.62395, saving model to temp/a4\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6255 - acc: 0.6653 - val_loss: 0.6230 - val_acc: 0.6541\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62395 to 0.62304, saving model to temp/a4\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6222 - acc: 0.6686 - val_loss: 0.6331 - val_acc: 0.6251\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62304\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6200 - acc: 0.6756 - val_loss: 0.6169 - val_acc: 0.6920\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62304 to 0.61688, saving model to temp/a4\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6171 - acc: 0.6787 - val_loss: 0.6134 - val_acc: 0.6823\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61688 to 0.61335, saving model to temp/a4\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6161 - acc: 0.6781 - val_loss: 0.6120 - val_acc: 0.6799\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61335 to 0.61204, saving model to temp/a4\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6173 - acc: 0.6752 - val_loss: 0.6107 - val_acc: 0.6925\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61204 to 0.61072, saving model to temp/a4\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6143 - acc: 0.6794 - val_loss: 0.6091 - val_acc: 0.6814\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61072 to 0.60911, saving model to temp/a4\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6141 - acc: 0.6775 - val_loss: 0.6271 - val_acc: 0.6429\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.60911\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6126 - acc: 0.6774 - val_loss: 0.6104 - val_acc: 0.6936\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.60911\n",
      "Epoch 00012: early stopping\n",
      "temp/a5\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6595 - acc: 0.6108 - val_loss: 0.6368 - val_acc: 0.6259\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63679, saving model to temp/a5\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6356 - acc: 0.6512 - val_loss: 0.6470 - val_acc: 0.6569\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.63679\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6304 - acc: 0.6588 - val_loss: 0.6292 - val_acc: 0.6325\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63679 to 0.62922, saving model to temp/a5\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6243 - acc: 0.6693 - val_loss: 0.6196 - val_acc: 0.6725\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62922 to 0.61955, saving model to temp/a5\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6222 - acc: 0.6703 - val_loss: 0.6223 - val_acc: 0.6884\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.61955\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6201 - acc: 0.6727 - val_loss: 0.6151 - val_acc: 0.6775\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61955 to 0.61506, saving model to temp/a5\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6177 - acc: 0.6745 - val_loss: 0.6151 - val_acc: 0.6931\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61506\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6168 - acc: 0.6760 - val_loss: 0.6155 - val_acc: 0.6910\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61506\n",
      "Epoch 00008: early stopping\n",
      "temp/a6\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6651 - acc: 0.6009 - val_loss: 0.6397 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63974, saving model to temp/a6\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6403 - acc: 0.6420 - val_loss: 0.6285 - val_acc: 0.6575\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63974 to 0.62854, saving model to temp/a6\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6303 - acc: 0.6589 - val_loss: 0.6240 - val_acc: 0.6598\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62854 to 0.62400, saving model to temp/a6\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6263 - acc: 0.6652 - val_loss: 0.6205 - val_acc: 0.6618\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62400 to 0.62049, saving model to temp/a6\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6221 - acc: 0.6724 - val_loss: 0.6219 - val_acc: 0.6525\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62049\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6215 - acc: 0.6749 - val_loss: 0.6224 - val_acc: 0.6795\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.62049\n",
      "Epoch 00006: early stopping\n",
      "temp/a7\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 158us/step - loss: 0.6615 - acc: 0.6102 - val_loss: 0.6367 - val_acc: 0.6703\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63669, saving model to temp/a7\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6382 - acc: 0.6462 - val_loss: 0.6401 - val_acc: 0.6053\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.63669\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6314 - acc: 0.6547 - val_loss: 0.6245 - val_acc: 0.6594\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63669 to 0.62448, saving model to temp/a7\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6260 - acc: 0.6662 - val_loss: 0.6214 - val_acc: 0.6746\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62448 to 0.62136, saving model to temp/a7\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6233 - acc: 0.6703 - val_loss: 0.6249 - val_acc: 0.6419\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62136\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6184 - acc: 0.6774 - val_loss: 0.6182 - val_acc: 0.6583\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62136 to 0.61819, saving model to temp/a7\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6164 - acc: 0.6764 - val_loss: 0.6136 - val_acc: 0.6906\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61819 to 0.61359, saving model to temp/a7\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6171 - acc: 0.6761 - val_loss: 0.6136 - val_acc: 0.6942\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61359 to 0.61357, saving model to temp/a7\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6146 - acc: 0.6786 - val_loss: 0.6304 - val_acc: 0.6568\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61357\n",
      "Epoch 00009: early stopping\n",
      "temp/a8\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6617 - acc: 0.6083 - val_loss: 0.6358 - val_acc: 0.6610\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63583, saving model to temp/a8\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6367 - acc: 0.6483 - val_loss: 0.6296 - val_acc: 0.6787\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63583 to 0.62962, saving model to temp/a8\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6305 - acc: 0.6565 - val_loss: 0.6259 - val_acc: 0.6605\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62962 to 0.62586, saving model to temp/a8\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6265 - acc: 0.6647 - val_loss: 0.6293 - val_acc: 0.6754\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62586\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6246 - acc: 0.6684 - val_loss: 0.6255 - val_acc: 0.6407\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62586 to 0.62555, saving model to temp/a8\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6192 - acc: 0.6748 - val_loss: 0.6196 - val_acc: 0.6557\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62555 to 0.61955, saving model to temp/a8\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6182 - acc: 0.6763 - val_loss: 0.6176 - val_acc: 0.6596\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61955 to 0.61759, saving model to temp/a8\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6168 - acc: 0.6787 - val_loss: 0.6197 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61759\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6142 - acc: 0.6791 - val_loss: 0.6122 - val_acc: 0.6724\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61759 to 0.61217, saving model to temp/a8\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6128 - acc: 0.6828 - val_loss: 0.6084 - val_acc: 0.6850\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61217 to 0.60837, saving model to temp/a8\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6122 - acc: 0.6826 - val_loss: 0.6158 - val_acc: 0.6600\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.60837\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6136 - acc: 0.6770 - val_loss: 0.6162 - val_acc: 0.6839\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.60837\n",
      "Epoch 00012: early stopping\n",
      "temp/a9\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6629 - acc: 0.6023 - val_loss: 0.6355 - val_acc: 0.6649\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63550, saving model to temp/a9\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6388 - acc: 0.6434 - val_loss: 0.6332 - val_acc: 0.6232\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63550 to 0.63324, saving model to temp/a9\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6304 - acc: 0.6589 - val_loss: 0.6259 - val_acc: 0.6514\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63324 to 0.62586, saving model to temp/a9\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6251 - acc: 0.6669 - val_loss: 0.6199 - val_acc: 0.6733\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62586 to 0.61987, saving model to temp/a9\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6226 - acc: 0.6736 - val_loss: 0.6181 - val_acc: 0.6652\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.61987 to 0.61812, saving model to temp/a9\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6204 - acc: 0.6712 - val_loss: 0.6145 - val_acc: 0.6823\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61812 to 0.61455, saving model to temp/a9\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6197 - acc: 0.6742 - val_loss: 0.6127 - val_acc: 0.6909\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61455 to 0.61267, saving model to temp/a9\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6180 - acc: 0.6776 - val_loss: 0.6113 - val_acc: 0.6917\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61267 to 0.61131, saving model to temp/a9\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6139 - acc: 0.6827 - val_loss: 0.6103 - val_acc: 0.6867\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61131 to 0.61034, saving model to temp/a9\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6146 - acc: 0.6788 - val_loss: 0.6190 - val_acc: 0.6775\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61034\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6133 - acc: 0.6808 - val_loss: 0.6119 - val_acc: 0.6946\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.61034\n",
      "Epoch 00011: early stopping\n",
      "temp/a10\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6618 - acc: 0.6123 - val_loss: 0.6392 - val_acc: 0.6726\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63917, saving model to temp/a10\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6368 - acc: 0.6503 - val_loss: 0.6273 - val_acc: 0.6716\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63917 to 0.62729, saving model to temp/a10\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6316 - acc: 0.6574 - val_loss: 0.6328 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.62729\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6268 - acc: 0.6639 - val_loss: 0.6226 - val_acc: 0.6504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss improved from 0.62729 to 0.62255, saving model to temp/a10\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6226 - acc: 0.6707 - val_loss: 0.6267 - val_acc: 0.6412\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62255\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6208 - acc: 0.6731 - val_loss: 0.6148 - val_acc: 0.6845\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62255 to 0.61483, saving model to temp/a10\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6170 - acc: 0.6771 - val_loss: 0.6279 - val_acc: 0.6644\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61483\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6162 - acc: 0.6790 - val_loss: 0.6198 - val_acc: 0.6771\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61483\n",
      "Epoch 00008: early stopping\n",
      "temp/a11\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6621 - acc: 0.6103 - val_loss: 0.6541 - val_acc: 0.5734\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65413, saving model to temp/a11\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6398 - acc: 0.6404 - val_loss: 0.6299 - val_acc: 0.6808\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65413 to 0.62987, saving model to temp/a11\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6294 - acc: 0.6615 - val_loss: 0.6322 - val_acc: 0.6271\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.62987\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6254 - acc: 0.6682 - val_loss: 0.6204 - val_acc: 0.6627\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62987 to 0.62037, saving model to temp/a11\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6217 - acc: 0.6722 - val_loss: 0.6167 - val_acc: 0.6753\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62037 to 0.61673, saving model to temp/a11\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6196 - acc: 0.6739 - val_loss: 0.6142 - val_acc: 0.6814\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61673 to 0.61419, saving model to temp/a11\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6189 - acc: 0.6733 - val_loss: 0.6474 - val_acc: 0.6148\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61419\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6174 - acc: 0.6747 - val_loss: 0.6111 - val_acc: 0.6876\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61419 to 0.61114, saving model to temp/a11\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6146 - acc: 0.6815 - val_loss: 0.6126 - val_acc: 0.6836\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61114\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6146 - acc: 0.6803 - val_loss: 0.6148 - val_acc: 0.6641\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61114\n",
      "Epoch 00010: early stopping\n",
      "temp/a12\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 158us/step - loss: 0.6619 - acc: 0.6032 - val_loss: 0.6421 - val_acc: 0.6664\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64211, saving model to temp/a12\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6369 - acc: 0.6470 - val_loss: 0.6299 - val_acc: 0.6783\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64211 to 0.62990, saving model to temp/a12\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6316 - acc: 0.6537 - val_loss: 0.6260 - val_acc: 0.6475\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62990 to 0.62600, saving model to temp/a12\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6267 - acc: 0.6617 - val_loss: 0.6191 - val_acc: 0.6668\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62600 to 0.61909, saving model to temp/a12\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6229 - acc: 0.6721 - val_loss: 0.6176 - val_acc: 0.6879\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.61909 to 0.61757, saving model to temp/a12\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6195 - acc: 0.6746 - val_loss: 0.6146 - val_acc: 0.6902\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61757 to 0.61458, saving model to temp/a12\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6181 - acc: 0.6775 - val_loss: 0.6135 - val_acc: 0.6925\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61458 to 0.61346, saving model to temp/a12\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6147 - acc: 0.6784 - val_loss: 0.6149 - val_acc: 0.6876\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61346\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6148 - acc: 0.6831 - val_loss: 0.6106 - val_acc: 0.6818\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61346 to 0.61065, saving model to temp/a12\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6135 - acc: 0.6801 - val_loss: 0.6141 - val_acc: 0.6880\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61065\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6146 - acc: 0.6775 - val_loss: 0.6086 - val_acc: 0.6944\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.61065 to 0.60864, saving model to temp/a12\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6154 - acc: 0.6766 - val_loss: 0.6137 - val_acc: 0.6680\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.60864\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 142us/step - loss: 0.6137 - acc: 0.6780 - val_loss: 0.6191 - val_acc: 0.6773\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.60864\n",
      "Epoch 00013: early stopping\n",
      "temp/a13\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 155us/step - loss: 0.6608 - acc: 0.6121 - val_loss: 0.6376 - val_acc: 0.6291\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63755, saving model to temp/a13\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6369 - acc: 0.6487 - val_loss: 0.6279 - val_acc: 0.6593\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63755 to 0.62785, saving model to temp/a13\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6289 - acc: 0.6632 - val_loss: 0.6236 - val_acc: 0.6613\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62785 to 0.62364, saving model to temp/a13\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6269 - acc: 0.6656 - val_loss: 0.6224 - val_acc: 0.6582\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62364 to 0.62239, saving model to temp/a13\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 143us/step - loss: 0.6213 - acc: 0.6739 - val_loss: 0.6206 - val_acc: 0.6596\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62239 to 0.62063, saving model to temp/a13\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6206 - acc: 0.6722 - val_loss: 0.6221 - val_acc: 0.6793\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.62063\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6189 - acc: 0.6756 - val_loss: 0.6144 - val_acc: 0.6788\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.62063 to 0.61443, saving model to temp/a13\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6188 - acc: 0.6761 - val_loss: 0.6284 - val_acc: 0.6638\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61443\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6155 - acc: 0.6778 - val_loss: 0.6115 - val_acc: 0.6912\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61443 to 0.61148, saving model to temp/a13\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6143 - acc: 0.6805 - val_loss: 0.6108 - val_acc: 0.6752\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61148 to 0.61083, saving model to temp/a13\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6143 - acc: 0.6783 - val_loss: 0.6092 - val_acc: 0.6900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00011: val_loss improved from 0.61083 to 0.60919, saving model to temp/a13\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6122 - acc: 0.6818 - val_loss: 0.6086 - val_acc: 0.6853\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.60919 to 0.60859, saving model to temp/a13\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6125 - acc: 0.6800 - val_loss: 0.6070 - val_acc: 0.6882\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.60859 to 0.60698, saving model to temp/a13\n",
      "Epoch 14/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6121 - acc: 0.6820 - val_loss: 0.6088 - val_acc: 0.6943\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.60698\n",
      "Epoch 15/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6121 - acc: 0.6818 - val_loss: 0.6115 - val_acc: 0.6734\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.60698\n",
      "Epoch 00015: early stopping\n",
      "temp/a14\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 159us/step - loss: 0.6620 - acc: 0.6147 - val_loss: 0.6409 - val_acc: 0.6118\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64086, saving model to temp/a14\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6359 - acc: 0.6507 - val_loss: 0.6337 - val_acc: 0.6809\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64086 to 0.63372, saving model to temp/a14\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6314 - acc: 0.6532 - val_loss: 0.6233 - val_acc: 0.6689\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63372 to 0.62334, saving model to temp/a14\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6263 - acc: 0.6635 - val_loss: 0.6249 - val_acc: 0.6853\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62334\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6230 - acc: 0.6700 - val_loss: 0.6182 - val_acc: 0.6911\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62334 to 0.61822, saving model to temp/a14\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6199 - acc: 0.6765 - val_loss: 0.6147 - val_acc: 0.6862\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61822 to 0.61472, saving model to temp/a14\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6189 - acc: 0.6747 - val_loss: 0.6134 - val_acc: 0.6719\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61472 to 0.61343, saving model to temp/a14\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6166 - acc: 0.6791 - val_loss: 0.6114 - val_acc: 0.6890\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61343 to 0.61144, saving model to temp/a14\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6164 - acc: 0.6749 - val_loss: 0.6162 - val_acc: 0.6890\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61144\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6147 - acc: 0.6797 - val_loss: 0.6105 - val_acc: 0.6872\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61144 to 0.61048, saving model to temp/a14\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6125 - acc: 0.6818 - val_loss: 0.6103 - val_acc: 0.6936\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.61048 to 0.61034, saving model to temp/a14\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6123 - acc: 0.6835 - val_loss: 0.6107 - val_acc: 0.6836\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.61034\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6132 - acc: 0.6797 - val_loss: 0.6066 - val_acc: 0.6843\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.61034 to 0.60660, saving model to temp/a14\n",
      "Epoch 14/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6117 - acc: 0.6836 - val_loss: 0.6084 - val_acc: 0.6949\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.60660\n",
      "Epoch 15/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6125 - acc: 0.6802 - val_loss: 0.6118 - val_acc: 0.6720\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.60660\n",
      "Epoch 00015: early stopping\n",
      "temp/a15\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6619 - acc: 0.6046 - val_loss: 0.6370 - val_acc: 0.6701\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63696, saving model to temp/a15\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6360 - acc: 0.6509 - val_loss: 0.6287 - val_acc: 0.6842\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63696 to 0.62865, saving model to temp/a15\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6275 - acc: 0.6628 - val_loss: 0.6225 - val_acc: 0.6610\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62865 to 0.62252, saving model to temp/a15\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6265 - acc: 0.6636 - val_loss: 0.6192 - val_acc: 0.6848\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62252 to 0.61923, saving model to temp/a15\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6211 - acc: 0.6735 - val_loss: 0.6175 - val_acc: 0.6910\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.61923 to 0.61746, saving model to temp/a15\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6200 - acc: 0.6746 - val_loss: 0.6247 - val_acc: 0.6440\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61746\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6178 - acc: 0.6729 - val_loss: 0.6120 - val_acc: 0.6759\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61746 to 0.61198, saving model to temp/a15\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6156 - acc: 0.6796 - val_loss: 0.6096 - val_acc: 0.6881\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61198 to 0.60960, saving model to temp/a15\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6161 - acc: 0.6781 - val_loss: 0.6115 - val_acc: 0.6942\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.60960\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6158 - acc: 0.6756 - val_loss: 0.6091 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.60960 to 0.60905, saving model to temp/a15\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6130 - acc: 0.6773 - val_loss: 0.6084 - val_acc: 0.6922\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.60905 to 0.60839, saving model to temp/a15\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6124 - acc: 0.6804 - val_loss: 0.6105 - val_acc: 0.6824\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.60839\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6115 - acc: 0.6827 - val_loss: 0.6108 - val_acc: 0.6911\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.60839\n",
      "Epoch 00013: early stopping\n",
      "temp/a16\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6611 - acc: 0.6064 - val_loss: 0.6379 - val_acc: 0.6659\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63786, saving model to temp/a16\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6377 - acc: 0.6512 - val_loss: 0.6278 - val_acc: 0.6662\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63786 to 0.62783, saving model to temp/a16\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6281 - acc: 0.6633 - val_loss: 0.6254 - val_acc: 0.6821\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62783 to 0.62543, saving model to temp/a16\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6270 - acc: 0.6650 - val_loss: 0.6387 - val_acc: 0.6105\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62543\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6231 - acc: 0.6684 - val_loss: 0.6247 - val_acc: 0.6428\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62543 to 0.62467, saving model to temp/a16\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6210 - acc: 0.6746 - val_loss: 0.6191 - val_acc: 0.6574\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62467 to 0.61907, saving model to temp/a16\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6208 - acc: 0.6747 - val_loss: 0.6135 - val_acc: 0.6886\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61907 to 0.61352, saving model to temp/a16\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6166 - acc: 0.6793 - val_loss: 0.6227 - val_acc: 0.6729\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61352\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6164 - acc: 0.6775 - val_loss: 0.6144 - val_acc: 0.6903\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61352\n",
      "Epoch 00009: early stopping\n",
      "temp/a17\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6629 - acc: 0.6078 - val_loss: 0.6397 - val_acc: 0.6164\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63975, saving model to temp/a17\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6379 - acc: 0.6476 - val_loss: 0.6369 - val_acc: 0.6763\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63975 to 0.63686, saving model to temp/a17\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6318 - acc: 0.6570 - val_loss: 0.6325 - val_acc: 0.6702\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63686 to 0.63251, saving model to temp/a17\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6273 - acc: 0.6665 - val_loss: 0.6246 - val_acc: 0.6422\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.63251 to 0.62455, saving model to temp/a17\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6218 - acc: 0.6756 - val_loss: 0.6154 - val_acc: 0.6858\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62455 to 0.61541, saving model to temp/a17\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6179 - acc: 0.6785 - val_loss: 0.6217 - val_acc: 0.6805\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61541\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6165 - acc: 0.6761 - val_loss: 0.6123 - val_acc: 0.6905\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61541 to 0.61228, saving model to temp/a17\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6159 - acc: 0.6806 - val_loss: 0.6119 - val_acc: 0.6937\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61228 to 0.61190, saving model to temp/a17\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6143 - acc: 0.6791 - val_loss: 0.6106 - val_acc: 0.6867\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61190 to 0.61060, saving model to temp/a17\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6136 - acc: 0.6823 - val_loss: 0.6287 - val_acc: 0.6422\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61060\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 151us/step - loss: 0.6131 - acc: 0.6782 - val_loss: 0.6122 - val_acc: 0.6931\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.61060\n",
      "Epoch 00011: early stopping\n",
      "temp/a18\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6629 - acc: 0.6053 - val_loss: 0.6422 - val_acc: 0.6063\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64215, saving model to temp/a18\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6366 - acc: 0.6486 - val_loss: 0.6317 - val_acc: 0.6813\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64215 to 0.63173, saving model to temp/a18\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6293 - acc: 0.6632 - val_loss: 0.6227 - val_acc: 0.6716\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63173 to 0.62266, saving model to temp/a18\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6276 - acc: 0.6622 - val_loss: 0.6340 - val_acc: 0.6235\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62266\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6251 - acc: 0.6650 - val_loss: 0.6170 - val_acc: 0.6884\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62266 to 0.61703, saving model to temp/a18\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6203 - acc: 0.6707 - val_loss: 0.6151 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61703 to 0.61506, saving model to temp/a18\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6181 - acc: 0.6765 - val_loss: 0.6131 - val_acc: 0.6824\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61506 to 0.61311, saving model to temp/a18\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6157 - acc: 0.6808 - val_loss: 0.6144 - val_acc: 0.6906\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61311\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6150 - acc: 0.6825 - val_loss: 0.6099 - val_acc: 0.6841\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61311 to 0.60991, saving model to temp/a18\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6132 - acc: 0.6822 - val_loss: 0.6273 - val_acc: 0.6610\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.60991\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6153 - acc: 0.6766 - val_loss: 0.6090 - val_acc: 0.6944\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.60991 to 0.60896, saving model to temp/a18\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 143us/step - loss: 0.6120 - acc: 0.6827 - val_loss: 0.6115 - val_acc: 0.6709\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.60896\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 2s 141us/step - loss: 0.6111 - acc: 0.6843 - val_loss: 0.6060 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.60896 to 0.60601, saving model to temp/a18\n",
      "Epoch 14/20\n",
      "17748/17748 [==============================] - 3s 143us/step - loss: 0.6102 - acc: 0.6853 - val_loss: 0.6067 - val_acc: 0.6816\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.60601\n",
      "Epoch 15/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6122 - acc: 0.6804 - val_loss: 0.6086 - val_acc: 0.6929\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.60601\n",
      "Epoch 00015: early stopping\n",
      "temp/a19\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 155us/step - loss: 0.6621 - acc: 0.6113 - val_loss: 0.6444 - val_acc: 0.6660\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64442, saving model to temp/a19\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6374 - acc: 0.6491 - val_loss: 0.6262 - val_acc: 0.6766\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64442 to 0.62622, saving model to temp/a19\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6288 - acc: 0.6632 - val_loss: 0.6215 - val_acc: 0.6786\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62622 to 0.62151, saving model to temp/a19\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6259 - acc: 0.6653 - val_loss: 0.6238 - val_acc: 0.6840\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62151\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6204 - acc: 0.6731 - val_loss: 0.6186 - val_acc: 0.6606\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62151 to 0.61864, saving model to temp/a19\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6204 - acc: 0.6703 - val_loss: 0.6157 - val_acc: 0.6782\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61864 to 0.61566, saving model to temp/a19\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6166 - acc: 0.6783 - val_loss: 0.6149 - val_acc: 0.6708\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61566 to 0.61490, saving model to temp/a19\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6164 - acc: 0.6740 - val_loss: 0.6121 - val_acc: 0.6806\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61490 to 0.61214, saving model to temp/a19\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6170 - acc: 0.6769 - val_loss: 0.6107 - val_acc: 0.6941\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61214 to 0.61065, saving model to temp/a19\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6150 - acc: 0.6799 - val_loss: 0.6188 - val_acc: 0.6533\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61065\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6141 - acc: 0.6800 - val_loss: 0.6161 - val_acc: 0.6628\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.61065\n",
      "Epoch 00011: early stopping\n",
      "temp/a20\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6620 - acc: 0.6092 - val_loss: 0.6405 - val_acc: 0.6160\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64052, saving model to temp/a20\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6372 - acc: 0.6511 - val_loss: 0.6280 - val_acc: 0.6515\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64052 to 0.62800, saving model to temp/a20\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6295 - acc: 0.6631 - val_loss: 0.6213 - val_acc: 0.6843\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62800 to 0.62132, saving model to temp/a20\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6254 - acc: 0.6682 - val_loss: 0.6210 - val_acc: 0.6548\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62132 to 0.62095, saving model to temp/a20\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6225 - acc: 0.6685 - val_loss: 0.6164 - val_acc: 0.6899\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62095 to 0.61637, saving model to temp/a20\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6198 - acc: 0.6748 - val_loss: 0.6168 - val_acc: 0.6929\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61637\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6179 - acc: 0.6786 - val_loss: 0.6180 - val_acc: 0.6839\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61637\n",
      "Epoch 00007: early stopping\n",
      "temp/a21\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 158us/step - loss: 0.6619 - acc: 0.6074 - val_loss: 0.6505 - val_acc: 0.5856\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65046, saving model to temp/a21\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6376 - acc: 0.6476 - val_loss: 0.6271 - val_acc: 0.6635\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65046 to 0.62711, saving model to temp/a21\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6306 - acc: 0.6589 - val_loss: 0.6269 - val_acc: 0.6867\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62711 to 0.62689, saving model to temp/a21\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6273 - acc: 0.6631 - val_loss: 0.6246 - val_acc: 0.6848\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62689 to 0.62463, saving model to temp/a21\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6213 - acc: 0.6716 - val_loss: 0.6196 - val_acc: 0.6897\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62463 to 0.61959, saving model to temp/a21\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6202 - acc: 0.6727 - val_loss: 0.6147 - val_acc: 0.6922\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61959 to 0.61465, saving model to temp/a21\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6184 - acc: 0.6752 - val_loss: 0.6278 - val_acc: 0.6414\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61465\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 150us/step - loss: 0.6184 - acc: 0.6757 - val_loss: 0.6129 - val_acc: 0.6849\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61465 to 0.61291, saving model to temp/a21\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6154 - acc: 0.6765 - val_loss: 0.6140 - val_acc: 0.6929\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61291\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6133 - acc: 0.6807 - val_loss: 0.6100 - val_acc: 0.6813\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61291 to 0.61004, saving model to temp/a21\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6129 - acc: 0.6820 - val_loss: 0.6091 - val_acc: 0.6791\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.61004 to 0.60911, saving model to temp/a21\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6114 - acc: 0.6843 - val_loss: 0.6097 - val_acc: 0.6780\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.60911\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6115 - acc: 0.6829 - val_loss: 0.6073 - val_acc: 0.6849\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.60911 to 0.60728, saving model to temp/a21\n",
      "Epoch 14/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6103 - acc: 0.6836 - val_loss: 0.6132 - val_acc: 0.6891\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.60728\n",
      "Epoch 15/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6149 - acc: 0.6791 - val_loss: 0.6103 - val_acc: 0.6939\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.60728\n",
      "Epoch 00015: early stopping\n",
      "temp/a22\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6610 - acc: 0.6148 - val_loss: 0.6365 - val_acc: 0.6693\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63653, saving model to temp/a22\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6360 - acc: 0.6491 - val_loss: 0.6302 - val_acc: 0.6817\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63653 to 0.63021, saving model to temp/a22\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6322 - acc: 0.6530 - val_loss: 0.6812 - val_acc: 0.5980\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.63021\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6254 - acc: 0.6646 - val_loss: 0.6270 - val_acc: 0.6379\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.63021 to 0.62702, saving model to temp/a22\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6239 - acc: 0.6673 - val_loss: 0.6175 - val_acc: 0.6890\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62702 to 0.61751, saving model to temp/a22\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6198 - acc: 0.6766 - val_loss: 0.6273 - val_acc: 0.6672\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61751\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6172 - acc: 0.6748 - val_loss: 0.6175 - val_acc: 0.6881\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61751 to 0.61746, saving model to temp/a22\n",
      "Epoch 00007: early stopping\n",
      "temp/a23\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 158us/step - loss: 0.6641 - acc: 0.6077 - val_loss: 0.6351 - val_acc: 0.6549\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63507, saving model to temp/a23\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 143us/step - loss: 0.6374 - acc: 0.6501 - val_loss: 0.6278 - val_acc: 0.6514\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63507 to 0.62780, saving model to temp/a23\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6294 - acc: 0.6597 - val_loss: 0.6233 - val_acc: 0.6588\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62780 to 0.62328, saving model to temp/a23\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6263 - acc: 0.6653 - val_loss: 0.6542 - val_acc: 0.6256\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62328\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6222 - acc: 0.6703 - val_loss: 0.6176 - val_acc: 0.6926\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62328 to 0.61763, saving model to temp/a23\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6194 - acc: 0.6747 - val_loss: 0.6296 - val_acc: 0.6676\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61763\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6172 - acc: 0.6769 - val_loss: 0.6285 - val_acc: 0.6607\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61763\n",
      "Epoch 00007: early stopping\n",
      "temp/a24\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 158us/step - loss: 0.6599 - acc: 0.6106 - val_loss: 0.6414 - val_acc: 0.6688\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64136, saving model to temp/a24\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6352 - acc: 0.6502 - val_loss: 0.6354 - val_acc: 0.6167\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64136 to 0.63538, saving model to temp/a24\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6301 - acc: 0.6604 - val_loss: 0.6244 - val_acc: 0.6587\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63538 to 0.62444, saving model to temp/a24\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6280 - acc: 0.6645 - val_loss: 0.6211 - val_acc: 0.6891\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62444 to 0.62115, saving model to temp/a24\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6229 - acc: 0.6686 - val_loss: 0.6235 - val_acc: 0.6832\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62115\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6197 - acc: 0.6759 - val_loss: 0.6195 - val_acc: 0.6877\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62115 to 0.61952, saving model to temp/a24\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6177 - acc: 0.6763 - val_loss: 0.6213 - val_acc: 0.6507\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61952\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6196 - acc: 0.6724 - val_loss: 0.6116 - val_acc: 0.6911\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61952 to 0.61161, saving model to temp/a24\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6166 - acc: 0.6748 - val_loss: 0.6290 - val_acc: 0.6371\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61161\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6152 - acc: 0.6780 - val_loss: 0.6195 - val_acc: 0.6752\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61161\n",
      "Epoch 00010: early stopping\n",
      "temp/a25\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6639 - acc: 0.6002 - val_loss: 0.6397 - val_acc: 0.6706\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63971, saving model to temp/a25\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6367 - acc: 0.6449 - val_loss: 0.6255 - val_acc: 0.6597\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63971 to 0.62553, saving model to temp/a25\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6291 - acc: 0.6623 - val_loss: 0.6257 - val_acc: 0.6440\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.62553\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6253 - acc: 0.6631 - val_loss: 0.6208 - val_acc: 0.6874\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62553 to 0.62076, saving model to temp/a25\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6225 - acc: 0.6685 - val_loss: 0.6229 - val_acc: 0.6813\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62076\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6217 - acc: 0.6695 - val_loss: 0.6184 - val_acc: 0.6865\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62076 to 0.61841, saving model to temp/a25\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6182 - acc: 0.6761 - val_loss: 0.6165 - val_acc: 0.6639\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61841 to 0.61651, saving model to temp/a25\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6155 - acc: 0.6797 - val_loss: 0.6122 - val_acc: 0.6934\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61651 to 0.61220, saving model to temp/a25\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6151 - acc: 0.6804 - val_loss: 0.6149 - val_acc: 0.6623\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61220\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6132 - acc: 0.6788 - val_loss: 0.6109 - val_acc: 0.6770\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61220 to 0.61085, saving model to temp/a25\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6131 - acc: 0.6796 - val_loss: 0.6098 - val_acc: 0.6854\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.61085 to 0.60976, saving model to temp/a25\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6133 - acc: 0.6805 - val_loss: 0.6116 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.60976\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6116 - acc: 0.6836 - val_loss: 0.6098 - val_acc: 0.6879\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.60976\n",
      "Epoch 00013: early stopping\n",
      "temp/a26\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 155us/step - loss: 0.6618 - acc: 0.6077 - val_loss: 0.6381 - val_acc: 0.6300\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63805, saving model to temp/a26\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6374 - acc: 0.6486 - val_loss: 0.6403 - val_acc: 0.6699\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.63805\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6322 - acc: 0.6549 - val_loss: 0.6276 - val_acc: 0.6840\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63805 to 0.62764, saving model to temp/a26\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6271 - acc: 0.6635 - val_loss: 0.6251 - val_acc: 0.6850\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62764 to 0.62505, saving model to temp/a26\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6230 - acc: 0.6705 - val_loss: 0.6174 - val_acc: 0.6845\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62505 to 0.61735, saving model to temp/a26\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6197 - acc: 0.6762 - val_loss: 0.6218 - val_acc: 0.6816\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61735\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6178 - acc: 0.6779 - val_loss: 0.6157 - val_acc: 0.6644\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61735 to 0.61567, saving model to temp/a26\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6153 - acc: 0.6816 - val_loss: 0.6108 - val_acc: 0.6809\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61567 to 0.61085, saving model to temp/a26\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6143 - acc: 0.6783 - val_loss: 0.6117 - val_acc: 0.6921\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61085\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6143 - acc: 0.6808 - val_loss: 0.6126 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61085\n",
      "Epoch 00010: early stopping\n",
      "temp/a27\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 158us/step - loss: 0.6624 - acc: 0.6040 - val_loss: 0.6352 - val_acc: 0.6617\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63525, saving model to temp/a27\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6365 - acc: 0.6525 - val_loss: 0.6675 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.63525\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6324 - acc: 0.6541 - val_loss: 0.6239 - val_acc: 0.6773\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63525 to 0.62386, saving model to temp/a27\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6258 - acc: 0.6662 - val_loss: 0.6279 - val_acc: 0.6810\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62386\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6234 - acc: 0.6666 - val_loss: 0.6177 - val_acc: 0.6725\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62386 to 0.61769, saving model to temp/a27\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6200 - acc: 0.6756 - val_loss: 0.6213 - val_acc: 0.6826\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61769\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6184 - acc: 0.6735 - val_loss: 0.6176 - val_acc: 0.6909\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61769 to 0.61762, saving model to temp/a27\n",
      "Epoch 00007: early stopping\n",
      "temp/a28\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6634 - acc: 0.6056 - val_loss: 0.6460 - val_acc: 0.5945\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64600, saving model to temp/a28\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6382 - acc: 0.6499 - val_loss: 0.6293 - val_acc: 0.6382\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64600 to 0.62927, saving model to temp/a28\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6313 - acc: 0.6566 - val_loss: 0.6279 - val_acc: 0.6385\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62927 to 0.62787, saving model to temp/a28\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6284 - acc: 0.6601 - val_loss: 0.6210 - val_acc: 0.6789\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62787 to 0.62096, saving model to temp/a28\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6217 - acc: 0.6716 - val_loss: 0.6177 - val_acc: 0.6907\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62096 to 0.61774, saving model to temp/a28\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6202 - acc: 0.6747 - val_loss: 0.6362 - val_acc: 0.6473\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61774\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6178 - acc: 0.6790 - val_loss: 0.6140 - val_acc: 0.6853\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61774 to 0.61402, saving model to temp/a28\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6169 - acc: 0.6758 - val_loss: 0.6243 - val_acc: 0.6697\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61402\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6162 - acc: 0.6786 - val_loss: 0.6138 - val_acc: 0.6921\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61402 to 0.61381, saving model to temp/a28\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 143us/step - loss: 0.6148 - acc: 0.6796 - val_loss: 0.6113 - val_acc: 0.6946\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61381 to 0.61130, saving model to temp/a28\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6137 - acc: 0.6799 - val_loss: 0.6093 - val_acc: 0.6948\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.61130 to 0.60930, saving model to temp/a28\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6115 - acc: 0.6835 - val_loss: 0.6088 - val_acc: 0.6818\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.60930 to 0.60876, saving model to temp/a28\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6125 - acc: 0.6815 - val_loss: 0.6075 - val_acc: 0.6904\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.60876 to 0.60752, saving model to temp/a28\n",
      "Epoch 14/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6130 - acc: 0.6833 - val_loss: 0.6075 - val_acc: 0.6833\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.60752 to 0.60750, saving model to temp/a28\n",
      "Epoch 15/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6129 - acc: 0.6815 - val_loss: 0.6134 - val_acc: 0.6722\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.60750\n",
      "Epoch 00015: early stopping\n",
      "temp/a29\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 155us/step - loss: 0.6602 - acc: 0.6199 - val_loss: 0.6462 - val_acc: 0.6618\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64622, saving model to temp/a29\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6382 - acc: 0.6452 - val_loss: 0.6293 - val_acc: 0.6748\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64622 to 0.62933, saving model to temp/a29\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6322 - acc: 0.6582 - val_loss: 0.6242 - val_acc: 0.6820\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62933 to 0.62423, saving model to temp/a29\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6263 - acc: 0.6630 - val_loss: 0.6205 - val_acc: 0.6705\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62423 to 0.62051, saving model to temp/a29\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6244 - acc: 0.6707 - val_loss: 0.6165 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62051 to 0.61645, saving model to temp/a29\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6191 - acc: 0.6758 - val_loss: 0.6208 - val_acc: 0.6511\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61645\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6176 - acc: 0.6774 - val_loss: 0.6248 - val_acc: 0.6763\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61645\n",
      "Epoch 00007: early stopping\n",
      "temp/a30\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6612 - acc: 0.6065 - val_loss: 0.6542 - val_acc: 0.6464\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65425, saving model to temp/a30\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6350 - acc: 0.6542 - val_loss: 0.6266 - val_acc: 0.6709\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65425 to 0.62655, saving model to temp/a30\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6302 - acc: 0.6592 - val_loss: 0.6240 - val_acc: 0.6822\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62655 to 0.62404, saving model to temp/a30\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6272 - acc: 0.6661 - val_loss: 0.6202 - val_acc: 0.6894\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62404 to 0.62022, saving model to temp/a30\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6206 - acc: 0.6768 - val_loss: 0.6220 - val_acc: 0.6817\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62022\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6204 - acc: 0.6706 - val_loss: 0.6199 - val_acc: 0.6549\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62022 to 0.61993, saving model to temp/a30\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6171 - acc: 0.6766 - val_loss: 0.6121 - val_acc: 0.6844\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61993 to 0.61213, saving model to temp/a30\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6161 - acc: 0.6797 - val_loss: 0.6123 - val_acc: 0.6916\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61213\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6166 - acc: 0.6768 - val_loss: 0.6174 - val_acc: 0.6586\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61213\n",
      "Epoch 00009: early stopping\n",
      "temp/a31\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 155us/step - loss: 0.6617 - acc: 0.6152 - val_loss: 0.6369 - val_acc: 0.6515\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63691, saving model to temp/a31\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6382 - acc: 0.6464 - val_loss: 0.6311 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63691 to 0.63114, saving model to temp/a31\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6318 - acc: 0.6571 - val_loss: 0.6234 - val_acc: 0.6540\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63114 to 0.62340, saving model to temp/a31\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6264 - acc: 0.6654 - val_loss: 0.6251 - val_acc: 0.6849\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62340\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6221 - acc: 0.6698 - val_loss: 0.6154 - val_acc: 0.6826\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62340 to 0.61543, saving model to temp/a31\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6192 - acc: 0.6762 - val_loss: 0.6176 - val_acc: 0.6917\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61543\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6182 - acc: 0.6763 - val_loss: 0.6191 - val_acc: 0.6537\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61543\n",
      "Epoch 00007: early stopping\n",
      "temp/a32\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6594 - acc: 0.6114 - val_loss: 0.6363 - val_acc: 0.6266\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63633, saving model to temp/a32\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6389 - acc: 0.6433 - val_loss: 0.6317 - val_acc: 0.6808\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63633 to 0.63168, saving model to temp/a32\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6306 - acc: 0.6602 - val_loss: 0.6239 - val_acc: 0.6858\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63168 to 0.62386, saving model to temp/a32\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6259 - acc: 0.6680 - val_loss: 0.6192 - val_acc: 0.6730\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62386 to 0.61916, saving model to temp/a32\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6212 - acc: 0.6698 - val_loss: 0.6231 - val_acc: 0.6802\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.61916\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6205 - acc: 0.6736 - val_loss: 0.6171 - val_acc: 0.6637\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61916 to 0.61710, saving model to temp/a32\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6180 - acc: 0.6776 - val_loss: 0.6180 - val_acc: 0.6896\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61710\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6151 - acc: 0.6826 - val_loss: 0.6164 - val_acc: 0.6847\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61710 to 0.61643, saving model to temp/a32\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6159 - acc: 0.6775 - val_loss: 0.6112 - val_acc: 0.6913\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61643 to 0.61122, saving model to temp/a32\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6136 - acc: 0.6814 - val_loss: 0.6099 - val_acc: 0.6869\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61122 to 0.60990, saving model to temp/a32\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6132 - acc: 0.6819 - val_loss: 0.6113 - val_acc: 0.6812\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.60990\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6144 - acc: 0.6789 - val_loss: 0.6085 - val_acc: 0.6937\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.60990 to 0.60852, saving model to temp/a32\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6141 - acc: 0.6791 - val_loss: 0.6134 - val_acc: 0.6694\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.60852\n",
      "Epoch 14/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6137 - acc: 0.6786 - val_loss: 0.6085 - val_acc: 0.6939\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.60852 to 0.60851, saving model to temp/a32\n",
      "Epoch 00014: early stopping\n",
      "temp/a33\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 159us/step - loss: 0.6625 - acc: 0.6100 - val_loss: 0.6383 - val_acc: 0.6684\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63833, saving model to temp/a33\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6369 - acc: 0.6498 - val_loss: 0.6298 - val_acc: 0.6813\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63833 to 0.62982, saving model to temp/a33\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6304 - acc: 0.6576 - val_loss: 0.6505 - val_acc: 0.6315\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.62982\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6245 - acc: 0.6682 - val_loss: 0.6206 - val_acc: 0.6588\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62982 to 0.62059, saving model to temp/a33\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6210 - acc: 0.6723 - val_loss: 0.6195 - val_acc: 0.6907\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62059 to 0.61950, saving model to temp/a33\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6191 - acc: 0.6749 - val_loss: 0.6147 - val_acc: 0.6920\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61950 to 0.61472, saving model to temp/a33\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6176 - acc: 0.6777 - val_loss: 0.6133 - val_acc: 0.6814\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61472 to 0.61326, saving model to temp/a33\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6161 - acc: 0.6796 - val_loss: 0.6130 - val_acc: 0.6912\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61326 to 0.61303, saving model to temp/a33\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6132 - acc: 0.6818 - val_loss: 0.6199 - val_acc: 0.6789\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61303\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6133 - acc: 0.6805 - val_loss: 0.6090 - val_acc: 0.6941\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61303 to 0.60903, saving model to temp/a33\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6132 - acc: 0.6786 - val_loss: 0.6080 - val_acc: 0.6835\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.60903 to 0.60805, saving model to temp/a33\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6141 - acc: 0.6788 - val_loss: 0.6096 - val_acc: 0.6935\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.60805\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6146 - acc: 0.6788 - val_loss: 0.6081 - val_acc: 0.6924\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.60805\n",
      "Epoch 00013: early stopping\n",
      "temp/a34\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6603 - acc: 0.6169 - val_loss: 0.6700 - val_acc: 0.5557\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66997, saving model to temp/a34\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6368 - acc: 0.6476 - val_loss: 0.6292 - val_acc: 0.6726\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.66997 to 0.62915, saving model to temp/a34\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6314 - acc: 0.6570 - val_loss: 0.6318 - val_acc: 0.6242\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.62915\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6267 - acc: 0.6642 - val_loss: 0.6247 - val_acc: 0.6560\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62915 to 0.62467, saving model to temp/a34\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6219 - acc: 0.6715 - val_loss: 0.6317 - val_acc: 0.6596\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62467\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6208 - acc: 0.6753 - val_loss: 0.6351 - val_acc: 0.6300\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.62467\n",
      "Epoch 00006: early stopping\n",
      "temp/a35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 158us/step - loss: 0.6646 - acc: 0.5991 - val_loss: 0.6423 - val_acc: 0.6108\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64228, saving model to temp/a35\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6383 - acc: 0.6443 - val_loss: 0.6459 - val_acc: 0.5944\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.64228\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6292 - acc: 0.6579 - val_loss: 0.6226 - val_acc: 0.6632\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.64228 to 0.62264, saving model to temp/a35\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6238 - acc: 0.6689 - val_loss: 0.6197 - val_acc: 0.6624\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62264 to 0.61966, saving model to temp/a35\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6218 - acc: 0.6726 - val_loss: 0.6160 - val_acc: 0.6863\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.61966 to 0.61603, saving model to temp/a35\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6184 - acc: 0.6771 - val_loss: 0.6160 - val_acc: 0.6913\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61603 to 0.61597, saving model to temp/a35\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6184 - acc: 0.6762 - val_loss: 0.6117 - val_acc: 0.6867\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61597 to 0.61168, saving model to temp/a35\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6174 - acc: 0.6762 - val_loss: 0.6164 - val_acc: 0.6861\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61168\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6153 - acc: 0.6785 - val_loss: 0.6202 - val_acc: 0.6802\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61168\n",
      "Epoch 00009: early stopping\n",
      "temp/a36\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6620 - acc: 0.6105 - val_loss: 0.6372 - val_acc: 0.6420\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63717, saving model to temp/a36\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6357 - acc: 0.6502 - val_loss: 0.6274 - val_acc: 0.6516\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63717 to 0.62744, saving model to temp/a36\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6289 - acc: 0.6637 - val_loss: 0.6257 - val_acc: 0.6467\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62744 to 0.62572, saving model to temp/a36\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6267 - acc: 0.6636 - val_loss: 0.6508 - val_acc: 0.5973\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62572\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6237 - acc: 0.6711 - val_loss: 0.6172 - val_acc: 0.6688\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62572 to 0.61723, saving model to temp/a36\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6199 - acc: 0.6751 - val_loss: 0.6180 - val_acc: 0.6638\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61723\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6167 - acc: 0.6775 - val_loss: 0.6126 - val_acc: 0.6897\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61723 to 0.61262, saving model to temp/a36\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6166 - acc: 0.6806 - val_loss: 0.6301 - val_acc: 0.6555\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61262\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6155 - acc: 0.6814 - val_loss: 0.6289 - val_acc: 0.6416\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61262\n",
      "Epoch 00009: early stopping\n",
      "temp/a37\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 152us/step - loss: 0.6660 - acc: 0.5911 - val_loss: 0.6432 - val_acc: 0.6671\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64320, saving model to temp/a37\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6362 - acc: 0.6487 - val_loss: 0.6310 - val_acc: 0.6332\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64320 to 0.63102, saving model to temp/a37\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6282 - acc: 0.6581 - val_loss: 0.6238 - val_acc: 0.6847\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63102 to 0.62380, saving model to temp/a37\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6247 - acc: 0.6704 - val_loss: 0.6199 - val_acc: 0.6881\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62380 to 0.61992, saving model to temp/a37\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6222 - acc: 0.6740 - val_loss: 0.6163 - val_acc: 0.6752\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.61992 to 0.61629, saving model to temp/a37\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6204 - acc: 0.6709 - val_loss: 0.6159 - val_acc: 0.6849\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61629 to 0.61587, saving model to temp/a37\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6199 - acc: 0.6737 - val_loss: 0.6212 - val_acc: 0.6508\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61587\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6166 - acc: 0.6757 - val_loss: 0.6104 - val_acc: 0.6863\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61587 to 0.61042, saving model to temp/a37\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6134 - acc: 0.6808 - val_loss: 0.6140 - val_acc: 0.6888\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61042\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6132 - acc: 0.6827 - val_loss: 0.6128 - val_acc: 0.6882\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61042\n",
      "Epoch 00010: early stopping\n",
      "temp/a38\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 155us/step - loss: 0.6627 - acc: 0.6094 - val_loss: 0.6410 - val_acc: 0.6687\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64101, saving model to temp/a38\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6360 - acc: 0.6510 - val_loss: 0.6317 - val_acc: 0.6820\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64101 to 0.63174, saving model to temp/a38\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6286 - acc: 0.6650 - val_loss: 0.6249 - val_acc: 0.6835\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63174 to 0.62491, saving model to temp/a38\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6259 - acc: 0.6688 - val_loss: 0.6210 - val_acc: 0.6884\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62491 to 0.62103, saving model to temp/a38\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6229 - acc: 0.6699 - val_loss: 0.6192 - val_acc: 0.6632\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62103 to 0.61920, saving model to temp/a38\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6196 - acc: 0.6743 - val_loss: 0.6237 - val_acc: 0.6779\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61920\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6196 - acc: 0.6725 - val_loss: 0.6169 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61920 to 0.61688, saving model to temp/a38\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6160 - acc: 0.6793 - val_loss: 0.6114 - val_acc: 0.6867\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61688 to 0.61143, saving model to temp/a38\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6161 - acc: 0.6783 - val_loss: 0.6127 - val_acc: 0.6889\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61143\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6160 - acc: 0.6764 - val_loss: 0.6140 - val_acc: 0.6880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61143\n",
      "Epoch 00010: early stopping\n",
      "temp/a39\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6637 - acc: 0.6047 - val_loss: 0.6383 - val_acc: 0.6582\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63828, saving model to temp/a39\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6381 - acc: 0.6484 - val_loss: 0.6279 - val_acc: 0.6769\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63828 to 0.62790, saving model to temp/a39\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6292 - acc: 0.6629 - val_loss: 0.6223 - val_acc: 0.6733\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62790 to 0.62225, saving model to temp/a39\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6251 - acc: 0.6681 - val_loss: 0.6395 - val_acc: 0.6555\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62225\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6225 - acc: 0.6694 - val_loss: 0.6285 - val_acc: 0.6337\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62225\n",
      "Epoch 00005: early stopping\n",
      "temp/a40\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6622 - acc: 0.6037 - val_loss: 0.6371 - val_acc: 0.6261\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63714, saving model to temp/a40\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6348 - acc: 0.6488 - val_loss: 0.6281 - val_acc: 0.6804\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63714 to 0.62805, saving model to temp/a40\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6294 - acc: 0.6622 - val_loss: 0.6304 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.62805\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6255 - acc: 0.6659 - val_loss: 0.6220 - val_acc: 0.6877\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62805 to 0.62201, saving model to temp/a40\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6212 - acc: 0.6746 - val_loss: 0.6169 - val_acc: 0.6822\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62201 to 0.61687, saving model to temp/a40\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6218 - acc: 0.6724 - val_loss: 0.6394 - val_acc: 0.6422\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61687\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 150us/step - loss: 0.6185 - acc: 0.6730 - val_loss: 0.6270 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61687\n",
      "Epoch 00007: early stopping\n",
      "temp/a41\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 153us/step - loss: 0.6621 - acc: 0.6090 - val_loss: 0.6380 - val_acc: 0.6678\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63797, saving model to temp/a41\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6369 - acc: 0.6460 - val_loss: 0.6288 - val_acc: 0.6521\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63797 to 0.62882, saving model to temp/a41\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6289 - acc: 0.6607 - val_loss: 0.6241 - val_acc: 0.6887\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62882 to 0.62414, saving model to temp/a41\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6260 - acc: 0.6651 - val_loss: 0.6207 - val_acc: 0.6686\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62414 to 0.62066, saving model to temp/a41\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6230 - acc: 0.6673 - val_loss: 0.6186 - val_acc: 0.6645\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62066 to 0.61862, saving model to temp/a41\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6193 - acc: 0.6762 - val_loss: 0.6209 - val_acc: 0.6549\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61862\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6183 - acc: 0.6733 - val_loss: 0.6289 - val_acc: 0.6399\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61862\n",
      "Epoch 00007: early stopping\n",
      "temp/a42\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 161us/step - loss: 0.6627 - acc: 0.6105 - val_loss: 0.6465 - val_acc: 0.5957\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64651, saving model to temp/a42\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6375 - acc: 0.6470 - val_loss: 0.6299 - val_acc: 0.6817\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64651 to 0.62991, saving model to temp/a42\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6297 - acc: 0.6618 - val_loss: 0.6236 - val_acc: 0.6596\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62991 to 0.62357, saving model to temp/a42\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6258 - acc: 0.6672 - val_loss: 0.6218 - val_acc: 0.6564\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62357 to 0.62185, saving model to temp/a42\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6223 - acc: 0.6694 - val_loss: 0.6185 - val_acc: 0.6632\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62185 to 0.61854, saving model to temp/a42\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6205 - acc: 0.6737 - val_loss: 0.6167 - val_acc: 0.6905\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61854 to 0.61673, saving model to temp/a42\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6190 - acc: 0.6755 - val_loss: 0.6157 - val_acc: 0.6921\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61673 to 0.61567, saving model to temp/a42\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6166 - acc: 0.6775 - val_loss: 0.6119 - val_acc: 0.6951\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61567 to 0.61191, saving model to temp/a42\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 143us/step - loss: 0.6169 - acc: 0.6752 - val_loss: 0.6104 - val_acc: 0.6857\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61191 to 0.61038, saving model to temp/a42\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6129 - acc: 0.6828 - val_loss: 0.6098 - val_acc: 0.6935\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61038 to 0.60980, saving model to temp/a42\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6133 - acc: 0.6811 - val_loss: 0.6107 - val_acc: 0.6941\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.60980\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6129 - acc: 0.6813 - val_loss: 0.6101 - val_acc: 0.6929\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.60980\n",
      "Epoch 00012: early stopping\n",
      "temp/a43\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 155us/step - loss: 0.6622 - acc: 0.6058 - val_loss: 0.6352 - val_acc: 0.6508\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63519, saving model to temp/a43\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6367 - acc: 0.6521 - val_loss: 0.6385 - val_acc: 0.6697\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.63519\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 143us/step - loss: 0.6290 - acc: 0.6566 - val_loss: 0.6246 - val_acc: 0.6864\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63519 to 0.62460, saving model to temp/a43\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6274 - acc: 0.6636 - val_loss: 0.6316 - val_acc: 0.6702\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62460\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6225 - acc: 0.6731 - val_loss: 0.6186 - val_acc: 0.6830\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62460 to 0.61856, saving model to temp/a43\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6201 - acc: 0.6722 - val_loss: 0.6135 - val_acc: 0.6918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_loss improved from 0.61856 to 0.61352, saving model to temp/a43\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6181 - acc: 0.6764 - val_loss: 0.6164 - val_acc: 0.6626\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61352\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6168 - acc: 0.6786 - val_loss: 0.6121 - val_acc: 0.6922\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61352 to 0.61213, saving model to temp/a43\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6149 - acc: 0.6789 - val_loss: 0.6122 - val_acc: 0.6803\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61213\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6146 - acc: 0.6778 - val_loss: 0.6096 - val_acc: 0.6925\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61213 to 0.60965, saving model to temp/a43\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6126 - acc: 0.6809 - val_loss: 0.6100 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.60965\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6131 - acc: 0.6806 - val_loss: 0.6128 - val_acc: 0.6690\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.60965\n",
      "Epoch 00012: early stopping\n",
      "temp/a44\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6647 - acc: 0.6064 - val_loss: 0.6375 - val_acc: 0.6434\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63750, saving model to temp/a44\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6389 - acc: 0.6464 - val_loss: 0.6328 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63750 to 0.63279, saving model to temp/a44\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6301 - acc: 0.6603 - val_loss: 0.6434 - val_acc: 0.6475\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.63279\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6244 - acc: 0.6688 - val_loss: 0.6188 - val_acc: 0.6761\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.63279 to 0.61883, saving model to temp/a44\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6215 - acc: 0.6723 - val_loss: 0.6160 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.61883 to 0.61597, saving model to temp/a44\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6207 - acc: 0.6719 - val_loss: 0.6159 - val_acc: 0.6912\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61597 to 0.61595, saving model to temp/a44\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6181 - acc: 0.6748 - val_loss: 0.6118 - val_acc: 0.6876\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61595 to 0.61180, saving model to temp/a44\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6169 - acc: 0.6752 - val_loss: 0.6106 - val_acc: 0.6801\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61180 to 0.61060, saving model to temp/a44\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6152 - acc: 0.6791 - val_loss: 0.6137 - val_acc: 0.6921\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61060\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6153 - acc: 0.6745 - val_loss: 0.6142 - val_acc: 0.6890\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61060\n",
      "Epoch 00010: early stopping\n",
      "temp/a45\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 155us/step - loss: 0.6641 - acc: 0.6011 - val_loss: 0.6407 - val_acc: 0.6660\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64066, saving model to temp/a45\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6352 - acc: 0.6543 - val_loss: 0.6277 - val_acc: 0.6559\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64066 to 0.62774, saving model to temp/a45\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6302 - acc: 0.6597 - val_loss: 0.6487 - val_acc: 0.6514\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.62774\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6253 - acc: 0.6671 - val_loss: 0.6247 - val_acc: 0.6562\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62774 to 0.62466, saving model to temp/a45\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6238 - acc: 0.6692 - val_loss: 0.6204 - val_acc: 0.6868\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62466 to 0.62040, saving model to temp/a45\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 2s 141us/step - loss: 0.6210 - acc: 0.6700 - val_loss: 0.6149 - val_acc: 0.6913\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62040 to 0.61488, saving model to temp/a45\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6172 - acc: 0.6787 - val_loss: 0.6139 - val_acc: 0.6924\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61488 to 0.61389, saving model to temp/a45\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6172 - acc: 0.6738 - val_loss: 0.6130 - val_acc: 0.6748\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61389 to 0.61300, saving model to temp/a45\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6154 - acc: 0.6814 - val_loss: 0.6233 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61300\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6143 - acc: 0.6804 - val_loss: 0.6113 - val_acc: 0.6742\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61300 to 0.61127, saving model to temp/a45\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6137 - acc: 0.6805 - val_loss: 0.6093 - val_acc: 0.6898\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.61127 to 0.60929, saving model to temp/a45\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6110 - acc: 0.6823 - val_loss: 0.6087 - val_acc: 0.6802\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.60929 to 0.60872, saving model to temp/a45\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6135 - acc: 0.6792 - val_loss: 0.6074 - val_acc: 0.6935\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.60872 to 0.60739, saving model to temp/a45\n",
      "Epoch 14/20\n",
      "17748/17748 [==============================] - 3s 142us/step - loss: 0.6125 - acc: 0.6831 - val_loss: 0.6116 - val_acc: 0.6717\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.60739\n",
      "Epoch 15/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6121 - acc: 0.6822 - val_loss: 0.6086 - val_acc: 0.6898\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.60739\n",
      "Epoch 00015: early stopping\n",
      "temp/a46\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6623 - acc: 0.6083 - val_loss: 0.6357 - val_acc: 0.6558\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63574, saving model to temp/a46\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6363 - acc: 0.6506 - val_loss: 0.6274 - val_acc: 0.6668\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63574 to 0.62739, saving model to temp/a46\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6293 - acc: 0.6609 - val_loss: 0.6276 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.62739\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6271 - acc: 0.6659 - val_loss: 0.6205 - val_acc: 0.6902\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62739 to 0.62052, saving model to temp/a46\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6234 - acc: 0.6688 - val_loss: 0.6187 - val_acc: 0.6809\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62052 to 0.61871, saving model to temp/a46\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 142us/step - loss: 0.6210 - acc: 0.6702 - val_loss: 0.6165 - val_acc: 0.6930\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61871 to 0.61653, saving model to temp/a46\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6176 - acc: 0.6791 - val_loss: 0.6152 - val_acc: 0.6709\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61653 to 0.61515, saving model to temp/a46\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6157 - acc: 0.6827 - val_loss: 0.6113 - val_acc: 0.6784\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61515 to 0.61129, saving model to temp/a46\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6151 - acc: 0.6773 - val_loss: 0.6101 - val_acc: 0.6840\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61129 to 0.61015, saving model to temp/a46\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6133 - acc: 0.6792 - val_loss: 0.6096 - val_acc: 0.6909\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61015 to 0.60962, saving model to temp/a46\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6135 - acc: 0.6802 - val_loss: 0.6084 - val_acc: 0.6912\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.60962 to 0.60843, saving model to temp/a46\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6121 - acc: 0.6820 - val_loss: 0.6083 - val_acc: 0.6919\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.60843 to 0.60834, saving model to temp/a46\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6136 - acc: 0.6797 - val_loss: 0.6103 - val_acc: 0.6734\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.60834\n",
      "Epoch 00013: early stopping\n",
      "temp/a47\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 152us/step - loss: 0.6609 - acc: 0.6091 - val_loss: 0.6387 - val_acc: 0.6200\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63872, saving model to temp/a47\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6384 - acc: 0.6406 - val_loss: 0.6271 - val_acc: 0.6563\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63872 to 0.62713, saving model to temp/a47\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6315 - acc: 0.6531 - val_loss: 0.6243 - val_acc: 0.6556\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62713 to 0.62428, saving model to temp/a47\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6245 - acc: 0.6686 - val_loss: 0.6241 - val_acc: 0.6877\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62428 to 0.62414, saving model to temp/a47\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6218 - acc: 0.6728 - val_loss: 0.6202 - val_acc: 0.6581\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62414 to 0.62019, saving model to temp/a47\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6177 - acc: 0.6797 - val_loss: 0.6163 - val_acc: 0.6718\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62019 to 0.61634, saving model to temp/a47\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6177 - acc: 0.6757 - val_loss: 0.6134 - val_acc: 0.6762\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61634 to 0.61342, saving model to temp/a47\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6161 - acc: 0.6795 - val_loss: 0.6121 - val_acc: 0.6948\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61342 to 0.61214, saving model to temp/a47\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6147 - acc: 0.6797 - val_loss: 0.6118 - val_acc: 0.6960\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61214 to 0.61177, saving model to temp/a47\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6143 - acc: 0.6814 - val_loss: 0.6219 - val_acc: 0.6760\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61177\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6153 - acc: 0.6802 - val_loss: 0.6229 - val_acc: 0.6495\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.61177\n",
      "Epoch 00011: early stopping\n",
      "temp/a48\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6665 - acc: 0.5966 - val_loss: 0.6429 - val_acc: 0.6093\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64285, saving model to temp/a48\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6360 - acc: 0.6487 - val_loss: 0.6265 - val_acc: 0.6628\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64285 to 0.62647, saving model to temp/a48\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6289 - acc: 0.6598 - val_loss: 0.6225 - val_acc: 0.6820\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62647 to 0.62251, saving model to temp/a48\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6239 - acc: 0.6723 - val_loss: 0.6193 - val_acc: 0.6885\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62251 to 0.61934, saving model to temp/a48\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6227 - acc: 0.6700 - val_loss: 0.6166 - val_acc: 0.6906\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.61934 to 0.61660, saving model to temp/a48\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6190 - acc: 0.6763 - val_loss: 0.6189 - val_acc: 0.6595\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61660\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6173 - acc: 0.6784 - val_loss: 0.6142 - val_acc: 0.6750\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61660 to 0.61418, saving model to temp/a48\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6168 - acc: 0.6762 - val_loss: 0.6146 - val_acc: 0.6924\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61418\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6171 - acc: 0.6768 - val_loss: 0.6142 - val_acc: 0.6624\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61418 to 0.61415, saving model to temp/a48\n",
      "Epoch 00009: early stopping\n",
      "temp/a49\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6631 - acc: 0.6077 - val_loss: 0.6371 - val_acc: 0.6727\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63708, saving model to temp/a49\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6365 - acc: 0.6475 - val_loss: 0.6273 - val_acc: 0.6786\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63708 to 0.62730, saving model to temp/a49\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6303 - acc: 0.6601 - val_loss: 0.6245 - val_acc: 0.6755\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62730 to 0.62446, saving model to temp/a49\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 143us/step - loss: 0.6256 - acc: 0.6658 - val_loss: 0.6197 - val_acc: 0.6866\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62446 to 0.61968, saving model to temp/a49\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6217 - acc: 0.6728 - val_loss: 0.6188 - val_acc: 0.6597\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.61968 to 0.61881, saving model to temp/a49\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6197 - acc: 0.6769 - val_loss: 0.6171 - val_acc: 0.6609\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61881 to 0.61707, saving model to temp/a49\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6187 - acc: 0.6762 - val_loss: 0.6134 - val_acc: 0.6899\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61707 to 0.61336, saving model to temp/a49\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6172 - acc: 0.6759 - val_loss: 0.6110 - val_acc: 0.6916\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61336 to 0.61102, saving model to temp/a49\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6166 - acc: 0.6779 - val_loss: 0.6143 - val_acc: 0.6656\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61102\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6135 - acc: 0.6807 - val_loss: 0.6095 - val_acc: 0.6798\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61102 to 0.60947, saving model to temp/a49\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6117 - acc: 0.6841 - val_loss: 0.6071 - val_acc: 0.6940\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.60947 to 0.60707, saving model to temp/a49\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6125 - acc: 0.6828 - val_loss: 0.6083 - val_acc: 0.6867\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.60707\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6113 - acc: 0.6817 - val_loss: 0.6164 - val_acc: 0.6841\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.60707\n",
      "Epoch 00013: early stopping\n",
      "temp/a50\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6638 - acc: 0.6013 - val_loss: 0.6383 - val_acc: 0.6566\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63828, saving model to temp/a50\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 142us/step - loss: 0.6384 - acc: 0.6426 - val_loss: 0.6326 - val_acc: 0.6822\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63828 to 0.63259, saving model to temp/a50\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6309 - acc: 0.6584 - val_loss: 0.6237 - val_acc: 0.6814\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63259 to 0.62365, saving model to temp/a50\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6264 - acc: 0.6644 - val_loss: 0.6265 - val_acc: 0.6801\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62365\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6229 - acc: 0.6716 - val_loss: 0.6181 - val_acc: 0.6919\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62365 to 0.61807, saving model to temp/a50\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6202 - acc: 0.6727 - val_loss: 0.6146 - val_acc: 0.6759\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61807 to 0.61455, saving model to temp/a50\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6188 - acc: 0.6759 - val_loss: 0.6142 - val_acc: 0.6920\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61455 to 0.61416, saving model to temp/a50\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6149 - acc: 0.6804 - val_loss: 0.6138 - val_acc: 0.6697\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61416 to 0.61385, saving model to temp/a50\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6146 - acc: 0.6793 - val_loss: 0.6238 - val_acc: 0.6500\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61385\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6150 - acc: 0.6786 - val_loss: 0.6127 - val_acc: 0.6922\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61385 to 0.61268, saving model to temp/a50\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 2s 137us/step - loss: 0.6141 - acc: 0.6808 - val_loss: 0.6149 - val_acc: 0.6849\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.61268\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6123 - acc: 0.6826 - val_loss: 0.6170 - val_acc: 0.6832\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.61268\n",
      "Epoch 00012: early stopping\n",
      "temp/a51\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6618 - acc: 0.6122 - val_loss: 0.6343 - val_acc: 0.6660\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63427, saving model to temp/a51\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6370 - acc: 0.6490 - val_loss: 0.6294 - val_acc: 0.6811\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63427 to 0.62944, saving model to temp/a51\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6283 - acc: 0.6644 - val_loss: 0.6262 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62944 to 0.62623, saving model to temp/a51\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6256 - acc: 0.6670 - val_loss: 0.6197 - val_acc: 0.6746\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62623 to 0.61967, saving model to temp/a51\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6226 - acc: 0.6703 - val_loss: 0.6216 - val_acc: 0.6837\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.61967\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6197 - acc: 0.6747 - val_loss: 0.6137 - val_acc: 0.6858\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61967 to 0.61374, saving model to temp/a51\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6162 - acc: 0.6790 - val_loss: 0.6170 - val_acc: 0.6619\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61374\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6154 - acc: 0.6797 - val_loss: 0.6123 - val_acc: 0.6939\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61374 to 0.61232, saving model to temp/a51\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6138 - acc: 0.6803 - val_loss: 0.6157 - val_acc: 0.6823\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61232\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6136 - acc: 0.6797 - val_loss: 0.6079 - val_acc: 0.6851\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61232 to 0.60789, saving model to temp/a51\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6140 - acc: 0.6802 - val_loss: 0.6079 - val_acc: 0.6941\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.60789\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6130 - acc: 0.6808 - val_loss: 0.6121 - val_acc: 0.6928\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.60789\n",
      "Epoch 00012: early stopping\n",
      "temp/a52\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6641 - acc: 0.6019 - val_loss: 0.6359 - val_acc: 0.6491\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63594, saving model to temp/a52\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6352 - acc: 0.6494 - val_loss: 0.6285 - val_acc: 0.6654\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63594 to 0.62854, saving model to temp/a52\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6313 - acc: 0.6573 - val_loss: 0.6280 - val_acc: 0.6404\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62854 to 0.62798, saving model to temp/a52\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6271 - acc: 0.6648 - val_loss: 0.6216 - val_acc: 0.6912\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62798 to 0.62162, saving model to temp/a52\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6227 - acc: 0.6702 - val_loss: 0.6340 - val_acc: 0.6284\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62162\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6200 - acc: 0.6733 - val_loss: 0.6169 - val_acc: 0.6920\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62162 to 0.61691, saving model to temp/a52\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6182 - acc: 0.6755 - val_loss: 0.6128 - val_acc: 0.6879\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61691 to 0.61278, saving model to temp/a52\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6164 - acc: 0.6799 - val_loss: 0.6138 - val_acc: 0.6729\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61278\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6171 - acc: 0.6756 - val_loss: 0.6101 - val_acc: 0.6859\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61278 to 0.61013, saving model to temp/a52\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6134 - acc: 0.6802 - val_loss: 0.6245 - val_acc: 0.6713\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61013\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6131 - acc: 0.6828 - val_loss: 0.6139 - val_acc: 0.6672\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.61013\n",
      "Epoch 00011: early stopping\n",
      "temp/a53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6629 - acc: 0.6066 - val_loss: 0.6365 - val_acc: 0.6551\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63653, saving model to temp/a53\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6360 - acc: 0.6488 - val_loss: 0.6282 - val_acc: 0.6813\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63653 to 0.62818, saving model to temp/a53\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6305 - acc: 0.6560 - val_loss: 0.6336 - val_acc: 0.6248\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.62818\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6263 - acc: 0.6644 - val_loss: 0.6186 - val_acc: 0.6785\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62818 to 0.61855, saving model to temp/a53\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6232 - acc: 0.6687 - val_loss: 0.6266 - val_acc: 0.6823\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.61855\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6205 - acc: 0.6771 - val_loss: 0.6141 - val_acc: 0.6892\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61855 to 0.61412, saving model to temp/a53\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6184 - acc: 0.6760 - val_loss: 0.6186 - val_acc: 0.6555\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61412\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6164 - acc: 0.6792 - val_loss: 0.6158 - val_acc: 0.6909\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61412\n",
      "Epoch 00008: early stopping\n",
      "temp/a54\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 155us/step - loss: 0.6626 - acc: 0.6087 - val_loss: 0.6427 - val_acc: 0.6651\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64265, saving model to temp/a54\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6366 - acc: 0.6477 - val_loss: 0.6271 - val_acc: 0.6751\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64265 to 0.62712, saving model to temp/a54\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6293 - acc: 0.6594 - val_loss: 0.6218 - val_acc: 0.6692\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62712 to 0.62179, saving model to temp/a54\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6257 - acc: 0.6667 - val_loss: 0.6212 - val_acc: 0.6888\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62179 to 0.62124, saving model to temp/a54\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6215 - acc: 0.6729 - val_loss: 0.6242 - val_acc: 0.6779\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62124\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6214 - acc: 0.6705 - val_loss: 0.6152 - val_acc: 0.6909\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62124 to 0.61519, saving model to temp/a54\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6161 - acc: 0.6794 - val_loss: 0.6134 - val_acc: 0.6934\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61519 to 0.61345, saving model to temp/a54\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6145 - acc: 0.6788 - val_loss: 0.6149 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61345\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6152 - acc: 0.6777 - val_loss: 0.6127 - val_acc: 0.6918\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61345 to 0.61269, saving model to temp/a54\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6140 - acc: 0.6808 - val_loss: 0.6119 - val_acc: 0.6926\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61269 to 0.61192, saving model to temp/a54\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 151us/step - loss: 0.6123 - acc: 0.6819 - val_loss: 0.6096 - val_acc: 0.6767\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.61192 to 0.60962, saving model to temp/a54\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6121 - acc: 0.6820 - val_loss: 0.6086 - val_acc: 0.6820\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.60962 to 0.60863, saving model to temp/a54\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6119 - acc: 0.6822 - val_loss: 0.6116 - val_acc: 0.6912\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.60863\n",
      "Epoch 14/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6118 - acc: 0.6829 - val_loss: 0.6073 - val_acc: 0.6943\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.60863 to 0.60728, saving model to temp/a54\n",
      "Epoch 15/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6127 - acc: 0.6813 - val_loss: 0.6114 - val_acc: 0.6901\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.60728\n",
      "Epoch 16/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6112 - acc: 0.6824 - val_loss: 0.6152 - val_acc: 0.6830\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.60728\n",
      "Epoch 00016: early stopping\n",
      "temp/a55\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 158us/step - loss: 0.6631 - acc: 0.6068 - val_loss: 0.6349 - val_acc: 0.6428\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63491, saving model to temp/a55\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6362 - acc: 0.6510 - val_loss: 0.6317 - val_acc: 0.6796\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63491 to 0.63169, saving model to temp/a55\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6308 - acc: 0.6588 - val_loss: 0.6293 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63169 to 0.62929, saving model to temp/a55\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6253 - acc: 0.6651 - val_loss: 0.6204 - val_acc: 0.6799\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62929 to 0.62039, saving model to temp/a55\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6221 - acc: 0.6706 - val_loss: 0.6225 - val_acc: 0.6508\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62039\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6209 - acc: 0.6751 - val_loss: 0.6242 - val_acc: 0.6447\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.62039\n",
      "Epoch 00006: early stopping\n",
      "temp/a56\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6626 - acc: 0.6127 - val_loss: 0.6356 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63564, saving model to temp/a56\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6385 - acc: 0.6455 - val_loss: 0.6318 - val_acc: 0.6805\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63564 to 0.63180, saving model to temp/a56\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6305 - acc: 0.6581 - val_loss: 0.6351 - val_acc: 0.6664\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.63180\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6258 - acc: 0.6623 - val_loss: 0.6340 - val_acc: 0.6235\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.63180\n",
      "Epoch 00004: early stopping\n",
      "temp/a57\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 155us/step - loss: 0.6611 - acc: 0.6077 - val_loss: 0.6499 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64987, saving model to temp/a57\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6367 - acc: 0.6465 - val_loss: 0.6372 - val_acc: 0.6718\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64987 to 0.63718, saving model to temp/a57\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6298 - acc: 0.6633 - val_loss: 0.6321 - val_acc: 0.6757\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63718 to 0.63208, saving model to temp/a57\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6268 - acc: 0.6660 - val_loss: 0.6184 - val_acc: 0.6848\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.63208 to 0.61841, saving model to temp/a57\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6231 - acc: 0.6680 - val_loss: 0.6160 - val_acc: 0.6774\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.61841 to 0.61602, saving model to temp/a57\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6191 - acc: 0.6744 - val_loss: 0.6179 - val_acc: 0.6659\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61602\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6182 - acc: 0.6769 - val_loss: 0.6242 - val_acc: 0.6429\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61602\n",
      "Epoch 00007: early stopping\n",
      "temp/a58\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6611 - acc: 0.6102 - val_loss: 0.6399 - val_acc: 0.6145\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63986, saving model to temp/a58\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6361 - acc: 0.6503 - val_loss: 0.6333 - val_acc: 0.6800\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63986 to 0.63335, saving model to temp/a58\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6304 - acc: 0.6615 - val_loss: 0.6227 - val_acc: 0.6699\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63335 to 0.62273, saving model to temp/a58\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 143us/step - loss: 0.6257 - acc: 0.6645 - val_loss: 0.6193 - val_acc: 0.6747\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62273 to 0.61928, saving model to temp/a58\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6223 - acc: 0.6672 - val_loss: 0.6193 - val_acc: 0.6570\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.61928\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6199 - acc: 0.6745 - val_loss: 0.6138 - val_acc: 0.6860\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61928 to 0.61383, saving model to temp/a58\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6178 - acc: 0.6753 - val_loss: 0.6214 - val_acc: 0.6759\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61383\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6171 - acc: 0.6771 - val_loss: 0.6251 - val_acc: 0.6418\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61383\n",
      "Epoch 00008: early stopping\n",
      "temp/a59\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6606 - acc: 0.6106 - val_loss: 0.6365 - val_acc: 0.6669\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63649, saving model to temp/a59\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6370 - acc: 0.6484 - val_loss: 0.6285 - val_acc: 0.6772\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63649 to 0.62853, saving model to temp/a59\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6296 - acc: 0.6605 - val_loss: 0.6235 - val_acc: 0.6567\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62853 to 0.62351, saving model to temp/a59\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6254 - acc: 0.6660 - val_loss: 0.6198 - val_acc: 0.6899\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62351 to 0.61985, saving model to temp/a59\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6215 - acc: 0.6721 - val_loss: 0.6211 - val_acc: 0.6574\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.61985\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6205 - acc: 0.6728 - val_loss: 0.6178 - val_acc: 0.6870\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61985 to 0.61780, saving model to temp/a59\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6167 - acc: 0.6764 - val_loss: 0.6169 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61780 to 0.61692, saving model to temp/a59\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6158 - acc: 0.6790 - val_loss: 0.6157 - val_acc: 0.6595\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61692 to 0.61573, saving model to temp/a59\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6143 - acc: 0.6814 - val_loss: 0.6109 - val_acc: 0.6901\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61573 to 0.61088, saving model to temp/a59\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6156 - acc: 0.6766 - val_loss: 0.6093 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61088 to 0.60928, saving model to temp/a59\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6128 - acc: 0.6824 - val_loss: 0.6090 - val_acc: 0.6802\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.60928 to 0.60897, saving model to temp/a59\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6139 - acc: 0.6795 - val_loss: 0.6088 - val_acc: 0.6924\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.60897 to 0.60876, saving model to temp/a59\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6134 - acc: 0.6798 - val_loss: 0.6111 - val_acc: 0.6708\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.60876\n",
      "Epoch 14/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6122 - acc: 0.6822 - val_loss: 0.6081 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.60876 to 0.60811, saving model to temp/a59\n",
      "Epoch 15/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6110 - acc: 0.6823 - val_loss: 0.6080 - val_acc: 0.6938\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.60811 to 0.60795, saving model to temp/a59\n",
      "Epoch 16/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6119 - acc: 0.6799 - val_loss: 0.6186 - val_acc: 0.6775\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.60795\n",
      "Epoch 17/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6118 - acc: 0.6810 - val_loss: 0.6087 - val_acc: 0.6937\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.60795\n",
      "Epoch 00017: early stopping\n",
      "temp/a60\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 155us/step - loss: 0.6617 - acc: 0.6155 - val_loss: 0.6361 - val_acc: 0.6614\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63610, saving model to temp/a60\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6387 - acc: 0.6445 - val_loss: 0.6339 - val_acc: 0.6240\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63610 to 0.63392, saving model to temp/a60\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6306 - acc: 0.6566 - val_loss: 0.6243 - val_acc: 0.6553\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63392 to 0.62427, saving model to temp/a60\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6269 - acc: 0.6654 - val_loss: 0.6323 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62427\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 143us/step - loss: 0.6234 - acc: 0.6692 - val_loss: 0.6174 - val_acc: 0.6789\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62427 to 0.61737, saving model to temp/a60\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6203 - acc: 0.6717 - val_loss: 0.6151 - val_acc: 0.6855\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61737 to 0.61514, saving model to temp/a60\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6171 - acc: 0.6766 - val_loss: 0.6149 - val_acc: 0.6937\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61514 to 0.61490, saving model to temp/a60\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6158 - acc: 0.6800 - val_loss: 0.6108 - val_acc: 0.6811\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61490 to 0.61083, saving model to temp/a60\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6163 - acc: 0.6789 - val_loss: 0.6129 - val_acc: 0.6706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61083\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6146 - acc: 0.6787 - val_loss: 0.6123 - val_acc: 0.6771\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61083\n",
      "Epoch 00010: early stopping\n",
      "temp/a61\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 163us/step - loss: 0.6655 - acc: 0.6005 - val_loss: 0.6397 - val_acc: 0.6390\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63966, saving model to temp/a61\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6380 - acc: 0.6482 - val_loss: 0.6330 - val_acc: 0.6283\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63966 to 0.63296, saving model to temp/a61\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6307 - acc: 0.6597 - val_loss: 0.6251 - val_acc: 0.6877\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63296 to 0.62512, saving model to temp/a61\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6283 - acc: 0.6624 - val_loss: 0.6227 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62512 to 0.62266, saving model to temp/a61\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6232 - acc: 0.6693 - val_loss: 0.6184 - val_acc: 0.6930\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62266 to 0.61843, saving model to temp/a61\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6200 - acc: 0.6754 - val_loss: 0.6174 - val_acc: 0.6919\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61843 to 0.61743, saving model to temp/a61\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6181 - acc: 0.6771 - val_loss: 0.6399 - val_acc: 0.6369\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61743\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6163 - acc: 0.6783 - val_loss: 0.6117 - val_acc: 0.6941\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61743 to 0.61167, saving model to temp/a61\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6158 - acc: 0.6784 - val_loss: 0.6106 - val_acc: 0.6921\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61167 to 0.61061, saving model to temp/a61\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6149 - acc: 0.6784 - val_loss: 0.6109 - val_acc: 0.6951\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61061\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6133 - acc: 0.6811 - val_loss: 0.6091 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.61061 to 0.60906, saving model to temp/a61\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6119 - acc: 0.6826 - val_loss: 0.6113 - val_acc: 0.6703\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.60906\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6136 - acc: 0.6778 - val_loss: 0.6108 - val_acc: 0.6737\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.60906\n",
      "Epoch 00013: early stopping\n",
      "temp/a62\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6616 - acc: 0.6096 - val_loss: 0.6352 - val_acc: 0.6684\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63518, saving model to temp/a62\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6369 - acc: 0.6453 - val_loss: 0.6286 - val_acc: 0.6757\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63518 to 0.62864, saving model to temp/a62\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6324 - acc: 0.6538 - val_loss: 0.6311 - val_acc: 0.6284\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.62864\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6260 - acc: 0.6673 - val_loss: 0.6190 - val_acc: 0.6903\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62864 to 0.61904, saving model to temp/a62\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6216 - acc: 0.6742 - val_loss: 0.6332 - val_acc: 0.6587\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.61904\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6182 - acc: 0.6772 - val_loss: 0.6198 - val_acc: 0.6552\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61904\n",
      "Epoch 00006: early stopping\n",
      "temp/a63\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6605 - acc: 0.6058 - val_loss: 0.6384 - val_acc: 0.6694\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63843, saving model to temp/a63\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6377 - acc: 0.6463 - val_loss: 0.6349 - val_acc: 0.6228\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63843 to 0.63487, saving model to temp/a63\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6306 - acc: 0.6596 - val_loss: 0.6309 - val_acc: 0.6799\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63487 to 0.63094, saving model to temp/a63\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6280 - acc: 0.6632 - val_loss: 0.6205 - val_acc: 0.6756\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.63094 to 0.62045, saving model to temp/a63\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6235 - acc: 0.6699 - val_loss: 0.6279 - val_acc: 0.6793\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62045\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6204 - acc: 0.6742 - val_loss: 0.6165 - val_acc: 0.6884\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62045 to 0.61645, saving model to temp/a63\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6190 - acc: 0.6732 - val_loss: 0.6360 - val_acc: 0.6284\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61645\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 143us/step - loss: 0.6175 - acc: 0.6753 - val_loss: 0.6166 - val_acc: 0.6648\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61645\n",
      "Epoch 00008: early stopping\n",
      "temp/a64\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6641 - acc: 0.6030 - val_loss: 0.6347 - val_acc: 0.6516\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63465, saving model to temp/a64\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6379 - acc: 0.6414 - val_loss: 0.6350 - val_acc: 0.6755\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.63465\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6294 - acc: 0.6609 - val_loss: 0.6301 - val_acc: 0.6284\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63465 to 0.63011, saving model to temp/a64\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6261 - acc: 0.6672 - val_loss: 0.6243 - val_acc: 0.6467\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.63011 to 0.62428, saving model to temp/a64\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6219 - acc: 0.6708 - val_loss: 0.6199 - val_acc: 0.6580\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62428 to 0.61990, saving model to temp/a64\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6190 - acc: 0.6746 - val_loss: 0.6163 - val_acc: 0.6915\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61990 to 0.61632, saving model to temp/a64\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6187 - acc: 0.6765 - val_loss: 0.6129 - val_acc: 0.6930\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61632 to 0.61290, saving model to temp/a64\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6177 - acc: 0.6781 - val_loss: 0.6114 - val_acc: 0.6906\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61290 to 0.61145, saving model to temp/a64\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6162 - acc: 0.6773 - val_loss: 0.6102 - val_acc: 0.6802\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61145 to 0.61020, saving model to temp/a64\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6152 - acc: 0.6772 - val_loss: 0.6116 - val_acc: 0.6876\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61020\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6130 - acc: 0.6818 - val_loss: 0.6138 - val_acc: 0.6673\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.61020\n",
      "Epoch 00011: early stopping\n",
      "temp/a65\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 158us/step - loss: 0.6632 - acc: 0.6058 - val_loss: 0.6392 - val_acc: 0.6682\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63915, saving model to temp/a65\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6364 - acc: 0.6468 - val_loss: 0.6341 - val_acc: 0.6349\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63915 to 0.63411, saving model to temp/a65\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6293 - acc: 0.6628 - val_loss: 0.6221 - val_acc: 0.6743\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63411 to 0.62211, saving model to temp/a65\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6263 - acc: 0.6664 - val_loss: 0.6277 - val_acc: 0.6431\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62211\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6247 - acc: 0.6708 - val_loss: 0.6210 - val_acc: 0.6895\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62211 to 0.62097, saving model to temp/a65\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6222 - acc: 0.6709 - val_loss: 0.6275 - val_acc: 0.6645\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.62097\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6186 - acc: 0.6713 - val_loss: 0.6203 - val_acc: 0.6490\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.62097 to 0.62030, saving model to temp/a65\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6168 - acc: 0.6777 - val_loss: 0.6136 - val_acc: 0.6936\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.62030 to 0.61360, saving model to temp/a65\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6146 - acc: 0.6796 - val_loss: 0.6138 - val_acc: 0.6908\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61360\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6141 - acc: 0.6809 - val_loss: 0.6100 - val_acc: 0.6910\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61360 to 0.60995, saving model to temp/a65\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6140 - acc: 0.6802 - val_loss: 0.6121 - val_acc: 0.6729\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.60995\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6136 - acc: 0.6785 - val_loss: 0.6128 - val_acc: 0.6723\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.60995\n",
      "Epoch 00012: early stopping\n",
      "temp/a66\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6646 - acc: 0.6017 - val_loss: 0.6446 - val_acc: 0.6626\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64461, saving model to temp/a66\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6370 - acc: 0.6449 - val_loss: 0.6297 - val_acc: 0.6456\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64461 to 0.62970, saving model to temp/a66\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6315 - acc: 0.6551 - val_loss: 0.6264 - val_acc: 0.6557\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62970 to 0.62638, saving model to temp/a66\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6257 - acc: 0.6650 - val_loss: 0.6245 - val_acc: 0.6853\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62638 to 0.62451, saving model to temp/a66\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6226 - acc: 0.6688 - val_loss: 0.6177 - val_acc: 0.6906\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62451 to 0.61770, saving model to temp/a66\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6191 - acc: 0.6757 - val_loss: 0.6141 - val_acc: 0.6829\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61770 to 0.61410, saving model to temp/a66\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6177 - acc: 0.6770 - val_loss: 0.6151 - val_acc: 0.6693\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61410\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6176 - acc: 0.6766 - val_loss: 0.6153 - val_acc: 0.6920\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61410\n",
      "Epoch 00008: early stopping\n",
      "temp/a67\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 158us/step - loss: 0.6617 - acc: 0.6129 - val_loss: 0.6418 - val_acc: 0.6068\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64176, saving model to temp/a67\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6374 - acc: 0.6445 - val_loss: 0.6281 - val_acc: 0.6734\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64176 to 0.62812, saving model to temp/a67\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6293 - acc: 0.6598 - val_loss: 0.6266 - val_acc: 0.6834\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62812 to 0.62661, saving model to temp/a67\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6259 - acc: 0.6681 - val_loss: 0.6256 - val_acc: 0.6851\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62661 to 0.62557, saving model to temp/a67\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6242 - acc: 0.6660 - val_loss: 0.6208 - val_acc: 0.6538\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62557 to 0.62083, saving model to temp/a67\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6201 - acc: 0.6744 - val_loss: 0.6142 - val_acc: 0.6789\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62083 to 0.61422, saving model to temp/a67\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6183 - acc: 0.6765 - val_loss: 0.6207 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61422\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6170 - acc: 0.6766 - val_loss: 0.6136 - val_acc: 0.6743\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61422 to 0.61358, saving model to temp/a67\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6159 - acc: 0.6784 - val_loss: 0.6109 - val_acc: 0.6936\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61358 to 0.61088, saving model to temp/a67\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6125 - acc: 0.6804 - val_loss: 0.6105 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61088 to 0.61051, saving model to temp/a67\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6139 - acc: 0.6809 - val_loss: 0.6087 - val_acc: 0.6871\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.61051 to 0.60874, saving model to temp/a67\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6117 - acc: 0.6844 - val_loss: 0.6083 - val_acc: 0.6943\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.60874 to 0.60833, saving model to temp/a67\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6110 - acc: 0.6827 - val_loss: 0.6063 - val_acc: 0.6942\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.60833 to 0.60629, saving model to temp/a67\n",
      "Epoch 14/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6123 - acc: 0.6816 - val_loss: 0.6088 - val_acc: 0.6936\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.60629\n",
      "Epoch 15/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6109 - acc: 0.6838 - val_loss: 0.6101 - val_acc: 0.6932\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.60629\n",
      "Epoch 00015: early stopping\n",
      "temp/a68\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6619 - acc: 0.6082 - val_loss: 0.6353 - val_acc: 0.6636\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63535, saving model to temp/a68\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6382 - acc: 0.6459 - val_loss: 0.6302 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63535 to 0.63024, saving model to temp/a68\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6311 - acc: 0.6548 - val_loss: 0.6225 - val_acc: 0.6786\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63024 to 0.62254, saving model to temp/a68\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6254 - acc: 0.6689 - val_loss: 0.6199 - val_acc: 0.6678\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62254 to 0.61991, saving model to temp/a68\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6221 - acc: 0.6741 - val_loss: 0.6162 - val_acc: 0.6926\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.61991 to 0.61625, saving model to temp/a68\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6197 - acc: 0.6752 - val_loss: 0.6141 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61625 to 0.61407, saving model to temp/a68\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6192 - acc: 0.6744 - val_loss: 0.6152 - val_acc: 0.6775\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61407\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6157 - acc: 0.6758 - val_loss: 0.6127 - val_acc: 0.6725\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61407 to 0.61273, saving model to temp/a68\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6158 - acc: 0.6786 - val_loss: 0.6107 - val_acc: 0.6933\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61273 to 0.61075, saving model to temp/a68\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6150 - acc: 0.6769 - val_loss: 0.6089 - val_acc: 0.6942\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61075 to 0.60894, saving model to temp/a68\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6122 - acc: 0.6820 - val_loss: 0.6082 - val_acc: 0.6876\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.60894 to 0.60820, saving model to temp/a68\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6113 - acc: 0.6831 - val_loss: 0.6118 - val_acc: 0.6898\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.60820\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6129 - acc: 0.6797 - val_loss: 0.6088 - val_acc: 0.6817\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.60820\n",
      "Epoch 00013: early stopping\n",
      "temp/a69\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6609 - acc: 0.6124 - val_loss: 0.6412 - val_acc: 0.6690\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64120, saving model to temp/a69\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6360 - acc: 0.6499 - val_loss: 0.6312 - val_acc: 0.6344\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64120 to 0.63123, saving model to temp/a69\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6302 - acc: 0.6623 - val_loss: 0.6248 - val_acc: 0.6766\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63123 to 0.62480, saving model to temp/a69\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6265 - acc: 0.6646 - val_loss: 0.6216 - val_acc: 0.6806\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62480 to 0.62156, saving model to temp/a69\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6242 - acc: 0.6664 - val_loss: 0.6183 - val_acc: 0.6902\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62156 to 0.61835, saving model to temp/a69\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6205 - acc: 0.6729 - val_loss: 0.6304 - val_acc: 0.6356\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61835\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6178 - acc: 0.6751 - val_loss: 0.6147 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61835 to 0.61472, saving model to temp/a69\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6158 - acc: 0.6793 - val_loss: 0.6120 - val_acc: 0.6943\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61472 to 0.61202, saving model to temp/a69\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6153 - acc: 0.6787 - val_loss: 0.6138 - val_acc: 0.6893\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61202\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6142 - acc: 0.6811 - val_loss: 0.6109 - val_acc: 0.6911\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61202 to 0.61090, saving model to temp/a69\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6120 - acc: 0.6833 - val_loss: 0.6105 - val_acc: 0.6838\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.61090 to 0.61049, saving model to temp/a69\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6130 - acc: 0.6800 - val_loss: 0.6096 - val_acc: 0.6947\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.61049 to 0.60962, saving model to temp/a69\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6120 - acc: 0.6829 - val_loss: 0.6112 - val_acc: 0.6931\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.60962\n",
      "Epoch 14/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6113 - acc: 0.6827 - val_loss: 0.6099 - val_acc: 0.6794\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.60962\n",
      "Epoch 00014: early stopping\n",
      "temp/a70\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 160us/step - loss: 0.6604 - acc: 0.6125 - val_loss: 0.6349 - val_acc: 0.6420\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63490, saving model to temp/a70\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6377 - acc: 0.6415 - val_loss: 0.6266 - val_acc: 0.6602\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63490 to 0.62665, saving model to temp/a70\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6271 - acc: 0.6615 - val_loss: 0.6216 - val_acc: 0.6782\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62665 to 0.62161, saving model to temp/a70\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6245 - acc: 0.6680 - val_loss: 0.6273 - val_acc: 0.6746\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62161\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 143us/step - loss: 0.6229 - acc: 0.6697 - val_loss: 0.6222 - val_acc: 0.6500\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62161\n",
      "Epoch 00005: early stopping\n",
      "temp/a71\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 155us/step - loss: 0.6632 - acc: 0.6075 - val_loss: 0.6357 - val_acc: 0.6664\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63570, saving model to temp/a71\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6391 - acc: 0.6455 - val_loss: 0.6337 - val_acc: 0.6248\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63570 to 0.63369, saving model to temp/a71\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6290 - acc: 0.6627 - val_loss: 0.6301 - val_acc: 0.6787\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63369 to 0.63009, saving model to temp/a71\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6267 - acc: 0.6646 - val_loss: 0.6175 - val_acc: 0.6809\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.63009 to 0.61746, saving model to temp/a71\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6219 - acc: 0.6702 - val_loss: 0.6182 - val_acc: 0.6617\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.61746\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6185 - acc: 0.6774 - val_loss: 0.6182 - val_acc: 0.6598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61746\n",
      "Epoch 00006: early stopping\n",
      "temp/a72\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6625 - acc: 0.6087 - val_loss: 0.6372 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63724, saving model to temp/a72\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6375 - acc: 0.6464 - val_loss: 0.6296 - val_acc: 0.6396\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63724 to 0.62959, saving model to temp/a72\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6300 - acc: 0.6592 - val_loss: 0.6229 - val_acc: 0.6638\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62959 to 0.62291, saving model to temp/a72\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6251 - acc: 0.6686 - val_loss: 0.6198 - val_acc: 0.6698\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62291 to 0.61981, saving model to temp/a72\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6238 - acc: 0.6702 - val_loss: 0.6164 - val_acc: 0.6810\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.61981 to 0.61636, saving model to temp/a72\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6202 - acc: 0.6717 - val_loss: 0.6204 - val_acc: 0.6534\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61636\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6173 - acc: 0.6768 - val_loss: 0.6130 - val_acc: 0.6930\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61636 to 0.61297, saving model to temp/a72\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6174 - acc: 0.6758 - val_loss: 0.6199 - val_acc: 0.6548\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61297\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6155 - acc: 0.6801 - val_loss: 0.6104 - val_acc: 0.6854\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61297 to 0.61044, saving model to temp/a72\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6132 - acc: 0.6809 - val_loss: 0.6176 - val_acc: 0.6553\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61044\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6132 - acc: 0.6815 - val_loss: 0.6416 - val_acc: 0.6265\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.61044\n",
      "Epoch 00011: early stopping\n",
      "temp/a73\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 154us/step - loss: 0.6620 - acc: 0.6128 - val_loss: 0.6353 - val_acc: 0.6634\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63528, saving model to temp/a73\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6376 - acc: 0.6462 - val_loss: 0.6393 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.63528\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6298 - acc: 0.6600 - val_loss: 0.6248 - val_acc: 0.6811\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63528 to 0.62482, saving model to temp/a73\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6265 - acc: 0.6645 - val_loss: 0.6212 - val_acc: 0.6596\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62482 to 0.62121, saving model to temp/a73\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6230 - acc: 0.6701 - val_loss: 0.6212 - val_acc: 0.6571\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62121\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6201 - acc: 0.6740 - val_loss: 0.6153 - val_acc: 0.6903\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62121 to 0.61526, saving model to temp/a73\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6169 - acc: 0.6782 - val_loss: 0.6164 - val_acc: 0.6905\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61526\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6171 - acc: 0.6785 - val_loss: 0.6118 - val_acc: 0.6838\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61526 to 0.61180, saving model to temp/a73\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6151 - acc: 0.6777 - val_loss: 0.6111 - val_acc: 0.6871\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61180 to 0.61111, saving model to temp/a73\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6154 - acc: 0.6760 - val_loss: 0.6120 - val_acc: 0.6725\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61111\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6123 - acc: 0.6823 - val_loss: 0.6107 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.61111 to 0.61068, saving model to temp/a73\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6129 - acc: 0.6814 - val_loss: 0.6102 - val_acc: 0.6911\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.61068 to 0.61018, saving model to temp/a73\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6144 - acc: 0.6784 - val_loss: 0.6104 - val_acc: 0.6784\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.61018\n",
      "Epoch 14/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6135 - acc: 0.6779 - val_loss: 0.6092 - val_acc: 0.6844\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.61018 to 0.60922, saving model to temp/a73\n",
      "Epoch 15/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6128 - acc: 0.6818 - val_loss: 0.6081 - val_acc: 0.6823\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.60922 to 0.60809, saving model to temp/a73\n",
      "Epoch 16/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6126 - acc: 0.6788 - val_loss: 0.6297 - val_acc: 0.6545\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.60809\n",
      "Epoch 17/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6126 - acc: 0.6802 - val_loss: 0.6073 - val_acc: 0.6932\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.60809 to 0.60734, saving model to temp/a73\n",
      "Epoch 18/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6114 - acc: 0.6818 - val_loss: 0.6117 - val_acc: 0.6871\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.60734\n",
      "Epoch 19/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6113 - acc: 0.6835 - val_loss: 0.6171 - val_acc: 0.6593\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.60734\n",
      "Epoch 00019: early stopping\n",
      "temp/a74\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 158us/step - loss: 0.6623 - acc: 0.6095 - val_loss: 0.6479 - val_acc: 0.6611\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64787, saving model to temp/a74\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6360 - acc: 0.6499 - val_loss: 0.6279 - val_acc: 0.6728\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64787 to 0.62788, saving model to temp/a74\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6302 - acc: 0.6592 - val_loss: 0.6247 - val_acc: 0.6756\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62788 to 0.62468, saving model to temp/a74\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6265 - acc: 0.6623 - val_loss: 0.6214 - val_acc: 0.6890\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62468 to 0.62139, saving model to temp/a74\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6224 - acc: 0.6703 - val_loss: 0.6162 - val_acc: 0.6845\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62139 to 0.61622, saving model to temp/a74\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6213 - acc: 0.6698 - val_loss: 0.6170 - val_acc: 0.6691\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61622\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6168 - acc: 0.6766 - val_loss: 0.6124 - val_acc: 0.6826\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61622 to 0.61239, saving model to temp/a74\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6170 - acc: 0.6749 - val_loss: 0.6122 - val_acc: 0.6880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00008: val_loss improved from 0.61239 to 0.61222, saving model to temp/a74\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6172 - acc: 0.6768 - val_loss: 0.6118 - val_acc: 0.6729\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61222 to 0.61184, saving model to temp/a74\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6133 - acc: 0.6811 - val_loss: 0.6299 - val_acc: 0.6555\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61184\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6143 - acc: 0.6788 - val_loss: 0.6086 - val_acc: 0.6855\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.61184 to 0.60862, saving model to temp/a74\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6131 - acc: 0.6802 - val_loss: 0.6124 - val_acc: 0.6692\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.60862\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6121 - acc: 0.6832 - val_loss: 0.6083 - val_acc: 0.6948\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.60862 to 0.60833, saving model to temp/a74\n",
      "Epoch 14/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6121 - acc: 0.6809 - val_loss: 0.6155 - val_acc: 0.6596\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.60833\n",
      "Epoch 15/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6126 - acc: 0.6801 - val_loss: 0.6086 - val_acc: 0.6756\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.60833\n",
      "Epoch 00015: early stopping\n",
      "temp/a75\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6610 - acc: 0.6125 - val_loss: 0.6350 - val_acc: 0.6472\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63500, saving model to temp/a75\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6386 - acc: 0.6450 - val_loss: 0.6277 - val_acc: 0.6529\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63500 to 0.62771, saving model to temp/a75\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6305 - acc: 0.6580 - val_loss: 0.6347 - val_acc: 0.6168\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.62771\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6251 - acc: 0.6677 - val_loss: 0.6244 - val_acc: 0.6851\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62771 to 0.62442, saving model to temp/a75\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6222 - acc: 0.6730 - val_loss: 0.6195 - val_acc: 0.6552\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62442 to 0.61947, saving model to temp/a75\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6203 - acc: 0.6720 - val_loss: 0.6200 - val_acc: 0.6561\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61947\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6197 - acc: 0.6742 - val_loss: 0.6314 - val_acc: 0.6557\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61947\n",
      "Epoch 00007: early stopping\n",
      "temp/a76\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6636 - acc: 0.6047 - val_loss: 0.6407 - val_acc: 0.6138\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64065, saving model to temp/a76\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6360 - acc: 0.6503 - val_loss: 0.6288 - val_acc: 0.6460\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64065 to 0.62883, saving model to temp/a76\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6312 - acc: 0.6576 - val_loss: 0.6231 - val_acc: 0.6625\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62883 to 0.62307, saving model to temp/a76\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6258 - acc: 0.6667 - val_loss: 0.6218 - val_acc: 0.6879\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62307 to 0.62185, saving model to temp/a76\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6219 - acc: 0.6727 - val_loss: 0.6309 - val_acc: 0.6632\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62185\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6216 - acc: 0.6714 - val_loss: 0.6151 - val_acc: 0.6809\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62185 to 0.61512, saving model to temp/a76\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6177 - acc: 0.6779 - val_loss: 0.6139 - val_acc: 0.6941\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61512 to 0.61388, saving model to temp/a76\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6168 - acc: 0.6783 - val_loss: 0.6115 - val_acc: 0.6907\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61388 to 0.61147, saving model to temp/a76\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6153 - acc: 0.6774 - val_loss: 0.6202 - val_acc: 0.6762\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61147\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6145 - acc: 0.6814 - val_loss: 0.6104 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61147 to 0.61041, saving model to temp/a76\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6136 - acc: 0.6817 - val_loss: 0.6241 - val_acc: 0.6678\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.61041\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6134 - acc: 0.6809 - val_loss: 0.6101 - val_acc: 0.6774\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.61041 to 0.61006, saving model to temp/a76\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6114 - acc: 0.6830 - val_loss: 0.6129 - val_acc: 0.6677\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.61006\n",
      "Epoch 14/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6139 - acc: 0.6782 - val_loss: 0.6204 - val_acc: 0.6744\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.61006\n",
      "Epoch 00014: early stopping\n",
      "temp/a77\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 161us/step - loss: 0.6637 - acc: 0.6033 - val_loss: 0.6380 - val_acc: 0.6682\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63796, saving model to temp/a77\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6381 - acc: 0.6466 - val_loss: 0.6371 - val_acc: 0.6227\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63796 to 0.63711, saving model to temp/a77\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6311 - acc: 0.6583 - val_loss: 0.6332 - val_acc: 0.6238\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63711 to 0.63323, saving model to temp/a77\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6258 - acc: 0.6682 - val_loss: 0.6213 - val_acc: 0.6880\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.63323 to 0.62128, saving model to temp/a77\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6210 - acc: 0.6733 - val_loss: 0.6173 - val_acc: 0.6659\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62128 to 0.61730, saving model to temp/a77\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6198 - acc: 0.6734 - val_loss: 0.6154 - val_acc: 0.6695\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61730 to 0.61537, saving model to temp/a77\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6183 - acc: 0.6743 - val_loss: 0.6123 - val_acc: 0.6795\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61537 to 0.61231, saving model to temp/a77\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6166 - acc: 0.6770 - val_loss: 0.6100 - val_acc: 0.6855\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61231 to 0.61005, saving model to temp/a77\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6153 - acc: 0.6795 - val_loss: 0.6242 - val_acc: 0.6662\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61005\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6136 - acc: 0.6811 - val_loss: 0.6337 - val_acc: 0.6516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61005\n",
      "Epoch 00010: early stopping\n",
      "temp/a78\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 154us/step - loss: 0.6632 - acc: 0.6081 - val_loss: 0.6545 - val_acc: 0.6457\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65448, saving model to temp/a78\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6368 - acc: 0.6508 - val_loss: 0.6354 - val_acc: 0.6769\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65448 to 0.63541, saving model to temp/a78\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6314 - acc: 0.6590 - val_loss: 0.6298 - val_acc: 0.6296\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63541 to 0.62978, saving model to temp/a78\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6270 - acc: 0.6621 - val_loss: 0.6195 - val_acc: 0.6773\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62978 to 0.61954, saving model to temp/a78\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6235 - acc: 0.6657 - val_loss: 0.6187 - val_acc: 0.6660\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.61954 to 0.61865, saving model to temp/a78\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6195 - acc: 0.6731 - val_loss: 0.6156 - val_acc: 0.6700\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61865 to 0.61559, saving model to temp/a78\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6163 - acc: 0.6798 - val_loss: 0.6122 - val_acc: 0.6949\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61559 to 0.61218, saving model to temp/a78\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6159 - acc: 0.6784 - val_loss: 0.6160 - val_acc: 0.6868\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61218\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6151 - acc: 0.6782 - val_loss: 0.6106 - val_acc: 0.6864\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61218 to 0.61063, saving model to temp/a78\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6141 - acc: 0.6800 - val_loss: 0.6101 - val_acc: 0.6795\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61063 to 0.61008, saving model to temp/a78\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6136 - acc: 0.6779 - val_loss: 0.6123 - val_acc: 0.6718\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.61008\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6122 - acc: 0.6833 - val_loss: 0.6083 - val_acc: 0.6827\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.61008 to 0.60825, saving model to temp/a78\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6132 - acc: 0.6797 - val_loss: 0.6087 - val_acc: 0.6828\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.60825\n",
      "Epoch 14/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6129 - acc: 0.6829 - val_loss: 0.6166 - val_acc: 0.6609\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.60825\n",
      "Epoch 00014: early stopping\n",
      "temp/a79\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6610 - acc: 0.6080 - val_loss: 0.6373 - val_acc: 0.6245\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63729, saving model to temp/a79\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6364 - acc: 0.6503 - val_loss: 0.6282 - val_acc: 0.6774\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63729 to 0.62818, saving model to temp/a79\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6307 - acc: 0.6577 - val_loss: 0.6236 - val_acc: 0.6570\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62818 to 0.62362, saving model to temp/a79\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6257 - acc: 0.6656 - val_loss: 0.6194 - val_acc: 0.6651\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62362 to 0.61936, saving model to temp/a79\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6219 - acc: 0.6722 - val_loss: 0.6177 - val_acc: 0.6916\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.61936 to 0.61765, saving model to temp/a79\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6203 - acc: 0.6724 - val_loss: 0.6195 - val_acc: 0.6852\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61765\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6168 - acc: 0.6780 - val_loss: 0.6130 - val_acc: 0.6777\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61765 to 0.61303, saving model to temp/a79\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6172 - acc: 0.6772 - val_loss: 0.6122 - val_acc: 0.6861\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61303 to 0.61223, saving model to temp/a79\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6167 - acc: 0.6762 - val_loss: 0.6117 - val_acc: 0.6794\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61223 to 0.61167, saving model to temp/a79\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6166 - acc: 0.6765 - val_loss: 0.6114 - val_acc: 0.6935\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61167 to 0.61136, saving model to temp/a79\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6127 - acc: 0.6822 - val_loss: 0.6142 - val_acc: 0.6651\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.61136\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6116 - acc: 0.6835 - val_loss: 0.6100 - val_acc: 0.6919\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.61136 to 0.60999, saving model to temp/a79\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6135 - acc: 0.6811 - val_loss: 0.6101 - val_acc: 0.6742\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.60999\n",
      "Epoch 14/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6128 - acc: 0.6806 - val_loss: 0.6111 - val_acc: 0.6914\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.60999\n",
      "Epoch 00014: early stopping\n",
      "temp/a80\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 155us/step - loss: 0.6595 - acc: 0.6133 - val_loss: 0.6352 - val_acc: 0.6606\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63518, saving model to temp/a80\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6369 - acc: 0.6488 - val_loss: 0.6291 - val_acc: 0.6392\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63518 to 0.62910, saving model to temp/a80\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6305 - acc: 0.6603 - val_loss: 0.6227 - val_acc: 0.6707\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62910 to 0.62266, saving model to temp/a80\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6223 - acc: 0.6740 - val_loss: 0.6194 - val_acc: 0.6854\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62266 to 0.61936, saving model to temp/a80\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6223 - acc: 0.6694 - val_loss: 0.6457 - val_acc: 0.6153\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.61936\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6191 - acc: 0.6752 - val_loss: 0.6283 - val_acc: 0.6688\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61936\n",
      "Epoch 00006: early stopping\n",
      "temp/a81\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 159us/step - loss: 0.6661 - acc: 0.6028 - val_loss: 0.6442 - val_acc: 0.6063\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64422, saving model to temp/a81\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6382 - acc: 0.6460 - val_loss: 0.6293 - val_acc: 0.6480\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64422 to 0.62926, saving model to temp/a81\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6303 - acc: 0.6575 - val_loss: 0.6248 - val_acc: 0.6575\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62926 to 0.62482, saving model to temp/a81\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6270 - acc: 0.6656 - val_loss: 0.6209 - val_acc: 0.6655\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62482 to 0.62091, saving model to temp/a81\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6237 - acc: 0.6670 - val_loss: 0.6237 - val_acc: 0.6451\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62091\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6206 - acc: 0.6739 - val_loss: 0.6182 - val_acc: 0.6608\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62091 to 0.61815, saving model to temp/a81\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6190 - acc: 0.6769 - val_loss: 0.6199 - val_acc: 0.6811\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61815\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6157 - acc: 0.6794 - val_loss: 0.6235 - val_acc: 0.6476\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61815\n",
      "Epoch 00008: early stopping\n",
      "temp/a82\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6653 - acc: 0.5976 - val_loss: 0.6527 - val_acc: 0.5780\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65268, saving model to temp/a82\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6376 - acc: 0.6455 - val_loss: 0.6331 - val_acc: 0.6261\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65268 to 0.63308, saving model to temp/a82\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6309 - acc: 0.6615 - val_loss: 0.6244 - val_acc: 0.6455\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63308 to 0.62443, saving model to temp/a82\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6253 - acc: 0.6695 - val_loss: 0.6216 - val_acc: 0.6862\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62443 to 0.62163, saving model to temp/a82\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6218 - acc: 0.6702 - val_loss: 0.6198 - val_acc: 0.6548\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62163 to 0.61976, saving model to temp/a82\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6201 - acc: 0.6748 - val_loss: 0.6146 - val_acc: 0.6850\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61976 to 0.61463, saving model to temp/a82\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6166 - acc: 0.6757 - val_loss: 0.6215 - val_acc: 0.6504\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61463\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6155 - acc: 0.6799 - val_loss: 0.6159 - val_acc: 0.6686\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61463\n",
      "Epoch 00008: early stopping\n",
      "temp/a83\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 154us/step - loss: 0.6651 - acc: 0.5986 - val_loss: 0.6421 - val_acc: 0.6135\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64211, saving model to temp/a83\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6397 - acc: 0.6446 - val_loss: 0.6284 - val_acc: 0.6755\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64211 to 0.62843, saving model to temp/a83\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6306 - acc: 0.6597 - val_loss: 0.6251 - val_acc: 0.6881\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62843 to 0.62508, saving model to temp/a83\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6251 - acc: 0.6649 - val_loss: 0.6270 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62508\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6225 - acc: 0.6708 - val_loss: 0.6245 - val_acc: 0.6424\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62508 to 0.62449, saving model to temp/a83\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6183 - acc: 0.6735 - val_loss: 0.6156 - val_acc: 0.6929\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62449 to 0.61560, saving model to temp/a83\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6177 - acc: 0.6764 - val_loss: 0.6149 - val_acc: 0.6679\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61560 to 0.61490, saving model to temp/a83\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6162 - acc: 0.6760 - val_loss: 0.6198 - val_acc: 0.6558\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61490\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6146 - acc: 0.6798 - val_loss: 0.6285 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61490\n",
      "Epoch 00009: early stopping\n",
      "temp/a84\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6669 - acc: 0.5965 - val_loss: 0.6373 - val_acc: 0.6543\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63730, saving model to temp/a84\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6383 - acc: 0.6446 - val_loss: 0.6613 - val_acc: 0.6315\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.63730\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6330 - acc: 0.6538 - val_loss: 0.6347 - val_acc: 0.6191\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63730 to 0.63472, saving model to temp/a84\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6265 - acc: 0.6614 - val_loss: 0.6234 - val_acc: 0.6853\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.63472 to 0.62343, saving model to temp/a84\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6229 - acc: 0.6702 - val_loss: 0.6186 - val_acc: 0.6898\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62343 to 0.61861, saving model to temp/a84\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6204 - acc: 0.6759 - val_loss: 0.6220 - val_acc: 0.6477\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61861\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6186 - acc: 0.6730 - val_loss: 0.6140 - val_acc: 0.6926\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61861 to 0.61403, saving model to temp/a84\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6158 - acc: 0.6777 - val_loss: 0.6154 - val_acc: 0.6889\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61403\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6143 - acc: 0.6806 - val_loss: 0.6201 - val_acc: 0.6542\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61403\n",
      "Epoch 00009: early stopping\n",
      "temp/a85\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6622 - acc: 0.6118 - val_loss: 0.6392 - val_acc: 0.6251\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63920, saving model to temp/a85\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6374 - acc: 0.6490 - val_loss: 0.6391 - val_acc: 0.6681\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63920 to 0.63908, saving model to temp/a85\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6315 - acc: 0.6569 - val_loss: 0.6221 - val_acc: 0.6780\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63908 to 0.62213, saving model to temp/a85\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6254 - acc: 0.6694 - val_loss: 0.6185 - val_acc: 0.6844\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62213 to 0.61849, saving model to temp/a85\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6231 - acc: 0.6726 - val_loss: 0.6169 - val_acc: 0.6908\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.61849 to 0.61692, saving model to temp/a85\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6196 - acc: 0.6743 - val_loss: 0.6146 - val_acc: 0.6899\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61692 to 0.61457, saving model to temp/a85\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6182 - acc: 0.6731 - val_loss: 0.6231 - val_acc: 0.6761\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61457\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6149 - acc: 0.6830 - val_loss: 0.6126 - val_acc: 0.6909\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61457 to 0.61256, saving model to temp/a85\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6142 - acc: 0.6826 - val_loss: 0.6137 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61256\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6150 - acc: 0.6791 - val_loss: 0.6104 - val_acc: 0.6899\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61256 to 0.61041, saving model to temp/a85\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6154 - acc: 0.6780 - val_loss: 0.6155 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.61041\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6161 - acc: 0.6726 - val_loss: 0.6148 - val_acc: 0.6847\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.61041\n",
      "Epoch 00012: early stopping\n",
      "temp/a86\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6619 - acc: 0.6091 - val_loss: 0.6401 - val_acc: 0.6731\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64012, saving model to temp/a86\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6357 - acc: 0.6481 - val_loss: 0.6281 - val_acc: 0.6741\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64012 to 0.62806, saving model to temp/a86\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6297 - acc: 0.6581 - val_loss: 0.6223 - val_acc: 0.6733\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62806 to 0.62233, saving model to temp/a86\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6251 - acc: 0.6684 - val_loss: 0.6202 - val_acc: 0.6867\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62233 to 0.62020, saving model to temp/a86\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6235 - acc: 0.6711 - val_loss: 0.6337 - val_acc: 0.6563\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62020\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6189 - acc: 0.6752 - val_loss: 0.6170 - val_acc: 0.6906\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62020 to 0.61697, saving model to temp/a86\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6181 - acc: 0.6800 - val_loss: 0.6134 - val_acc: 0.6823\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61697 to 0.61336, saving model to temp/a86\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6163 - acc: 0.6766 - val_loss: 0.6280 - val_acc: 0.6595\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61336\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6152 - acc: 0.6777 - val_loss: 0.6103 - val_acc: 0.6919\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61336 to 0.61027, saving model to temp/a86\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6138 - acc: 0.6809 - val_loss: 0.6131 - val_acc: 0.6702\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61027\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6153 - acc: 0.6789 - val_loss: 0.6230 - val_acc: 0.6690\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.61027\n",
      "Epoch 00011: early stopping\n",
      "temp/a87\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 159us/step - loss: 0.6622 - acc: 0.6084 - val_loss: 0.6596 - val_acc: 0.6404\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65962, saving model to temp/a87\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6380 - acc: 0.6454 - val_loss: 0.6355 - val_acc: 0.6142\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65962 to 0.63548, saving model to temp/a87\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6290 - acc: 0.6591 - val_loss: 0.6221 - val_acc: 0.6634\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63548 to 0.62213, saving model to temp/a87\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6266 - acc: 0.6654 - val_loss: 0.6235 - val_acc: 0.6851\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62213\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6208 - acc: 0.6750 - val_loss: 0.6180 - val_acc: 0.6670\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62213 to 0.61797, saving model to temp/a87\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6200 - acc: 0.6729 - val_loss: 0.6239 - val_acc: 0.6447\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61797\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6170 - acc: 0.6771 - val_loss: 0.6178 - val_acc: 0.6565\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61797 to 0.61780, saving model to temp/a87\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6143 - acc: 0.6807 - val_loss: 0.6135 - val_acc: 0.6702\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61780 to 0.61349, saving model to temp/a87\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6149 - acc: 0.6792 - val_loss: 0.6141 - val_acc: 0.6619\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61349\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6147 - acc: 0.6797 - val_loss: 0.6093 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61349 to 0.60929, saving model to temp/a87\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6129 - acc: 0.6802 - val_loss: 0.6149 - val_acc: 0.6845\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.60929\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6111 - acc: 0.6828 - val_loss: 0.6077 - val_acc: 0.6858\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.60929 to 0.60768, saving model to temp/a87\n",
      "Epoch 13/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6126 - acc: 0.6817 - val_loss: 0.6069 - val_acc: 0.6907\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.60768 to 0.60693, saving model to temp/a87\n",
      "Epoch 14/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6111 - acc: 0.6834 - val_loss: 0.6111 - val_acc: 0.6904\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.60693\n",
      "Epoch 15/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6119 - acc: 0.6795 - val_loss: 0.6351 - val_acc: 0.6439\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.60693\n",
      "Epoch 00015: early stopping\n",
      "temp/a88\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 155us/step - loss: 0.6597 - acc: 0.6105 - val_loss: 0.6361 - val_acc: 0.6367\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63610, saving model to temp/a88\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6385 - acc: 0.6471 - val_loss: 0.6287 - val_acc: 0.6788\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63610 to 0.62869, saving model to temp/a88\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6300 - acc: 0.6635 - val_loss: 0.6273 - val_acc: 0.6845\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62869 to 0.62725, saving model to temp/a88\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6264 - acc: 0.6665 - val_loss: 0.6206 - val_acc: 0.6724\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62725 to 0.62064, saving model to temp/a88\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6229 - acc: 0.6720 - val_loss: 0.6161 - val_acc: 0.6887\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62064 to 0.61615, saving model to temp/a88\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6205 - acc: 0.6714 - val_loss: 0.6262 - val_acc: 0.6404\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61615\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6171 - acc: 0.6774 - val_loss: 0.6188 - val_acc: 0.6833\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61615\n",
      "Epoch 00007: early stopping\n",
      "temp/a89\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6625 - acc: 0.6092 - val_loss: 0.6454 - val_acc: 0.6627\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64541, saving model to temp/a89\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6386 - acc: 0.6479 - val_loss: 0.6286 - val_acc: 0.6570\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64541 to 0.62861, saving model to temp/a89\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6317 - acc: 0.6562 - val_loss: 0.6234 - val_acc: 0.6632\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62861 to 0.62344, saving model to temp/a89\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6249 - acc: 0.6684 - val_loss: 0.6306 - val_acc: 0.6306\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62344\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6229 - acc: 0.6688 - val_loss: 0.6248 - val_acc: 0.6826\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62344\n",
      "Epoch 00005: early stopping\n",
      "temp/a90\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 154us/step - loss: 0.6629 - acc: 0.6102 - val_loss: 0.6384 - val_acc: 0.6713\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63836, saving model to temp/a90\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6382 - acc: 0.6451 - val_loss: 0.6293 - val_acc: 0.6459\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63836 to 0.62930, saving model to temp/a90\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6292 - acc: 0.6578 - val_loss: 0.6229 - val_acc: 0.6628\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62930 to 0.62292, saving model to temp/a90\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6256 - acc: 0.6672 - val_loss: 0.6204 - val_acc: 0.6855\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62292 to 0.62037, saving model to temp/a90\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6229 - acc: 0.6703 - val_loss: 0.6186 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62037 to 0.61856, saving model to temp/a90\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6195 - acc: 0.6759 - val_loss: 0.6152 - val_acc: 0.6915\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61856 to 0.61524, saving model to temp/a90\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6178 - acc: 0.6746 - val_loss: 0.6125 - val_acc: 0.6871\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61524 to 0.61252, saving model to temp/a90\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6155 - acc: 0.6757 - val_loss: 0.6143 - val_acc: 0.6919\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61252\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6141 - acc: 0.6829 - val_loss: 0.6103 - val_acc: 0.6882\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61252 to 0.61030, saving model to temp/a90\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6130 - acc: 0.6822 - val_loss: 0.6107 - val_acc: 0.6806\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61030\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6138 - acc: 0.6791 - val_loss: 0.6115 - val_acc: 0.6913\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.61030\n",
      "Epoch 00011: early stopping\n",
      "temp/a91\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6626 - acc: 0.6078 - val_loss: 0.6409 - val_acc: 0.6082\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64091, saving model to temp/a91\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6354 - acc: 0.6493 - val_loss: 0.6291 - val_acc: 0.6451\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64091 to 0.62906, saving model to temp/a91\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6309 - acc: 0.6604 - val_loss: 0.6231 - val_acc: 0.6567\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62906 to 0.62314, saving model to temp/a91\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6248 - acc: 0.6671 - val_loss: 0.6200 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62314 to 0.62001, saving model to temp/a91\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6215 - acc: 0.6744 - val_loss: 0.6266 - val_acc: 0.6673\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62001\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6193 - acc: 0.6747 - val_loss: 0.6152 - val_acc: 0.6709\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62001 to 0.61516, saving model to temp/a91\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6158 - acc: 0.6799 - val_loss: 0.6210 - val_acc: 0.6485\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61516\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6169 - acc: 0.6749 - val_loss: 0.6123 - val_acc: 0.6930\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61516 to 0.61229, saving model to temp/a91\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6160 - acc: 0.6769 - val_loss: 0.6116 - val_acc: 0.6925\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61229 to 0.61162, saving model to temp/a91\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6147 - acc: 0.6780 - val_loss: 0.6277 - val_acc: 0.6591\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61162\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6144 - acc: 0.6801 - val_loss: 0.6187 - val_acc: 0.6607\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.61162\n",
      "Epoch 00011: early stopping\n",
      "temp/a92\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 155us/step - loss: 0.6628 - acc: 0.6053 - val_loss: 0.6359 - val_acc: 0.6569\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63588, saving model to temp/a92\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6361 - acc: 0.6476 - val_loss: 0.6338 - val_acc: 0.6788\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63588 to 0.63385, saving model to temp/a92\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6300 - acc: 0.6585 - val_loss: 0.6476 - val_acc: 0.5987\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.63385\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6274 - acc: 0.6655 - val_loss: 0.6278 - val_acc: 0.6786\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.63385 to 0.62775, saving model to temp/a92\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6230 - acc: 0.6685 - val_loss: 0.6165 - val_acc: 0.6863\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62775 to 0.61646, saving model to temp/a92\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6193 - acc: 0.6756 - val_loss: 0.6168 - val_acc: 0.6894\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61646\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6183 - acc: 0.6740 - val_loss: 0.6144 - val_acc: 0.6690\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61646 to 0.61442, saving model to temp/a92\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6173 - acc: 0.6760 - val_loss: 0.6199 - val_acc: 0.6773\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61442\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6143 - acc: 0.6837 - val_loss: 0.6217 - val_acc: 0.6721\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61442\n",
      "Epoch 00009: early stopping\n",
      "temp/a93\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17748/17748 [==============================] - 3s 158us/step - loss: 0.6650 - acc: 0.6027 - val_loss: 0.6394 - val_acc: 0.6687\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63942, saving model to temp/a93\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6368 - acc: 0.6460 - val_loss: 0.6266 - val_acc: 0.6485\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63942 to 0.62657, saving model to temp/a93\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6292 - acc: 0.6611 - val_loss: 0.6358 - val_acc: 0.6197\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.62657\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 144us/step - loss: 0.6256 - acc: 0.6660 - val_loss: 0.6195 - val_acc: 0.6664\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62657 to 0.61950, saving model to temp/a93\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6224 - acc: 0.6700 - val_loss: 0.6165 - val_acc: 0.6917\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.61950 to 0.61646, saving model to temp/a93\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6201 - acc: 0.6730 - val_loss: 0.6145 - val_acc: 0.6891\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61646 to 0.61450, saving model to temp/a93\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6189 - acc: 0.6739 - val_loss: 0.6127 - val_acc: 0.6818\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61450 to 0.61272, saving model to temp/a93\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6163 - acc: 0.6791 - val_loss: 0.6105 - val_acc: 0.6826\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61272 to 0.61046, saving model to temp/a93\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6149 - acc: 0.6791 - val_loss: 0.6155 - val_acc: 0.6833\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61046\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6140 - acc: 0.6821 - val_loss: 0.6087 - val_acc: 0.6925\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61046 to 0.60872, saving model to temp/a93\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6133 - acc: 0.6795 - val_loss: 0.6144 - val_acc: 0.6832\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.60872\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6127 - acc: 0.6808 - val_loss: 0.6197 - val_acc: 0.6760\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.60872\n",
      "Epoch 00012: early stopping\n",
      "temp/a94\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 158us/step - loss: 0.6597 - acc: 0.6121 - val_loss: 0.6377 - val_acc: 0.6215\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63767, saving model to temp/a94\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6373 - acc: 0.6471 - val_loss: 0.6385 - val_acc: 0.6105\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.63767\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6319 - acc: 0.6595 - val_loss: 0.6256 - val_acc: 0.6843\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63767 to 0.62557, saving model to temp/a94\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6283 - acc: 0.6651 - val_loss: 0.6214 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62557 to 0.62139, saving model to temp/a94\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6226 - acc: 0.6704 - val_loss: 0.6210 - val_acc: 0.6519\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62139 to 0.62101, saving model to temp/a94\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6210 - acc: 0.6733 - val_loss: 0.6184 - val_acc: 0.6922\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62101 to 0.61839, saving model to temp/a94\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6180 - acc: 0.6775 - val_loss: 0.6141 - val_acc: 0.6917\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61839 to 0.61414, saving model to temp/a94\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 150us/step - loss: 0.6163 - acc: 0.6787 - val_loss: 0.6132 - val_acc: 0.6922\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61414 to 0.61323, saving model to temp/a94\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6145 - acc: 0.6792 - val_loss: 0.6427 - val_acc: 0.6429\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61323\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6139 - acc: 0.6804 - val_loss: 0.6206 - val_acc: 0.6783\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61323\n",
      "Epoch 00010: early stopping\n",
      "temp/a95\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 156us/step - loss: 0.6609 - acc: 0.6169 - val_loss: 0.6379 - val_acc: 0.6200\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63789, saving model to temp/a95\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6366 - acc: 0.6480 - val_loss: 0.6290 - val_acc: 0.6740\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63789 to 0.62902, saving model to temp/a95\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6308 - acc: 0.6579 - val_loss: 0.6260 - val_acc: 0.6830\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62902 to 0.62604, saving model to temp/a95\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6258 - acc: 0.6676 - val_loss: 0.6215 - val_acc: 0.6887\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62604 to 0.62152, saving model to temp/a95\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6234 - acc: 0.6694 - val_loss: 0.6170 - val_acc: 0.6895\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62152 to 0.61703, saving model to temp/a95\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6203 - acc: 0.6728 - val_loss: 0.6138 - val_acc: 0.6893\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61703 to 0.61376, saving model to temp/a95\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6187 - acc: 0.6747 - val_loss: 0.6146 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61376\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6152 - acc: 0.6793 - val_loss: 0.6110 - val_acc: 0.6894\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61376 to 0.61100, saving model to temp/a95\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6156 - acc: 0.6798 - val_loss: 0.6138 - val_acc: 0.6896\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61100\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6149 - acc: 0.6777 - val_loss: 0.6109 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61100 to 0.61088, saving model to temp/a95\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6140 - acc: 0.6791 - val_loss: 0.6134 - val_acc: 0.6906\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.61088\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6139 - acc: 0.6795 - val_loss: 0.6131 - val_acc: 0.6871\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.61088\n",
      "Epoch 00012: early stopping\n",
      "temp/a96\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 162us/step - loss: 0.6619 - acc: 0.6120 - val_loss: 0.6362 - val_acc: 0.6378\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63619, saving model to temp/a96\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6340 - acc: 0.6517 - val_loss: 0.6288 - val_acc: 0.6809\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63619 to 0.62883, saving model to temp/a96\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6329 - acc: 0.6515 - val_loss: 0.6267 - val_acc: 0.6702\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62883 to 0.62668, saving model to temp/a96\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6249 - acc: 0.6690 - val_loss: 0.6309 - val_acc: 0.6290\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62668\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6237 - acc: 0.6698 - val_loss: 0.6215 - val_acc: 0.6515\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62668 to 0.62149, saving model to temp/a96\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6200 - acc: 0.6706 - val_loss: 0.6245 - val_acc: 0.6457\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.62149\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6185 - acc: 0.6725 - val_loss: 0.6127 - val_acc: 0.6857\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.62149 to 0.61272, saving model to temp/a96\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6164 - acc: 0.6786 - val_loss: 0.6140 - val_acc: 0.6918\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61272\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6153 - acc: 0.6772 - val_loss: 0.6117 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61272 to 0.61173, saving model to temp/a96\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6150 - acc: 0.6783 - val_loss: 0.6102 - val_acc: 0.6908\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.61173 to 0.61018, saving model to temp/a96\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6138 - acc: 0.6799 - val_loss: 0.6154 - val_acc: 0.6856\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.61018\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6132 - acc: 0.6804 - val_loss: 0.6230 - val_acc: 0.6705\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.61018\n",
      "Epoch 00012: early stopping\n",
      "temp/a97\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6626 - acc: 0.6118 - val_loss: 0.6354 - val_acc: 0.6599\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63536, saving model to temp/a97\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6366 - acc: 0.6496 - val_loss: 0.6322 - val_acc: 0.6777\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63536 to 0.63221, saving model to temp/a97\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6312 - acc: 0.6596 - val_loss: 0.6328 - val_acc: 0.6701\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.63221\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6258 - acc: 0.6648 - val_loss: 0.6193 - val_acc: 0.6862\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.63221 to 0.61934, saving model to temp/a97\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6237 - acc: 0.6695 - val_loss: 0.6204 - val_acc: 0.6868\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.61934\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6187 - acc: 0.6757 - val_loss: 0.6170 - val_acc: 0.6715\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61934 to 0.61696, saving model to temp/a97\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6171 - acc: 0.6766 - val_loss: 0.6139 - val_acc: 0.6926\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61696 to 0.61391, saving model to temp/a97\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6157 - acc: 0.6802 - val_loss: 0.6123 - val_acc: 0.6921\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61391 to 0.61226, saving model to temp/a97\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6135 - acc: 0.6814 - val_loss: 0.6094 - val_acc: 0.6842\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61226 to 0.60938, saving model to temp/a97\n",
      "Epoch 10/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6160 - acc: 0.6772 - val_loss: 0.6082 - val_acc: 0.6891\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.60938 to 0.60816, saving model to temp/a97\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 148us/step - loss: 0.6137 - acc: 0.6783 - val_loss: 0.6092 - val_acc: 0.6933\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.60816\n",
      "Epoch 12/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6127 - acc: 0.6813 - val_loss: 0.6215 - val_acc: 0.6720\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.60816\n",
      "Epoch 00012: early stopping\n",
      "temp/a98\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 157us/step - loss: 0.6645 - acc: 0.6033 - val_loss: 0.6375 - val_acc: 0.6348\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63749, saving model to temp/a98\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6360 - acc: 0.6520 - val_loss: 0.6274 - val_acc: 0.6584\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63749 to 0.62737, saving model to temp/a98\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6299 - acc: 0.6582 - val_loss: 0.6252 - val_acc: 0.6799\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62737 to 0.62524, saving model to temp/a98\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 149us/step - loss: 0.6257 - acc: 0.6691 - val_loss: 0.6206 - val_acc: 0.6901\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62524 to 0.62056, saving model to temp/a98\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6244 - acc: 0.6673 - val_loss: 0.6263 - val_acc: 0.6376\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62056\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6186 - acc: 0.6790 - val_loss: 0.6163 - val_acc: 0.6906\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62056 to 0.61630, saving model to temp/a98\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6176 - acc: 0.6777 - val_loss: 0.6134 - val_acc: 0.6782\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61630 to 0.61345, saving model to temp/a98\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6161 - acc: 0.6753 - val_loss: 0.6192 - val_acc: 0.6789\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61345\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6166 - acc: 0.6755 - val_loss: 0.6224 - val_acc: 0.6757\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.61345\n",
      "Epoch 00009: early stopping\n",
      "temp/a99\n",
      "Train on 17748 samples, validate on 17748 samples\n",
      "Epoch 1/20\n",
      "17748/17748 [==============================] - 3s 154us/step - loss: 0.6623 - acc: 0.6094 - val_loss: 0.6388 - val_acc: 0.6218\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63883, saving model to temp/a99\n",
      "Epoch 2/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6379 - acc: 0.6445 - val_loss: 0.6286 - val_acc: 0.6453\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63883 to 0.62862, saving model to temp/a99\n",
      "Epoch 3/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6322 - acc: 0.6589 - val_loss: 0.6267 - val_acc: 0.6427\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62862 to 0.62674, saving model to temp/a99\n",
      "Epoch 4/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6256 - acc: 0.6651 - val_loss: 0.6264 - val_acc: 0.6398\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62674 to 0.62639, saving model to temp/a99\n",
      "Epoch 5/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6230 - acc: 0.6687 - val_loss: 0.6161 - val_acc: 0.6882\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62639 to 0.61611, saving model to temp/a99\n",
      "Epoch 6/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6196 - acc: 0.6744 - val_loss: 0.6151 - val_acc: 0.6730\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61611 to 0.61511, saving model to temp/a99\n",
      "Epoch 7/20\n",
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6174 - acc: 0.6789 - val_loss: 0.6124 - val_acc: 0.6879\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61511 to 0.61244, saving model to temp/a99\n",
      "Epoch 8/20\n",
      "17748/17748 [==============================] - 3s 146us/step - loss: 0.6168 - acc: 0.6768 - val_loss: 0.6129 - val_acc: 0.6933\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61244\n",
      "Epoch 9/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6167 - acc: 0.6746 - val_loss: 0.6092 - val_acc: 0.6855\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61244 to 0.60917, saving model to temp/a99\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17748/17748 [==============================] - 3s 145us/step - loss: 0.6158 - acc: 0.6765 - val_loss: 0.6110 - val_acc: 0.6752\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.60917\n",
      "Epoch 11/20\n",
      "17748/17748 [==============================] - 3s 147us/step - loss: 0.6123 - acc: 0.6801 - val_loss: 0.6115 - val_acc: 0.6776\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.60917\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "for idx, model_name in enumerate(model_names):\n",
    "    print(model_name)\n",
    "\n",
    "    if type(models[idx]) is list:\n",
    "        #clear session\n",
    "        keras.backend.clear_session() \n",
    "        #get model according to specification\n",
    "        model = get_model(models[idx], [0.2] * len(models), len(inputs), 2)\n",
    "        callbacks = [ModelCheckpoint(model_name, verbose= verbosity, monitor='val_loss',save_best_only=True), \n",
    "                     EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose= verbosity, mode='auto')]\n",
    "        model.compile(optimizer = optimizers.SGD(lr = 0.01, momentum = 0.9, ), loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "        #print(len(X), len(y))\n",
    "        model.fit(X, y, epochs = 20, validation_data = (x_val, y_val), callbacks = callbacks, batch_size = 32, verbose = verbosity)\n",
    "    else:\n",
    "        models[idx].fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6887124381894257\n",
      "0.6959692373526056\n",
      "0.6909590148345378\n",
      "0.6927598421453024\n",
      "0.6925577691137315\n",
      "0.6922011696462533\n",
      "0.6937107740585774\n",
      "0.6935562476226702\n",
      "0.6897822365918601\n",
      "0.6903171357930772\n",
      "0.6960524438950172\n",
      "0.6918802301255229\n",
      "0.6967180962343097\n",
      "0.694370483073412\n",
      "0.6916008938759985\n",
      "0.6967180962343096\n",
      "0.6964506466337009\n",
      "0.6951728318752377\n",
      "0.6913453309243058\n",
      "0.6923854127044503\n",
      "0.6874465100798782\n",
      "0.6923259794598707\n",
      "0.6942872765310003\n",
      "0.6937464340053253\n",
      "0.6918742868010651\n",
      "0.6948221757322176\n",
      "0.6948340623811335\n",
      "0.6961475370863446\n",
      "0.685402006466337\n",
      "0.6913631608976797\n",
      "0.6939128470901483\n",
      "0.6942278432864207\n",
      "0.695678014454165\n",
      "0.6899605363255991\n",
      "0.6962129136553823\n",
      "0.6938831304678585\n",
      "0.6964209300114113\n",
      "0.6900377995435527\n",
      "0.699315329022442\n",
      "0.6961653670597185\n",
      "0.6972708254089007\n",
      "0.6874049068086725\n",
      "0.690941184861164\n",
      "0.6902814758463294\n",
      "0.6929975751236211\n",
      "0.6945071795359452\n",
      "0.6906975085583871\n",
      "0.6948221757322176\n",
      "0.6924032426778242\n",
      "0.6997610783567896\n",
      "0.699493628756181\n",
      "0.6919158900722708\n",
      "0.6886767782426778\n",
      "0.6942932198554583\n",
      "0.6962129136553823\n",
      "0.6971638455686573\n",
      "0.6991726892354507\n",
      "0.6972351654621529\n",
      "0.6988814663370102\n",
      "0.6877733929250665\n",
      "0.6957493343476606\n",
      "0.6932353081019399\n",
      "0.6930272917459108\n",
      "0.6933482312666414\n",
      "0.6878031095473564\n",
      "0.6970390357550399\n",
      "0.6919337200456447\n",
      "0.6836249524534044\n",
      "0.6938831304678584\n",
      "0.6930748383415747\n",
      "0.6887778147584633\n",
      "0.6858834157474325\n",
      "0.6928965386078356\n",
      "0.6941446367440091\n",
      "0.6922962628375807\n",
      "0.699125142639787\n",
      "0.6994401388360593\n",
      "0.6925874857360212\n",
      "0.7043909281095474\n",
      "0.6908936382655002\n",
      "0.6896277101559529\n",
      "0.6896871434005325\n",
      "0.6929381418790413\n",
      "0.6879814092810955\n",
      "0.6896098801825788\n",
      "0.6933838912133892\n",
      "0.7011339863065804\n",
      "0.6946082160517306\n",
      "0.6927657854697604\n",
      "0.6977581780144542\n",
      "0.6938950171167745\n",
      "0.6914285374667174\n",
      "0.6912205211106884\n",
      "0.6948340623811335\n",
      "0.6948102890833016\n",
      "0.6973243153290225\n",
      "0.6972589387599848\n",
      "0.7012528527957398\n",
      "0.6917673069608216\n",
      "0.6998323982502852\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "nb_test = 2000\n",
    "metrics_dicts = []\n",
    "\n",
    "perturbed_df = gen_data_perturbed(SIZE = nb_test)\n",
    "y_test2 = perturbed_df[target]\n",
    "x_test2 = normalize(perturbed_df[inputs].values)\n",
    "for idx, model_name in enumerate(model_names):\n",
    "\n",
    "    if type(models[idx]) is list:\n",
    "        keras.backend.clear_session()\n",
    "        model = load_model(model_name)\n",
    "    else:\n",
    "        model = models[idx]\n",
    "    y_pred2 = model.predict(x_test2)[:,1]\n",
    "    print(roc_auc_score(y_test2, y_pred2))\n",
    "    metrics_dicts.append(roc_auc_score(y_test2, y_pred2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times =  0\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19755.81767410945\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19579.887626306852\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19720.110870680455\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19596.357207275876\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19782.248146819966\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19907.824283908376\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20615.056183258414\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19377.959718932198\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19710.18521989853\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19617.61615169618\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19653.191435783687\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19567.847931912467\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19398.635474889048\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19630.73118872183\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19676.407102895384\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19511.64278270036\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19667.029951645996\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19617.61615169618\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19529.640908131332\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19468.272130012196\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19571.831453255207\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19657.774019032076\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19312.86243814123\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19508.13122261235\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19522.35350385011\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19630.73118872183\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19715.132738850094\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19328.361088977726\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19705.268282006677\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19690.70063981382\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19725.119647435058\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19872.063732313647\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19380.142831803634\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19667.029951645996\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20827.178165233086\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19533.328755190556\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19563.894087261444\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19657.774019032076\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19671.703360047224\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20111.068357125212\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19766.29699819784\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20322.081643700498\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19477.84328686858\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19481.091877531515\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19787.62724431845\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19484.369604056417\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19508.13122261235\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19474.623811968933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20338.181396742188\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19494.37780268803\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19474.623811968933\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19735.22926500854\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19695.526023230366\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19681.141210252492\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19455.917291969396\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19798.478720981417\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19484.36960405642\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20008.819677393956\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19667.029951646\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19424.328891559664\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19750.624342303756\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19798.478720981417\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19588.062809020743\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19571.831453255203\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19848.856609095463\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19567.847931912464\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19809.45480935277\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19446.955524793808\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19609.022792251417\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19452.901080579722\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19907.824283908372\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19957.291452362173\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19700.38189357614\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19406.042199460164\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19455.917291969396\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20633.591524728356\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19766.29699819784\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19681.141210252492\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19695.52602323037\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19353.981978109958\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19571.83145325519\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20463.403609147314\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19690.70063981382\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20055.614997162356\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19418.96043581197\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19468.2721300122\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19511.64278270036\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19533.328755190556\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19671.70336004722\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20446.247247511918\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19648.639068907716\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19508.13122261235\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20048.83171835054\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19446.955524793808\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19462.0366746645\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19644.116889423243\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19525.982498623285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19613.304490377745\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19820.55579611208\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19657.774019032076\n",
      "Times =  1\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19626.329473265407\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19444.02614378868\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19552.21036629748\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19384.594619902404\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19695.526023230374\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19771.583058424123\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20322.081643700516\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19308.167139452646\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19563.89408726144\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19537.0460628199\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19592.19509320966\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19452.901080579715\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19357.69415207422\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19481.09187753152\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19522.35350385011\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19380.142831803638\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19522.353503850114\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19522.35350385011\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19389.160581783937\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19369.512167409666\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19435.4113726105\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19533.32875519055\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19304.91639255748\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19369.51216740967\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19421.630278854027\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19462.036674664494\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19592.19509320966\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19294.93067245359\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19522.353503850114\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19529.640908131336\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19563.894087261437\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19671.703360047206\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19324.36205713698\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19515.183668215275\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20690.087655274176\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19474.623811968933\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19468.272130012192\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19518.753901189226\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19487.67648673696\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19919.999024807035\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19653.191435783683\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20118.14912928996\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19396.22384466035\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19384.594619902404\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19639.624868564784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19367.471440323297\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19389.16058178394\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19357.69415207422\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20118.149129289963\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19398.63547488905\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19371.581346151805\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19552.21036629748\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19484.369604056432\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19452.901080579715\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19371.5813461518\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19700.381893576166\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19363.4752917154\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19843.133630507466\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19540.79285423242\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19352.168432852606\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19644.116889423232\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19705.268282006662\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19411.123362013303\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19455.91729196941\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19725.11964743505\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19401.07573413479\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19617.61615169619\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19353.981978109958\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19462.03667466449\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19355.823880796186\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19809.454809352777\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19782.248146819966\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19529.64090813134\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19328.361088977723\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19361.519845931478\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20387.318568764054\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19710.185219898514\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19544.569152841344\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19596.357207275887\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19304.91639255748\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19525.982498623303\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20403.9784940953\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19563.89408726144\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20015.406716583886\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19324.36205713698\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19380.142831803638\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19384.594619902404\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19396.223844660348\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19508.13122261235\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20420.779648537107\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19487.676486736964\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19393.84082872869\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20002.26518742246\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19389.16058178394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19357.69415207422\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19494.377802688028\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19413.706993475123\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19477.843286868563\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19695.526023230377\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19556.075328969\n",
      "Times =  2\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20097.006501583026\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19843.133630507444\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19950.995785480118\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19750.62434230377\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20139.591408259756\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20362.59243738748\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "21146.621140788367\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19600.54917746634\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20015.40671658387\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19963.61932189007\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19976.37183874938\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19926.134305435415\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19685.90571239265\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19957.291452362188\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20015.40671658387\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19745.461852412776\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19950.995785480118\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19963.61932189007\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19750.624342303774\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19730.159101380166\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19860.397052818545\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20035.3635759848\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19406.042199460164\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19725.119647435044\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19843.133630507447\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19895.77703405328\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20083.077243278356\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19497.77227750144\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19969.979436464622\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19950.995785480118\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19989.253678407935\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20048.83171835054\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19657.774019032084\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19976.371838749375\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "21214.861916840346\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19944.732279100557\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19848.85660909548\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19938.500891341282\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19932.301580575517\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20463.403609147328\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20198.245966587325\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20817.138433026856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19766.296998197846\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19798.47872098145\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20183.380261852224\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19715.132738850076\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19720.11087068045\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19715.132738850076\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20888.22709432792\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19740.330171038127\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19705.268282006637\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20097.006501583022\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19982.796571667117\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19895.77703405328\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19695.526023230388\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20183.38026185222\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19793.03742395616\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20354.420491830202\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19982.796571667117\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19705.268282006637\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20062.431142499816\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20168.649539186015\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19798.478720981446\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19950.995785480118\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20125.26319584822\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19803.951170874465\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20035.363575984797\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19681.141210252514\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19837.442097183077\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19676.407102895366\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20298.192278603412\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20175.99805344189\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19938.500891341286\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19671.703360047177\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19695.52602323039\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "21157.891564757258\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20125.26319584822\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19950.995785480118\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20028.678620959112\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19617.61615169619\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19944.732279100554\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "21169.20308059349\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20055.61499716232\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20624.30539171271\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19644.116889423218\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19793.037423956157\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19782.248146819966\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19750.624342303774\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19907.824283908387\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20982.434540132213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19976.371838749375\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19782.248146819966\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20551.33985680277\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19740.33017103813\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19676.407102895366\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19889.80111817658\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19798.47872098145\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "19913.895697832588\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20062.431142499816\n",
      "['genes --> density', 'estrogen --> genes', 'age --> genes', 'estrogen --> insomnia', 'estrogen --> density', 'bmi --> estrogen', 'density --> cancer']\n",
      "20002.265187422454\n",
      "Model_name =  temp/a0 Violations =  0.0\n",
      "Average_violations =  19826.384549652626 198.5262017793805\n",
      "MSE =  0.7267092932536504 0.006882225362434128\n",
      "Model_name =  temp/a1 Violations =  0.0\n",
      "Average_violations =  19622.349133534324 165.67826489064794\n",
      "MSE =  0.7261241420901836 0.009475967817171062\n",
      "Model_name =  temp/a2 Violations =  0.0\n",
      "Average_violations =  19741.105674152685 163.47892620109803\n",
      "MSE =  0.7216018905396998 0.008864144996753223\n",
      "Model_name =  temp/a3 Violations =  0.0\n",
      "Average_violations =  19577.19205649402 150.04425274714552\n",
      "MSE =  0.7222932586808607 0.010224429569971267\n",
      "Model_name =  temp/a4 Violations =  0.0\n",
      "Average_violations =  19872.455192770034 192.18307226808727\n",
      "MSE =  0.7183254200722479 0.0070893353274467065\n",
      "Model_name =  temp/a5 Violations =  0.0\n",
      "Average_violations =  20013.999926573328 252.689495662364\n",
      "MSE =  0.7276911262822615 0.010518573052657859\n",
      "Model_name =  temp/a6 Violations =  0.0\n",
      "Average_violations =  20694.586322582432 341.2820220483788\n",
      "MSE =  0.7259542605228017 0.010670737948093301\n",
      "Model_name =  temp/a7 Violations =  0.0\n",
      "Average_violations =  19428.892011950393 124.67928962563305\n",
      "MSE =  0.720364841902076 0.0075917431046979315\n",
      "Model_name =  temp/a8 Violations =  0.0\n",
      "Average_violations =  19763.162007914612 188.09717109478223\n",
      "MSE =  0.7233050255031004 0.008313130711567546\n",
      "Model_name =  temp/a9 Violations =  0.0\n",
      "Average_violations =  19706.093845468717 185.04488295170742\n",
      "MSE =  0.7232268881953926 0.009669112058381609\n",
      "Model_name =  temp/a10 Violations =  0.0\n",
      "Average_violations =  19740.58612258091 168.57504048872136\n",
      "MSE =  0.7285145212473078 0.01152738659005837\n",
      "Model_name =  temp/a11 Violations =  0.0\n",
      "Average_violations =  19648.961105975864 201.53069498517365\n",
      "MSE =  0.7230160204174982 0.011849326708385104\n",
      "Model_name =  temp/a12 Violations =  0.0\n",
      "Average_violations =  19480.74511311864 146.03013752718002\n",
      "MSE =  0.7215461013251344 0.010934696496370435\n",
      "Model_name =  temp/a13 Violations =  0.0\n",
      "Average_violations =  19689.70483953851 198.82978882319316\n",
      "MSE =  0.7156324205149608 0.013042087813812907\n",
      "Model_name =  temp/a14 Violations =  0.0\n",
      "Average_violations =  19738.05577444312 205.9543423099264\n",
      "MSE =  0.7175353758966553 0.009592978345511887\n",
      "Model_name =  temp/a15 Violations =  0.0\n",
      "Average_violations =  19545.749155638925 151.07819214118533\n",
      "MSE =  0.7249010578130956 0.010116229030046811\n",
      "Model_name =  temp/a16 Violations =  0.0\n",
      "Average_violations =  19713.459746992074 178.04558559004485\n",
      "MSE =  0.7230748665191987 0.009137469246920356\n",
      "Model_name =  temp/a17 Violations =  0.0\n",
      "Average_violations =  19701.19632581212 189.59274717414127\n",
      "MSE =  0.7251128254967617 0.009658846411551167\n",
      "Model_name =  temp/a18 Violations =  0.0\n",
      "Average_violations =  19556.47527740635 148.78188750220104\n",
      "MSE =  0.7148504098684562 0.008787990420953958\n",
      "Model_name =  temp/a19 Violations =  0.0\n",
      "Average_violations =  19522.647799600676 152.1711488817273\n",
      "MSE =  0.7230254210519641 0.010473889513372888\n",
      "Model_name =  temp/a20 Violations =  0.0\n",
      "Average_violations =  19622.546626228082 177.16701851214793\n",
      "MSE =  0.7260874827661853 0.009718284817736664\n",
      "Model_name =  temp/a21 Violations =  0.0\n",
      "Average_violations =  19742.15545006914 213.46336102537805\n",
      "MSE =  0.722808599225696 0.012261153954488731\n",
      "Model_name =  temp/a22 Violations =  0.0\n",
      "Average_violations =  19341.273676719622 45.91300515539206\n",
      "MSE =  0.7256271260235957 0.010979494568846943\n",
      "Model_name =  temp/a23 Violations =  0.0\n",
      "Average_violations =  19534.25434581902 146.34658209663883\n",
      "MSE =  0.7263764654078616 0.006794394671230571\n",
      "Model_name =  temp/a24 Violations =  0.0\n",
      "Average_violations =  19595.70580440386 179.72513988711137\n",
      "MSE =  0.7244461754480346 0.011338719659097926\n",
      "Model_name =  temp/a25 Violations =  0.0\n",
      "Average_violations =  19662.848299146535 178.52414669021277\n",
      "MSE =  0.7270112019648813 0.010050672635132934\n",
      "Model_name =  temp/a26 Violations =  0.0\n",
      "Average_violations =  19796.80169177937 208.55644274942284\n",
      "MSE =  0.7273080674166739 0.010319882417126513\n",
      "Model_name =  temp/a27 Violations =  0.0\n",
      "Average_violations =  19373.688012977585 88.79593351824138\n",
      "MSE =  0.726400253463645 0.011289736826924572\n",
      "Model_name =  temp/a28 Violations =  0.0\n",
      "Average_violations =  19732.533740773804 183.7567192759465\n",
      "MSE =  0.7244916514612975 0.008905425147892594\n",
      "Model_name =  temp/a29 Violations =  0.0\n",
      "Average_violations =  19723.779111141757 173.6003496836853\n",
      "MSE =  0.7255516576746289 0.008121171840777415\n",
      "Model_name =  temp/a30 Violations =  0.0\n",
      "Average_violations =  19759.42247103481 175.3381651272112\n",
      "MSE =  0.7257166509315214 0.00822926830668944\n",
      "Model_name =  temp/a31 Violations =  0.0\n",
      "Average_violations =  19864.199603570465 154.06239655803697\n",
      "MSE =  0.7243791782639134 0.006286393083485171\n",
      "Model_name =  temp/a32 Violations =  0.0\n",
      "Average_violations =  19454.092969324232 145.8134680866213\n",
      "MSE =  0.7191542610584211 0.012803108413082336\n",
      "Model_name =  temp/a33 Violations =  0.0\n",
      "Average_violations =  19719.528486203548 191.90397643642072\n",
      "MSE =  0.722258187580529 0.010482234758678868\n",
      "Model_name =  temp/a34 Violations =  0.0\n",
      "Average_violations =  20910.70924578254 222.23124212390226\n",
      "MSE =  0.7270643347875915 0.011114511197599635\n",
      "Model_name =  temp/a35 Violations =  0.0\n",
      "Average_violations =  19650.89494875335 209.1520182687612\n",
      "MSE =  0.7230841699781937 0.014039897066635066\n",
      "Model_name =  temp/a36 Violations =  0.0\n",
      "Average_violations =  19627.007608789707 161.65523783564484\n",
      "MSE =  0.7217991419668209 0.0119585794451943\n",
      "Model_name =  temp/a37 Violations =  0.0\n",
      "Average_violations =  19705.009603854192 174.58576553381675\n",
      "MSE =  0.7224623688601804 0.009550672628566582\n",
      "Model_name =  temp/a38 Violations =  0.0\n",
      "Average_violations =  19697.227142453234 182.41247420606643\n",
      "MSE =  0.7262363711015705 0.010237739978523324\n",
      "Model_name =  temp/a39 Violations =  0.0\n",
      "Average_violations =  20164.823663693194 225.0768169739311\n",
      "MSE =  0.7254625006871022 0.011450717429547846\n",
      "Model_name =  temp/a40 Violations =  0.0\n",
      "Average_violations =  19872.578133522948 234.86573495210732\n",
      "MSE =  0.725225568840732 0.007682862583749347\n",
      "Model_name =  temp/a41 Violations =  0.0\n",
      "Average_violations =  20419.123068672438 293.49536090040255\n",
      "MSE =  0.7268968164139156 0.008147456495732614\n",
      "Model_name =  temp/a42 Violations =  0.0\n",
      "Average_violations =  19546.78804324226 158.75257334579044\n",
      "MSE =  0.7234733048016881 0.007287985089491314\n",
      "Model_name =  temp/a43 Violations =  0.0\n",
      "Average_violations =  19554.721739471788 176.80691862124687\n",
      "MSE =  0.7229140115821734 0.013348274415584092\n",
      "Model_name =  temp/a44 Violations =  0.0\n",
      "Average_violations =  19870.210791578484 229.53941366837572\n",
      "MSE =  0.7254406774687056 0.010234499033641578\n",
      "Model_name =  temp/a45 Violations =  0.0\n",
      "Average_violations =  19522.324594409933 144.44729279309894\n",
      "MSE =  0.7253523237819346 0.009257723356002188\n",
      "Model_name =  temp/a46 Violations =  0.0\n",
      "Average_violations =  19539.13422502558 136.87686205527248\n",
      "MSE =  0.7238474210097358 0.0069500920695866305\n",
      "Model_name =  temp/a47 Violations =  0.0\n",
      "Average_violations =  19515.816900964408 148.8024165682409\n",
      "MSE =  0.721188327520523 0.01326735751014596\n",
      "Model_name =  temp/a48 Violations =  0.0\n",
      "Average_violations =  20448.185873453356 323.8628894368981\n",
      "MSE =  0.7242575253218776 0.009256047633326804\n",
      "Model_name =  temp/a49 Violations =  0.0\n",
      "Average_violations =  19544.44781620507 143.9191232007283\n",
      "MSE =  0.7263673965814572 0.008614975403709839\n",
      "Model_name =  temp/a50 Violations =  0.0\n",
      "Average_violations =  19517.15781337579 139.50770288603098\n",
      "MSE =  0.7214920884082172 0.009318988134722038\n",
      "Model_name =  temp/a51 Violations =  0.0\n",
      "Average_violations =  19794.815377629682 226.36782143006405\n",
      "MSE =  0.7236505901737088 0.010790653255270448\n",
      "Model_name =  temp/a52 Violations =  0.0\n",
      "Average_violations =  19720.897399651305 204.2712911245165\n",
      "MSE =  0.7232297080443401 0.006424793645512373\n",
      "Model_name =  temp/a53 Violations =  0.0\n",
      "Average_violations =  19676.606441628497 180.83178303999333\n",
      "MSE =  0.7220994046686761 0.011527172680894194\n",
      "Model_name =  temp/a54 Violations =  0.0\n",
      "Average_violations =  19507.67488711719 137.22044294613477\n",
      "MSE =  0.718855475239281 0.011130778957053819\n",
      "Model_name =  temp/a55 Violations =  0.0\n",
      "Average_violations =  19894.0802921366 208.4491964401838\n",
      "MSE =  0.7221190331270173 0.009562804811073662\n",
      "Model_name =  temp/a56 Violations =  0.0\n",
      "Average_violations =  19546.960773242663 180.8667045255198\n",
      "MSE =  0.726599860624772 0.013719735733663604\n",
      "Model_name =  temp/a57 Violations =  0.0\n",
      "Average_violations =  20068.791266577206 212.99609898178468\n",
      "MSE =  0.7263817489418161 0.009110357000154595\n",
      "Model_name =  temp/a58 Violations =  0.0\n",
      "Average_violations =  19730.206459181845 185.89472806733588\n",
      "MSE =  0.7242065414694557 0.00959616414993126\n",
      "Model_name =  temp/a59 Violations =  0.0\n",
      "Average_violations =  19493.9218688063 152.32041388618507\n",
      "MSE =  0.7198414037772247 0.005922449906387406\n",
      "Model_name =  temp/a60 Violations =  0.0\n",
      "Average_violations =  19819.057458075604 177.49933724277602\n",
      "MSE =  0.725353462070328 0.00947995168358334\n",
      "Model_name =  temp/a61 Violations =  0.0\n",
      "Average_violations =  19890.798847391365 200.12129935206974\n",
      "MSE =  0.7245800714690738 0.010391687588346816\n",
      "Model_name =  temp/a62 Violations =  0.0\n",
      "Average_violations =  19599.22163067183 158.33389404633775\n",
      "MSE =  0.7281636435840836 0.012302311063170696\n",
      "Model_name =  temp/a63 Violations =  0.0\n",
      "Average_violations =  19659.58151023491 211.42490086579102\n",
      "MSE =  0.7233539387227291 0.007170409327522461\n",
      "Model_name =  temp/a64 Violations =  0.0\n",
      "Average_violations =  19899.746484126244 167.27432432914293\n",
      "MSE =  0.7260533532789216 0.01022919029562159\n",
      "Model_name =  temp/a65 Violations =  0.0\n",
      "Average_violations =  19590.958278973907 165.28303096612063\n",
      "MSE =  0.7238229743016676 0.010807148275244299\n",
      "Model_name =  temp/a66 Violations =  0.0\n",
      "Average_violations =  19820.811512344586 170.73363006366435\n",
      "MSE =  0.726234747532696 0.008357980704531481\n",
      "Model_name =  temp/a67 Violations =  0.0\n",
      "Average_violations =  19494.026237718757 137.6469634135921\n",
      "MSE =  0.7277365195947344 0.010201556902452297\n",
      "Model_name =  temp/a68 Violations =  0.0\n",
      "Average_violations =  19636.167188032992 154.4558652837973\n",
      "MSE =  0.7289365164080093 0.008559747572205689\n",
      "Model_name =  temp/a69 Violations =  0.0\n",
      "Average_violations =  19495.044021423757 134.22722319537348\n",
      "MSE =  0.7154603204285185 0.010015001663081715\n",
      "Model_name =  temp/a70 Violations =  0.0\n",
      "Average_violations =  20005.157123954854 211.06292865085558\n",
      "MSE =  0.7223815326600174 0.009849464879489204\n",
      "Model_name =  temp/a71 Violations =  0.0\n",
      "Average_violations =  19971.84588420801 161.07683643934212\n",
      "MSE =  0.7268265847281588 0.00942634336193647\n",
      "Model_name =  temp/a72 Violations =  0.0\n",
      "Average_violations =  19722.841231016257 167.67018786155967\n",
      "MSE =  0.7274811741221101 0.011413792276298573\n",
      "Model_name =  temp/a73 Violations =  0.0\n",
      "Average_violations =  19468.702216161688 147.00495920332995\n",
      "MSE =  0.7148515151423854 0.009207454665837434\n",
      "Model_name =  temp/a74 Violations =  0.0\n",
      "Average_violations =  19504.32105371042 140.58739803996156\n",
      "MSE =  0.7189593267964893 0.004504272608845327\n",
      "Model_name =  temp/a75 Violations =  0.0\n",
      "Average_violations =  20726.267219416553 321.3381125107329\n",
      "MSE =  0.7270819315197764 0.00803622515012793\n",
      "Model_name =  temp/a76 Violations =  0.0\n",
      "Average_violations =  19867.24847131486 183.87646482171704\n",
      "MSE =  0.7272500207771451 0.00947850130024357\n",
      "Model_name =  temp/a77 Violations =  0.0\n",
      "Average_violations =  19725.56871619132 168.87076793114397\n",
      "MSE =  0.7267045194822801 0.00752650706749294\n",
      "Model_name =  temp/a78 Violations =  0.0\n",
      "Average_violations =  19773.52061715512 184.91046246473064\n",
      "MSE =  0.7230886842860414 0.008846077373236843\n",
      "Model_name =  temp/a79 Violations =  0.0\n",
      "Average_violations =  19425.504840787875 137.31211346317397\n",
      "MSE =  0.7170830922343656 0.010132076472160335\n",
      "Model_name =  temp/a80 Violations =  0.0\n",
      "Average_violations =  19680.84874365968 187.5303030939969\n",
      "MSE =  0.7261240496725803 0.012940902239639502\n",
      "Model_name =  temp/a81 Violations =  0.0\n",
      "Average_violations =  20678.861727945365 347.5714004942755\n",
      "MSE =  0.725880262146512 0.008513365005743224\n",
      "Model_name =  temp/a82 Violations =  0.0\n",
      "Average_violations =  19770.069908079193 208.44179661986058\n",
      "MSE =  0.7230199280827926 0.011332192232368074\n",
      "Model_name =  temp/a83 Violations =  0.0\n",
      "Average_violations =  20231.775701819653 278.04537344640414\n",
      "MSE =  0.7238392169694357 0.007427172110786336\n",
      "Model_name =  temp/a84 Violations =  0.0\n",
      "Average_violations =  19462.479794124058 134.11745872809144\n",
      "MSE =  0.7211784398873622 0.011218853701838656\n",
      "Model_name =  temp/a85 Violations =  0.0\n",
      "Average_violations =  19547.150795257334 177.55162428683286\n",
      "MSE =  0.7236297613467088 0.011492983315097986\n",
      "Model_name =  temp/a86 Violations =  0.0\n",
      "Average_violations =  19559.495183140913 165.8301762616122\n",
      "MSE =  0.7256811478270841 0.011919467094266101\n",
      "Model_name =  temp/a87 Violations =  0.0\n",
      "Average_violations =  19560.058980718226 145.91277508623\n",
      "MSE =  0.7235843481632281 0.003707870846709654\n",
      "Model_name =  temp/a88 Violations =  0.0\n",
      "Average_violations =  19695.886288855985 164.06755995905482\n",
      "MSE =  0.7260127643408151 0.006644257545698668\n",
      "Model_name =  temp/a89 Violations =  0.0\n",
      "Average_violations =  20616.487145393745 258.9726773700129\n",
      "MSE =  0.7270282204811201 0.012362004870333326\n",
      "Model_name =  temp/a90 Violations =  0.0\n",
      "Average_violations =  19704.229131464686 203.34449924688482\n",
      "MSE =  0.7231865118916492 0.00640663859657803\n",
      "Model_name =  temp/a91 Violations =  0.0\n",
      "Average_violations =  19561.406732720337 162.98010342710305\n",
      "MSE =  0.7267219482764152 0.01247664438063686\n",
      "Model_name =  temp/a92 Violations =  0.0\n",
      "Average_violations =  20200.812254191922 248.58842906130275\n",
      "MSE =  0.7238652050837532 0.010692388006188111\n",
      "Model_name =  temp/a93 Violations =  0.0\n",
      "Average_violations =  19525.482092538627 153.7418539534705\n",
      "MSE =  0.7244494864591169 0.008753584439561283\n",
      "Model_name =  temp/a94 Violations =  0.0\n",
      "Average_violations =  19498.712643211362 132.67336144812822\n",
      "MSE =  0.7260877134936546 0.010428417560408326\n",
      "Model_name =  temp/a95 Violations =  0.0\n",
      "Average_violations =  19676.098603429284 163.00720256287806\n",
      "MSE =  0.7237110494058404 0.007946978072291735\n",
      "Model_name =  temp/a96 Violations =  0.0\n",
      "Average_violations =  19579.389404359954 161.55812938861882\n",
      "MSE =  0.723724464048467 0.007828184455644047\n",
      "Model_name =  temp/a97 Violations =  0.0\n",
      "Average_violations =  19668.3478250263 182.2228540255204\n",
      "MSE =  0.7262025799390569 0.010379585201640373\n",
      "Model_name =  temp/a98 Violations =  0.0\n",
      "Average_violations =  19859.50432061409 152.29922807244319\n",
      "MSE =  0.7238763056326444 0.007807029818322628\n",
      "Model_name =  temp/a99 Violations =  0.0\n",
      "Average_violations =  19738.70484514118 190.93401374594376\n",
      "MSE =  0.7199700675210986 0.009419922477808564\n",
      "[0.72670929 0.72612414 0.72160189 0.72229326 0.71832542 0.72769113\n",
      " 0.72595426 0.72036484 0.72330503 0.72322689 0.72851452 0.72301602\n",
      " 0.7215461  0.71563242 0.71753538 0.72490106 0.72307487 0.72511283\n",
      " 0.71485041 0.72302542 0.72608748 0.7228086  0.72562713 0.72637647\n",
      " 0.72444618 0.7270112  0.72730807 0.72640025 0.72449165 0.72555166\n",
      " 0.72571665 0.72437918 0.71915426 0.72225819 0.72706433 0.72308417\n",
      " 0.72179914 0.72246237 0.72623637 0.7254625  0.72522557 0.72689682\n",
      " 0.7234733  0.72291401 0.72544068 0.72535232 0.72384742 0.72118833\n",
      " 0.72425753 0.7263674  0.72149209 0.72365059 0.72322971 0.7220994\n",
      " 0.71885548 0.72211903 0.72659986 0.72638175 0.72420654 0.7198414\n",
      " 0.72535346 0.72458007 0.72816364 0.72335394 0.72605335 0.72382297\n",
      " 0.72623475 0.72773652 0.72893652 0.71546032 0.72238153 0.72682658\n",
      " 0.72748117 0.71485152 0.71895933 0.72708193 0.72725002 0.72670452\n",
      " 0.72308868 0.71708309 0.72612405 0.72588026 0.72301993 0.72383922\n",
      " 0.72117844 0.72362976 0.72568115 0.72358435 0.72601276 0.72702822\n",
      " 0.72318651 0.72672195 0.72386521 0.72444949 0.72608771 0.72371105\n",
      " 0.72372446 0.72620258 0.72387631 0.71997007] [0.00688223 0.00947597 0.00886414 0.01022443 0.00708934 0.01051857\n",
      " 0.01067074 0.00759174 0.00831313 0.00966911 0.01152739 0.01184933\n",
      " 0.0109347  0.01304209 0.00959298 0.01011623 0.00913747 0.00965885\n",
      " 0.00878799 0.01047389 0.00971828 0.01226115 0.01097949 0.00679439\n",
      " 0.01133872 0.01005067 0.01031988 0.01128974 0.00890543 0.00812117\n",
      " 0.00822927 0.00628639 0.01280311 0.01048223 0.01111451 0.0140399\n",
      " 0.01195858 0.00955067 0.01023774 0.01145072 0.00768286 0.00814746\n",
      " 0.00728799 0.01334827 0.0102345  0.00925772 0.00695009 0.01326736\n",
      " 0.00925605 0.00861498 0.00931899 0.01079065 0.00642479 0.01152717\n",
      " 0.01113078 0.0095628  0.01371974 0.00911036 0.00959616 0.00592245\n",
      " 0.00947995 0.01039169 0.01230231 0.00717041 0.01022919 0.01080715\n",
      " 0.00835798 0.01020156 0.00855975 0.010015   0.00984946 0.00942634\n",
      " 0.01141379 0.00920745 0.00450427 0.00803623 0.0094785  0.00752651\n",
      " 0.00884608 0.01013208 0.0129409  0.00851337 0.01133219 0.00742717\n",
      " 0.01121885 0.01149298 0.01191947 0.00370787 0.00664426 0.012362\n",
      " 0.00640664 0.01247664 0.01069239 0.00875358 0.01042842 0.00794698\n",
      " 0.00782818 0.01037959 0.00780703 0.00941992] [19826.38454965 19622.34913353 19741.10567415 19577.19205649\n",
      " 19872.45519277 20013.99992657 20694.58632258 19428.89201195\n",
      " 19763.16200791 19706.09384547 19740.58612258 19648.96110598\n",
      " 19480.74511312 19689.70483954 19738.05577444 19545.74915564\n",
      " 19713.45974699 19701.19632581 19556.47527741 19522.6477996\n",
      " 19622.54662623 19742.15545007 19341.27367672 19534.25434582\n",
      " 19595.7058044  19662.84829915 19796.80169178 19373.68801298\n",
      " 19732.53374077 19723.77911114 19759.42247103 19864.19960357\n",
      " 19454.09296932 19719.5284862  20910.70924578 19650.89494875\n",
      " 19627.00760879 19705.00960385 19697.22714245 20164.82366369\n",
      " 19872.57813352 20419.12306867 19546.78804324 19554.72173947\n",
      " 19870.21079158 19522.32459441 19539.13422503 19515.81690096\n",
      " 20448.18587345 19544.44781621 19517.15781338 19794.81537763\n",
      " 19720.89739965 19676.60644163 19507.67488712 19894.08029214\n",
      " 19546.96077324 20068.79126658 19730.20645918 19493.92186881\n",
      " 19819.05745808 19890.79884739 19599.22163067 19659.58151023\n",
      " 19899.74648413 19590.95827897 19820.81151234 19494.02623772\n",
      " 19636.16718803 19495.04402142 20005.15712395 19971.84588421\n",
      " 19722.84123102 19468.70221616 19504.32105371 20726.26721942\n",
      " 19867.24847131 19725.56871619 19773.52061716 19425.50484079\n",
      " 19680.84874366 20678.86172795 19770.06990808 20231.77570182\n",
      " 19462.47979412 19547.15079526 19559.49518314 19560.05898072\n",
      " 19695.88628886 20616.48714539 19704.22913146 19561.40673272\n",
      " 20200.81225419 19525.48209254 19498.71264321 19676.09860343\n",
      " 19579.38940436 19668.34782503 19859.50432061 19738.70484514] [198.52620178 165.67826489 163.4789262  150.04425275 192.18307227\n",
      " 252.68949566 341.28202205 124.67928963 188.09717109 185.04488295\n",
      " 168.57504049 201.53069499 146.03013753 198.82978882 205.95434231\n",
      " 151.07819214 178.04558559 189.59274717 148.7818875  152.17114888\n",
      " 177.16701851 213.46336103  45.91300516 146.3465821  179.72513989\n",
      " 178.52414669 208.55644275  88.79593352 183.75671928 173.60034968\n",
      " 175.33816513 154.06239656 145.81346809 191.90397644 222.23124212\n",
      " 209.15201827 161.65523784 174.58576553 182.41247421 225.07681697\n",
      " 234.86573495 293.4953609  158.75257335 176.80691862 229.53941367\n",
      " 144.44729279 136.87686206 148.80241657 323.86288944 143.9191232\n",
      " 139.50770289 226.36782143 204.27129112 180.83178304 137.22044295\n",
      " 208.44919644 180.86670453 212.99609898 185.89472807 152.32041389\n",
      " 177.49933724 200.12129935 158.33389405 211.42490087 167.27432433\n",
      " 165.28303097 170.73363006 137.64696341 154.45586528 134.2272232\n",
      " 211.06292865 161.07683644 167.67018786 147.0049592  140.58739804\n",
      " 321.33811251 183.87646482 168.87076793 184.91046246 137.31211346\n",
      " 187.53030309 347.57140049 208.44179662 278.04537345 134.11745873\n",
      " 177.55162429 165.83017626 145.91277509 164.06755996 258.97267737\n",
      " 203.34449925 162.98010343 248.58842906 153.74185395 132.67336145\n",
      " 163.00720256 161.55812939 182.22285403 152.29922807 190.93401375]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1332x756 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#the number of times to sample\n",
    "times = 3\n",
    "## the size of the test set\n",
    "\n",
    "\n",
    "violations = np.zeros(len(models))\n",
    "violation_mean = np.zeros((len(models), times))\n",
    "violation_mean2 = np.zeros((len(models), times))\n",
    "mean = np.zeros((len(models), times))\n",
    "\n",
    "fold = 0\n",
    "\n",
    "\n",
    "\n",
    "for t in range(times):\n",
    "    print(\"Times = \", t)\n",
    "    df_test = gen_data(SIZE = nb_test)\n",
    "    x_test = df_test[inputs].values\n",
    "    x_test_norm = normalize(df_test[inputs].values)\n",
    "    y_test = df_test[target].values\n",
    "    #bic_orig = get_bic(df_test,prior)\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        if type(models[idx]) is list:\n",
    "            keras.backend.clear_session()\n",
    "            model = load_model(model_name)\n",
    "        else:\n",
    "            model = models[idx]\n",
    "            \n",
    "        predicted = model.predict(x_test_norm)[:,1]\n",
    "        test_df = pd.DataFrame(x_test, columns = inputs)\n",
    "        test_targets = pd.DataFrame(predicted,columns = target)\n",
    "        test_df = test_df.join(test_targets)\n",
    "       \n",
    "        \n",
    "    \n",
    "        mean[idx][t] = roc_auc_score(y_test, predicted) \n",
    "        test_df[test_df['cancer'] > 0.5] = 1\n",
    "        test_df[test_df['cancer'] <= 0.5] = 0\n",
    "        bic_pred = get_bic(test_df,prior)\n",
    "        \n",
    "        #bic_pred = get_bic(df_test.join(pd.DataFrame(model.predict(x_test), columns = ['target'])), prior)\n",
    "        \n",
    "        print(tetrad.getEdges())\n",
    "        print(bic_pred)\n",
    "        violation_mean[idx][t] = bic_pred\n",
    "        violation_mean2[idx][t] = bic_pred\n",
    "        #print(bic_orig - bic_pred)\n",
    "metric = []\n",
    "metric_err = []\n",
    "viol = []\n",
    "viol_err = []\n",
    "\n",
    "#normalize the violations for prettier graphing.\n",
    "#also violations are always positive, so just divide by max.\n",
    "\n",
    "#TMK\n",
    "#violation_mean = violation_mean / np.max(violation_mean)\n",
    "\n",
    "for i in range(len(violations)):\n",
    "    print(\"Model_name = \", model_names[i], \"Violations = \", violations[i])\n",
    "    print(\"Average_violations = \", np.mean(violation_mean[i]), np.std(violation_mean[i]))\n",
    "    print(\"MSE = \", np.mean(mean[i]), np.std(mean[i]))\n",
    "    #print(\"mean = \", mean[i])\n",
    "    metric.append(np.mean(mean[i]))\n",
    "    metric_err.append(np.std(mean[i]))\n",
    "    viol.append(np.mean(violation_mean[i]))\n",
    "    #viol.append(violations[i]/times)\n",
    "    viol_err.append(np.std(violation_mean[i]))\n",
    "print(np.array(metric), \n",
    "         np.array(metric_err), \n",
    "         np.array(viol), \n",
    "         np.array(viol_err))    \n",
    "\n",
    "bar_plot(model_names, \n",
    "         np.array(metric), \n",
    "         np.array(metric_err), \n",
    "         np.array(viol), \n",
    "         np.array(viol_err))\n",
    "\n",
    "\n",
    "    \n",
    "MSE = []\n",
    "VIO = []\n",
    "VIO2 = []\n",
    "AUS = []\n",
    "for i, m in enumerate(models):\n",
    "    MSE.append(np.mean(mean[i]))\n",
    "    VIO.append(np.mean(violation_mean[i]))\n",
    "    VIO2.append(np.mean(violation_mean2[i]))\n",
    "    AUS.append(metrics_dicts[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best by BIC =  0.693430843476607\n",
      "Best by AUC =  0.693782688284519\n",
      "Best by MET =  0.692839779859262\n",
      "Random =  0.6931610165462153\n",
      "-0.029339015216299124\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030409193256815382\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3X98VPWd7/HXhwQQsEoQ7KooPwTpgg0gEfFqFfQWf7H+aqtQbLFa3bXa3dLear1blWJ7W6uuXbpYL0Wr27WlFlvlWgW0KtZqtiSCClgKgihWJWLwN5Ifn/vHOUmHMJlMOHNmzsy8n49HHmROznznc4bkfOb729wdERGRvdWj0AGIiEhxUyIREZFIlEhERCQSJRIREYlEiURERCJRIhERkUiUSEREJBIlEhERiUSJREREIqksdAD5MHDgQB86dGihwxARKSr19fVvuvugrs6LNZGY2anAvwMVwEJ3/0GHn98CTAkf9gUOdPf+4c9mAd8Of/Zdd78rPP44cBDwYfizqe6+LVMcQ4cOpa6uLvoFiYiUETPbks15sSUSM6sA5gOfBrYCK81sibuvazvH3WennP9VYHz4/QDgOqAGcKA+fG5jePpMd1dmEBFJgDj7SCYCG919k7vvAhYBZ2U4fwbwy/D7U4CH3f2tMHk8DJwaY6wiIrKX4kwkhwCvpDzeGh7bg5kNAYYBj2b53J+Z2Wozu8bMrJMyLzWzOjOra2ho2NtrEBGRLsSZSNLd4Dtbs346sNjdW7J47kx3/yTwqfDrC+kKdPcF7l7j7jWDBnXZVyQiInspzkSyFTg05fFg4K+dnDudvzVrZXyuu78a/vsu8AuCJjQRESmQOBPJSmCkmQ0zs14EyWJJx5PMbBRQBTydcngZMNXMqsysCpgKLDOzSjMbGD6vJzANWBPjNYiISBdiG7Xl7s1mdgVBUqgA7nD3tWY2F6hz97akMgNY5ClbNbr7W2Z2PUEyApgbHutHkFB6hmU+Avw0rmsQkfyp39JI7abtTBp+ABOGVBU6HOkGK4etdmtqalzzSESSq35LIzMX1rKruZVelT24+8uTlEwSwMzq3b2mq/O0RIqIFFztpu3sam6l1aGpuZXaTdsLHZJ0gxKJiBTcpOEH0KuyBxUGPSt7MGn4AYUOSbqhLNbaEpFkmzCkiru/PEl9JEVKiUREEmHCkColkCKlpi0REYlEiURERCJRIhERkUiUSLqhoqKCcePGceSRR/K5z32ODz74oNAhxW7GjBlUV1dzyy238Oc//5lx48Yxfvx4Xnzxxd3O27x5M8cccwwjR47k/PPPZ9euXWnL+/73v8+IESMYNWoUy5YtA2Dnzp1MnDiRsWPHMmbMGK677rqsy128eDFmpv1mRApIiaQb+vTpw+rVq1mzZg29evXitttui1xmS0tL1ycVyOuvv85TTz3Fc889x+zZs7nvvvs466yzWLVqFYcffvhu51511VXMnj2bDRs2UFVVxe23375HeevWrWPRokWsXbuWpUuX8pWvfIWWlhZ69+7No48+yrPPPsvq1atZunQptbW1XZb77rvvMm/ePI455ph43wgRyUiJZC996lOfYuPGjQD813/9FxMnTmTcuHH84z/+Y3tyuOyyy6ipqdnjU/bQoUOZO3cuxx9/PL/+9a+ZN28eo0ePprq6munTpwPw1ltvcfbZZ1NdXc2kSZN47rnnAJgzZw4XXXQRkydPZvjw4cybNy9tfEuXLuWoo45i7NixnHzyyRnLfP/997nooos4+uijGT9+PPfffz8AU6dOZdu2bYwbN47vfOc7/OhHP2LhwoVMmTJlt9dydx599FE++9nPAjBr1izuu+++PWK6//77mT59Or1792bYsGGMGDGCP/3pT5gZ++67LwBNTU00NTVhZl2We80113DllVeyzz77ZP3/JiIxcPeS/5owYYLnQr9+/dzdvampyc8880y/9dZbfd26dT5t2jTftWuXu7tfdtllftddd7m7+/bt293dvbm52U888UR/9tln3d19yJAhfsMNN7SXe9BBB/nOnTvd3b2xsdHd3a+44gqfM2eOu7v//ve/97Fjx7q7+3XXXefHHnus79y50xsaGnzAgAHtr91m27ZtPnjwYN+0adNucXRW5tVXX+0///nP219/5MiR/t577/nmzZt9zJgx7eVed911fuONN7Y/Pu200/zVV1/1hoYGP/zww9uPv/zyy7s9r83ll1/e/jru7hdddJH/+te/bn+Pxo4d6/369fMrr7zS3T1juc8884yfe+657u5+4okn+sqVK/d4PRGJhmBdxC7vsZpH0g0ffvgh48aNA4IaycUXX8yCBQuor6/n6KOPbj/nwAMPBOCee+5hwYIFNDc389prr7Fu3Tqqq6sBOP/889vLra6uZubMmZx99tmcffbZADz55JPce++9AJx00kls376dt99+G4AzzjiD3r1707t3bw488EDeeOMNBg8e3F5ebW0tJ5xwAsOGDQNgwIABGctcvnw5S5Ys4aabbgKCPouXX36ZPn36ZHw/HnzwQQDSbRyWbr8xT7OuW9t5FRUVrF69mh07dnDOOeewZs0aPv7xj6c9v7W1ldmzZ3PnnXdmjE9E8kOJpBva+khSuTuzZs3i+9///m7HN2/ezE033cTKlSupqqriwgsvZOfOne0/79evX/v3v/vd73jiiSdYsmQJ119/PWvXrs140+3du3f7sYqKCpqbm/eIqTs3cnfn3nvvZdSoUbv97KWXXtrj/HQGDhzIjh07aG5uprKykq1bt3LwwQfvcd7gwYN55ZW/bXyZ7rz+/fszefJkli5dyje+8Y205b777rusWbOGyZMnA0FfzplnnsmSJUuoqelyfTkRyTH1kUR08skns3jxYrZt2wYE/RBbtmzhnXfeoV+/fuy///688cYbPPTQQ2mf39rayiuvvMKUKVP44Q9/yI4dO3jvvfc44YQTuPvuuwF4/PHHGThwIPvtt19WMR177LGsWLGCzZs3t8cEdFrmKaecwo9//OP2RLNq1apuvQdmxpQpU1i8eDEAd911F2edddYe55155pksWrSIjz76iM2bN7NhwwYmTpxIQ0MDO3bsAIIa3SOPPMInPvGJTsvdf//9efPNN3nppZd46aWXmDRpkpKISAGpRhLR6NGj+e53v8vUqVNpbW2lZ8+ezJ8/n0mTJjF+/HjGjBnD8OHDOe6449I+v6WlhQsuuIC3334bd2f27Nn079+fOXPm8KUvfYnq6mr69u3LXXfdlXVMgwYNYsGCBZx77rm0trZy4IEH8vDDD3da5jXXXMPXvvY1qqurcXeGDh3KAw880OXrnH766SxcuJCDDz6YG264genTp/Ptb3+b8ePHc/HFFwOwZMkS6urqmDt3LmPGjOG8885j9OjRVFZWMn/+fCoqKnjttdeYNWsWLS0ttLa2ct555zFt2jSATssVkeTQfiQiIpKW9iMREZG8UCIREZFIlEhERCQSJRIREYlEiURERCJRIhERkUiUSEREJBIlEhERiUSJREREIlEiERGRSJRIREQkEiUSERGJRIlEREpW/ZZG5j+2kfotjYUOpaRpGXkRKUn1WxqZubCWXc2t9Krswd1fnsSEIVU5Kbd203YmDT8gJ+WVAiUSESlJtZu2s6u5lVaHpuZWajdtj3zjjys5FTs1bYlISZo0/AB6VfagwqBnZQ8mDT8gcpnpkpOoRiIiJWrCkCru/vKknDZDtSWnpubWnCWnUqAdEkVEuqGc+kiy3SFRNRIRkW6YMKSq5BNId6mPREREIlEiERGRSJRIREQkklgTiZmdambrzWyjmX0rzc9vMbPV4ddfzGxHys9mmdmG8GtWyvEJZvZ8WOY8M7M4r0FERDKLrbPdzCqA+cCnga3ASjNb4u7r2s5x99kp538VGB9+PwC4DqgBHKgPn9sI/AS4FKgFHgROBR6K6zpERCSzOGskE4GN7r7J3XcBi4CzMpw/A/hl+P0pwMPu/laYPB4GTjWzg4D93P1pD8Yt/ydwdnyXICIiXYkzkRwCvJLyeGt4bA9mNgQYBjzaxXMPCb/vskwREcmPOBNJur6LzmY/TgcWu3tLF8/Nukwzu9TM6sysrqGhoctgRSR5kr56b9Ljy5c4JyRuBQ5NeTwY+Gsn504HLu/w3Mkdnvt4eHxwNmW6+wJgAQQz27MPW0SSIOkLJCY9vnyKs0ayEhhpZsPMrBdBsljS8SQzGwVUAU+nHF4GTDWzKjOrAqYCy9z9NeBdM5sUjtb6InB/jNcgIgWS9AUSkx5fPsWWSNy9GbiCICm8ANzj7mvNbK6ZnZly6gxgkacs+uXubwHXEySjlcDc8BjAZcBCYCPwIhqxJVJwcTTxxLF6by6li69cm7q0aKOIRBJnE0/SF0hMjQ8ouaYuLdooInkRxwZSbbJdILFQCSc1vvmPbYztfUg6JZIikvRPZ1KeCr1HRyE7vVP/Jgv9PhSSEkmR0AgRSao4NpDqjjhrRJmk+5ss5PtQSEokRaJQfywi2SjkHh2Fqgmk+5u8fMqIsvy7VCIpEuVcbRaBzpt2C1Uj0t/k32jUVhFRH4mUq6Q27Zb636RGbZUgbfEp5SqpTbv6mwxoYysRSbykT04sd6qRiEjiFXpkmGSmRCIiRUHNSMmlpi0REYlEiURERCJRIhERkUiUSEREJBIlkjJXrvsniEjuaNRWGUvqbGERKS6qkZQxbRUqIrmgRFLGNFtYRHJBTVtlTLOFRSQXlEjKnGYLF4ekrjJbDnEl9RqTpNNEYmaDgEHuvq7D8THANndviDs4EUnOoIiON9SkxJUuzlzFldRrTJpMfSQ/BgalOT4Y+Pd4whGRjpIwKKLthnrz8vXMXFjbnlQKHVc6uYirbVj8b57ZmshrTJpMTVufdPcVHQ+6+zIzuznGmEQkRRJ24kt3c05CXOlEjSu1FlLZw6is6EFLS7KuMWkyJZKee/kzEcmhJAyKSHdzTkJc6USNKzVptrQ65088lEP690nUNSZNpkSywcxOd/cHUw+a2WnApnjDEpFUhR4U0dnNudBxdSZKXB2T5meOGpy2rO52wpdyp32ne7ab2RHAA8BTQH14uAY4Fpjm7n/JS4Q5UCp7totIfnR10+9uJ3yxdtpnu2d7p53tYaL4JLACGBp+rQCqiymJiIh014QhVVw+ZUSnN/vudugndWBCrmScR+LuH5nZ40AD4MAL7r4zH4GJiHQmF81EUcrobod+Ugcm5Eqmpq39gIXABGA1Qe1lLEEz18Xu/k6+goxKTVsipSMXzUS5KqPU+0iybdrKVCOZB6wDprt7a1ioAdcA/wF8MReBJlkx/seLlLp0zURRRmbtbRmddeh3dt9I6sCEXMiUSI5z9wtTD3hQfZlrZhtijSoBirVzTJJDH0TikYtmoriamgpx30jC71mmRGJ5iyKBcvGJRcqXPojkXuoNM+r8ldThzFV9e7V3fkf9P8r3fSMpv2eZEskfzexa4HpP6Ugxs2uA2tgjK7BS7xyTeOmDSG6lu2FePmVEpDLb/j9yeSPO930jKb9nmRLJV4HbgY1mtppg1NZ4YBXw5TzEVlBJnbUrxUEfRHIrrhtmrsvN930jKb9nnSaScFTW58zscGA0QVPXVe7+Yr6CK7RS7hyTeOmDyJ7yOdy2kOV2574RtX8jKb9nnQ7/7fQJZqOA/+Xul8QTUu5p+K9IYRViuG2hy83mdZPQv5FJ5OG/ZlYN3AQcDNxHsKz8rcAxgFb/FZGsxTncNqpCtTwkpX8jFzLtR/JT4BfAZwhmtj9DsFjjCHe/JQ+xiUiJaGtCqjDUZxQqpfck08z21e4+LuXxK8BQd2/JV3C5oqYtkcJLwnyHpEn6e5KLme37mNl4/jaf5D2gOpzdjrs/k0UQpxLsplgBLHT3H6Q55zxgDsGosGfd/fPh8RuAM8LTrnf3X4XH7wROBN4Of3ahu6/uKhYRKay4mpCSfjPOpFQG9GRKJK8D/9bJYwdOylSwmVUA84FPA1uBlWa2JHUPeDMbCVxNMIu+0cwODI+fARwFjAN6AyvM7KGU9b2+6e6Ls7xGESlRxdBhXQ4yDf+dHLHsicBGd98EYGaLgLMI1u9qcwkw390bw9fcFh4fDaxw92ag2cyeBU4F7okYk4iUkFLqsC5mnXa2m9kFZvaFNMcvMbPPZ1H2IcArKY+3hsdSHQEcYWZ/NLPasCkM4FngNDPra2YDgSnAoSnP+56ZPWdmt5hZ7yxiEZGEqd/SyPzHNlK/pXGvy8imwzoXr5ON+i2N/Otvn+d///b53V4rX69fSJmatr4BnJDm+K+AxwhGdGWSbq2ujj37lcBIYDIwGPiDmR3p7svN7GiC3RkbgKeB5vA5VxM0s/UCFgBXAXP3eHGzS4FLAQ477LAuQhWRfMpVk1RXE/Ly1fRVv6WRGT8NXgdgcd0r/PLSY4HcLsGSVJmG/1a4+7sdD4b9FD2zKHsru9ciBgN/TXPO/e7e5O6bgfUEiQV3/567j3P3TxMkpQ3h8dc88BHwM4ImtD24+wJ3r3H3mkGDBmURrojkSy53DMy0m2GudybsrHZRu2k7TWESAWhqcWo3bS/5nRHbZKqR9DSzfu7+fupBM/sYQW2gKyuBkWY2DHgVmA50bBK7D5gB3Bk2YR0BbAo76vu7+/ZwYmQ1sDx8/YPc/bVw9NjZwJosYhGRBMnXGlG5fJ1MtZtJww+gZ2WP9hpJRYW1v1YS1sKKW6ZEcjuw2Mwuc/eXAMxsKMFIrNu7Ktjdm83sCmAZwfDfO9x9rZnNBercfUn4s6lmtg5oIRiNtd3M9iFo5gJ4B7gg7HgHuNvMBhHUUlYD/9TNaxaRAsvXGlG5fJ1MHfsThlQx5x/GcM19z9Pif2vqScpaWHHLNGrrJjN7j2Do7b7h4feAH7j7T7Ip3N0fBB7scOzalO8d+Hr4lXrOToKRW+nKzDjsWESKQ67mUHScR9Lxca5ep6vaTeMHu9o7gVtavT3RFGquSD7n12SqkeDutwG3hYnE0vWZiEh5K+SEwI7NTddOG8PcB9bG0rndVe0iKUu6Q/7n12RatPHrHQ65mb0JPBl2jItImSv0hMCOzU0PrXkt1nklmWoX2Ywgy1fCzff8mkw1ko+lOTYU+Fczm+Pui+IJSUSKRaEnBHasBZx25EGsfOmtgtUKOks0+U64+a4dZeoj+U6642Y2AHgEUCIRKXNx3bC68+n9M0cNxsN/JwypYtTffWyP56Yrr5RrCPnu5M/YR5KOu7/VtnCjiJS3OG5Y2X5673jeZ44a3B5T6vnpyoP8ThQsRP9JPjv5u51IzOwkoHTn+hehYl79VIpfrm9Y2X56j3IekNMaQld/gzlNuC3N8OFb8MFbwb8fNkKvfaGyNzR9AE07oflDaPoQPj4GDh6/96+VpUyd7c+z55ImAwhmp8+KMyjJXqE7O0VyLdtP7xnPa22F382G+ju5HLg8dUW+FcE/exxbsfcxTwi/MpWRzTmxuPpV6L1v1+dFkKlGMq3DYwe2d5zpLoVV6M5OKXIfvAV/+in8923Bp9sEmAD8uYJgGjMECyFFOK9sDJ4IPfukfPWFI06JPYlA5s72LR2PmVk/M5sJfN7dz0jzNMmzJI1dL1nu0LgZ1t0PGx6BLU8WOiJJkKerv8fFq4bxUXOwAnGUVoH5j23k5uXraXWoMPj61FFcPmVEjiPOvS77SMysF3A6wTpZpwL3ArfFHJdkKVFLMDTvgoY/wxtr4PXnw3/XJOaTrhTQ0E/BGTfDoFF5fdm4mn53K3dVMBGy8YNdkf8Gi/WDYaY+kk8TLKh4CsGy8T8HJrr7l/IUW+lqaQo6wpp3hp1jH3Z4vLPD9+HP31gLLyzZo7iCtb1K7lgPGDsDqs+HocdDj4qun5NnxdgfF1fTb8dyGz/YlZOaQ6I+GHZDphrJMuAPwPFtM9nN7N/zElVSbH4C7vqHQkchbQZ9AoadEHwddiz0G1joiMpKMfbH7e0n/K5GYXVVbpSRlMW4j3umRDKBYOn3R8xsE8EExOR9TIpTw/p4yu1RCZV9oOc+QadY+/d9oXKf3TvMUn/2YSP8ZRn0HQB9qqDPAOh7AHgLfPQe7HdwcLz3vjDgcDjgcPjYwdAj07YzItnp7OaZq+HncQxj35tP+NnUvDKVW4w1t6gydbavAlYBV5nZcQTNXL3M7CHgt+6+IE8xFs7ES4KvJDn9xkJHIGUq3c0zVzfN7pbTnaTT3U/42da8Oiu3GGtuUWX1UdXd/+juVxDsuf4j4NhYoxKRROq4G2GudgDsTjltSefm5euZubC2fbfCXO2N3nEf+Kq+vbpVbjb7yJeabs1sd/dWgr6TZfGEIyLFJFejjLpTTmdJJ1fNSak1r6q+vbq9LH2xdphH0e0lUkREUpuWcnHT7M7Nt6pvL3qYgXt70sl1c1Jbs9X8xzbuVbnF2GEehRKJFJTWCSs+6fozcjX0tavfgfotjcx9YC2t7vToYVw7bUz7c+KYf1Gs8zryLdM8kr5Ak7s3hY9HEUxM3OLuv8lTfFLCynF0SynId2dy6oeN1Nc2nMYPdgHxNSd1p9zUOIGy+oCUqUayFLgY2GBmI4CngbuBaWZ2tLtfnY8ApXSV4+iWfIi7lpfPT+npttLt7LXjak7KtqbUFmdlDwMzmlvK5wNSpkRS5e4bwu9nAb9096+GS6bUA0okEomaDXIvH7W8fHYmp5tBnsSO7N3ibHHAccrnA1KmRJK6hPxJwI0A7r7LzFpjjUrKQjmObolbvmp5+epMTvdhI4kd2alxVoQ1kpaW8vmAlCmRPGdmNwGvAiOA5QBm1j8fgUl5SOJNoZgltZa3t1vd5vLDRnea/LrbPNgxTiivPhJz77h3VfgDsz7AvwAHAXe4+7Ph8f8BHO7uP89blBHV1NR4XV1docMQyYtCjITL9JpJ2Oq2O01+GgTyN2ZW7+41XZ2XaYmUD4EfhIX1MrMjwx+tdPenchOmiORavmt5Xd1487HVbVe60+SnQSDdl81+JCcC/wm8BBhwqJnNcvcnYo5NRIpAVzfezprbsmmCy1XtqjtNfkltHkyyTpu22k8wqyfYEXF9+PgIghFcE/IQX06oaUskPm01krYbb7qmoL3pI8l1E1OcfSSlKnLTVoqebUkEwN3/YmY9I0UnIiUjmw7xdM1tXTXBdVbT2dubfHea/DqeW86TDbORTSKpM7PbCXZIBJhJMI9ERASIp18mXRNTITrCy32yYTayWUb+MmAt8M8Eo7jWAf8UZ1AixSJXS5fLntpqOl+fOqr9hp2rZeu7o+Nkw6Y8v34xyKZGYgRzSJYBL7r7znhDEikOGiYav441nUJ0hJf7ZMNsZFq0sRL4P8BFwBaC2stgM/sZ8K9tizmKlCsNE82Pjn0i+V4NobPJhlV9e7XXSMr9/z1TjeRG4GPAMHd/F8DM9gNuCr/+Jf7wRJJLw0Tj11mtL9837nSvGUdttFhHi2VKJNOAIzxlfLC7v2NmlwF/RolEypzWCotfUmt9ccRVzE2lGRdt9DSTTNy9xcwyTz4RKRNaKyxeSa31tcW1q6kVM6Oqb6/IZSY1aWYj06itdWb2xY4HzewCghqJiEis0o3cSoIJQ6q4dtoYevQwWt2Z+8DayCP32pJThZGopJmNTDWSy4HfmNlFBPNGHDga6AOck4fYREQSW+tr/GAXre453Se+WJtKMy3a+CpwjJmdBIwhGAb8kLv/Pl/BiYhkUsjO6Tia3ZKaNLvS5TwSd38UeDQPsYiIZK1QndOpyatYaxC5ls3M9r1mZqea2Xoz22hm3+rknPPMbJ2ZrTWzX6Qcv8HM1oRf56ccH2Zm/21mG8zsV+HWvyJSZgoxy70ted28fD0zF9YCcPmUEWWdRCDGRGJmFcB84DRgNDDDzEZ3OGckwd7vx7n7GOBr4fEzgKOAccAxwDfDOSwANwC3uPtIoBG4OK5rEJHkmjT8ACp7GAZU9LC8dE4XInkVgzhrJBOBje6+yd13AYuAszqccwkw390bAdx9W3h8NLDC3Zvd/X3gWeBUMzOC/eMXh+fdBZwd4zWISMK0rW+2/vV3wSw42PZvzIp5ZFWcsllra28dAryS8ngrQe0i1REAZvZHoAKY4+5LCRLHdWb2b0BfYArBYpEHADvcvTmlzEPSvbiZXQpcCnDYYYfl4npEJA+y3ba3hwVDbx1oacnPvItiHlkVpzgTSbqPCB0nMlYCI4HJwGDgD2Z2pLsvN7OjgaeABuBpoDnLMoOD7guABRBsbLU3F1CMinWJBRHo3ra9uNOjh2F4XmsHxTqyKk5xJpKtwKEpjwcDf01zTm24AORmM1tPkFhWuvv3gO8BhJ3wG4A3gf5mVhnWStKVWbaKeYkFEej+tr3XThtD4we79vjgVAwfqIohxmzFmUhWAiPNbBjwKjAd+HyHc+4DZgB3mtlAgqauTWFHfX93325m1UA1sNzd3cweAz5L0OcyC7g/xmsoKsW8xIIIdD03I5umpWL4QFUMMXZHbInE3ZvN7AqCfUwqgDvcfa2ZzQXq3H1J+LOpZrYOaAG+GSaPfQiauQDeAS5I6Re5ClhkZt8FVgG3x3UNxSap6xKJZGtvt+1NVQwfqIohxu6Is0aCuz8IPNjh2LUp3zvw9fAr9ZydBCO30pW5iWBEmHSgjkApBVH7IIrhA1UxxNgdlmaB35JTU1PjdXV1hQ5DRPKkGPofiiFGM6t395quzou1RiIiUgjFMLKqGGLMVqxLpIiISOlTIhERkUiUSERKTNsSIlE3WsqlJMYkuaM+EpESksT5CbmOqRg6qcuNEolICUni/IRcxpTERClq2hIpKUlcnTaXMWkZ92RSjUSkhCRxUmouYyq1iXylQhMSRaSoqI8kfzQhUURKUilN5CsV6iMRyYKGr4p0TjUSkS5opJBIZqqRiHRBI4UCqpV1X7m8Z6qRiHRBI4VUK9sb5fSeKZGIdCGJQ2rzLYkTHZOunN4zJRKRLJT7SCHVyrqvnN4zzSORRNJcgeQp1f+TOK+r2N8zzSORolVObcvFpBRrZXH/rpXie5aORm1J4miUlOSLftdyQ4lEEieJCw9KadLvWm6oj0QSqdjblqV46Hetc+ojkaJWLm380gLyAAAI1ElEQVTLUnj6XYtOTVsiIhKJEomIiESiRCIiIpEokYiISCRKJCIiEokSiYiIRKJEIiIikSiRiIhIJEokImWmXHbtk/zRzHaRMqKVlSUOqpGIlBGtditxUCIRKSNa7VbioKYtkTKi/eclDkokImWmWFe71XLvyaVEIiKJp0ECyRZrH4mZnWpm681so5l9q5NzzjOzdWa21sx+kXL8h+GxF8xsnplZePzxsMzV4deBcV6DiBSeBgkkW2w1EjOrAOYDnwa2AivNbIm7r0s5ZyRwNXCcuze2JQUz+x/AcUB1eOqTwInA4+Hjme6uLQ9FykTbIIGm5lYNEkigOJu2JgIb3X0TgJktAs4C1qWccwkw390bAdx9W3jcgX2AXoABPYE3YoxVRBJMgwSSLc5EcgjwSsrjrcAxHc45AsDM/ghUAHPcfam7P21mjwGvESSS/3D3F1Ke9zMzawHuBb7r5bDxvEiZK9ZBAuUgzj4SS3Os4w2/EhgJTAZmAAvNrL+ZjQD+HhhMkJBOMrMTwufMdPdPAp8Kv76Q9sXNLjWzOjOra2hoiHwxIiKSXpyJZCtwaMrjwcBf05xzv7s3uftmYD1BYjkHqHX399z9PeAhYBKAu78a/vsu8AuCJrQ9uPsCd69x95pBgwbl8LJERCRVnIlkJTDSzIaZWS9gOrCkwzn3AVMAzGwgQVPXJuBl4EQzqzSzngQd7S+EjweG5/cEpgFrYrwGERHpQmx9JO7ebGZXAMsI+j/ucPe1ZjYXqHP3JeHPpprZOqAF+Ka7bzezxcBJwPMEzWFL3f3/mVk/YFmYRCqAR4CfxnUNIiLSNSuHfuqamhqvq9NoYRGR7jCzenev6eo8LdooIiKRKJGIiEgkSiQiIhKJEomIiESiRCIiIpEokYiISCRKJCIiMavf0sj8xzZSv6Wx0KHEQhtbiYjEqBw25VKNREQkRuWwKZcSiYhIjNo25aowSnZTLjVtiYjEqBw25VIiERGJWalvyqWmLRERiUSJREREIlEiERGRSJRIREQkEiUSERGJRIlEREQiKYutds2sAdhS6Dg6GAi8Wegg8kzXXB50zaVjiLsP6uqkskgkSWRmddnshVxKdM3lQddcftS0JSIikSiRiIhIJEokhbOg0AEUgK65POiay4z6SEREJBLVSEREJBIlkpiZ2almtt7MNprZt9L8/DAze8zMVpnZc2Z2eiHizBUzu8PMtpnZmk5+bmY2L3w/njOzo/IdY65lcc0zw2t9zsyeMrOx+Y4x17q65pTzjjazFjP7bL5ii0s212xmk81stZmtNbMV+YyvkJRIYmRmFcB84DRgNDDDzEZ3OO3bwD3uPh6YDtya3yhz7k7g1Aw/Pw0YGX5dCvwkDzHF7U4yX/Nm4ER3rwaupzTa0+8k8zW3/f7fACzLR0B5cCcZrtnM+hP8/Z7p7mOAz+UproJTIonXRGCju29y913AIuCsDuc4sF/4/f7AX/MYX865+xPAWxlOOQv4Tw/UAv3N7KD8RBePrq7Z3Z9y98bwYS0wOC+BxSiL/2eArwL3Atvijyh+WVzz54HfuPvL4fklcd3ZUCKJ1yHAKymPt4bHUs0BLjCzrcCDBH98pSyb96SUXQw8VOgg4mZmhwDnALcVOpY8OgKoMrPHzazezL5Y6IDyRTskxsvSHOs4TG4GcKe732xmxwI/N7Mj3b01/vAKIpv3pCSZ2RSCRHJ8oWPJgx8BV7l7i1m6//KSVAlMAE4G+gBPm1mtu/+lsGHFT4kkXluBQ1MeD2bPpquLCdtd3f1pM9uHYN2eUq0WZ/OelBwzqwYWAqe5+/ZCx5MHNcCiMIkMBE43s2Z3v6+wYcVqK/Cmu78PvG9mTwBjgZJPJGraitdKYKSZDTOzXgSd6Us6nPMywScYzOzvgX2AhrxGmV9LgC+Go7cmAW+7+2uFDipOZnYY8BvgC+Xw6RTA3Ye5+1B3HwosBr5S4kkE4H7gU2ZWaWZ9gWOAFwocU16oRhIjd282sysIRq1UAHe4+1ozmwvUufsS4BvAT81sNkETz4VexLNEzeyXwGRgYNjvcx3QE8DdbyPoBzod2Ah8AHypMJHmThbXfC1wAHBr+Am9udgX+MvimktOV9fs7i+Y2VLgOaAVWOjuGYdHlwrNbBcRkUjUtCUiIpEokYiISCRKJCIiEokSiYiIRKJEIiIikSiRiGRgZn9nZovM7EUzW2dmD5rZERHLvDPdarhmVmNm86KUnVLWhWb2H7koS6Qrmkci0gkLJn38FrjL3aeHx8YBHyeG2cruXgfU5bpckbipRiLSuSlAU+oEO3dfDTxpZjea2Roze97Mzof2vShWmNk9ZvYXM/tBuBfJn8LzDk8p+3+a2R/C86alPP+B8Ps54f4Xj5vZJjP757YnmtkFYZmrzez/hsu1Y2ZfCstbARwX/9sjElCNRKRzRwL1aY6fC4wjWEdpILAyXFeJ8NjfEyw3volgdvNEM/sXgpWdvxaeNxQ4ETgceMzMRqR5nU8QJLOPAevN7CfACOB84Dh3bzKzW4GZZvYw8B2CRQPfBh4DVkW4dpGsKZGIdN/xwC/dvQV4I6wBHA28A6xsWzvMzF4ElofPeZ4gKbS5J1zheYOZbSJIGh39zt0/Aj4ys20ETWonEySLleFyK30IFvg8Bnjc3RvC1/4VwbLmIrFTIhHp3Fog3RaxmdZF/yjl+9aUx63s/vfWcW2idGsVpZbVEj7fCPpsrt4tILOzOylDJHbqIxHp3KNAbzO7pO2AmR0NNALnm1mFmQ0CTgD+1M2yP2dmPcJ+k+HA+iyf93vgs2Z2YBjPADMbAvw3MNnMDjCznpTRNq9SeKqRiHTC3d3MzgF+ZGbfAnYCLxH0c+wLPEtQC7jS3V83s3TNU51ZD6wgaK76J3ffmc0GUO6+zsy+DSw3sx5AE3C5u9ea2RzgaeA14BmCFadFYqfVf0VEJBI1bYmISCRKJCIiEokSiYiIRKJEIiIikSiRiIhIJEokIiISiRKJiIhEokQiIiKR/H+Qj2EivTbg1QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.030409193256815382"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.polynomial.polynomial import polyfit  \n",
    "from scipy.stats import pearsonr\n",
    "from pylab import text\n",
    "\n",
    "def norm(a):\n",
    "    return (a - np.min(a)) / a.ptp()\n",
    "METRIC = norm(-np.array(VIO)) + np.array(MSE)\n",
    "n_low = int(num_models * 0.2)\n",
    "sorted_aus = [AUS for _,AUS in sorted(zip(VIO,AUS))]\n",
    "\n",
    "print(\"Best by BIC = \", np.mean(sorted_aus[:n_low]))\n",
    "\n",
    "sorted_aus = [AUS for _,AUS in sorted(zip(MSE,AUS))]\n",
    "print(\"Best by AUC = \", np.mean(sorted_aus[:n_low]))\n",
    "\n",
    "\n",
    "sorted_aus = [AUS for _,AUS in sorted(zip(METRIC,AUS))]\n",
    "print(\"Best by MET = \", np.mean(sorted_aus[:n_low]))\n",
    "\n",
    "#sorted_aus = [AUS for _,AUS in sorted(zip(METRIC,AUS))]\n",
    "print(\"Random = \", np.mean(AUS[:n_low]))\n",
    "\n",
    "print(pearsonr(VIO,AUS)[0])\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(VIO,AUS, 1)\n",
    "ax.plot(VIO,AUS, '.')\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(VIO,AUS)[0])[0:6], ha='left', va='center', transform=ax.transAxes)\n",
    "plt.plot(VIO, b + m * np.array(VIO), '-')\n",
    "ax.set_xlabel(\"BIC\")\n",
    "ax.set_ylabel(\"OoS AUCROC\")\n",
    "fig.savefig('Ex4VIOVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(pearsonr(METRIC,AUS)[0])\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(METRIC,AUS, 1)\n",
    "ax.plot(METRIC,AUS, '.')\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(METRIC,AUS)[0])[0:6], ha='left', va='center', transform=ax.transAxes)\n",
    "plt.plot(METRIC, b + m * np.array(METRIC), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"Combined\")\n",
    "ax.set_ylabel(\"OoS AUCROC\")\n",
    "fig.savefig('Ex4ProposedVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(MSE,AUS, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(MSE,AUS)[0])[0:6], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(MSE,AUS, '.')\n",
    "plt.plot(MSE, b + m * np.array(MSE), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"AUC\")\n",
    "ax.set_ylabel(\"OoS AUCROC\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "MSE = np.array(MSE)\n",
    "\n",
    "x = []\n",
    "y1 = []\n",
    "y2 = []\n",
    "y3 = []\n",
    "for split in range(10, len(AUS), 5):\n",
    "    #print(\"******\", split, \"*******\")\n",
    "    sorted_aus = [AUS for _,AUS in sorted(zip(VIO,AUS))]\n",
    "    sorted_mse = [MSE for _,MSE in sorted(zip(VIO,MSE))]\n",
    "\n",
    "    low = []\n",
    "    high = []\n",
    "    low = sorted_aus[:split]\n",
    "    high = sorted_aus[split:]\n",
    "\n",
    "    x.append(split)\n",
    "    \n",
    "    \n",
    "    #print(\"Low Violations = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High Violations = \", np.mean(high), \"for\", len(high))\n",
    "    y1.append(np.mean(low)) \n",
    "    sorted_aus_by_mse = [AUS for _,AUS in sorted(zip(MSE,AUS))]\n",
    "    low = sorted_aus_by_mse[:split]\n",
    "    high = sorted_aus_by_mse[split:]\n",
    "    #print(\"Low AUS by MSE = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High AUS by MSE = \", np.mean(high), \"for\", len(high))\n",
    "    y2.append(np.mean(low))\n",
    "    sorted_aus = [AUS for _,AUS in sorted(zip(METRIC,AUS))]\n",
    "    sorted_mse = [MSE for _,MSE in sorted(zip(METRIC,MSE))]\n",
    "\n",
    "    low = []\n",
    "    high = []\n",
    "    low = sorted_aus[:split]\n",
    "    high = sorted_aus[split:]\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"Low Metric = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High Metric = \", np.mean(high), \"for\", len(high))\n",
    "    y3.append(np.mean(low))\n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x,y1, '-', label = 'BIC')\n",
    "ax.plot(x,y2, '-', label = 'MSE')\n",
    "ax.plot(x,y3, '-', label = 'METRIC')\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"Out of Sample AUCROC\")\n",
    "plt.show()  \n",
    "pearsonr(METRIC,AUS)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69748163, 0.30251837],\n",
       "       [0.33234844, 0.6676516 ],\n",
       "       [0.52037275, 0.47962725],\n",
       "       ...,\n",
       "       [0.3269573 , 0.6730427 ],\n",
       "       [0.904066  , 0.09593401],\n",
       "       [0.8770836 , 0.12291645]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot clone object '<keras.engine.training.Model object at 0x7f9e101656a0>' (type <class 'keras.engine.training.Model'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6375e1beb2eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msfm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectFromModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_selection/from_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    166\u001b[0m             raise NotFittedError(\n\u001b[1;32m    167\u001b[0m                 \"Since 'prefit=True', call transform directly\")\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     56\u001b[0m                             \u001b[0;34m\"it does not seem to be a scikit-learn estimator \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                             \u001b[0;34m\"as it does not implement a 'get_params' methods.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                             % (repr(estimator), type(estimator)))\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mnew_object_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot clone object '<keras.engine.training.Model object at 0x7f9e101656a0>' (type <class 'keras.engine.training.Model'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods."
     ]
    }
   ],
   "source": [
    "sfm = SelectFromModel(model, threshold=0.25)\n",
    "sfm.fit(X, y)\n",
    "n_features = sfm.transform(X).shape[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
