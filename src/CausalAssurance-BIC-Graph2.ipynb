{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes \n",
    "### Required installing Oracle JAVA 8 to get javabridge installed\n",
    "### Then, I was able to install py-causal from https://bd2kccd.github.io/docs/py-causal/\n",
    "### GFCI is slower than RFCI, but more accurate (SPIRTES), GFCI and RFCI account for unobserved variables, FGES assumes no unobserved variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure Learning Performance Guarantees If the assumptions in the previous section hold, then in the large sample limit, the CBN structure output by GFCId will contain an edge of one of four kinds between Xand Y   if and only if Xand Yare not independent conditional on any subset of the other measured variables of less than or equal to a specified size. In addition, there is (1) an arc X->Y   if and only if Xdirectly or indirectly causes Y, and Y   does not directly or indirectly cause X; (2) an edge X <-->Y   if and only if X   is not a direct or indirect cause of Yand Y   is not a direct or indirect cause of X(which can only occur if there are latent confounders of Xand some other variable or Yand some other variable; (3) an edge Xo->Y   only if Yis not a direct or indirect cause of X, but Xmay or may not be an indirect cause of Y; (4) an edge X oâ€“o Y   indicates that Xand Y   are dependent no matter what subset of observed variables is conditioned on, but contains no orientation information (X   may be a direct or indirect cause of Y, and Ymay be an indirect cause of X, or there may be a latent common cause of Xand Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying some various ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import configparser\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, \\\n",
    "                        Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "import keras.optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "import glob, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "# select your GPU Here\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" #Comment this line out if you want all GPUS (2 hehe)\n",
    "\n",
    "# python full-display web browser\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "def get_model(dense, dropouts, inputs):\n",
    "    # dense is an ordered list of the number of dense neurons like [1024, 2048, 1024]\n",
    "    # dropouts is an ordered list of the dropout masks like [0.2, 0.3, 0.4]\n",
    "    inputs = keras.Input(shape = (inputs,))\n",
    "\n",
    "    x = keras.layers.Dense(dense[0], activation = 'relu')(inputs)\n",
    "    x = keras.layers.Dropout(dropouts[0])(x, training=True)\n",
    "    for den, drop in zip(dense[1:], dropouts[1:]):\n",
    "        x = keras.layers.Dense(den, activation = 'relu')(x)\n",
    "        x = keras.layers.Dropout(drop)(x, training=True)\n",
    "    outputs = keras.layers.Dense(1, activation = 'linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256]] ['temp/e0', 'temp/e1', 'temp/e2', 'temp/e3', 'temp/e4', 'temp/e5', 'temp/e6', 'temp/e7', 'temp/e8', 'temp/e9', 'temp/e10', 'temp/e11', 'temp/e12', 'temp/e13', 'temp/e14', 'temp/e15', 'temp/e16', 'temp/e17', 'temp/e18', 'temp/e19', 'temp/e20', 'temp/e21', 'temp/e22', 'temp/e23', 'temp/e24', 'temp/e25', 'temp/e26', 'temp/e27', 'temp/e28', 'temp/e29', 'temp/e30', 'temp/e31', 'temp/e32', 'temp/e33', 'temp/e34', 'temp/e35', 'temp/e36', 'temp/e37', 'temp/e38', 'temp/e39', 'temp/e40', 'temp/e41', 'temp/e42', 'temp/e43', 'temp/e44', 'temp/e45', 'temp/e46', 'temp/e47', 'temp/e48', 'temp/e49']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from pycausal import search as s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def discrete_gauss(low, high, samples, std = 20):\n",
    "    x = np.arange(low, high)\n",
    "    xU, xL = x + 0.5, x - 0.5 \n",
    "    prob = ss.norm.cdf(xU, scale = std) - ss.norm.cdf(xL, scale = std)\n",
    "    prob = prob / prob.sum() #normalize the probabilities so their sum is 1\n",
    "    nums = np.random.choice(x, size = samples, p = prob)\n",
    "    return nums\n",
    "\n",
    "\n",
    "\n",
    "def bar_plot(x_ax, val1, val1std, val2, val2std):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ## the data\n",
    "    N = len(x_ax)\n",
    "\n",
    "    ## necessary variables\n",
    "    ind = np.arange(N)                # the x locations for the groups\n",
    "    width = 0.35                      # the width of the bars\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    ## the bars\n",
    "    rects1 = ax.bar(ind, val1, width,\n",
    "                    color='gray',\n",
    "                    yerr=val1std,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='blue'))\n",
    "\n",
    "    rects2 = ax.bar(ind+width, val2, width,\n",
    "                        color='blue',\n",
    "                        #yerr=val2std,\n",
    "                        error_kw=dict(elinewidth=2,ecolor='gray'))\n",
    "\n",
    "    # axes and labels\n",
    "    ax.set_xlim(-width,len(ind)+width)\n",
    "    #ax.set_ylim(0,45)\n",
    "    ax.set_ylabel('Percentage')\n",
    "    ax.set_title('')\n",
    "    plt.xticks(ind + width / 2, x_ax, rotation=75, size = 14)\n",
    "    ## add a legend\n",
    "    ax.legend( (rects1[0], rects2[0]), ('Accuracy', '% Violations') )\n",
    "    fig.savefig(\"violations.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 5000):\n",
    "    a = np.random.gumbel(mean, var, SIZE)\n",
    "    b = np.random.gumbel(mean, var, SIZE)\n",
    "    c = np.random.gumbel(mean, var, SIZE)\n",
    "    d = np.random.gumbel(mean, var, SIZE)\n",
    "    e = np.random.gumbel(mean, var, SIZE)\n",
    "    f= a + b + c + d + e + np.random.gumbel(mean, var, SIZE)\n",
    "    g = f + np.random.gumbel(mean,var, SIZE)\n",
    "    g = np.rint(g)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 5000):\n",
    "    a = np.random.gumbel(mean, var, SIZE)\n",
    "    b = np.random.gumbel(mean, var, SIZE)\n",
    "    c = np.random.gumbel(mean, var, SIZE)\n",
    "    d = np.random.gumbel(mean, var, SIZE)\n",
    "\n",
    "    f= a + b + c + d + np.random.gumbel(mean, var, SIZE)\n",
    "    g = f + np.random.gumbel(mean,var, SIZE)\n",
    "    \n",
    "    \n",
    "    g = np.rint(g)\n",
    "    e = g + np.random.gumbel(mean,var,SIZE)\n",
    "    \n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 400000):\n",
    "    f = np.random.normal(mean, var, SIZE)\n",
    "    a = f + np.random.normal(mean, var, SIZE)\n",
    "    b = f + np.random.normal(mean, var, SIZE)\n",
    "    c = f + np.random.normal(mean, var, SIZE)\n",
    "    d = f + np.random.normal(mean, var, SIZE)\n",
    "    e = f + np.random.normal(mean, var, SIZE)\n",
    "    g = a + b + c + d  + e + np.random.normal(mean, var, SIZE)\n",
    "\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 20000):\n",
    "    a = np.random.normal(mean, var, SIZE)\n",
    "    b = np.random.normal(mean, var, SIZE)\n",
    "    c = np.random.normal(mean, var, SIZE)\n",
    "    d = np.random.normal(mean, var, SIZE)\n",
    "    e = np.random.normal(mean, var, SIZE)\n",
    "    f= a + b + c + d + e + np.random.normal(mean, var, SIZE)\n",
    "    g = f + np.random.normal(mean,var, SIZE)\n",
    "    #g = np.rint(g)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 20000):\n",
    "    a = np.random.normal(mean, var, SIZE)\n",
    "    b = a + np.random.normal(mean, var, SIZE)\n",
    "    c = a + np.random.normal(mean, var, SIZE)\n",
    "    d = a + np.random.normal(mean, var, SIZE)\n",
    "    e = a + np.random.normal(mean, var, SIZE)\n",
    "    f= a + np.random.normal(mean, var, SIZE)\n",
    "    g = f + np.random.normal(mean,var, SIZE)\n",
    "    #g = np.rint(g)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def get_CG(df, tetrad):\n",
    "    tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "    #tetrad.run(algoId = 'fges-mb', targetName = 'g', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "    #       structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "    #       completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "def get_MB(graph, var, pc):\n",
    "    parents = set()\n",
    "    children = set()\n",
    "    for i in pc.extractTetradGraphEdges(graph):\n",
    "        if i[-1] == var and i[3:5] == '->':\n",
    "            parents.add(i[0])\n",
    "        if i[0] == var and i[3:5] == '->':\n",
    "            children.add(i[-1])\n",
    "    return parents, children\n",
    "\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from collections import defaultdict\n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '5000M')\n",
    "tetrad = s.tetradrunner()\n",
    "\n",
    "\n",
    "verbosity = 1\n",
    "\n",
    "\n",
    "\n",
    "models = []\n",
    "model_names = []\n",
    "\n",
    "num_models = 50\n",
    "model_layers = [512, 256]\n",
    "for i in range(num_models):\n",
    "    models.append(model_layers)\n",
    "    model_names.append('temp/e' + str(i))\n",
    "\n",
    "print(models, model_names)\n",
    "\n",
    "\n",
    "df = gen_data()\n",
    "X = df[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "y = df['g'].values\n",
    "\n",
    "val_df = gen_data(SIZE = 2000)\n",
    "x_val = df[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "y_val = df['g'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp/e0\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1080 - mean_squared_error: 1.1080 - val_loss: 1.0235 - val_mean_squared_error: 1.0235\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02349, saving model to temp/e0\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 112us/step - loss: 1.0212 - mean_squared_error: 1.0212 - val_loss: 1.0198 - val_mean_squared_error: 1.0198\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02349 to 1.01977, saving model to temp/e0\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 115us/step - loss: 1.0153 - mean_squared_error: 1.0153 - val_loss: 1.0095 - val_mean_squared_error: 1.0095\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01977 to 1.00948, saving model to temp/e0\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 113us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0068 - val_mean_squared_error: 1.0068\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00948 to 1.00684, saving model to temp/e0\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 117us/step - loss: 1.0139 - mean_squared_error: 1.0139 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00684\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0028 - val_mean_squared_error: 1.0028\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00684 to 1.00277, saving model to temp/e0\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 116us/step - loss: 1.0072 - mean_squared_error: 1.0072 - val_loss: 1.0077 - val_mean_squared_error: 1.0077\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00277\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 115us/step - loss: 1.0109 - mean_squared_error: 1.0109 - val_loss: 0.9999 - val_mean_squared_error: 0.9999\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00277 to 0.99986, saving model to temp/e0\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 98us/step - loss: 1.0037 - mean_squared_error: 1.0037 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99986\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 2s 81us/step - loss: 1.0019 - mean_squared_error: 1.0019 - val_loss: 0.9979 - val_mean_squared_error: 0.9979\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.99986 to 0.99791, saving model to temp/e0\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 2s 80us/step - loss: 1.0015 - mean_squared_error: 1.0015 - val_loss: 0.9963 - val_mean_squared_error: 0.9963\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.99791 to 0.99631, saving model to temp/e0\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 2s 81us/step - loss: 1.0035 - mean_squared_error: 1.0035 - val_loss: 1.0016 - val_mean_squared_error: 1.0016\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.99631\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 2s 81us/step - loss: 1.0010 - mean_squared_error: 1.0010 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.99631\n",
      "Epoch 00013: early stopping\n",
      "temp/e1\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 121us/step - loss: 1.1010 - mean_squared_error: 1.1010 - val_loss: 1.0251 - val_mean_squared_error: 1.0251\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02509, saving model to temp/e1\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 115us/step - loss: 1.0205 - mean_squared_error: 1.0205 - val_loss: 1.0136 - val_mean_squared_error: 1.0136\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02509 to 1.01356, saving model to temp/e1\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 113us/step - loss: 1.0154 - mean_squared_error: 1.0154 - val_loss: 1.0046 - val_mean_squared_error: 1.0046\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01356 to 1.00464, saving model to temp/e1\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 117us/step - loss: 1.0144 - mean_squared_error: 1.0144 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00464\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 117us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0159 - val_mean_squared_error: 1.0159\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00464\n",
      "Epoch 00005: early stopping\n",
      "temp/e2\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 126us/step - loss: 1.0979 - mean_squared_error: 1.0979 - val_loss: 1.0193 - val_mean_squared_error: 1.0193\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01930, saving model to temp/e2\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 117us/step - loss: 1.0194 - mean_squared_error: 1.0194 - val_loss: 1.0126 - val_mean_squared_error: 1.0126\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01930 to 1.01260, saving model to temp/e2\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0139 - mean_squared_error: 1.0139 - val_loss: 1.0116 - val_mean_squared_error: 1.0116\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01260 to 1.01157, saving model to temp/e2\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0118 - mean_squared_error: 1.0118 - val_loss: 1.0036 - val_mean_squared_error: 1.0036\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01157 to 1.00356, saving model to temp/e2\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0061 - mean_squared_error: 1.0061 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00356\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 120us/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 1.0009 - val_mean_squared_error: 1.0009\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00356 to 1.00087, saving model to temp/e2\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 120us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0116 - val_mean_squared_error: 1.0116\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00087\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0086 - mean_squared_error: 1.0086 - val_loss: 1.0036 - val_mean_squared_error: 1.0036\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00087\n",
      "Epoch 00008: early stopping\n",
      "temp/e3\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 126us/step - loss: 1.1019 - mean_squared_error: 1.1019 - val_loss: 1.0259 - val_mean_squared_error: 1.0259\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02592, saving model to temp/e3\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0238 - mean_squared_error: 1.0238 - val_loss: 1.0128 - val_mean_squared_error: 1.0128\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02592 to 1.01283, saving model to temp/e3\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0185 - mean_squared_error: 1.0185 - val_loss: 1.0096 - val_mean_squared_error: 1.0096\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01283 to 1.00960, saving model to temp/e3\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0150 - mean_squared_error: 1.0150 - val_loss: 1.0022 - val_mean_squared_error: 1.0022\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00960 to 1.00218, saving model to temp/e3\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 116us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0056 - val_mean_squared_error: 1.0056\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00218\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 113us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0022 - val_mean_squared_error: 1.0022\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00218\n",
      "Epoch 00006: early stopping\n",
      "temp/e4\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.1048 - mean_squared_error: 1.1048 - val_loss: 1.0264 - val_mean_squared_error: 1.0264\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02643, saving model to temp/e4\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 2s 108us/step - loss: 1.0229 - mean_squared_error: 1.0229 - val_loss: 1.0103 - val_mean_squared_error: 1.0103\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02643 to 1.01026, saving model to temp/e4\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 113us/step - loss: 1.0147 - mean_squared_error: 1.0147 - val_loss: 1.0147 - val_mean_squared_error: 1.0147\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01026\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 114us/step - loss: 1.0148 - mean_squared_error: 1.0148 - val_loss: 1.0048 - val_mean_squared_error: 1.0048\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01026 to 1.00477, saving model to temp/e4\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0021 - val_mean_squared_error: 1.0021\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00477 to 1.00205, saving model to temp/e4\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 116us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0031 - val_mean_squared_error: 1.0031\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00205\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 113us/step - loss: 1.0076 - mean_squared_error: 1.0076 - val_loss: 0.9997 - val_mean_squared_error: 0.9997\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00205 to 0.99968, saving model to temp/e4\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 115us/step - loss: 1.0043 - mean_squared_error: 1.0043 - val_loss: 1.0035 - val_mean_squared_error: 1.0035\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99968\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 115us/step - loss: 1.0051 - mean_squared_error: 1.0051 - val_loss: 1.0012 - val_mean_squared_error: 1.0012\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99968\n",
      "Epoch 00009: early stopping\n",
      "temp/e5\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 114us/step - loss: 1.0867 - mean_squared_error: 1.0867 - val_loss: 1.0231 - val_mean_squared_error: 1.0231\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02308, saving model to temp/e5\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 121us/step - loss: 1.0204 - mean_squared_error: 1.0204 - val_loss: 1.0223 - val_mean_squared_error: 1.0223\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02308 to 1.02234, saving model to temp/e5\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 113us/step - loss: 1.0163 - mean_squared_error: 1.0163 - val_loss: 1.0106 - val_mean_squared_error: 1.0106\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02234 to 1.01055, saving model to temp/e5\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.0127 - mean_squared_error: 1.0127 - val_loss: 1.0076 - val_mean_squared_error: 1.0076\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01055 to 1.00755, saving model to temp/e5\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 98us/step - loss: 1.0116 - mean_squared_error: 1.0116 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00755 to 1.00586, saving model to temp/e5\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.0058 - mean_squared_error: 1.0058 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00586 to 1.00236, saving model to temp/e5\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0106 - val_mean_squared_error: 1.0106\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00236\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0043 - mean_squared_error: 1.0043 - val_loss: 0.9984 - val_mean_squared_error: 0.9984\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00236 to 0.99838, saving model to temp/e5\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0039 - mean_squared_error: 1.0039 - val_loss: 1.0082 - val_mean_squared_error: 1.0082\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99838\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 2s 95us/step - loss: 1.0020 - mean_squared_error: 1.0020 - val_loss: 1.0020 - val_mean_squared_error: 1.0020\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99838\n",
      "Epoch 00010: early stopping\n",
      "temp/e6\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 105us/step - loss: 1.0941 - mean_squared_error: 1.0941 - val_loss: 1.0191 - val_mean_squared_error: 1.0191\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01914, saving model to temp/e6\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0195 - mean_squared_error: 1.0195 - val_loss: 1.0136 - val_mean_squared_error: 1.0136\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01914 to 1.01362, saving model to temp/e6\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0177 - val_mean_squared_error: 1.0177\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01362\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.0123 - mean_squared_error: 1.0123 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01362 to 1.00927, saving model to temp/e6\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0043 - val_mean_squared_error: 1.0043\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00927 to 1.00430, saving model to temp/e6\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00430 to 1.00245, saving model to temp/e6\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0063 - mean_squared_error: 1.0063 - val_loss: 0.9985 - val_mean_squared_error: 0.9985\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00245 to 0.99847, saving model to temp/e6\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 1.0033 - mean_squared_error: 1.0033 - val_loss: 0.9981 - val_mean_squared_error: 0.9981\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.99847 to 0.99814, saving model to temp/e6\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0037 - mean_squared_error: 1.0037 - val_loss: 1.0000 - val_mean_squared_error: 1.0000\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99814\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 1.0025 - mean_squared_error: 1.0025 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99814\n",
      "Epoch 00010: early stopping\n",
      "temp/e7\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 104us/step - loss: 1.0813 - mean_squared_error: 1.0813 - val_loss: 1.0229 - val_mean_squared_error: 1.0229\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02285, saving model to temp/e7\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0226 - mean_squared_error: 1.0226 - val_loss: 1.0154 - val_mean_squared_error: 1.0154\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02285 to 1.01541, saving model to temp/e7\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0211 - mean_squared_error: 1.0211 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01541 to 1.00748, saving model to temp/e7\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00748 to 1.00625, saving model to temp/e7\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0089 - mean_squared_error: 1.0089 - val_loss: 1.0039 - val_mean_squared_error: 1.0039\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00625 to 1.00388, saving model to temp/e7\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 2s 95us/step - loss: 1.0063 - mean_squared_error: 1.0063 - val_loss: 1.0084 - val_mean_squared_error: 1.0084\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00388\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0065 - mean_squared_error: 1.0065 - val_loss: 1.0082 - val_mean_squared_error: 1.0082\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00388\n",
      "Epoch 00007: early stopping\n",
      "temp/e8\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 104us/step - loss: 1.0961 - mean_squared_error: 1.0961 - val_loss: 1.0214 - val_mean_squared_error: 1.0214\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02136, saving model to temp/e8\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0235 - mean_squared_error: 1.0235 - val_loss: 1.0151 - val_mean_squared_error: 1.0151\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02136 to 1.01512, saving model to temp/e8\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 95us/step - loss: 1.0170 - mean_squared_error: 1.0170 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01512 to 1.00544, saving model to temp/e8\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 1.0094 - val_mean_squared_error: 1.0094\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00544\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0002 - val_mean_squared_error: 1.0002\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00544 to 1.00022, saving model to temp/e8\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0069 - mean_squared_error: 1.0069 - val_loss: 1.0072 - val_mean_squared_error: 1.0072\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00022\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 95us/step - loss: 1.0051 - mean_squared_error: 1.0051 - val_loss: 1.0017 - val_mean_squared_error: 1.0017\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00022\n",
      "Epoch 00007: early stopping\n",
      "temp/e9\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 107us/step - loss: 1.0910 - mean_squared_error: 1.0910 - val_loss: 1.0292 - val_mean_squared_error: 1.0292\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02924, saving model to temp/e9\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0193 - mean_squared_error: 1.0193 - val_loss: 1.0132 - val_mean_squared_error: 1.0132\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02924 to 1.01319, saving model to temp/e9\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 101us/step - loss: 1.0153 - mean_squared_error: 1.0153 - val_loss: 1.0107 - val_mean_squared_error: 1.0107\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01319 to 1.01069, saving model to temp/e9\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0147 - mean_squared_error: 1.0147 - val_loss: 1.0082 - val_mean_squared_error: 1.0082\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01069 to 1.00820, saving model to temp/e9\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0037 - val_mean_squared_error: 1.0037\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00820 to 1.00373, saving model to temp/e9\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0068 - mean_squared_error: 1.0068 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00373\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 98us/step - loss: 1.0073 - mean_squared_error: 1.0073 - val_loss: 1.0007 - val_mean_squared_error: 1.0007\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00373 to 1.00075, saving model to temp/e9\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 95us/step - loss: 1.0051 - mean_squared_error: 1.0051 - val_loss: 1.0044 - val_mean_squared_error: 1.0044\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00075\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0029 - mean_squared_error: 1.0029 - val_loss: 1.0007 - val_mean_squared_error: 1.0007\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00075 to 1.00069, saving model to temp/e9\n",
      "Epoch 00009: early stopping\n",
      "temp/e10\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 101us/step - loss: 1.0928 - mean_squared_error: 1.0928 - val_loss: 1.0200 - val_mean_squared_error: 1.0200\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01998, saving model to temp/e10\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0261 - mean_squared_error: 1.0261 - val_loss: 1.0129 - val_mean_squared_error: 1.0129\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01998 to 1.01292, saving model to temp/e10\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0195 - mean_squared_error: 1.0195 - val_loss: 1.0138 - val_mean_squared_error: 1.0138\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01292\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0137 - mean_squared_error: 1.0137 - val_loss: 1.0045 - val_mean_squared_error: 1.0045\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01292 to 1.00449, saving model to temp/e10\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 96us/step - loss: 1.0107 - mean_squared_error: 1.0107 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00449 to 1.00396, saving model to temp/e10\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 96us/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 1.0029 - val_mean_squared_error: 1.0029\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00396 to 1.00294, saving model to temp/e10\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 97us/step - loss: 1.0058 - mean_squared_error: 1.0058 - val_loss: 0.9995 - val_mean_squared_error: 0.9995\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00294 to 0.99949, saving model to temp/e10\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 95us/step - loss: 1.0063 - mean_squared_error: 1.0063 - val_loss: 0.9992 - val_mean_squared_error: 0.9992\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.99949 to 0.99924, saving model to temp/e10\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0038 - mean_squared_error: 1.0038 - val_loss: 1.0007 - val_mean_squared_error: 1.0007\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99924\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0029 - mean_squared_error: 1.0029 - val_loss: 0.9966 - val_mean_squared_error: 0.9966\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.99924 to 0.99657, saving model to temp/e10\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0014 - mean_squared_error: 1.0014 - val_loss: 0.9942 - val_mean_squared_error: 0.9942\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.99657 to 0.99424, saving model to temp/e10\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0029 - mean_squared_error: 1.0029 - val_loss: 0.9955 - val_mean_squared_error: 0.9955\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.99424\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 2s 98us/step - loss: 1.0014 - mean_squared_error: 1.0014 - val_loss: 0.9955 - val_mean_squared_error: 0.9955\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.99424\n",
      "Epoch 00013: early stopping\n",
      "temp/e11\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 103us/step - loss: 1.0880 - mean_squared_error: 1.0880 - val_loss: 1.0154 - val_mean_squared_error: 1.0154\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01542, saving model to temp/e11\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0199 - mean_squared_error: 1.0199 - val_loss: 1.0294 - val_mean_squared_error: 1.0294\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01542\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 2s 95us/step - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 1.0081 - val_mean_squared_error: 1.0081\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01542 to 1.00813, saving model to temp/e11\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0131 - val_mean_squared_error: 1.0131\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00813\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 95us/step - loss: 1.0111 - mean_squared_error: 1.0111 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00813\n",
      "Epoch 00005: early stopping\n",
      "temp/e12\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 106us/step - loss: 1.1083 - mean_squared_error: 1.1083 - val_loss: 1.0184 - val_mean_squared_error: 1.0184\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01841, saving model to temp/e12\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 96us/step - loss: 1.0209 - mean_squared_error: 1.0209 - val_loss: 1.0142 - val_mean_squared_error: 1.0142\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01841 to 1.01422, saving model to temp/e12\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 97us/step - loss: 1.0146 - mean_squared_error: 1.0146 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01422 to 1.00912, saving model to temp/e12\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 96us/step - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 1.0169 - val_mean_squared_error: 1.0169\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00912\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0075 - mean_squared_error: 1.0075 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00912 to 1.00567, saving model to temp/e12\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 96us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0074 - val_mean_squared_error: 1.0074\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00567\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0026 - mean_squared_error: 1.0026 - val_loss: 1.0042 - val_mean_squared_error: 1.0042\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00567 to 1.00422, saving model to temp/e12\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 95us/step - loss: 1.0055 - mean_squared_error: 1.0055 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00422 to 1.00244, saving model to temp/e12\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0029 - mean_squared_error: 1.0029 - val_loss: 1.0005 - val_mean_squared_error: 1.0005\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00244 to 1.00054, saving model to temp/e12\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.0026 - mean_squared_error: 1.0026 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.00054\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0052 - mean_squared_error: 1.0052 - val_loss: 0.9986 - val_mean_squared_error: 0.9986\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.00054 to 0.99861, saving model to temp/e12\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 0.9994 - mean_squared_error: 0.9994 - val_loss: 0.9976 - val_mean_squared_error: 0.9976\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.99861 to 0.99762, saving model to temp/e12\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 2s 101us/step - loss: 1.0057 - mean_squared_error: 1.0057 - val_loss: 0.9994 - val_mean_squared_error: 0.9994\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.99762\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0007 - mean_squared_error: 1.0007 - val_loss: 0.9920 - val_mean_squared_error: 0.9920\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.99762 to 0.99200, saving model to temp/e12\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0035 - mean_squared_error: 1.0035 - val_loss: 1.0121 - val_mean_squared_error: 1.0121\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.99200\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0000 - mean_squared_error: 1.0000 - val_loss: 0.9979 - val_mean_squared_error: 0.9979\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.99200\n",
      "Epoch 00016: early stopping\n",
      "temp/e13\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 109us/step - loss: 1.1020 - mean_squared_error: 1.1020 - val_loss: 1.0175 - val_mean_squared_error: 1.0175\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01754, saving model to temp/e13\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0220 - mean_squared_error: 1.0220 - val_loss: 1.0114 - val_mean_squared_error: 1.0114\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01754 to 1.01139, saving model to temp/e13\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 98us/step - loss: 1.0159 - mean_squared_error: 1.0159 - val_loss: 1.0101 - val_mean_squared_error: 1.0101\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01139 to 1.01009, saving model to temp/e13\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.0134 - mean_squared_error: 1.0134 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01009 to 1.00540, saving model to temp/e13\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 1.0050 - val_mean_squared_error: 1.0050\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00540 to 1.00498, saving model to temp/e13\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.0112 - mean_squared_error: 1.0112 - val_loss: 1.0046 - val_mean_squared_error: 1.0046\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00498 to 1.00462, saving model to temp/e13\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0054 - mean_squared_error: 1.0054 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00462 to 1.00243, saving model to temp/e13\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0058 - mean_squared_error: 1.0058 - val_loss: 0.9987 - val_mean_squared_error: 0.9987\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00243 to 0.99870, saving model to temp/e13\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0057 - mean_squared_error: 1.0057 - val_loss: 0.9964 - val_mean_squared_error: 0.9964\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.99870 to 0.99640, saving model to temp/e13\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0029 - mean_squared_error: 1.0029 - val_loss: 0.9965 - val_mean_squared_error: 0.9965\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99640\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 0.9985 - mean_squared_error: 0.9985 - val_loss: 1.0011 - val_mean_squared_error: 1.0011\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99640\n",
      "Epoch 00011: early stopping\n",
      "temp/e14\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 102us/step - loss: 1.0973 - mean_squared_error: 1.0973 - val_loss: 1.0235 - val_mean_squared_error: 1.0235\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02352, saving model to temp/e14\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 1.0181 - val_mean_squared_error: 1.0181\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02352 to 1.01812, saving model to temp/e14\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.0161 - mean_squared_error: 1.0161 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01812 to 1.00910, saving model to temp/e14\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 2s 101us/step - loss: 1.0066 - mean_squared_error: 1.0066 - val_loss: 1.0073 - val_mean_squared_error: 1.0073\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00910 to 1.00728, saving model to temp/e14\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0098 - mean_squared_error: 1.0098 - val_loss: 1.0139 - val_mean_squared_error: 1.0139\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00728\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0059 - mean_squared_error: 1.0059 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00728 to 1.00474, saving model to temp/e14\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0057 - mean_squared_error: 1.0057 - val_loss: 1.0022 - val_mean_squared_error: 1.0022\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00474 to 1.00221, saving model to temp/e14\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0047 - mean_squared_error: 1.0047 - val_loss: 1.0092 - val_mean_squared_error: 1.0092\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00221\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0021 - mean_squared_error: 1.0021 - val_loss: 1.0048 - val_mean_squared_error: 1.0048\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00221\n",
      "Epoch 00009: early stopping\n",
      "temp/e15\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 113us/step - loss: 1.0942 - mean_squared_error: 1.0942 - val_loss: 1.0239 - val_mean_squared_error: 1.0239\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02390, saving model to temp/e15\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0242 - mean_squared_error: 1.0242 - val_loss: 1.0103 - val_mean_squared_error: 1.0103\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02390 to 1.01025, saving model to temp/e15\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0148 - mean_squared_error: 1.0148 - val_loss: 1.0196 - val_mean_squared_error: 1.0196\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01025\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 96us/step - loss: 1.0144 - mean_squared_error: 1.0144 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01025 to 1.00606, saving model to temp/e15\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0127 - mean_squared_error: 1.0127 - val_loss: 1.0063 - val_mean_squared_error: 1.0063\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00606\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0135 - val_mean_squared_error: 1.0135\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00606\n",
      "Epoch 00006: early stopping\n",
      "temp/e16\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 108us/step - loss: 1.1128 - mean_squared_error: 1.1128 - val_loss: 1.0183 - val_mean_squared_error: 1.0183\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01826, saving model to temp/e16\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 95us/step - loss: 1.0306 - mean_squared_error: 1.0306 - val_loss: 1.0164 - val_mean_squared_error: 1.0164\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01826 to 1.01639, saving model to temp/e16\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 98us/step - loss: 1.0175 - mean_squared_error: 1.0175 - val_loss: 1.0039 - val_mean_squared_error: 1.0039\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01639 to 1.00387, saving model to temp/e16\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 95us/step - loss: 1.0106 - mean_squared_error: 1.0106 - val_loss: 1.0113 - val_mean_squared_error: 1.0113\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00387\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 97us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0022 - val_mean_squared_error: 1.0022\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00387 to 1.00223, saving model to temp/e16\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 96us/step - loss: 1.0118 - mean_squared_error: 1.0118 - val_loss: 1.0100 - val_mean_squared_error: 1.0100\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00223\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 97us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 1.0018 - val_mean_squared_error: 1.0018\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00223 to 1.00184, saving model to temp/e16\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 97us/step - loss: 1.0052 - mean_squared_error: 1.0052 - val_loss: 1.0002 - val_mean_squared_error: 1.0002\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00184 to 1.00021, saving model to temp/e16\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 96us/step - loss: 1.0027 - mean_squared_error: 1.0027 - val_loss: 1.0014 - val_mean_squared_error: 1.0014\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00021\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 2s 98us/step - loss: 1.0053 - mean_squared_error: 1.0053 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.00021\n",
      "Epoch 00010: early stopping\n",
      "temp/e17\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 105us/step - loss: 1.0945 - mean_squared_error: 1.0945 - val_loss: 1.0378 - val_mean_squared_error: 1.0378\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03777, saving model to temp/e17\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 96us/step - loss: 1.0195 - mean_squared_error: 1.0195 - val_loss: 1.0123 - val_mean_squared_error: 1.0123\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03777 to 1.01230, saving model to temp/e17\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 95us/step - loss: 1.0125 - mean_squared_error: 1.0125 - val_loss: 1.0135 - val_mean_squared_error: 1.0135\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01230\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 97us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01230 to 1.00520, saving model to temp/e17\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 95us/step - loss: 1.0069 - mean_squared_error: 1.0069 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00520\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 98us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0118 - val_mean_squared_error: 1.0118\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00520\n",
      "Epoch 00006: early stopping\n",
      "temp/e18\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 103us/step - loss: 1.0952 - mean_squared_error: 1.0952 - val_loss: 1.0188 - val_mean_squared_error: 1.0188\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01885, saving model to temp/e18\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 97us/step - loss: 1.0206 - mean_squared_error: 1.0206 - val_loss: 1.0121 - val_mean_squared_error: 1.0121\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01885 to 1.01210, saving model to temp/e18\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 1.0118 - mean_squared_error: 1.0118 - val_loss: 1.0072 - val_mean_squared_error: 1.0072\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01210 to 1.00720, saving model to temp/e18\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 95us/step - loss: 1.0127 - mean_squared_error: 1.0127 - val_loss: 1.0182 - val_mean_squared_error: 1.0182\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00720\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.0073 - mean_squared_error: 1.0073 - val_loss: 1.0021 - val_mean_squared_error: 1.0021\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00720 to 1.00213, saving model to temp/e18\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0060 - mean_squared_error: 1.0060 - val_loss: 1.0005 - val_mean_squared_error: 1.0005\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00213 to 1.00054, saving model to temp/e18\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0061 - mean_squared_error: 1.0061 - val_loss: 1.0017 - val_mean_squared_error: 1.0017\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00054\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0054 - mean_squared_error: 1.0054 - val_loss: 1.0043 - val_mean_squared_error: 1.0043\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00054\n",
      "Epoch 00008: early stopping\n",
      "temp/e19\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 101us/step - loss: 1.1034 - mean_squared_error: 1.1034 - val_loss: 1.0221 - val_mean_squared_error: 1.0221\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02209, saving model to temp/e19\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0154 - mean_squared_error: 1.0154 - val_loss: 1.0132 - val_mean_squared_error: 1.0132\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02209 to 1.01325, saving model to temp/e19\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.0205 - mean_squared_error: 1.0205 - val_loss: 1.0107 - val_mean_squared_error: 1.0107\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01325 to 1.01071, saving model to temp/e19\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 101us/step - loss: 1.0137 - mean_squared_error: 1.0137 - val_loss: 1.0167 - val_mean_squared_error: 1.0167\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.01071\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01071 to 1.00265, saving model to temp/e19\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 97us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 0.9988 - val_mean_squared_error: 0.9988\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00265 to 0.99877, saving model to temp/e19\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.0079 - mean_squared_error: 1.0079 - val_loss: 0.9984 - val_mean_squared_error: 0.9984\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.99877 to 0.99838, saving model to temp/e19\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0042 - mean_squared_error: 1.0042 - val_loss: 0.9996 - val_mean_squared_error: 0.9996\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99838\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.0045 - mean_squared_error: 1.0045 - val_loss: 0.9985 - val_mean_squared_error: 0.9985\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99838\n",
      "Epoch 00009: early stopping\n",
      "temp/e20\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 107us/step - loss: 1.0993 - mean_squared_error: 1.0993 - val_loss: 1.0139 - val_mean_squared_error: 1.0139\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01392, saving model to temp/e20\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.0209 - mean_squared_error: 1.0209 - val_loss: 1.0143 - val_mean_squared_error: 1.0143\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01392\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0144 - mean_squared_error: 1.0144 - val_loss: 1.0138 - val_mean_squared_error: 1.0138\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01392 to 1.01378, saving model to temp/e20\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0122 - val_mean_squared_error: 1.0122\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01378 to 1.01216, saving model to temp/e20\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0020 - val_mean_squared_error: 1.0020\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01216 to 1.00196, saving model to temp/e20\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0066 - mean_squared_error: 1.0066 - val_loss: 1.0041 - val_mean_squared_error: 1.0041\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00196\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0059 - mean_squared_error: 1.0059 - val_loss: 0.9995 - val_mean_squared_error: 0.9995\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00196 to 0.99947, saving model to temp/e20\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0009 - mean_squared_error: 1.0009 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99947\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0047 - mean_squared_error: 1.0047 - val_loss: 1.0021 - val_mean_squared_error: 1.0021\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99947\n",
      "Epoch 00009: early stopping\n",
      "temp/e21\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 102us/step - loss: 1.1019 - mean_squared_error: 1.1019 - val_loss: 1.0274 - val_mean_squared_error: 1.0274\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02743, saving model to temp/e21\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0250 - mean_squared_error: 1.0250 - val_loss: 1.0103 - val_mean_squared_error: 1.0103\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02743 to 1.01028, saving model to temp/e21\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 1.0154 - mean_squared_error: 1.0154 - val_loss: 1.0078 - val_mean_squared_error: 1.0078\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01028 to 1.00784, saving model to temp/e21\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0055 - val_mean_squared_error: 1.0055\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00784 to 1.00547, saving model to temp/e21\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 1.0107 - mean_squared_error: 1.0107 - val_loss: 1.0007 - val_mean_squared_error: 1.0007\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00547 to 1.00067, saving model to temp/e21\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 96us/step - loss: 1.0078 - mean_squared_error: 1.0078 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00067\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 1.0047 - mean_squared_error: 1.0047 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00067\n",
      "Epoch 00007: early stopping\n",
      "temp/e22\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 105us/step - loss: 1.1069 - mean_squared_error: 1.1069 - val_loss: 1.0193 - val_mean_squared_error: 1.0193\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01933, saving model to temp/e22\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 1.0228 - mean_squared_error: 1.0228 - val_loss: 1.0202 - val_mean_squared_error: 1.0202\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01933\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0179 - mean_squared_error: 1.0179 - val_loss: 1.0147 - val_mean_squared_error: 1.0147\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01933 to 1.01468, saving model to temp/e22\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 1.0137 - mean_squared_error: 1.0137 - val_loss: 1.0064 - val_mean_squared_error: 1.0064\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01468 to 1.00638, saving model to temp/e22\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 97us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 1.0085 - val_mean_squared_error: 1.0085\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00638\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 2s 98us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.0018 - val_mean_squared_error: 1.0018\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00638 to 1.00179, saving model to temp/e22\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 97us/step - loss: 1.0049 - mean_squared_error: 1.0049 - val_loss: 1.0019 - val_mean_squared_error: 1.0019\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00179\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 97us/step - loss: 1.0042 - mean_squared_error: 1.0042 - val_loss: 1.0014 - val_mean_squared_error: 1.0014\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00179 to 1.00137, saving model to temp/e22\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 96us/step - loss: 1.0022 - mean_squared_error: 1.0022 - val_loss: 1.0008 - val_mean_squared_error: 1.0008\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00137 to 1.00078, saving model to temp/e22\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 2s 98us/step - loss: 1.0029 - mean_squared_error: 1.0029 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.00078\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.0037 - mean_squared_error: 1.0037 - val_loss: 1.0009 - val_mean_squared_error: 1.0009\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.00078\n",
      "Epoch 00011: early stopping\n",
      "temp/e23\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 104us/step - loss: 1.0996 - mean_squared_error: 1.0996 - val_loss: 1.0268 - val_mean_squared_error: 1.0268\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02678, saving model to temp/e23\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 98us/step - loss: 1.0193 - mean_squared_error: 1.0193 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02678 to 1.00687, saving model to temp/e23\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 97us/step - loss: 1.0178 - mean_squared_error: 1.0178 - val_loss: 1.0145 - val_mean_squared_error: 1.0145\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00687\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 95us/step - loss: 1.0186 - mean_squared_error: 1.0186 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00687\n",
      "Epoch 00004: early stopping\n",
      "temp/e24\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 104us/step - loss: 1.0884 - mean_squared_error: 1.0884 - val_loss: 1.0200 - val_mean_squared_error: 1.0200\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02000, saving model to temp/e24\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 97us/step - loss: 1.0244 - mean_squared_error: 1.0244 - val_loss: 1.0145 - val_mean_squared_error: 1.0145\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02000 to 1.01449, saving model to temp/e24\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 95us/step - loss: 1.0137 - mean_squared_error: 1.0137 - val_loss: 1.0072 - val_mean_squared_error: 1.0072\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01449 to 1.00724, saving model to temp/e24\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 98us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00724 to 1.00303, saving model to temp/e24\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 95us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0043 - val_mean_squared_error: 1.0043\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00303\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 0.9980 - val_mean_squared_error: 0.9980\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00303 to 0.99803, saving model to temp/e24\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 96us/step - loss: 1.0072 - mean_squared_error: 1.0072 - val_loss: 1.0017 - val_mean_squared_error: 1.0017\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99803\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.0058 - mean_squared_error: 1.0058 - val_loss: 0.9977 - val_mean_squared_error: 0.9977\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.99803 to 0.99769, saving model to temp/e24\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 96us/step - loss: 1.0032 - mean_squared_error: 1.0032 - val_loss: 0.9985 - val_mean_squared_error: 0.9985\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99769\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.0027 - mean_squared_error: 1.0027 - val_loss: 0.9986 - val_mean_squared_error: 0.9986\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99769\n",
      "Epoch 00010: early stopping\n",
      "temp/e25\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 104us/step - loss: 1.0982 - mean_squared_error: 1.0982 - val_loss: 1.0305 - val_mean_squared_error: 1.0305\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03054, saving model to temp/e25\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 1.0254 - mean_squared_error: 1.0254 - val_loss: 1.0178 - val_mean_squared_error: 1.0178\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03054 to 1.01782, saving model to temp/e25\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 96us/step - loss: 1.0176 - mean_squared_error: 1.0176 - val_loss: 1.0163 - val_mean_squared_error: 1.0163\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01782 to 1.01631, saving model to temp/e25\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 1.0072 - val_mean_squared_error: 1.0072\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01631 to 1.00721, saving model to temp/e25\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0125 - mean_squared_error: 1.0125 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00721 to 1.00377, saving model to temp/e25\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.0093 - mean_squared_error: 1.0093 - val_loss: 0.9992 - val_mean_squared_error: 0.9992\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00377 to 0.99925, saving model to temp/e25\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 0.9999 - val_mean_squared_error: 0.9999\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99925\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.0043 - mean_squared_error: 1.0043 - val_loss: 1.0097 - val_mean_squared_error: 1.0097\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99925\n",
      "Epoch 00008: early stopping\n",
      "temp/e26\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 115us/step - loss: 1.1080 - mean_squared_error: 1.1080 - val_loss: 1.0213 - val_mean_squared_error: 1.0213\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02129, saving model to temp/e26\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.0212 - mean_squared_error: 1.0212 - val_loss: 1.0137 - val_mean_squared_error: 1.0137\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02129 to 1.01370, saving model to temp/e26\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0186 - mean_squared_error: 1.0186 - val_loss: 1.0084 - val_mean_squared_error: 1.0084\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01370 to 1.00839, saving model to temp/e26\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.0106 - mean_squared_error: 1.0106 - val_loss: 1.0070 - val_mean_squared_error: 1.0070\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00839 to 1.00701, saving model to temp/e26\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00701 to 1.00245, saving model to temp/e26\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0082 - mean_squared_error: 1.0082 - val_loss: 1.0078 - val_mean_squared_error: 1.0078\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00245\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0054 - mean_squared_error: 1.0054 - val_loss: 1.0072 - val_mean_squared_error: 1.0072\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00245\n",
      "Epoch 00007: early stopping\n",
      "temp/e27\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 101us/step - loss: 1.1083 - mean_squared_error: 1.1083 - val_loss: 1.0249 - val_mean_squared_error: 1.0249\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02486, saving model to temp/e27\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0227 - mean_squared_error: 1.0227 - val_loss: 1.0208 - val_mean_squared_error: 1.0208\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02486 to 1.02079, saving model to temp/e27\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0182 - mean_squared_error: 1.0182 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02079 to 1.00616, saving model to temp/e27\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0042 - val_mean_squared_error: 1.0042\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00616 to 1.00419, saving model to temp/e27\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0056 - mean_squared_error: 1.0056 - val_loss: 1.0048 - val_mean_squared_error: 1.0048\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00419\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 101us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0006 - val_mean_squared_error: 1.0006\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00419 to 1.00059, saving model to temp/e27\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 1.0079 - mean_squared_error: 1.0079 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00059\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0020 - mean_squared_error: 1.0020 - val_loss: 1.0007 - val_mean_squared_error: 1.0007\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00059\n",
      "Epoch 00008: early stopping\n",
      "temp/e28\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 101us/step - loss: 1.1001 - mean_squared_error: 1.1001 - val_loss: 1.0274 - val_mean_squared_error: 1.0274\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02744, saving model to temp/e28\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0235 - mean_squared_error: 1.0235 - val_loss: 1.0128 - val_mean_squared_error: 1.0128\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02744 to 1.01283, saving model to temp/e28\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 101us/step - loss: 1.0157 - mean_squared_error: 1.0157 - val_loss: 1.0153 - val_mean_squared_error: 1.0153\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01283\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 102us/step - loss: 1.0107 - mean_squared_error: 1.0107 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01283 to 1.00693, saving model to temp/e28\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 101us/step - loss: 1.0061 - mean_squared_error: 1.0061 - val_loss: 1.0077 - val_mean_squared_error: 1.0077\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00693\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 101us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0060 - val_mean_squared_error: 1.0060\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00693 to 1.00598, saving model to temp/e28\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0070 - mean_squared_error: 1.0070 - val_loss: 1.0056 - val_mean_squared_error: 1.0056\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00598 to 1.00559, saving model to temp/e28\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 101us/step - loss: 1.0031 - mean_squared_error: 1.0031 - val_loss: 0.9954 - val_mean_squared_error: 0.9954\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00559 to 0.99543, saving model to temp/e28\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 101us/step - loss: 1.0047 - mean_squared_error: 1.0047 - val_loss: 1.0013 - val_mean_squared_error: 1.0013\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99543\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0039 - mean_squared_error: 1.0039 - val_loss: 0.9960 - val_mean_squared_error: 0.9960\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99543\n",
      "Epoch 00010: early stopping\n",
      "temp/e29\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 110us/step - loss: 1.1007 - mean_squared_error: 1.1007 - val_loss: 1.0172 - val_mean_squared_error: 1.0172\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01718, saving model to temp/e29\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0239 - mean_squared_error: 1.0239 - val_loss: 1.0108 - val_mean_squared_error: 1.0108\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01718 to 1.01075, saving model to temp/e29\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0140 - mean_squared_error: 1.0140 - val_loss: 1.0187 - val_mean_squared_error: 1.0187\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01075\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01075 to 1.00574, saving model to temp/e29\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0020 - val_mean_squared_error: 1.0020\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00574 to 1.00200, saving model to temp/e29\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 1.0081 - val_mean_squared_error: 1.0081\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00200\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0045 - mean_squared_error: 1.0045 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00200\n",
      "Epoch 00007: early stopping\n",
      "temp/e30\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 108us/step - loss: 1.0890 - mean_squared_error: 1.0890 - val_loss: 1.0288 - val_mean_squared_error: 1.0288\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02882, saving model to temp/e30\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 97us/step - loss: 1.0207 - mean_squared_error: 1.0207 - val_loss: 1.0128 - val_mean_squared_error: 1.0128\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02882 to 1.01280, saving model to temp/e30\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 1.0137 - mean_squared_error: 1.0137 - val_loss: 1.0141 - val_mean_squared_error: 1.0141\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01280\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 1.0076 - val_mean_squared_error: 1.0076\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01280 to 1.00756, saving model to temp/e30\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 98us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0041 - val_mean_squared_error: 1.0041\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00756 to 1.00406, saving model to temp/e30\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00406\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 2s 102us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0002 - val_mean_squared_error: 1.0002\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00406 to 1.00023, saving model to temp/e30\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 98us/step - loss: 1.0073 - mean_squared_error: 1.0073 - val_loss: 0.9989 - val_mean_squared_error: 0.9989\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00023 to 0.99892, saving model to temp/e30\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 98us/step - loss: 1.0058 - mean_squared_error: 1.0058 - val_loss: 0.9956 - val_mean_squared_error: 0.9956\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.99892 to 0.99560, saving model to temp/e30\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0016 - mean_squared_error: 1.0016 - val_loss: 0.9987 - val_mean_squared_error: 0.9987\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99560\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 1.0010 - mean_squared_error: 1.0010 - val_loss: 0.9965 - val_mean_squared_error: 0.9965\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99560\n",
      "Epoch 00011: early stopping\n",
      "temp/e31\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 1.0904 - mean_squared_error: 1.0904 - val_loss: 1.0331 - val_mean_squared_error: 1.0331\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03314, saving model to temp/e31\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 114us/step - loss: 1.0201 - mean_squared_error: 1.0201 - val_loss: 1.0136 - val_mean_squared_error: 1.0136\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03314 to 1.01359, saving model to temp/e31\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0172 - mean_squared_error: 1.0172 - val_loss: 1.0082 - val_mean_squared_error: 1.0082\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01359 to 1.00821, saving model to temp/e31\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0118 - mean_squared_error: 1.0118 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00821\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0070 - mean_squared_error: 1.0070 - val_loss: 1.0107 - val_mean_squared_error: 1.0107\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00821\n",
      "Epoch 00005: early stopping\n",
      "temp/e32\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 123us/step - loss: 1.1010 - mean_squared_error: 1.1010 - val_loss: 1.0169 - val_mean_squared_error: 1.0169\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01687, saving model to temp/e32\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 115us/step - loss: 1.0250 - mean_squared_error: 1.0250 - val_loss: 1.0130 - val_mean_squared_error: 1.0130\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01687 to 1.01304, saving model to temp/e32\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0178 - mean_squared_error: 1.0178 - val_loss: 1.0138 - val_mean_squared_error: 1.0138\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01304\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0098 - mean_squared_error: 1.0098 - val_loss: 1.0058 - val_mean_squared_error: 1.0058\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01304 to 1.00582, saving model to temp/e32\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 117us/step - loss: 1.0098 - mean_squared_error: 1.0098 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00582 to 1.00382, saving model to temp/e32\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 117us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0134 - val_mean_squared_error: 1.0134\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00382\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0044 - mean_squared_error: 1.0044 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00382\n",
      "Epoch 00007: early stopping\n",
      "temp/e33\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 125us/step - loss: 1.0927 - mean_squared_error: 1.0927 - val_loss: 1.0340 - val_mean_squared_error: 1.0340\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03398, saving model to temp/e33\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0238 - mean_squared_error: 1.0238 - val_loss: 1.0118 - val_mean_squared_error: 1.0118\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03398 to 1.01184, saving model to temp/e33\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0152 - mean_squared_error: 1.0152 - val_loss: 1.0072 - val_mean_squared_error: 1.0072\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01184 to 1.00724, saving model to temp/e33\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 120us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 1.0166 - val_mean_squared_error: 1.0166\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00724\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0114 - val_mean_squared_error: 1.0114\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00724\n",
      "Epoch 00005: early stopping\n",
      "temp/e34\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 1.0910 - mean_squared_error: 1.0910 - val_loss: 1.0188 - val_mean_squared_error: 1.0188\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01876, saving model to temp/e34\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0202 - mean_squared_error: 1.0202 - val_loss: 1.0154 - val_mean_squared_error: 1.0154\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01876 to 1.01539, saving model to temp/e34\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0180 - mean_squared_error: 1.0180 - val_loss: 1.0113 - val_mean_squared_error: 1.0113\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01539 to 1.01126, saving model to temp/e34\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 1.0070 - val_mean_squared_error: 1.0070\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01126 to 1.00699, saving model to temp/e34\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00699 to 1.00612, saving model to temp/e34\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0082 - mean_squared_error: 1.0082 - val_loss: 1.0022 - val_mean_squared_error: 1.0022\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00612 to 1.00221, saving model to temp/e34\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 116us/step - loss: 1.0065 - mean_squared_error: 1.0065 - val_loss: 1.0004 - val_mean_squared_error: 1.0004\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00221 to 1.00038, saving model to temp/e34\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0066 - mean_squared_error: 1.0066 - val_loss: 0.9992 - val_mean_squared_error: 0.9992\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00038 to 0.99919, saving model to temp/e34\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0026 - mean_squared_error: 1.0026 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99919\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0027 - mean_squared_error: 1.0027 - val_loss: 1.0014 - val_mean_squared_error: 1.0014\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99919\n",
      "Epoch 00010: early stopping\n",
      "temp/e35\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 126us/step - loss: 1.0855 - mean_squared_error: 1.0855 - val_loss: 1.0268 - val_mean_squared_error: 1.0268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02684, saving model to temp/e35\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 1.0268 - val_mean_squared_error: 1.0268\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.02684\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 120us/step - loss: 1.0159 - mean_squared_error: 1.0159 - val_loss: 1.0120 - val_mean_squared_error: 1.0120\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02684 to 1.01197, saving model to temp/e35\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 120us/step - loss: 1.0138 - mean_squared_error: 1.0138 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01197 to 1.00606, saving model to temp/e35\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0125 - mean_squared_error: 1.0125 - val_loss: 1.0074 - val_mean_squared_error: 1.0074\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00606\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 121us/step - loss: 1.0058 - mean_squared_error: 1.0058 - val_loss: 0.9998 - val_mean_squared_error: 0.9998\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00606 to 0.99983, saving model to temp/e35\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0048 - mean_squared_error: 1.0048 - val_loss: 1.0017 - val_mean_squared_error: 1.0017\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99983\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 121us/step - loss: 1.0004 - mean_squared_error: 1.0004 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99983\n",
      "Epoch 00008: early stopping\n",
      "temp/e36\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0862 - mean_squared_error: 1.0862 - val_loss: 1.0212 - val_mean_squared_error: 1.0212\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02123, saving model to temp/e36\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 120us/step - loss: 1.0210 - mean_squared_error: 1.0210 - val_loss: 1.0148 - val_mean_squared_error: 1.0148\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02123 to 1.01485, saving model to temp/e36\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0148 - mean_squared_error: 1.0148 - val_loss: 1.0252 - val_mean_squared_error: 1.0252\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01485\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0041 - val_mean_squared_error: 1.0041\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01485 to 1.00412, saving model to temp/e36\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 120us/step - loss: 1.0128 - mean_squared_error: 1.0128 - val_loss: 1.0064 - val_mean_squared_error: 1.0064\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00412\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0070 - mean_squared_error: 1.0070 - val_loss: 1.0025 - val_mean_squared_error: 1.0025\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00412 to 1.00254, saving model to temp/e36\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 121us/step - loss: 1.0069 - mean_squared_error: 1.0069 - val_loss: 1.0016 - val_mean_squared_error: 1.0016\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00254 to 1.00157, saving model to temp/e36\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0043 - mean_squared_error: 1.0043 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00157\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 120us/step - loss: 1.0015 - mean_squared_error: 1.0015 - val_loss: 1.0077 - val_mean_squared_error: 1.0077\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00157\n",
      "Epoch 00009: early stopping\n",
      "temp/e37\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0949 - mean_squared_error: 1.0949 - val_loss: 1.0268 - val_mean_squared_error: 1.0268\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02680, saving model to temp/e37\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 120us/step - loss: 1.0275 - mean_squared_error: 1.0275 - val_loss: 1.0107 - val_mean_squared_error: 1.0107\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02680 to 1.01067, saving model to temp/e37\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0165 - mean_squared_error: 1.0165 - val_loss: 1.0068 - val_mean_squared_error: 1.0068\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01067 to 1.00680, saving model to temp/e37\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 120us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0050 - val_mean_squared_error: 1.0050\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00680 to 1.00500, saving model to temp/e37\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0089 - mean_squared_error: 1.0089 - val_loss: 1.0143 - val_mean_squared_error: 1.0143\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00500\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0027 - val_mean_squared_error: 1.0027\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00500 to 1.00267, saving model to temp/e37\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 117us/step - loss: 1.0042 - mean_squared_error: 1.0042 - val_loss: 0.9998 - val_mean_squared_error: 0.9998\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00267 to 0.99982, saving model to temp/e37\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0058 - mean_squared_error: 1.0058 - val_loss: 0.9997 - val_mean_squared_error: 0.9997\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.99982 to 0.99973, saving model to temp/e37\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 112us/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 0.9987 - val_mean_squared_error: 0.9987\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.99973 to 0.99869, saving model to temp/e37\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 2s 120us/step - loss: 1.0046 - mean_squared_error: 1.0046 - val_loss: 0.9936 - val_mean_squared_error: 0.9936\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.99869 to 0.99360, saving model to temp/e37\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 2s 116us/step - loss: 1.0004 - mean_squared_error: 1.0004 - val_loss: 0.9981 - val_mean_squared_error: 0.9981\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99360\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0019 - mean_squared_error: 1.0019 - val_loss: 0.9984 - val_mean_squared_error: 0.9984\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.99360\n",
      "Epoch 00012: early stopping\n",
      "temp/e38\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 125us/step - loss: 1.1052 - mean_squared_error: 1.1052 - val_loss: 1.0320 - val_mean_squared_error: 1.0320\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03201, saving model to temp/e38\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0208 - mean_squared_error: 1.0208 - val_loss: 1.0151 - val_mean_squared_error: 1.0151\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03201 to 1.01506, saving model to temp/e38\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0096 - val_mean_squared_error: 1.0096\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01506 to 1.00962, saving model to temp/e38\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0114 - mean_squared_error: 1.0114 - val_loss: 1.0112 - val_mean_squared_error: 1.0112\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00962\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 0.9986 - val_mean_squared_error: 0.9986\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00962 to 0.99856, saving model to temp/e38\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 2s 117us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 1.0028 - val_mean_squared_error: 1.0028\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99856\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0057 - mean_squared_error: 1.0057 - val_loss: 0.9991 - val_mean_squared_error: 0.9991\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99856\n",
      "Epoch 00007: early stopping\n",
      "temp/e39\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 126us/step - loss: 1.0978 - mean_squared_error: 1.0978 - val_loss: 1.0200 - val_mean_squared_error: 1.0200\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02004, saving model to temp/e39\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0237 - mean_squared_error: 1.0237 - val_loss: 1.0104 - val_mean_squared_error: 1.0104\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02004 to 1.01040, saving model to temp/e39\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 121us/step - loss: 1.0131 - mean_squared_error: 1.0131 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01040 to 1.00799, saving model to temp/e39\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0127 - mean_squared_error: 1.0127 - val_loss: 1.0153 - val_mean_squared_error: 1.0153\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00799\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 113us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0068 - val_mean_squared_error: 1.0068\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00799 to 1.00677, saving model to temp/e39\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 115us/step - loss: 1.0038 - mean_squared_error: 1.0038 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00677 to 1.00404, saving model to temp/e39\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 121us/step - loss: 1.0041 - mean_squared_error: 1.0041 - val_loss: 0.9976 - val_mean_squared_error: 0.9976\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00404 to 0.99761, saving model to temp/e39\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0047 - mean_squared_error: 1.0047 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99761\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 116us/step - loss: 1.0027 - mean_squared_error: 1.0027 - val_loss: 0.9981 - val_mean_squared_error: 0.9981\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99761\n",
      "Epoch 00009: early stopping\n",
      "temp/e40\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 125us/step - loss: 1.0880 - mean_squared_error: 1.0880 - val_loss: 1.0153 - val_mean_squared_error: 1.0153\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01532, saving model to temp/e40\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0199 - mean_squared_error: 1.0199 - val_loss: 1.0137 - val_mean_squared_error: 1.0137\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01532 to 1.01372, saving model to temp/e40\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0123 - mean_squared_error: 1.0123 - val_loss: 1.0109 - val_mean_squared_error: 1.0109\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01372 to 1.01090, saving model to temp/e40\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 113us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01090 to 1.00527, saving model to temp/e40\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 116us/step - loss: 1.0093 - mean_squared_error: 1.0093 - val_loss: 0.9963 - val_mean_squared_error: 0.9963\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00527 to 0.99631, saving model to temp/e40\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0105 - mean_squared_error: 1.0105 - val_loss: 1.0003 - val_mean_squared_error: 1.0003\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99631\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0081 - mean_squared_error: 1.0081 - val_loss: 1.0027 - val_mean_squared_error: 1.0027\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99631\n",
      "Epoch 00007: early stopping\n",
      "temp/e41\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 124us/step - loss: 1.0881 - mean_squared_error: 1.0881 - val_loss: 1.0250 - val_mean_squared_error: 1.0250\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02503, saving model to temp/e41\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 117us/step - loss: 1.0191 - mean_squared_error: 1.0191 - val_loss: 1.0128 - val_mean_squared_error: 1.0128\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02503 to 1.01279, saving model to temp/e41\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0169 - mean_squared_error: 1.0169 - val_loss: 1.0079 - val_mean_squared_error: 1.0079\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01279 to 1.00793, saving model to temp/e41\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 120us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0079 - val_mean_squared_error: 1.0079\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00793 to 1.00785, saving model to temp/e41\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 117us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0015 - val_mean_squared_error: 1.0015\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00785 to 1.00150, saving model to temp/e41\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 117us/step - loss: 1.0045 - mean_squared_error: 1.0045 - val_loss: 1.0014 - val_mean_squared_error: 1.0014\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00150 to 1.00137, saving model to temp/e41\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0053 - mean_squared_error: 1.0053 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00137\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 118us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 0.9956 - val_mean_squared_error: 0.9956\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00137 to 0.99562, saving model to temp/e41\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 117us/step - loss: 1.0032 - mean_squared_error: 1.0032 - val_loss: 1.0020 - val_mean_squared_error: 1.0020\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99562\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0021 - mean_squared_error: 1.0021 - val_loss: 0.9979 - val_mean_squared_error: 0.9979\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99562\n",
      "Epoch 00010: early stopping\n",
      "temp/e42\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 124us/step - loss: 1.0969 - mean_squared_error: 1.0969 - val_loss: 1.0204 - val_mean_squared_error: 1.0204\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02044, saving model to temp/e42\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 120us/step - loss: 1.0207 - mean_squared_error: 1.0207 - val_loss: 1.0160 - val_mean_squared_error: 1.0160\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02044 to 1.01600, saving model to temp/e42\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 117us/step - loss: 1.0136 - mean_squared_error: 1.0136 - val_loss: 1.0124 - val_mean_squared_error: 1.0124\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01600 to 1.01236, saving model to temp/e42\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 116us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0135 - val_mean_squared_error: 1.0135\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.01236\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 120us/step - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 1.0101 - val_mean_squared_error: 1.0101\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01236 to 1.01008, saving model to temp/e42\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0060 - mean_squared_error: 1.0060 - val_loss: 1.0045 - val_mean_squared_error: 1.0045\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.01008 to 1.00446, saving model to temp/e42\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 120us/step - loss: 1.0066 - mean_squared_error: 1.0066 - val_loss: 0.9971 - val_mean_squared_error: 0.9971\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00446 to 0.99710, saving model to temp/e42\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0055 - mean_squared_error: 1.0055 - val_loss: 0.9999 - val_mean_squared_error: 0.9999\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99710\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 120us/step - loss: 1.0038 - mean_squared_error: 1.0038 - val_loss: 0.9975 - val_mean_squared_error: 0.9975\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99710\n",
      "Epoch 00009: early stopping\n",
      "temp/e43\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 1.0982 - mean_squared_error: 1.0982 - val_loss: 1.0197 - val_mean_squared_error: 1.0197\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01975, saving model to temp/e43\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.0218 - mean_squared_error: 1.0218 - val_loss: 1.0066 - val_mean_squared_error: 1.0066\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01975 to 1.00663, saving model to temp/e43\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 115us/step - loss: 1.0138 - mean_squared_error: 1.0138 - val_loss: 1.0095 - val_mean_squared_error: 1.0095\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00663\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 115us/step - loss: 1.0150 - mean_squared_error: 1.0150 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00663 to 1.00537, saving model to temp/e43\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 114us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0072 - val_mean_squared_error: 1.0072\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00537\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 108us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0087 - val_mean_squared_error: 1.0087\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00537\n",
      "Epoch 00006: early stopping\n",
      "temp/e44\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 1.1079 - mean_squared_error: 1.1079 - val_loss: 1.0204 - val_mean_squared_error: 1.0204\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02038, saving model to temp/e44\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 120us/step - loss: 1.0247 - mean_squared_error: 1.0247 - val_loss: 1.0135 - val_mean_squared_error: 1.0135\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02038 to 1.01353, saving model to temp/e44\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 122us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01353 to 1.00493, saving model to temp/e44\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 123us/step - loss: 1.0079 - mean_squared_error: 1.0079 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00493\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 123us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0031 - val_mean_squared_error: 1.0031\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00493 to 1.00308, saving model to temp/e44\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 123us/step - loss: 1.0059 - mean_squared_error: 1.0059 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00308 to 1.00236, saving model to temp/e44\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 123us/step - loss: 1.0043 - mean_squared_error: 1.0043 - val_loss: 1.0027 - val_mean_squared_error: 1.0027\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00236\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 123us/step - loss: 1.0066 - mean_squared_error: 1.0066 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00236 to 1.00234, saving model to temp/e44\n",
      "Epoch 00008: early stopping\n",
      "temp/e45\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 2s 125us/step - loss: 1.0885 - mean_squared_error: 1.0885 - val_loss: 1.0315 - val_mean_squared_error: 1.0315\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03145, saving model to temp/e45\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 116us/step - loss: 1.0278 - mean_squared_error: 1.0278 - val_loss: 1.0165 - val_mean_squared_error: 1.0165\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03145 to 1.01653, saving model to temp/e45\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 114us/step - loss: 1.0142 - mean_squared_error: 1.0142 - val_loss: 1.0006 - val_mean_squared_error: 1.0006\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01653 to 1.00061, saving model to temp/e45\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 116us/step - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00061\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 117us/step - loss: 1.0128 - mean_squared_error: 1.0128 - val_loss: 1.0089 - val_mean_squared_error: 1.0089\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00061\n",
      "Epoch 00005: early stopping\n",
      "temp/e46\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 125us/step - loss: 1.0888 - mean_squared_error: 1.0888 - val_loss: 1.0198 - val_mean_squared_error: 1.0198\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01984, saving model to temp/e46\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 114us/step - loss: 1.0210 - mean_squared_error: 1.0210 - val_loss: 1.0081 - val_mean_squared_error: 1.0081\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01984 to 1.00805, saving model to temp/e46\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 121us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0142 - val_mean_squared_error: 1.0142\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00805\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 121us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0119 - val_mean_squared_error: 1.0119\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00805\n",
      "Epoch 00004: early stopping\n",
      "temp/e47\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0948 - mean_squared_error: 1.0948 - val_loss: 1.0280 - val_mean_squared_error: 1.0280\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02801, saving model to temp/e47\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 125us/step - loss: 1.0207 - mean_squared_error: 1.0207 - val_loss: 1.0105 - val_mean_squared_error: 1.0105\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02801 to 1.01047, saving model to temp/e47\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 124us/step - loss: 1.0164 - mean_squared_error: 1.0164 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01047 to 1.00544, saving model to temp/e47\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 124us/step - loss: 1.0155 - mean_squared_error: 1.0155 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00544 to 1.00466, saving model to temp/e47\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 123us/step - loss: 1.0125 - mean_squared_error: 1.0125 - val_loss: 1.0046 - val_mean_squared_error: 1.0046\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00466 to 1.00463, saving model to temp/e47\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 123us/step - loss: 1.0054 - mean_squared_error: 1.0054 - val_loss: 1.0058 - val_mean_squared_error: 1.0058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00463\n",
      "Epoch 00006: early stopping\n",
      "temp/e48\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.1056 - mean_squared_error: 1.1056 - val_loss: 1.0234 - val_mean_squared_error: 1.0234\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02336, saving model to temp/e48\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 123us/step - loss: 1.0213 - mean_squared_error: 1.0213 - val_loss: 1.0185 - val_mean_squared_error: 1.0185\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02336 to 1.01847, saving model to temp/e48\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 124us/step - loss: 1.0191 - mean_squared_error: 1.0191 - val_loss: 1.0116 - val_mean_squared_error: 1.0116\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01847 to 1.01164, saving model to temp/e48\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 121us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01164 to 1.00382, saving model to temp/e48\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 124us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00382\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 123us/step - loss: 1.0042 - mean_squared_error: 1.0042 - val_loss: 1.0128 - val_mean_squared_error: 1.0128\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00382\n",
      "Epoch 00006: early stopping\n",
      "temp/e49\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.1015 - mean_squared_error: 1.1015 - val_loss: 1.0183 - val_mean_squared_error: 1.0183\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01833, saving model to temp/e49\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 121us/step - loss: 1.0242 - mean_squared_error: 1.0242 - val_loss: 1.0165 - val_mean_squared_error: 1.0165\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01833 to 1.01653, saving model to temp/e49\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 122us/step - loss: 1.0144 - mean_squared_error: 1.0144 - val_loss: 1.0131 - val_mean_squared_error: 1.0131\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01653 to 1.01309, saving model to temp/e49\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 122us/step - loss: 1.0148 - mean_squared_error: 1.0148 - val_loss: 1.0046 - val_mean_squared_error: 1.0046\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01309 to 1.00460, saving model to temp/e49\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 123us/step - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 1.0017 - val_mean_squared_error: 1.0017\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00460 to 1.00171, saving model to temp/e49\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 123us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0097 - val_mean_squared_error: 1.0097\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00171\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 123us/step - loss: 1.0054 - mean_squared_error: 1.0054 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00171\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "for idx, model_name in enumerate(model_names):\n",
    "    print(model_name)\n",
    "\n",
    "    if type(models[idx]) is list:\n",
    "        #clear session\n",
    "        keras.backend.clear_session() \n",
    "        #get model according to specification\n",
    "        model = get_model(models[idx], [0.2] * len(models), 6)\n",
    "        callbacks = [ModelCheckpoint(model_name, verbose= verbosity, monitor='val_loss',save_best_only=True), \n",
    "                     EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose= verbosity, mode='auto')]\n",
    "        model.compile(optimizer = optimizers.SGD(lr = 0.001, momentum = 0.9, ), loss='mean_squared_error', metrics = ['mse'])\n",
    "        #print(len(X), len(y))\n",
    "        model.fit(X, y, epochs = 20, validation_data = (x_val, y_val), callbacks = callbacks, batch_size = 32, verbose = verbosity)\n",
    "    else:\n",
    "        models[idx].fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "0 4\n",
      "1 1\n",
      "1 2\n",
      "1 3\n",
      "1 4\n",
      "2 1\n",
      "2 2\n",
      "2 3\n",
      "2 4\n",
      "3 1\n",
      "3 2\n",
      "3 3\n",
      "3 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n            setA = get_MB(get_CG(perturbed_df, tetrad), \\'g\\', pc)\\n            if setA != {\\'f\\'}:\\n                print(\"Error in SETA markov blanket\")\\n                #setA = {\\'f\\'}\\n            setC = get_MB(get_CG(test_df2, tetrad), \\'g\\', pc)\\n\\n            if setA != setC:\\n                causal_dicts[idx][str(m) + \\'_\\' + str(v)].append(1)\\n            else:\\n                causal_dicts[idx][str(m) + \\'_\\' + str(v)].append(0)\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_test = 2000\n",
    "metrics_dicts = []\n",
    "for m in models:\n",
    "    metrics_dicts.append(defaultdict(list))\n",
    "\n",
    "\n",
    "#means = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "#variances = [1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0]\n",
    "means = [0, 1, 2,3]\n",
    "variances = [1,2,3,4]\n",
    "\n",
    "\n",
    "# ok at this point we need to check the model on various variances and means\n",
    "for m in means:\n",
    "    for v in variances:\n",
    "        print(m,v)\n",
    "        #t0 = time.time()\n",
    "        perturbed_df = gen_data(mean =m, var = v, SIZE = nb_test)\n",
    "        y_test2 = perturbed_df['g']\n",
    "        x_test2 = perturbed_df[['a', 'b', 'c', 'd', 'e', 'f']]\n",
    "        #t1 = time.time()\n",
    "        #print(\"Time for gen_data = \", t1 - t0)\n",
    "        for idx, model_name in enumerate(model_names):\n",
    "            #t0 = time.time()\n",
    "            if type(models[idx]) is list:\n",
    "                keras.backend.clear_session()\n",
    "                model = load_model(model_name)\n",
    "            else:\n",
    "                model = models[idx]\n",
    "            #t1 = time.time()\n",
    "            #print(\"Time to load model = \", t1 - t0)\n",
    "            \n",
    "            y_pred2 = model.predict(x_test2)\n",
    "            metrics_dicts[idx][str(m) + '_' + str(v)].append(mean_squared_error(y_test2, y_pred2))\n",
    "\n",
    "            test_df2 = pd.DataFrame(x_test2, columns = ['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "            test_targets2 = pd.DataFrame(model.predict(x_test2), columns = ['g'])\n",
    "            test_df2 = test_df2.join(test_targets2)\n",
    "'''\n",
    "            setA = get_MB(get_CG(perturbed_df, tetrad), 'g', pc)\n",
    "            if setA != {'f'}:\n",
    "                print(\"Error in SETA markov blanket\")\n",
    "                #setA = {'f'}\n",
    "            setC = get_MB(get_CG(test_df2, tetrad), 'g', pc)\n",
    "\n",
    "            if setA != setC:\n",
    "                causal_dicts[idx][str(m) + '_' + str(v)].append(1)\n",
    "            else:\n",
    "                causal_dicts[idx][str(m) + '_' + str(v)].append(0)\n",
    "\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times =  0\n",
      "-2.927721589387889 -2.2064530029963763\n",
      "-2.927721589387889 -2.167982918626032\n",
      "-2.927721589387889 -2.208854914716253\n",
      "-2.927721589387889 -2.1638865362977664\n",
      "-2.927721589387889 -2.221568998984088\n",
      "-2.927721589387889 -2.115036990055373\n",
      "-2.927721589387889 -2.1853203180834173\n",
      "-2.927721589387889 -2.13178717472847\n",
      "-2.927721589387889 -2.1490565651162123\n",
      "-2.927721589387889 -2.175294485795704\n",
      "-2.927721589387889 -2.2189115806717603\n",
      "-2.927721589387889 -2.1920015517576874\n",
      "-2.927721589387889 -2.2131588928664367\n",
      "-2.927721589387889 -2.1856149351073553\n",
      "-2.927721589387889 -2.1106706520092144\n",
      "-2.927721589387889 -2.1508753679287356\n",
      "-2.927721589387889 -2.2638710193741987\n",
      "-2.927721589387889 -2.231652372978537\n",
      "-2.927721589387889 -2.23211024330941\n",
      "-2.927721589387889 -2.19644035955901\n",
      "-2.927721589387889 -2.2381773252993264\n",
      "-2.927721589387889 -2.2353884938624247\n",
      "-2.927721589387889 -2.2296677135615064\n",
      "-2.927721589387889 -2.215181382195895\n",
      "-2.927721589387889 -2.1947740447952158\n",
      "-2.927721589387889 -2.1595035183322424\n",
      "-2.927721589387889 -2.133423127873282\n",
      "-2.927721589387889 -2.13648316632566\n",
      "-2.927721589387889 -2.2058695048463237\n",
      "-2.927721589387889 -2.2243721294222505\n",
      "-2.927721589387889 -2.087185071833506\n",
      "-2.927721589387889 -2.262213458648134\n",
      "-2.927721589387889 -2.22000913494603\n",
      "-2.927721589387889 -2.047643070774836\n",
      "-2.927721589387889 -2.0847099603506005\n",
      "-2.927721589387889 -2.0929308322686664\n",
      "-2.927721589387889 -2.174212597800462\n",
      "-2.927721589387889 -2.2246536072284595\n",
      "-2.927721589387889 -2.2309385744204238\n",
      "-2.927721589387889 -2.1996876316622203\n",
      "-2.927721589387889 -2.1394228049251\n",
      "-2.927721589387889 -2.177011020366613\n",
      "-2.927721589387889 -2.136526996554509\n",
      "-2.927721589387889 -2.1516995346208265\n",
      "-2.927721589387889 -2.154819331050939\n",
      "-2.927721589387889 -2.2040412154060687\n",
      "-2.927721589387889 -2.1597525668067954\n",
      "-2.927721589387889 -2.0349473936554974\n",
      "-2.927721589387889 -2.1756967827781484\n",
      "-2.927721589387889 -2.26511284790336\n",
      "Times =  1\n",
      "-2.8278865445824866 -2.19874776541997\n",
      "-2.8278865445824866 -2.1260292694664695\n",
      "-2.8278865445824866 -2.161509364651761\n",
      "-2.8278865445824866 -2.1348939275716075\n",
      "-2.8278865445824866 -2.1898383123901093\n",
      "-2.8278865445824866 -2.1346327375984733\n",
      "-2.8278865445824866 -2.1382768986143788\n",
      "-2.8278865445824866 -2.142038698215634\n",
      "-2.8278865445824866 -2.1184711525668103\n",
      "-2.8278865445824866 -2.1792759589098973\n",
      "-2.8278865445824866 -2.18657524367683\n",
      "-2.8278865445824866 -2.180941311158733\n",
      "-2.8278865445824866 -2.1950196796096773\n",
      "-2.8278865445824866 -2.1248192252751488\n",
      "-2.8278865445824866 -2.07437793669359\n",
      "-2.8278865445824866 -2.111524294329193\n",
      "-2.8278865445824866 -2.2270580202304804\n",
      "-2.8278865445824866 -2.175739107247032\n",
      "-2.8278865445824866 -2.228902630276028\n",
      "-2.8278865445824866 -2.170444602307342\n",
      "-2.8278865445824866 -2.1976839462008524\n",
      "-2.8278865445824866 -2.210527458339776\n",
      "-2.8278865445824866 -2.2057909691803084\n",
      "-2.8278865445824866 -2.1762394364402944\n",
      "-2.8278865445824866 -2.1572051508060746\n",
      "-2.8278865445824866 -2.1435885895437408\n",
      "-2.8278865445824866 -2.0973750247243363\n",
      "-2.8278865445824866 -2.1048772277704764\n",
      "-2.8278865445824866 -2.1750112611677808\n",
      "-2.8278865445824866 -2.183837655117614\n",
      "-2.8278865445824866 -2.0872684276575617\n",
      "-2.8278865445824866 -2.236009904101256\n",
      "-2.8278865445824866 -2.1254004373569866\n",
      "-2.8278865445824866 -2.066034303293327\n",
      "-2.8278865445824866 -2.1111598775307345\n",
      "-2.8278865445824866 -2.0905795846134896\n",
      "-2.8278865445824866 -2.184247410100481\n",
      "-2.8278865445824866 -2.2000640340669673\n",
      "-2.8278865445824866 -2.1832196700969506\n",
      "-2.8278865445824866 -2.2005507742536157\n",
      "-2.8278865445824866 -2.1279888019629465\n",
      "-2.8278865445824866 -2.1460598505851762\n",
      "-2.8278865445824866 -2.124776942609963\n",
      "-2.8278865445824866 -2.167022801798285\n",
      "-2.8278865445824866 -2.135845921455875\n",
      "-2.8278865445824866 -2.1725576776195537\n",
      "-2.8278865445824866 -2.111786006758299\n",
      "-2.8278865445824866 -2.018964132292686\n",
      "-2.8278865445824866 -2.1778363571822026\n",
      "-2.8278865445824866 -2.2089665376476457\n",
      "Times =  2\n",
      "-3.0147056979298603 -2.306463300246009\n",
      "-3.0147056979298603 -2.243966888172822\n",
      "-3.0147056979298603 -2.2553499172105647\n",
      "-3.0147056979298603 -2.2279437116731193\n",
      "-3.0147056979298603 -2.304022954505036\n",
      "-3.0147056979298603 -2.2136682106267376\n",
      "-3.0147056979298603 -2.265441943372469\n",
      "-3.0147056979298603 -2.234227128453249\n",
      "-3.0147056979298603 -2.2487072802634103\n",
      "-3.0147056979298603 -2.2734876460303894\n",
      "-3.0147056979298603 -2.286328771740574\n",
      "-3.0147056979298603 -2.2793051142617644\n",
      "-3.0147056979298603 -2.2998800843359333\n",
      "-3.0147056979298603 -2.238565111318802\n",
      "-3.0147056979298603 -2.190951115122204\n",
      "-3.0147056979298603 -2.211272691922001\n",
      "-3.0147056979298603 -2.3009546743230556\n",
      "-3.0147056979298603 -2.284824712957008\n",
      "-3.0147056979298603 -2.322657443383867\n",
      "-3.0147056979298603 -2.273638677265781\n",
      "-3.0147056979298603 -2.3060953029233833\n",
      "-3.0147056979298603 -2.312769500304217\n",
      "-3.0147056979298603 -2.3037963385400158\n",
      "-3.0147056979298603 -2.276496548897593\n",
      "-3.0147056979298603 -2.2625973093691183\n",
      "-3.0147056979298603 -2.2120207581571796\n",
      "-3.0147056979298603 -2.2152078709035607\n",
      "-3.0147056979298603 -2.216635780718171\n",
      "-3.0147056979298603 -2.286875637421167\n",
      "-3.0147056979298603 -2.2662770888215795\n",
      "-3.0147056979298603 -2.219897718913373\n",
      "-3.0147056979298603 -2.30547694366503\n",
      "-3.0147056979298603 -2.3268604686104464\n",
      "-3.0147056979298603 -2.1713060598041203\n",
      "-3.0147056979298603 -2.19530893285982\n",
      "-3.0147056979298603 -2.170221422251492\n",
      "-3.0147056979298603 -2.288007293485396\n",
      "-3.0147056979298603 -2.3073759660144493\n",
      "-3.0147056979298603 -2.3074169011950887\n",
      "-3.0147056979298603 -2.2570344397500564\n",
      "-3.0147056979298603 -2.234340495065237\n",
      "-3.0147056979298603 -2.265894142013136\n",
      "-3.0147056979298603 -2.223848593861397\n",
      "-3.0147056979298603 -2.2391266601916926\n",
      "-3.0147056979298603 -2.230777863111324\n",
      "-3.0147056979298603 -2.3027931760049136\n",
      "-3.0147056979298603 -2.242895122523281\n",
      "-3.0147056979298603 -2.1090309981575115\n",
      "-3.0147056979298603 -2.2488403275147713\n",
      "-3.0147056979298603 -2.3374343577160674\n"
     ]
    }
   ],
   "source": [
    "#the number of times to sample \n",
    "times =3\n",
    "\n",
    "## the size of the test set\n",
    "\n",
    "\n",
    "violations = np.zeros(len(models))\n",
    "violation_mean = np.zeros((len(models), times))\n",
    "violation_mean2 = np.zeros((len(models), times))\n",
    "mean = np.zeros((len(models), times))\n",
    "\n",
    "fold = 0\n",
    "\n",
    "#metrics_dicts = []\n",
    "causal_dicts = []\n",
    "for m in models:\n",
    "#    metrics_dicts.append(defaultdict(list))\n",
    "    causal_dicts.append(defaultdict(list))\n",
    "from pycausal import prior as p\n",
    "def get_bic(df):\n",
    "    prior = p.knowledge(requiredirect =  [['a', 'b'], ['a', 'c'], ['a', 'd'], ['a', 'e'],['a', 'f'],['f', 'g']])\n",
    "    tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "               structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, priorKnowledge = prior,\n",
    "               completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "    BIC = tetrad.getTetradGraph().getAllAttributes().toString()\n",
    "    BIC = float(BIC.split('=')[-1].split('}')[0])\n",
    "    return BIC / len(df)\n",
    "\n",
    "for t in range(times):\n",
    "    print(\"Times = \", t)\n",
    "    df_test = gen_data(SIZE = nb_test)\n",
    "    x_test = df_test[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "    y_test = df_test['g'].values\n",
    "    bic_orig = get_bic(df_test)\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        if type(models[idx]) is list:\n",
    "            keras.backend.clear_session()\n",
    "            model = load_model(model_name)\n",
    "        else:\n",
    "            model = models[idx]\n",
    "        test_df = pd.DataFrame(x_test, columns = ['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "        test_targets = pd.DataFrame(model.predict(x_test), columns = ['g'])\n",
    "        test_df = test_df.join(test_targets)\n",
    "        mean[idx][t] = mean_squared_error(y_test, model.predict(x_test))  \n",
    "        bic_pred = get_bic(test_df)\n",
    "        print(bic_orig, bic_pred)\n",
    "        violation_mean[idx][t] = bic_pred\n",
    "        violation_mean2[idx][t] = bic_pred \n",
    "        #print(bic_orig - bic_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_name =  temp/e0 Violations =  0.0\n",
      "Average_violations =  -2.2372213562207848 0.04906239415833261\n",
      "MSE =  1.0183695859015682 0.037234124013105725\n",
      "Model_name =  temp/e1 Violations =  0.0\n",
      "Average_violations =  -2.179326358755108 0.04881137641986537\n",
      "MSE =  1.0225596945520907 0.04494217679574547\n",
      "Model_name =  temp/e2 Violations =  0.0\n",
      "Average_violations =  -2.208571398859526 0.038310769696141264\n",
      "MSE =  1.0207502821974928 0.03914822571214926\n",
      "Model_name =  temp/e3 Violations =  0.0\n",
      "Average_violations =  -2.175574725180831 0.038876092909474504\n",
      "MSE =  1.0297381697492531 0.03972049922888406\n",
      "Model_name =  temp/e4 Violations =  0.0\n",
      "Average_violations =  -2.2384767552930778 0.04812440331253726\n",
      "MSE =  1.0276469498592895 0.03901217510922958\n",
      "Model_name =  temp/e5 Violations =  0.0\n",
      "Average_violations =  -2.1544459794268613 0.04263373337051249\n",
      "MSE =  1.0165690660513158 0.03477719015000928\n",
      "Model_name =  temp/e6 Violations =  0.0\n",
      "Average_violations =  -2.1963463866900885 0.05249709702304843\n",
      "MSE =  1.020355097925651 0.037432070551334\n",
      "Model_name =  temp/e7 Violations =  0.0\n",
      "Average_violations =  -2.1693510004657846 0.04606486311305248\n",
      "MSE =  1.02841957948096 0.03503246391189116\n",
      "Model_name =  temp/e8 Violations =  0.0\n",
      "Average_violations =  -2.172078332648811 0.05560493727835706\n",
      "MSE =  1.02406164500183 0.037177648123125406\n",
      "Model_name =  temp/e9 Violations =  0.0\n",
      "Average_violations =  -2.209352696911997 0.04537937715052109\n",
      "MSE =  1.0198641002663889 0.034694010343359165\n",
      "Model_name =  temp/e10 Violations =  0.0\n",
      "Average_violations =  -2.2306051986963884 0.04155516106871641\n",
      "MSE =  1.015833541365144 0.04067415637729373\n",
      "Model_name =  temp/e11 Violations =  0.0\n",
      "Average_violations =  -2.2174159923927284 0.0439945434860355\n",
      "MSE =  1.0228881280790285 0.040080408272970755\n",
      "Model_name =  temp/e12 Violations =  0.0\n",
      "Average_violations =  -2.2360195522706827 0.04575939561908752\n",
      "MSE =  1.022176494137974 0.04621609723977834\n",
      "Model_name =  temp/e13 Violations =  0.0\n",
      "Average_violations =  -2.182999757233769 0.04647336882162955\n",
      "MSE =  1.0252807120169822 0.03897157698583975\n",
      "Model_name =  temp/e14 Violations =  0.0\n",
      "Average_violations =  -2.125333234608336 0.04870708352724076\n",
      "MSE =  1.0191538505113653 0.03593567238574774\n",
      "Model_name =  temp/e15 Violations =  0.0\n",
      "Average_violations =  -2.1578907847266433 0.0410231459887342\n",
      "MSE =  1.019685151884861 0.04977451996569239\n",
      "Model_name =  temp/e16 Violations =  0.0\n",
      "Average_violations =  -2.2639612379759115 0.030168250154386903\n",
      "MSE =  1.0096089679610316 0.037693412410235334\n",
      "Model_name =  temp/e17 Violations =  0.0\n",
      "Average_violations =  -2.2307387310608586 0.04453869777652842\n",
      "MSE =  1.0210902034415297 0.03343390335614851\n",
      "Model_name =  temp/e18 Violations =  0.0\n",
      "Average_violations =  -2.2612234389897683 0.04346013397174916\n",
      "MSE =  1.0215017752491613 0.04170222321408601\n",
      "Model_name =  temp/e19 Violations =  0.0\n",
      "Average_violations =  -2.2135078797107113 0.043823353437244764\n",
      "MSE =  1.0108701832679738 0.04179314406712972\n",
      "Model_name =  temp/e20 Violations =  0.0\n",
      "Average_violations =  -2.2473188581411874 0.044728300389716556\n",
      "MSE =  1.012057597882592 0.03979855948084195\n",
      "Model_name =  temp/e21 Violations =  0.0\n",
      "Average_violations =  -2.2528951508354726 0.04353711876122574\n",
      "MSE =  1.0141548542717436 0.04311310747937088\n",
      "Model_name =  temp/e22 Violations =  0.0\n",
      "Average_violations =  -2.246418340427277 0.0417268956520082\n",
      "MSE =  1.017071866992989 0.04829155143025722\n",
      "Model_name =  temp/e23 Violations =  0.0\n",
      "Average_violations =  -2.222639122511261 0.04126811172101633\n",
      "MSE =  1.0268484618126574 0.036679417251560605\n",
      "Model_name =  temp/e24 Violations =  0.0\n",
      "Average_violations =  -2.204858834990136 0.043613102133809215\n",
      "MSE =  1.018698571639427 0.0359240265490116\n",
      "Model_name =  temp/e25 Violations =  0.0\n",
      "Average_violations =  -2.171704288677721 0.02923906659536018\n",
      "MSE =  1.023702034651491 0.04186754382071915\n",
      "Model_name =  temp/e26 Violations =  0.0\n",
      "Average_violations =  -2.1486686745003927 0.04929817392590896\n",
      "MSE =  1.0211334358622766 0.03178841362865422\n",
      "Model_name =  temp/e27 Violations =  0.0\n",
      "Average_violations =  -2.1526653916047693 0.0470382245305422\n",
      "MSE =  1.0190655938619653 0.033487270025742616\n",
      "Model_name =  temp/e28 Violations =  0.0\n",
      "Average_violations =  -2.2225854678117574 0.047173277972711834\n",
      "MSE =  1.0222138613268044 0.037225781997656665\n",
      "Model_name =  temp/e29 Violations =  0.0\n",
      "Average_violations =  -2.224828957787148 0.03365730803727342\n",
      "MSE =  1.0222475309742844 0.03876605545100318\n",
      "Model_name =  temp/e30 Violations =  0.0\n",
      "Average_violations =  -2.1314504061348134 0.06254170390152784\n",
      "MSE =  1.0158669381686682 0.041558217304696145\n",
      "Model_name =  temp/e31 Violations =  0.0\n",
      "Average_violations =  -2.26790010213814 0.028643449886207907\n",
      "MSE =  1.0209519048536382 0.028549980317707915\n",
      "Model_name =  temp/e32 Violations =  0.0\n",
      "Average_violations =  -2.224090013637821 0.08229631918190139\n",
      "MSE =  1.030324250671738 0.04645892577667389\n",
      "Model_name =  temp/e33 Violations =  0.0\n",
      "Average_violations =  -2.0949944779574277 0.05448028696293509\n",
      "MSE =  1.0213257259648967 0.036424444882279784\n",
      "Model_name =  temp/e34 Violations =  0.0\n",
      "Average_violations =  -2.1303929235803847 0.04715552795650259\n",
      "MSE =  1.0192642995866497 0.043753290640514\n",
      "Model_name =  temp/e35 Violations =  0.0\n",
      "Average_violations =  -2.117910613044549 0.03700178068956386\n",
      "MSE =  1.0232492539826257 0.04221009522755089\n",
      "Model_name =  temp/e36 Violations =  0.0\n",
      "Average_violations =  -2.215489100462113 0.051441491709810445\n",
      "MSE =  1.0261976229644894 0.02701753245194786\n",
      "Model_name =  temp/e37 Violations =  0.0\n",
      "Average_violations =  -2.2440312024366253 0.04590265846777578\n",
      "MSE =  1.0156631659080266 0.04354488609235364\n",
      "Model_name =  temp/e38 Violations =  0.0\n",
      "Average_violations =  -2.2405250485708206 0.05115442905613195\n",
      "MSE =  1.0171062244375655 0.03972729335279959\n",
      "Model_name =  temp/e39 Violations =  0.0\n",
      "Average_violations =  -2.2190909485552974 0.026832413816356905\n",
      "MSE =  1.0222912216272766 0.03969632460526364\n",
      "Model_name =  temp/e40 Violations =  0.0\n",
      "Average_violations =  -2.167250700651094 0.047668749314643925\n",
      "MSE =  1.018855625721344 0.03212411810109499\n",
      "Model_name =  temp/e41 Violations =  0.0\n",
      "Average_violations =  -2.196321670988308 0.05079199589886318\n",
      "MSE =  1.0180152540517973 0.03225951486619544\n",
      "Model_name =  temp/e42 Violations =  0.0\n",
      "Average_violations =  -2.1617175110086233 0.044194415441083656\n",
      "MSE =  1.022334884281449 0.04048800108821801\n",
      "Model_name =  temp/e43 Violations =  0.0\n",
      "Average_violations =  -2.185949665536935 0.03811863236119626\n",
      "MSE =  1.0297131880124812 0.04162045804854898\n",
      "Model_name =  temp/e44 Violations =  0.0\n",
      "Average_violations =  -2.1738143718727128 0.04101728959421894\n",
      "MSE =  1.0220097531218382 0.03987793590491637\n",
      "Model_name =  temp/e45 Violations =  0.0\n",
      "Average_violations =  -2.2264640230101786 0.055482177231725945\n",
      "MSE =  1.030390640235223 0.04209180119989967\n",
      "Model_name =  temp/e46 Violations =  0.0\n",
      "Average_violations =  -2.1714778986961254 0.0541634108669008\n",
      "MSE =  1.0304456986301591 0.0414899376187429\n",
      "Model_name =  temp/e47 Violations =  0.0\n",
      "Average_violations =  -2.0543141747018985 0.03923700836160115\n",
      "MSE =  1.0251976328761592 0.04318678811080519\n",
      "Model_name =  temp/e48 Violations =  0.0\n",
      "Average_violations =  -2.200791155825041 0.033987121283505034\n",
      "MSE =  1.020537873514739 0.043238394191224654\n",
      "Model_name =  temp/e49 Violations =  0.0\n",
      "Average_violations =  -2.2705045810890243 0.05258515816173571\n",
      "MSE =  1.0193874919291583 0.04217742543193586\n",
      "[1.01836959 1.02255969 1.02075028 1.02973817 1.02764695 1.01656907\n",
      " 1.0203551  1.02841958 1.02406165 1.0198641  1.01583354 1.02288813\n",
      " 1.02217649 1.02528071 1.01915385 1.01968515 1.00960897 1.0210902\n",
      " 1.02150178 1.01087018 1.0120576  1.01415485 1.01707187 1.02684846\n",
      " 1.01869857 1.02370203 1.02113344 1.01906559 1.02221386 1.02224753\n",
      " 1.01586694 1.0209519  1.03032425 1.02132573 1.0192643  1.02324925\n",
      " 1.02619762 1.01566317 1.01710622 1.02229122 1.01885563 1.01801525\n",
      " 1.02233488 1.02971319 1.02200975 1.03039064 1.0304457  1.02519763\n",
      " 1.02053787 1.01938749] [0.03723412 0.04494218 0.03914823 0.0397205  0.03901218 0.03477719\n",
      " 0.03743207 0.03503246 0.03717765 0.03469401 0.04067416 0.04008041\n",
      " 0.0462161  0.03897158 0.03593567 0.04977452 0.03769341 0.0334339\n",
      " 0.04170222 0.04179314 0.03979856 0.04311311 0.04829155 0.03667942\n",
      " 0.03592403 0.04186754 0.03178841 0.03348727 0.03722578 0.03876606\n",
      " 0.04155822 0.02854998 0.04645893 0.03642444 0.04375329 0.0422101\n",
      " 0.02701753 0.04354489 0.03972729 0.03969632 0.03212412 0.03225951\n",
      " 0.040488   0.04162046 0.03987794 0.0420918  0.04148994 0.04318679\n",
      " 0.04323839 0.04217743] [-2.23722136 -2.17932636 -2.2085714  -2.17557473 -2.23847676 -2.15444598\n",
      " -2.19634639 -2.169351   -2.17207833 -2.2093527  -2.2306052  -2.21741599\n",
      " -2.23601955 -2.18299976 -2.12533323 -2.15789078 -2.26396124 -2.23073873\n",
      " -2.26122344 -2.21350788 -2.24731886 -2.25289515 -2.24641834 -2.22263912\n",
      " -2.20485883 -2.17170429 -2.14866867 -2.15266539 -2.22258547 -2.22482896\n",
      " -2.13145041 -2.2679001  -2.22409001 -2.09499448 -2.13039292 -2.11791061\n",
      " -2.2154891  -2.2440312  -2.24052505 -2.21909095 -2.1672507  -2.19632167\n",
      " -2.16171751 -2.18594967 -2.17381437 -2.22646402 -2.1714779  -2.05431417\n",
      " -2.20079116 -2.27050458] [0.04906239 0.04881138 0.03831077 0.03887609 0.0481244  0.04263373\n",
      " 0.0524971  0.04606486 0.05560494 0.04537938 0.04155516 0.04399454\n",
      " 0.0457594  0.04647337 0.04870708 0.04102315 0.03016825 0.0445387\n",
      " 0.04346013 0.04382335 0.0447283  0.04353712 0.0417269  0.04126811\n",
      " 0.0436131  0.02923907 0.04929817 0.04703822 0.04717328 0.03365731\n",
      " 0.0625417  0.02864345 0.08229632 0.05448029 0.04715553 0.03700178\n",
      " 0.05144149 0.04590266 0.05115443 0.02683241 0.04766875 0.050792\n",
      " 0.04419442 0.03811863 0.04101729 0.05548218 0.05416341 0.03923701\n",
      " 0.03398712 0.05258516]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABEcAAAKYCAYAAACRnpZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3Xu4XWV9L/rvjwBGKIJcVBApcYuVawwstFar5YAWrRsEvIB4FD3CtvVSe/bps2mxovRxP27r7gZaL4UjUrWHeOkhxRq1Al5qvQaxyEU2aPGQRmmEKkQgEvKePzLJDiGXlZU115xrvZ/P86wnc4z5rt/8jTHnO9da34w5RrXWAgAAANCrHUbdAAAAAMAoCUcAAACArglHAAAAgK4JRwAAAICuCUcAAACArglHAAAAgK4JRwAAAICuCUcAAACArglHAAAAgK7tOOoGhmHvvfduBx544KjbAAAAAEbommuu+WlrbZ+tjZuT4ciBBx6YZcuWjboNAAAAYISq6keTGedjNQAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAkKRq3RfApniPmNuEI9thNkyO6e5xNmwzfejxtdjjNgMAfejx95xhbHOP+3G6CEfGiBfyePKmNT3GfZvHvb9eeV6mR49B+Wzocbr1+Dz3yPM8nnrcj7Nhm2dDj+Oup/ecrsKRcX4imD49Ps89vWnNJj3uxx63ebr1uA9nwzbPhh5ngx73o22e+/WAuaGrcITx5AcUjEavc88v2YwDr5vpYT8yVzlyGWaecAQAAADomnAEAAAA6NqOo26A6ffOd75zg6VzH7bu3HPP3c6a5260PPWa467HbQaAHvmZD6Mz7vNvS39bJVPrcdy3uVfCkW1kcjBVwwitxt10b7P5N54Esn0Yxvybbj2+Fm3zeP48nV09TvfP00fWHJdtnm62eTy3eTb0ON1mw+/cs8FIw5GquiTJi5P8W2vtsE3cX0kuSPKiJPcmOaO19p1teYweJ8dsMJMTeK4+z4KC8dTjHypMjx7/UOnxtd33Nifj+locdz3+odLr7zk9/iygD7Nh/o36yJFLk/xlko9s5v4XJjlo8PXMJB8Y/AvAFM2GH07TTSALAMCWjPSErK21ryS5awtDTkzykbbON5LsUVX7zkx3AAAAQA9GfeTI1jwxye0bLC8frPvxxgOr6qwkZyXJAQccMCPNAQAAD/eOd7xz64MAxsy4hyO1iXVtUwNbaxcluShJJiYmNjmmR9P9w6nHH3Y9bjPAlvT4vtjjNs8GnhdgJvnbam4b93BkeZInbbC8f5IVI+qFWWI2vMn0+MY67j0Oo79x3+Zh6HGb2X49zr/ZsM3jvg+HYTY8L4yn2TD/xr3H2bDNs0GP2zxdxj0cuSLJm6pqcdadiPXnrbVHfKRmssZ9Avf6Qu51u9k+Xjd98DwDPJzfP7dfj9sMU9XTe86oL+V7WZLfSrJ3VS3PulP+75QkrbUPJlmadZfxvTXrLuX72tF0Cn0Z5zetnvX0w2mYxn0/9vq80Aevb6bC6waYCSMNR1prp23l/pbkjTPUDgDMSf6wAADYspFeyhcAAABg1IQjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNeEIwAAAEDXhCMAAABA14QjAAAAQNdGGo5U1fFVdXNV3VpVZ2/i/jOqamVVfXfw9fpR9AkAAADMXTuO6oGral6S9yV5fpLlSb5dVVe01m7caOjHW2tvmvEGAQAAgC6M8siRZyS5tbX2w9baL5MsTnLiCPsBAAAAOjTKcOSJSW7fYHn5YN3GTqmq66rqU1X1pM0Vq6qzqmpZVS1buXLldPcKAAAAzFGjDEdqE+vaRsufTnJga+2IJFcm+evNFWutXdRam2itTeyzzz7T2CYAAAAwl40yHFmeZMMjQfZPsmLDAa21O1trqweLFyc5aoZ6AwAAADoxynDk20kOqqoFVbVzklOTXLHhgKrad4PFE5LcNIP9AQAAAB0Y2dVqWmtrqupNST6fZF6SS1prN1TVeUmWtdauSPKWqjohyZokdyU5Y1T9AgAAAHPTyMKRJGmtLU2ydKN1b9/g9h8l+aOZ7gsAAADoxyg/VgMAAAAwcsIRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrIw1Hqur4qrq5qm6tqrM3cf+jqurjg/u/WVUHznyXAAAAwFw2snCkquYleV+SFyY5JMlpVXXIRsP+jyT/3lp7SpL/keS/zWyXAAAAwFw3yiNHnpHk1tbaD1trv0yyOMmJG405MclfD25/KsmxVVUz2CMAAAAwx1VrbTQPXPXSJMe31l4/WP7fkzyztfamDcZcPxizfLD8g8GYn26i3llJzkqSAw444Kgf/ehHk+hh8/dNdbdMd81xrzeMmjNZbxg1x6HeMGqOe71h1Bz3esOoaf4Nt94wao5DvWHUHPd6w6g57vWGUdP82/56w6g57vWGUXPc6w2jpvm3/fWGUXPc6w2j5jB6fHj9uqa1NrG1caM8cmRTu2DjTZ/MmHUrW7uotTbRWpvYZ599trs5AAAAoA+jDEeWJ3nSBsv7J1mxuTFVtWOS3ZPcNSPdAQAAAF0YZTjy7SQHVdWCqto5yalJrthozBVJXjO4/dIkV7dRfQ4IAAAAmJN2HNUDt9bWVNWbknw+ybwkl7TWbqiq85Isa61dkeRDST5aVbdm3REjp46qXwAAAGBuGlk4kiSttaVJlm607u0b3L4/yctmui8AAACgH6P8WA0AAADAyAlHAAAAgK4JRwAAAICuCUcAAACArglHAAAAgK4JRwAAAICuCUcAAACArglHAAAAgK4JRwAAAICuCUcAAACArglHAAAAgK4JRwAAAICuCUcAAACArglHAAAAgK4JRwAAAICuCUcAAACArglHAAAAgK4JRwAAAICuCUcAAACArglHAAAAgK4JRwAAAICuCUcAAACArglHAAAAgK4JRwAAAICuCUcAAACArglHAAAAgK4JRwAAAICuCUcAAACArglHAAAAgK4JRwAAAICuCUcAAACArglHAAAAgK4JRwAAAICuCUcAAACArglHAAAAgK4JRwAAAICuCUcAAACArglHAAAAgK4JRwAAAICuCUcAAACArglHAAAAgK4JRwAAAICuCUcAAACArglHAAAAgK4JRwAAAICuCUcAAACArglHAAAAgK4JRwAAAICuCUcAAACArglHAAAAgK4JRwAAAICuCUcAAACArglHAAAAgK4JRwAAAICuCUcAAACArglHAAAAgK4JRwAAAICuCUcAAACArglHAAAAgK4JRwAAAICuTTocqapHV9WvDbMZAAAAgJk2qXCkqv5jku8m+dxg+elVdcUwGwMAAACYCZM9cuQdSZ6R5GdJ0lr7bpIDh9MSAAAAwMyZbDiyprX286F2AgAAADACO05y3PVV9cok86rqoCRvSfK14bUFAAAAMDMme+TIm5McmmR1ksuS3J3krcNqCgAAAGCmTOrIkdbavUnOGXwBAAAAzBmTCkeq6tNJ2karf55kWZK/aq3dP92NAQAAAMyEyX6s5odJViW5ePB1d5I7kjx1sLxNqmrPqvpCVd0y+Pexmxn3YFV9d/Dl0sEAAADAtJvsCVkXtdaeu8Hyp6vqK62151bVDVN43LOTXNVae3dVnT1Y/i+bGHdfa+3pU6gPAAAAMCmTPXJkn6o64KGFwe29B4u/nMLjnpjkrwe3/zrJS6ZQAwAAAGC7TfbIkf+c5KtV9YMklWRBkt+rql3zv0KObfH41tqPk6S19uOqetxmxs2vqmVJ1iR5d2ttyeYKVtVZSc5KkgMOOGBzwwAAAAAeZrJXq1laVQcleVrWhSPf3+AkrOdv6nuq6sokT9jEXdtyxZsDWmsrqurJSa6uqu+11n6wmR4vSnJRkkxMTGx88lgAAACATZrskSNJclCSX0syP8kRVZXW2kc2N7i1dtzm7quqO6pq38FRI/sm+bfN1Fgx+PeHVfWlJIuSbDIcAQAAAJiKSZ1zpKrOTfIXg69jkrwnyQnb8bhXJHnN4PZrkvzdJh7zsVX1qMHtvZM8O8mN2/GYAAAAAI8w2ROyvjTJsUl+0lp7bZKFSR61HY/77iTPr6pbkjx/sJyqmqiq/3sw5uAky6rqn5N8MevOOSIcAQAAAKbVZD9Wc19rbW1Vramqx2Tdx2CePNUHba3dmXVhy8brlyV5/eD215IcPtXHAAAAAJiMyYYjy6pqjyQXJ7kmyaok3xpaVwAAAAAzZLJXq/m9wc0PVtXnkjymtXbd8NoCAAAAmBmTPSHrVQ/dbq3d1lq7bsN1AAAAALPVFo8cqar5SXZJsndVPTZJDe56TJL9htwbAAAAwNBt7WM1/ynJW7MuCLkm/yscuTvJ+4bYFwAAAMCM2GI40lq7IMkFVfXm1tpfzFBPAAAAADNmsidk/Yuq+o0kB274Pa21jwypLwAAAIAZMalwpKo+muQ/JPlukgcHq1sS4QgAAAAwq00qHEkykeSQ1lobZjMAAAAAM21Sl/JNcn2SJwyzEQAAAIBRmOyRI3snubGqvpVk9UMrW2snDKUrAAAAgBky2XDkHcNsAgAAAGBUJnu1mi9X1a8mOai1dmVV7ZJk3nBbAwAAABi+SZ1zpKrOTPKpJH81WPXEJEuG1RQAAADATJnsCVnfmOTZSe5OktbaLUkeN6ymAAAAAGbKZMOR1a21Xz60UFU7JnFZXwAAAGDWm2w48uWq+uMkj66q5yf5ZJJPD68tAAAAgJkx2XDk7CQrk3wvyX9KsjTJ24bVFAAAAMBMmeylfB+d5JLW2sVJUlXzBuvuHVZjAAAAADNhskeOXJV1YchDHp3kyulvBwAAAGBmTTYcmd9aW/XQwuD2LsNpCQAAAGDmTDYc+UVVHfnQQlUdleS+4bQEAAAAMHMme86R30/yyapaMVjeN8krhtMSAAAAwMzZajhSVTsk2TnJ05L8WpJK8v3W2gND7g0AAABg6LYajrTW1lbVf2+tPSvJ9TPQEwAAAMCMmew5R/6hqk6pqhpqNwAAAAAzbLLnHPk/k+ya5MGqui/rPlrTWmuPGVpnAAAAADNgUuFIa223YTcCAAAAMAqT+lhNrfOqqvqTwfKTquoZw20NAAAAYPgme86R9yd5VpJXDpZXJXnfUDoCAAAAmEGTPefIM1trR1bVtUnSWvv3qtp5iH0BAAAAzIjJHjnyQFXNS9KSpKr2SbJ2aF0BAAAAzJDJhiMXJrk8yeOq6l1Jvprkvw6tKwAAAIAZMtmr1fxNVV2T5Nisu4zvS1prNw21MwAAAIAZsMVwpKrmJ3lDkqck+V6Sv2qtrZmJxgAAAABmwtY+VvPXSSayLhh5YZL3Dr0jAAAAgBm0tY/VHNJaOzxJqupDSb41/JYAAAAAZs7Wjhx54KEbPk4DAAAAzEVbO3JkYVXdPbhdSR49WK4krbX2mKF2BwAAADBkWwxHWmvzZqoRAAAAgFHY2sdqAAAAAOY04QgAAADQNeEIAAAA0DXhCAAAANA14QgAAADQNeEIAAAA0DXhCAAAANA14QgAAADQNeEIAAAA0DXhCAAAANA14QgAAADQNeEIAAAA0DXhCAAAANA14QgAAADQNeEIAAAA0DXhCAAAANA14QgAAADQNeEIAAAA0DXhCAAAANA14QgAAADQNeEIAAAA0DXhCAAAANA14QgAAADQNeEIAAAA0DXhCAAAANA14QgAAADQNeEIAAAA0DXhCAAAANA14QgAAADQtZGEI1X1sqq6oarWVtXEFsYdX1U3V9WtVXX2TPYIAAAA9GFUR45cn+TkJF/Z3ICqmpfkfUlemOSQJKdV1SEz0x4AAADQix1H8aCttZuSpKq2NOwZSW5trf1wMHZxkhOT3Dj0BgEAAIBujPM5R56Y5PYNlpcP1m1SVZ1VVcuqatnKlSuH3hwAAAAwNwztyJGqujLJEzZx1zmttb+bTIlNrGubG9xauyjJRUkyMTGx2XEAAAAAGxpaONJaO247SyxP8qQNlvdPsmI7awIAAAA8zDh/rObbSQ6qqgVVtXOSU5NcMeKeAAAAgDlmVJfyPamqlid5VpLPVNXnB+v3q6qlSdJaW5PkTUk+n+SmJJ9ord0win4BAACAuWtUV6u5PMnlm1i/IsmLNlhemmTpDLYGAAAAdGacP1YDAAAAMHTCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrIwlHquplVXVDVa2tqoktjLutqr5XVd+tqmUz2SMAAADQhx1H9LjXJzk5yV9NYuwxrbWfDrkfAAAAoFMjCUdaazclSVWN4uEBAAAA1hv3c460JP9QVddU1VlbGlhVZ1XVsqpatnLlyhlqDwAAAJjthnbkSFVdmeQJm7jrnNba302yzLNbayuq6nFJvlBV32+tfWVTA1trFyW5KEkmJibalJoGAAAAujO0cKS1dtw01Fgx+PffquryJM9IsslwBAAAAGAqxvZjNVW1a1Xt9tDtJC/IuhO5AgAAAEybUV3K96SqWp7kWUk+U1WfH6zfr6qWDoY9PslXq+qfk3wryWdaa58bRb8AAADA3DWqq9VcnuTyTaxfkeRFg9s/TLJwhlsDAAAAOjO2H6sBAAAAmAnCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrwhEAAACgayMJR6rqz6rq+1V1XVVdXlV7bGbc8VV1c1XdWlVnz3SfAAAAwNw3qiNHvpDksNbaEUn+Z5I/2nhAVc1L8r4kL0xySJLTquqQGe0SAAAAmPNGEo601v6htbZmsPiNJPtvYtgzktzaWvtha+2XSRYnOXGmegQAAAD6MA7nHHldks9uYv0Tk9y+wfLywbpNqqqzqmpZVS1buXLlNLcIAAAAzFU7DqtwVV2Z5AmbuOuc1trfDcack2RNkr/ZVIlNrGube7zW2kVJLkqSiYmJzY4DAAAA2NDQwpHW2nFbur+qXpPkxUmOba1tKsxYnuRJGyzvn2TF9HUIAAAAMLqr1Ryf5L8kOaG1du9mhn07yUFVtaCqdk5yapIrZqpHAAAAoA+jOufIXybZLckXquq7VfXBJKmq/apqaZIMTtj6piSfT3JTkk+01m4YUb8AAADAHDW0j9VsSWvtKZtZvyLJizZYXppk6Uz1BQAAAPRnHK5WAwAAADAywhEAAACgayP5WA0AAAAwXJu8Liyb5MgRAAAAoGvCEQAAAKBrwhEAAACga8IRAAAAoGvCEQAAAKBrrlYDAAAzxJUjAMaTI0cAAACArglHAAAAgK4JRwAAAICuOecIAACwnvOiAD0SjgDMEL9sAgDAeBKOAAAwEkLj6WE/Amw/5xwBAAAAuiYcAQAAALrmYzUAAFMw3R9l8NEIABgd4Qhjxy+HMHnmCwAAbD/hCAAAANvFf9gw2wlHYAocSg0wu3ifBWY772N98DyPjnCkMyYbAADQI38LsSXCkWlmwsHcYT4DAEAfhCMAAACMHf9RxUwSjjDneVMF2HYPPPBAli9fnvvvv39S4z/72c3fd9NN2/74011vNpo/f37233//7LTTTqNuBZiD/I4MDyccAQAeYfny5dltt91y4IEHpqq2Ov4Xv9j8fQcfvO2PP5XvmUtaa7nzzjuzfPnyLFiwYNTtAMB6czVY22HUDbBlrW3+C7bEawfYHvfff3/22muvSQUjTL+qyl577TXpI3cAgO3jyBFgJIQ0jAuvxc0TjIyW/c9c4X0WmA2EIwDAdpuYGE7dyy+/PCeffHJuuummPO1pTxvOg8xR/iCdHvYjU+F1A7OPcASYFD/koW/vfOc7p7XeueeeO6lxl112WZ7znOdk8eLFecc73jGtPTzkwQcfzLx584ZSe67xswDg4bwvzh3OOQLMGc6zMn629Jx4XtiaVatW5Z/+6Z/yoQ99KIsXL16//j3veU8OP/zwLFy4MGeffXaS5NZbb81xxx2XhQsX5sgjj8wPfvCDfOlLX8qLX/zi9d/3pje9KZdeemmS5MADD8x5552X5zznOfnkJz+Ziy++OEcffXQWLlyYU045Jffee2+S5I477shJJ52UhQsXZuHChfna176WP/mTP8kFF1ywvu4555yTCy+8cAb2CAAwLF0fOeIX8+lhPwIwDEuWLMnxxx+fpz71qdlzzz3zne98J3fccUeWLFmSb37zm9lll11y1113JUlOP/30nH322TnppJNy//33Z+3atbn99tu3WH/+/Pn56le/miS58847c+aZZyZJ3va2t+VDH/pQ3vzmN+ctb3lLnve85+Xyyy/Pgw8+mFWrVmW//fbLySefnN///d/P2rVrs3jx4nzrW98a7s4Ahsrvs0DX4QgATLfZ8Av2bOgxWfeRmre+9a1JklNPPTWXXXZZ1q5dm9e+9rXZZZddkiR77rln7rnnnvzrv/5rTjrppCTrQo/JeMUrXrH+9vXXX5+3ve1t+dnPfpZVq1blt3/7t5MkV199dT7ykY8kSebNm5fdd989u+++e/baa69ce+21ueOOO7Jo0aLstdde07bdwNbNlvcxYPYQjgDAmOvxj4A777wzV199da6//vpUVR588MFUVU455ZRHXMWlbWYH7bjjjlnwpKmiAAAgAElEQVS7du365Y0vi7vrrruuv33GGWdkyZIlWbhwYS699NJ86Utf2mJ/r3/963PppZfmJz/5SV73utdt49YBAOPGOUcAmFWcw6QPn/rUp/LqV786P/rRj3Lbbbfl9ttvz4IFC7LnnnvmkksuWX9OkLvuuiuPecxjsv/++2fJkiVJktWrV+fee+/Nr/7qr+bGG2/M6tWr8/Of/zxXXXXVZh/vnnvuyb777psHHnggf/M3f7N+/bHHHpsPfOADSdaduPXuu+9Okpx00kn53Oc+l29/+9vrjzIBAGYv4QgA6wkeGBeXXXbZ+o/JPOSUU07JihUrcsIJJ2RiYiJPf/rT8973vjdJ8tGPfjQXXnhhjjjiiPzGb/xGfvKTn+RJT3pSXv7yl+eII47I6aefnkWLFm328f70T/80z3zmM/P85z//YZcMvuCCC/LFL34xhx9+eI466qjccMMNSZKdd945xxxzTF7+8pe70s0YcRJoAKaqNnco6mw2MTHRli1bNuo2AGDWuummm3LwwQePuo2xtXbt2hx55JH55Cc/mYMOOmhoj+N5AIDtU1XXtNYmtjbOkSMAANvgxhtvzFOe8pQce+yxQw1GAICZ44SsAADb4JBDDskPf/jDUbcBAEwjR44AAAAAXROOAAAAAF0TjgAAAABdE44AAAAAXROOAABjZ+XKlXnOc56Tww47LEuWLFm//sQTT8yKFSseMf5LX/pSnvWsZz1s3Zo1a/L4xz8+P/7xj/P2t789V1555RYf87d+67eybNmyLY45//zzc++9965fftGLXpSf/exnk9kkAGCMCUcAgK2qmt6vrbnsssvymte8Jl//+tfzZ3/2Z0mST3/60znyyCOz3377PWL8c5/73Cxfvjy33Xbb+nVXXnllDjvssOy7774577zzctxxx233ftg4HFm6dGn22GOP7a4LAIyWcAQAGDs77bRT7rvvvqxevTo77LBD1qxZk/PPPz9/+Id/uMnxO+ywQ172spfl4x//+Pp1ixcvzmmnnZYkOeOMM/KpT30qSXLVVVdl0aJFOfzww/O6170uq1evfkS93/3d383ExEQOPfTQnHvuuUmSCy+8MCtWrMgxxxyTY445Jkly4IEH5qc//WmS5M///M9z2GGH5bDDDsv555+fJLntttty8MEH58wzz8yhhx6aF7zgBbnvvvvW1zvkkENyxBFH5NRTT52O3QYATJFwBAAYO6985Svz+c9/Pscff3ze8Y535P3vf39e/epXZ5dddtns95x22mlZvHhxkmT16tVZunRpTjnllIeNuf/++3PGGWfk4x//eL73ve9lzZo1+cAHPvCIWu9617uybNmyXHfddfnyl7+c6667Lm95y1uy33775Ytf/GK++MUvPmz8Nddckw9/+MP55je/mW984xu5+OKLc+211yZJbrnllrzxjW/MDTfckD322CN/+7d/myR597vfnWuvvTbXXXddPvjBD27X/gIAto9wBAAYO7vvvns+85nPZNmyZTnyyCPz93//9znllFNy5pln5qUvfWm+/vWvP+J7jj766KxatSo333xzPvvZz+bXf/3X89jHPvZhY26++eYsWLAgT33qU5Mkr3nNa/KVr3zlEbU+8YlP5Mgjj8yiRYtyww035MYbb9xiv1/96ldz0kknZdddd82v/Mqv5OSTT84//uM/JkkWLFiQpz/96UmSo446av1Hf4444oicfvrp+djHPpYdd9xxm/cRADB9hCMAwFg777zzcs455+Syyy7LUUcdlUsuuSR//Md/vMmxp556ahYvXvywj9RsqLW21cf7l3/5l7z3ve/NVVddleuuuy6/8zu/k/vvv3+L37Oluo961KPW3543b17WrFmTJPnMZz6TN77xjbnmmmty1FFHrV8PAMw84QgAMLZuueWWrFixIs973vNy7733ZocddkhVbTasOO200/Kxj30sV199dU444YRH3P+0pz0tt912W2699dYkyUc/+tE873nPe9iYu+++O7vuumt233333HHHHfnsZz+7/r7ddtst99xzzyPqPve5z82SJUty77335he/+EUuv/zy/OZv/uZmt2vt2rW5/fbbc8wxx+Q973lPfvazn2XVqlWT2icAwPRzDCcAMLbOOeecvOtd70qyLvh4yUtekgsuuCDnnXfeJscfcsgh2WWXXXLUUUdl1113fcT98+fPz4c//OG87GUvy5o1a3L00UfnDW94w8PGLFy4MIsWLcqhhx6aJz/5yXn2s5+9/r6zzjorL3zhC7Pvvvs+7LwjRx55ZM4444w84xnPSJK8/vWvz6JFix529ZwNPfjgg3nVq16Vn//852mt5Q/+4A9c9QYARqgmc3jpbDMxMdGWLVs26jYAYNa66aabcvDBB4+6je55HgBg+1TVNa21ia2N87EaAAAAoGvCEQAAAKBrwhEAAACga8IRAGCT5uJ5yWYT+x8AZo5wBAB4hPnz5+fOO+/0B/qItNZy5513Zv78+aNuBQC64FK+AMAj7L///lm+fHlWrlw56la6NX/+/Oy///6jbgMAuiAcAQAeYaeddsqCBQtG3QYAwIzwsRoAAACga8IRAAAAoGvCEQAAAKBrNRfPQl9VK5P8aBJD907y02l++OmuOe71hlFz3OsNo2aPPdrm8aw57vWGUXPc6w2jZo892ubxrDnu9YZRs8cebfN41hz3esOo2WOPPW7zxn61tbbP1gbNyXBksqpqWWttYpxrjnu9YdQc93rDqNljj7Z5PGuOe71h1Bz3esOo2WOPtnk8a457vWHU7LFH2zyeNce93jBq9thjj9s8VT5WAwAAAHRNOAIAAAB0rfdw5KJZUHPc6w2j5rjXG0bNHnu0zeNZc9zrDaPmuNcbRs0ee7TN41lz3OsNo2aPPdrm8aw57vWGUbPHHnvc5inp+pwjAAAAAL0fOQIAAAB0TjgCAAAAdE04AlNUVTXqHkap9+2HbWG+wOSYKzB55gtML+HIBmqdHbzRbLvp3mez4Xlo03zCng23d7q2/aE6w3htb7j9g/pj/Xwxdw3h/WfaX8vmC+Ni3OeLucK4GMZrz3xhrhr3ny2TJRwZqKod2jprH3qjGdc3mOn+w3mjdVP6A3rDffbQH+Kbe4xJ1lu70Rv+Dhvc3uaaVbVnVe2ymfu2eZur6vSq2m+q/WxKa61V1WOr6tEbBy/b87xM92u7qnapqoOq6viqWlRVjxrUH8uzOw9zHk9H7Y1rbPhanw7Tuf0bzu0pfO/eVfXowe1509VT8sigcnv34XT+smm+TF/tYc+VTT3G9taaSo/DnCvJ+M4Xc2V668+mny0P1Ru3+TLdc2XjmubL+NSeTfPF72LD1f3VaqrqiUlemOSQJHskuTXJVUm+NS5vMFX12CT/W5J5Sb7eWrt9O+s9NckJSR5I8pHW2r9vR635SY5JckuSf2mtPbjhfa21+wd/nK/dhpp7JnlrknduVG+31to9U+hxjyT/LclFrbVrBuv+Q5Ldk1y7rc/zoL9/TfL/JPnD1tpdg/U11ddMVe2Y5EVJ3pB1oeVfttb+frB/V0+lblU9LsmLkxyc5DFJvpfkm621b0+136r6nSQXJnlCkv+ZZE2Snyf5RpK/ba1du619TqeHtqmqds6697fV01R37yS/kuRH0/G+MJjTq1prD2x3c+vqPT3JDa21B7bndbhBvRck+bck32+t3T9YN2/D+TiFmn+e5J7W2rnb09sG9XZO8twkhyf56kOv6+2s+egk+yVZkOTHSW5prf1yO+qZL9v/WpzWuTKoOdbzZbrnyqDmWM+XXufKoPbYzpfpniuDmmM9X4YxVwZ1zZftr+t3Mb+LDV9rrduvJM9K8vkk9yT5xyR/m2RZkh8M1r9kG+vtk+SgJLtuYczuSR63DTUPS/LxrAsyfpl1f5S/ZKMxOyXZfRu2+QtJ7kryiyRXJHlikjcn+UiS30+yyzb0d2rWvSF/PMn7BnWemXUTZVWS+VN4Xn4364KWh5YXJnl/ksVJ/jLJ8dtY73VJbk6yW5Jdk/xfSa4f7Mu7k1ySZP9tqPf6wfPx/ST/kuSkaXgtvjjJPyf5zGBffmWwDz+a5LbBuoXbUO+ZSf7fJD9JcmWSnyZZO9gPn0iyaAo9LkqyPMmfJXl6klOS/OfB/vtmkm8lee4U6u64vftvE6/x7wy2eWmSowfra4r1XpHkjiT3JVmSZH7W/XB+XpLfnEr/ST6W5O1JXjp4fT9+sP59U3xufpLkoA2Wfy3JsUn+Y5KnTqHe7UleNbj92MG8/liSywfz8zFTqPnA4DX45SS/MQ3P83/Puvfu6wbP975JjkjytiS/99A+3YZ6xw3m9MrBe8P/l3XvlX+S5ElT6G/a58t0z5VBzbGeL9M9VwbfO9bzZbrnyqDm2M6XYcyVQd2x/tky+N6xni/TPVcGNcZ6vkz3XBnUHOv5Mt1zZVBzrH+2DGqO9XyZ7rkyqDPW82U658qUt2kmHmRcvwYvrk8l2XuwvF+SiSSvTfJ3gyfltG2od36Sa5P8jySnJzkq61LdR20w5g1JPrgNNS/NugDj0MHy+wdvfPs99AaT5KT/v70zj5ekqPL9N+699ALdTQNNY9uAQsveCNIKNLINmzgMgs2giIAIg6i8gXFhFQYRaMAFRD7OsDx8CAMiIiDLB3jNvonsPBRodmQRkVXZlO4b749ziptd1L03MyuyKqLynM8nPlWZlfXL3y8jT0TkyViA7+bEu0AdazwwBbgS+L9IQOgyvQkPLsBvd2AhcDlwgzrHTfr5IrAnsCEwoQDm1cDJGW13IQ/1FyEF/wvATgXz+cf6/dvALUhPki2AfYH7gJP091ELbXXS7wLL6PV7Bfhe5j7qz4PThHlZg4NuX4L09Lgc2A+4E+mp0peT48VIYGVJ3d4ZCQAeq1gPUiDYohjfB65osX8AKQgvRnpeLV0A88N67bYClqdFME2v88HkCNoBayFBpgtU81XqL0s28f08MCYH3rqq6XBga+BuvaefRoJW9wFfLngdV0cqpieQMuYexM+/rfs/xwgB1hZ4GyFvAUAaC/sgAcu/I/78W+CTBfA2RColhwRzzwFe1c/z9Rp8t8g9rhxfBj6J+PD/A7bL/N5f8BqujzQaNgNmIOX4KUgFfQ1SZvyMnI0lpLHZyOfVgIOQRtjlwENImbRSN/0ltK+k4C+hfSUFfwntKyn4S2hfqcJfQvtKCv4S2ldS8JfQvpKCv4T2lSr8JbSvpOAvoX0lBX8J7StlU6XgsSe9Wbcf4fczkcBB3gLmeeA2vdneZOit/dHADkiXoweA4/LehHpTbJrZnow83B6T2Xcz8IM8mIq3MUOBlceA09DgBdJN7xpg2ZyaJyHR628hBd1spKfGi0hk+w69obfLg6eYfwPW1u83qKNN1G2HBCR+DSyWE+9i4Ej9fg+wb9PveynPmTnx3mjkCRIIORQpnM+jQK+gJsyngK0y2/PJBLyQHjoPkDPKq/f2Jo1rpp93Alvq99uBE7O/58A8Ganoxuj2ANCX+X1Zvb5fzYuLBGsGgXeQtwtXAN9EKoVlkWDQbsCLOTkej75R0O0VVPdZmWO2LoD3fcXr1+1jkGDYt5GK9QzlPb1AXk9BKouTkcL/CKSceRHpHXYzcDrwxZx4JwGX6fed9T45EhiL+Pq1yLC3vA32bwI36PevaJ6uottTkd5lLwOzC2g+CW3MIY2mXyGV/LGU6112PPCbzPZ+mi/bII24f0MaFXOK5HPTvh8BP1S+twMXFOQY1F9C+0oK/hLaV1Lwl9C+koK/hPaVKvwltK+k4C+hfSUFfwntKyn4S2hfqcJfQvtKCv4S2ldS8JfQvlI2VQoec0J6TpyP9EZo2atBneUvwEdy4E1Donjb6PZkJOp4DhLZfAsZgjHI0IP/aAXWh9SRVm/av5XefKvr9uto9y8yBWQLvJWQh/CVdXuM8lknc8xMJJhRJIo9TQuBb2X2vYXMa7KFOs/yOTWvopx2Vy7PAKsiQZHGQ/6nkGExuYbCaCF1PzJ86FK0i1rm98WRMW1rjMZRnfNNvXYNPgPAHkhw7M/A3hSIviIBpquRAnoyEn0dBGZljpmg+Gvm4LgUElQ6sGn/YOae2QupqIoM8dpE77uDR/CZ+cAuo92LmeMvQgq+lZHK90K9hoN6r56F+NVFjWs9Ct4twH9krxFSKT3HkG/+hKHKYTS83wL7Z7avBH7OUAU9Fbge2DPP/Z3BmYJE7b+W2XcWcCsSbHwM+GGe64h0PzxKv5+HVC5jMr+viwRsP5OT21Z6/ywOzAV+2uKYXzAUcMzzoPIgsE9mexzyxuOvmmeb5r12+v//DZzauDaIX5/RdMx5wNyc1/BchgLM4/XzAmTuH5Cy7GFg/W75S2hfScVfQvpKCv4S2ldS8JfQvlKFv4T2lRT8JbSvpOAvoX0lBX8J7StV+EsVvhK7v4T2lRT8JbSvlE2VAaeQgG2RwMUpyHCaCSwafd0eeD0n1geBwxhmnhJgRaSHxksF+K2tN2tjvJnL/PZz5IF6HeCdnHjrqdbGA/aywP8i8yCP9Pz4cwGOjUJvFSQA8UWkW9RrFBhKk8HbCIkM3o1Ebx8CZjQdMxv4UwHM5fRaXarX7RYk8NKIun8pb74AByITDr2vIAKWRuZteR44qKDu3ZHg2U2q/zYywQ1k0tuXC+B9E+kquBMyNOlS4IGm65z7LUDmf4chPWfuRaLs6+u9vTFwIhLMyjv/jUMq5LktruWaSG+k65AK+jO6f9igE/J24+qm67aYfp4GXKffHwa+lAOvH3mLMCez70Jga/0+oJ/3oH5PvoZSoxKZoffiHrr9NLBr5rjGG5eRAmH9es3vQobL/ZXMHDj6+4By/Jc8HJG3HPcggd1/R3perZDlj/jlrgXwBoEPNR8PfAKp/O+nWO+yHZGeaQchQcUnyPSm02N+n+E4Wo+6PfS+aJSN6+m1bDTixivep0fLkyr8hcC+koq/ENBXUvAXKvCVVPyFGtUtKfgLFfhKCv5CYF+J3V+oad2Sgr9gbTEI1BYrmoIDppS0UNgTGYLwrt7UpyDR2Ks0g47Jk6F6zBR0chy9aQfIDP3QG+cO/Z4n8jqAdE36WAazESWdibz5/wNwVR5MxVuJEQpMpGvV1Xk1Z8+LrPpzAdIVbV6G86iFVAZrMSTQshXygH8wTb1YgOMy+Hk5ztCC5R9aODyNFK4P6jX8Rh48ZNzfjMb90yJfJiJzz5xZkN9EpDvaSUjw5/NIgOjLSPfBu4CfFLgXl0cCNQuQXjwXApvpb0vofV7oGmaw19L/P5rBfxIpxD+XvTaj4PQjkfQGr/fNp6L36yAwOQfeGGQOmfcNF0J6DT2BBAMXkGOoHFKR/AtDw5P6kMbHQOaYDyKTg43Krwm7ce98CRlL+XG9N9cskh9IRfElxG8vQSbY+uemY6Yj3WVzc0TeIN2C9KgaVI57IkG8c9Vnck0EhgQnT2jWn7kG6yj3QXIGVFX3qXoPXqzcHlKs5YD9kXI978PUVOTN1DtIL7ingUszv89Axg0Xymf970xkIunS/hLaVyr0l+1D+wtDDdi2fCUFf6nCV2L3l6b7biZx1y3vm5usjK+k4C9V+Urs/hLaVzL+cnsgf8k+0LbdFgvtK03+Ym2xNv0llK+k4i9U2BYrdF9UCZ5SQh7Gf6yFyiNIN6Bd0cl4RitgWuA1Fy4DyCSgjV4gZWZVdtn/IhPWDAJ76XbRhmIzx52Rh/BdSuL1I9HDV4C9A+XLRBatDD6PRA2/mJcjixbMqyKzJ1+EBMDmat675mPLXMMs7zJ4mf8vhlR6f0Umy/0uOpaSAsEmPX6VbEGH9Ow5jZxvATL/W6TSRHrKfATp4rkdOeepKch9d+CJMrqzvPVzf/WXB9rBa9I/F1keuR1+Ryuve9BuhCVxJiEV0kqZfVOQsZp35+WY8YXlkLcVVyBvqN5BKuLr0DGuZe/vYc67Q1FMpPfb0vr9Z3odn1SfObAE3o7AUchEao2yfzLylu7movmcLVc0Lyrzl3Z9JcM1Wn8J5SuKtSTyUNCuvzQC4x/Qa1a5v5TxFT1+KrCMfi/lLyxaB3wWmcixbX+BRXqwLoM0gjdHAghTQly3pvOV9pcqfSWDHY2/aJ627StN1yw6f2m6t9v2lRaYwfwli69cV0YmxdwutL+U9RWa2tWh/aVdX2nOP82bkG2xD2f2lalbGs96UbbFWPTZrO22WBNe0LZY0dS4sWprzrlGz4YFTfsneO/fKIk54L1f4Jwb54fWpe5DHvTf8DnXp86ukT0M5jTgP4Afee9fLLKmdgZvvPf+bd23MtKF6Urv/ZslNY9BIq8PIUNryMtJcVppznKcgXT/urxI/jjnHNL4as7nPu/9YJv8snky0HyOMhyzvDRfnixyHfV/793bTddwMlJQP1WGa4i12xs4SGE57PV3zq2BRJxvHy2vVC/DHeOcWw6Jit/svT9otLzKgbcqupKQ9/5/yq49r+fZD/iL9/78Ev91w53XObc68A2k0fCzNjgOIHMwjQH+4AusN6//HRzhOhbm5JzrV0zftH9LpAy723t/XRGOw90LzrkpSKPzee/9b/OUGc65pZD5lvqB33rvn8nLJSff9/mg5vXkPL6SB1P95WJkGOGo/tICrw8p/hvlZdv+ovn+dUr4iv5/xLLLObca0mOxK/7SnG8t8qQUn1ZYuq+Uv4xyninIuPU/5fEXvS8+g/TePcd7/0oIHor9vnM37kskfwr5y3DHOec+iPTOLOwrDcwWeb0G8vb9riL+0tRG6UeW+3zVe39uHj4t8Fqe1zm3JvKQe0dRX2k+VnmuhfjL/d77dwtybHVvN3r0FsJqxU/3bYO8WLrDe399CcxW92IjsPFJcviLc24cMsT6UaRNmL2G2XZoqTaa8gFtlzVw2qlbmjF133LIC8pbS/iLY6i3w0Ld13jpWchXMpj93vuF+vzyFeAV7/15ef+f8xyrI89rhf2lmSvSq2UA+H2Z+1txmvN6xLZkCfxtkADRXWX8ZRjMKcjLpRfytsVKnyvAM05ylqmMlkEmTd0PGYbwI2QlmUIPjMPgTUcm8Tk2IMfSmB3CWx443nt/fFG8UTj+wHs/NxDeikg+H1OiYd7JfD6RcPfi8sg1LMtxWSRi+7xvETTTwnoSMoTsxTLnqNqaGozjvfdvl21AjIRd4r/vVZDOucXKVnT6/2GDOWU4NipLWgQhYrHMg86IQbaCmAPAwjbydCYyBnyOcnsJ2M97f3HmmLHIOObXc2IGD7Yo5pZIj7BhMfP6S16Oee/FkfDK+koT5u3e+z8G5Pg77/3TRTmNgBcyn0fV3A2OzrnZyNvaWUjX+euQh/k5yEuWe5HJ/Qq9qGoKuJztvX+1LMcM3g5Id/th8RoPqHnun7wcC9yLVWgeNWhVpG5pwvwf7/3LAfFaam4DL0igLoO5APh5O/ninNsF6fV7FVKvzEdWWhyL9CpYrlUbbQS8UYMtRdsORQI4wNjRzpEXr4GZ01dGwhyL+HnuF7uhORa4hrmDAznzesQXgmU1h+JXBK9t8xV1SYk5MdQN9hSk+9QcZH6R03T/tsAh5FzJYxi8h4BTm/CWa5NjM+Z3gGkB8Q4DPtDmNTy9zDXsIMdW+RI6n0NrPjQwx0ML3os/RhqpJyET7s5CusSOzRzz1cY5cmIuiwz3GXYNeaTbe968zoO3dF7dOfEm5+VXUHPevA7KcTQ85CF6csH7u6OaleMyga/hUgXz+SxkAuS1dPu/kAbsBxl6IfFZMkt1j4I3E/gl0mD/BzLb/45Nx4wBlirAMQ/mOPKPMQ/KMSfeYhQb958Hc2ybHD/bAi/vWOtuaR7fBsfn27yGFyCTF45HejNeiUy8+DhwGTJW/eC8ehVzNjAPGd77pvridKRb+tnI8pfD+ntJvImRcjwAmYR+f2DxwPwKTbg/DObyyu1spIdjkXl08nAMqfkb5JzbYQTMy9q5jsjwloXIvBM3ICuf3KSfLyLzPWyYFxPYBVnt8pfAT5XXBkjPrzcot7RrUMwOc9wsBo458HKXDV3kmNv3hsHbv918bid17EQxJmTYxwb6/QmGxlnNQpZxWrebeClwNM29rxlpAN+GVMBvAi8A1yDjmXdAVlV6AOnpAvnmgQkacIkdLwWOpjmY5j8Dm2a2JyOTZx+T2XczQ8vVjTYJ9FkEDLYUwJyDLksYKV4KmosEwbqluWscEV/ZOPPfx5A34xN0+ydIXZN7fh4CB1xy4h2SF68KzB7SHJpj1/Aq4jgJWV72W8iwitnAXkhg5Fn1xYfIudoIgYMtBTBnkzOgWACvm0GmoBy7mC+hNXcNr91U+QliSwxVwrOQwqSxgsvr6NtB5E3iX8gR+QqNlwJH01wPzXr8NGTVnMYyWpOR4TrnILNIN2ZHHwTWzvIYBTdowCV2vBQ4muYgeB9CuoWu3rR/K+Tt4eq6/TqZVchGwQwabKkCM3a8FDjWTTNSNz0FrKzbY5B6ZJ3MMTORh72VRuLVgmOwgEtovBQ4muY4Net/piF10rcy+95Chu5sARwPLK/7R5sAM2iwpQrMOnI0zWE0t5MqP0GsCVk14AbkQXJ3dIld/W1fYL5+zzvrcVC8FDia5t7XjLwRPIymrtOZ31dEKvuXCugNGnCJHS8FjqY5mOa1kWX3dms+HunefjUySdk7OX3lQ4QPtgTFjB0vBY411bweMvxzTd1eFlneM7tqzWzgz3l8RY8PGnAJjZcCR9Mcp2Y9vhFkWQW4H+nt+DGkp3CpN+oEDLZUhVlHjqY5jOayqTLgmBO6JCmy3NA84EHg/yBj1XdB1iP/jh476pK7ofFS4Gia66FZj5uCjrVV/AFk8tXG7yejAZicHIMGXGLHS4GjaQ6meQDYhszDIUPzAM1E3rL/AbiqcfwoeEGDLVVgxo6XAseaam70bBx2vhPgWOBq/Z6np0zQgNkHi5oAABW5SURBVEtovBQ4muY4NWf9Rj8/jQzbuRmYp/saKxTmxaoi2BIUs44cTXMYze2kjp6s2wmYAPxTZnsAmfjlRaS72z3ImKdD0YnoGCE6FRovBY6muR6aRziPa9oeAE5gqMHcrYBL1HgpcDTNYTS38heGGrSHI28P99Lt0YYxBA22VIEZO14KHOuoeThfyWzvDNwF7JLHVzIcgwVcQuOlwNE0x6m5xX/7gYOQXlt7F/lvM0/9bDvYUhVmHTma5jCay6aOnCSWhCxbOAj8Ctgos38tZKbdA4DV82ZAaLwUOJrmemge5hyNgmtcZl8fstpIoYq9CTdIwCUVvBQ4muZyeCz6Nr2Vv0xTzKmtzpkXP4NdKNjSCczY8VLgWDfNGYzxmX0rA/9KiZUZmjlmtgsHXKrES4GjaY5Pc8ZfxgAbISuqueZzFOQXJNhSJWYdOZrmMJoLc+jGSbsidKgin4FMgnQp8IVY8FLgaJrroTmD26efywBfA34PvIoOywmRCBxwiR0vBY6muRxeFf5CBcGW0Jix46XAsW6aq/CVFucIGnAJjZcCR9Mch+Zh/OU1Cq4YlINfyGBLUMw6cjTNYTSX4tHJk3U7ZQqY5ZG3HM8CVwBbxYCXAkfTXA/NitXoMn0KMixnDjJ52Km6f1vgEGC5EjyDNIpjx0uBo2kOpjmPv3wHmGb5YpprrjmPrxyGrrLWC5pT4Giao9Xcyl8eBk7X/Y222NQ2+LUVbAmNWUeOpjl84K/0devmybudkAfJHyBLOR6eLVgoNx9DULwUOJrm3tesBdUG+v0JYAf9Pgu4FVi3AFbQgEvseClwNM1hNIf2l5wccwdbqsCMHS8FjnXUHNpXCnDMHXAJjZcCR9Mcp+bQ/jIMv9LBliow68jRNIfRHCJ17EQxpUZm6PeJwJ7IZGKXoMs6dhMvBY6mubc1MzRUZxbSC6UxudjraIWOzCr9F2DxEjyDNYpTwEuBo2luq7FZmb9YvpjmXtJcpa/Eqjk1jqY5Hs1V+kusmuvO0TSH0dxO6tiJYkvIm/XJme3JwFzgaSSCtVQ38VLgaJp7XzPwEeAGLaB2R1fu0N/2Bebr91EneiVwJR87XgocTXPwxmYQf7F8Mc29qjm0r6SiOXaOpjlOzaH9JQXNdeRomsMHyttJA9TMnHOzgQOR8U1LOuf+BPwGOMN7f5hz7mpgY2Qivlc7jZcCR9NcG819wONIFPd4YDrwO+fcMsDWwJeBs/XwPmSlgmHNa0mHFHyPAesAayKV+gv62xbAK977t5xzfd77YTFjx0uBo2kOoxnC+ovli2nuVc1Qv7olBY6mOU7NYHVLHTia5nBtsRBWi+CIc855771zbjXg58ALwDxgATAT+Dow0Tn3Q+/9jc652/W3juClwNE010OzYk4APuG9v163vwKcjBRcmyj+OshcJv+tf1s4EmYGO2ijOHa8FDia5vbwqvIXyxfT3Gua61y3pMDRNMel2eqWenE0zWE0BzHfwW4q3UoMTfhyBHBLZrsPWBqZSOw1YLNu4KXA0TTXQ7P+d0ukAPoVsFFm/1rAvwMHAKuTo7tz5r8TgH/KbA8APwVeRCLG9yCV+qHokCBGmDg2drwUOJrmYJqD+ovli2nuYc21q1tS4Giao9VsdUsNOJrmMJpDpo6cpNupcUGBE4C5wxxzI3C0fs87Zi8IXgocTXPtNM8AfgJcCnxhNF05dIeu5KPGS4GjaQ6CF9xfLF9Mcy9qrsJXYtecCkfTHJ/mKvwlds115Wiaw2gOmTp+wm4m4N+A24HNgcUy+ycAjwA76vZAN/BS4Giae19zozBCJnY9HJko6Qpgq7z6mvCCVvKx46XA0TQHfTgL5i+WL6Y5FsyKONaqbkmBo2mOU7NiWd3S4xxNc7i2WMjUdQIdEyqz4Q5qegr4HrKe8u7A6cB5FIvKBcVLgaNprofmFvjLAz8AbkMq6KmZ33J1cyN8ozhqvBQ4muYwmlucoy1/sXwxzb2quQV+z9ctKXA0zXFqboFvdUsEmLHjpcCxal9pN3WdQEdEDkWppiAPjRcCrzD0QPkusA/wcWQymLGdxEuBo2muh+Ym7P7M94nAnsCdwCXANm36ZNuN4pTwUuBomtvDq8pfLF9Mc69prspXYtacEkfTHJfmqvwlZs115miaw2huJ3X0ZLElZN3wrwLXAn8H3gb+BOwWA14KHE1zb2vWAmtyZnsyMBd4GjgFWKogXtBKPna8FDia5uCNzSD+YvlimntVs+LUqm5JgaNpjlOz4ljd0uMcTXPYQHk7qSsn7YpQGKOFywbAKmTmZcgcsz5wKrBFp/FS4Gia66FZj58NXIRM5nofcCUSbGmshrMZshLOh/PgNWGHbhRHjZcCR9PcdmOzEn+xfDHNvaa5Kl+JWXNKHE1zXJqr8peYNdeZo2kOo7nd1OiS35PmnOvz3g8659ZELvQ2yOSUzyBLBc0HHgUe9t4/12m8FDia5npoVkznvffOudWAy4AXgHnAAmAmsDZwDvBDPW4ssMB7vzAn/mzgQGAZYEmkJ8tvgDO89wudc5sBGwPneu+fSh0vBY6muTxelf5i+WKae0lz3euWFDia5ng0W91SP46mOYzmUNbrwZEB7/0C59y5wFTgDKTbzkbAmsDiyFv3s733xznnBgA/XAETGi8Fjqa5HpoVs18LpCOATwGb6XYfEsn9GlKQ7eC9v3E4nCbMoJV87HgpcDTNwTQH9RfLF9Pcw5prV7ekwNE0R6vZ6pYacDTN4QPlwcx3uKtKNxKyxOn2TfsGgE8ARwJb677+buClwNE0975mhoKlJwBzhznmRuBo/T7qCjgMdf08Argls90HLI10B30Nqfzz6I0aLwWOpjmY5qD+YvlimntYc+3qlhQ4muZoNVvdUgOOpjmM5ipSH/WwM5Fo1HvmvV/gvb/Te3+U936e7ssbmQqNlwJH09w+XtQcvZZQyHCcLZxzmzvnFmv87pybAEwD7tZdecqPQf2cANzU4OG9H/Tev+K9Pxa4H9hKzzEaZux4KXA0zQE0V+Avli+muSc117RuSYGjaY5Qs9UtteFomsNoDm49GxxxzrnM5rPAHOfckc65jZxzk7uNlwJH01wPzU3Ys4DTkQlczwKOcM7Ncc7tDpwI3AVcChKEGQ0vdCUfO14KHE1zsIezoP5i+WKay+ClwrFudUsKHE1znJr1P1a39DhH0xyuLRbaenbOETc0Zu8oYB9knN6zyPiml5AJKx8HrvLeP9tpvBQ4muZ6aM7gNsYCTgE2BXYFtlB8gIXA14F7kYmTXvLe/z0H7ixkaS6APwJnI7OuLwFsgkSQd/PeD7ZGSAsvBY6mOQhecH+xfDHNZfBi51jXuiUFjqY5Ps1Wt9SHo2kOozm4+S6O6elEAt5B1k2eBmwOHAL8ArgBeArYVI9z3cBLgaNprofmEc7zEWTpuGuBvwNvIxXybjn+2wjATgHmABcCryBd6waBd5EAz8eB6cDYlPFS4Giaw2gO7S+WL6a5VzWH9pVUNMfO0TTHqTm0v6SguY4cTXO1bbF2UsdP2FFxcmHnAVNb/LYCsBMwrlt4KXA0zfXQnPnvGGTN8Q2AVYDFWhyzPnAqsEVR/AxG6UZxingpcDTNxfE64S+WL6a5FzR3wldi05wqR9Pcfc2d8JfYNBtH0xxSczup4yfsiCiduRnYELgE2DcmvBQ4muZ6aG7CXFMx30K6uF0GnKQF1pbA9JL4QSv52PFS4Giay+NV6S+WL6a5lzRX6Suxak6No2mOR3OV/hKr5rpzNM3VBcrbST0550hmvN5JwO7IhC6/Aq4H7vTeP95NvBQ4muZ6aFbMAe/9AufcucBU4AxgIrARUkkvjhRmZ3vvj3PODSDzKi0cAbPPez/onFsTmAtsAzwCPAM8hsyL8ijwsPf+uRwco8ZLgaNpDqY5qL9YvpjmHtZcu7olBY6mOVrNVrfUgKNpDqO5SuvJ4EjDnHPbATOBtYAPA+OR8UzPAE8Dx3nvX+0WXgocTXM9NCvm7cCx3vvLMvsGgI8B/wzc5r2f53RC2FGwQlfyUeOlwNE0h9GcwQ7iL5YvprlXNWdwa1O3pMDRNMepOYNrdUsPczTNYdtilZjvQneVTifk7frKwM7AMcib9pvQLmzdxkuBo2nufc3IJEiHldU2DObtwPZN+waATwBHAlvrvv5ewEuBo2kOpjmov1i+mOYe1ly7uiUFjqY5Ws1Wt9SAo2kOo7mK1Os9R1YBlgMc8KD3/mXdPwkZs/eQczJMoRt4KXA0zb2tOXuMc+7TwNHI+NZ5ivtaXo3D4O8DLOu9n9sOTip4VWDGjlcFZqx4VfqL5UsYi51jXTTXvW6pAjN2vCowY8cLhWl1S/04muZ4baDbBEJbo4Bxzu0MHATMQNYG3xU43zm3lJfhBn8t8vAYCi8Fjqa5HprV+oCFzrmjkLcVk4FJyKRfLznn5gOPA1d575/NgUfTuZ8F9nXOLUbJSj52vBQ4muYwmgnsL5YvprkMXiIca1e3pMDRNMepGatbasHRNAdri1VqPdlzxDk3GbgbONN7P9c59xawuff+DufcXCQodJT3/s1u4KXA0TTXQ3MG9x1kJvSrgdWQ1XDWAaYhc5rs4b2/qamQGw6r33vfXMk/C7wAvIRMvFSkko8aLwWOpjmM5gx2EH+xfDHNvao5g1ubuiUFjqY5Ts0ZXKtbepijaQ7bFqvMfBfH9IRO6BglZBWPB/T7psDzwHjd3hmZzKjjeClwNM310NyEPR2J4E5t8dsKwE7AuBK47wB7IpX65sAhwC+AG4CngE31ONcLeClwNM1B8IL7i+WLae5FzdS0bkmBo2mOTzNWt9SGo2kOo7mq1NWTBxcz1BPmGOAi/X4icHHmmKOBefp9xAlfQuOlwNE010OzHtOnnxsClwD7jvafvInAlXzseClwNM3t4VXlL5YvprnXNFflKzFrTomjaY5Lc1X+ErPmOnM0zWE0V5n66CHzepWBXwMznXMfRaJTlwI456YD2yKTHHUcLwWOprkemhuw+vl5YGPgOOfcac65XZxzMwrgvGfOuUaZsgLwJvDZ953U+2e897/23r+TOl4KHE1zGM0E9hfLF9NcBi8RjrWrW1LgaJrj1IzVLbXgaJqDtcUqt4FuEwhljfF3TtZGng9ci6yjPBNY1Tm3N7AH8AbwS/3bYKfwUuBomuuhuWHevxdwuQYZ+7cWsAawHvCuc+4Z4GngOC8Tveax5kp+U+fcesD1wJ3e+8dz4qSClwJH0xxAcwX+YvlimntSc03rlhQ4muYINVvdUhuOpjmM5sqt5yZkdc4dDKwIHICMZ9oGWAIYBzyJdFd7rlt4KXA0zfXQ3AK/D5nwaxYyAdhqyHLBm3vvRw22NGFthwRv1lLM8cC7QJlGcfR4KXA0zWE0Z7CD+Ivli2kug5cKR8WtTd2SAkfTHKfmDK7VLT3M0TSHbYtVYT0THHHOreu9v885dycyD8Nc3T8JicA+AfQDrwDv+lGEh8ZLgaNprofmFvirIBWvQ5bVejmDP917/1Cj90oRXMUI1ihOAS8Fjqa57YezSvzF8sU095rmOtctKXA0zXFptrqlXhxNcxjNlZiPYOKTdhMyPOhW4HzgH8BBSIRqYtNxfwR26jReChxNcz00Z45vBEZ3Bu5EAiuDwC66f6nmY4skYBWkC90mwDKZ/ZOANYrixo6XAkfTXB6vSn+xfDHNvaS5Sl+JVXNqHE1zPJqr9JdYNdedo2kOo7nK1HUCQUTA0sB/IjPhDgL3IuOZzgIOBeZo+huwZKfxUuBomuuhuQl7MrK2+GG6/Rawvn6fC3wfWKIAXtBKPna8FDia5nCNzZD+YvlimntVc2hfSUhz1BxNc5yaQ/tLCprryNE0hw+UV5m6TiCoGPgUcIJ+fh+4CrgLuA94CDhTj+vrBl4KHE1z72tGl/kFdgce0O+bAs8D43V7Z+C2Ej4YulEcNV4KHE1z243NSvzF8sU095rmqnwlZs0pcTTNcWmuyl9i1lxnjqY5jOZOpK4TqFwgrAr8K9KdZ5zuKx2hCo2XAkfT3FuaGYrmHgNcpN9PROYzaRxzNDBPv/fnwAxayceOlwJH0xxMc1B/sXwxzT2suXZ1SwocTXO0mq1uqQFH0xw2UN6J1DNL+Q5n3vtHgEea9vlY8KrAjB2vCszY8arALIuXOebXwOeccx8FNgdOAXDOTQe2Bc4pQKcxkdJqwKP6fUfgd977t3X7o8g65zjn+r33CxPGS4GjaW4frwp/sXxpHy8FjrXTXNO6JQWOprl9vOCYVrfUhqNpbh+vo9bXbQJmZmadM+ec088BYD5wLXAGMsnrqs65vYHzgDeAX+rfBltALWJNlfzMTCV/qZ6vUclflodn7HgpcDTN7eNV4S+WL6a5DF7sHOtat6TA0TTHp9nqlvpwNM1hNHfUfATdVyxZstTZBBwM/BRZDedw4CbgbuAPwOXIsnF5sRpdQweAxYH/Bn6HrK5zHLA3cCMykexy2f+kiJcCR9McRnNof7F8Mc29qjm0r6SiOXaOpjlOzaH9JQXNdeRomsO2xTqVGgLMzMxqYM65db339znn7kTGtc7V/ZOANYAngH5kVul3fYECwjl3MLAicABwCLANsAQwDngS2Nd7/1yv4KXA0TS3h1eVv1i+mOYyeDFzrHPdkgJH0xyXZqtb6sXRNIfR3DHrdnTGkiVLnUlIFPdW4HwkgnsQ0oVzYtNxfwR2KoC7rn7eic5IrduTgA2AZYEPAGPI9yYlarwUOJrmIHjB/cXyxTT3ouYqfCV2zalwNM3xaa7CX2LXXFeOpjmM5k6nrhOwZMlSZxKwNPCfwDxk7Oq9SLe2s4BDgTma/gYsmRMzaCUfO14KHE1zMM1B/cXyxTT3sOba1S0pcDTN0Wq2uqUGHE1zuEB5p1PPr1ZjZmYm5r1/Bfiec+5TwD3AdcCWyIzRM5FCbSxwgff+dedcn/d+tAnzJgFXA5vo/78AfBp42jk3H5loDGAp4JocNGPHS4GjaQ6guQJ/sXwxzT2puaZ1SwocTXOEmq1uqQ1H0xxGc8fN5hwxMzPDObcqUjG/ANzlvX/HOed8zgJCK/ktWLSSn8JQJX+b937vnI3i6PFS4Giaw2ge5jyl/cXyxTT3quZhztHTdUsKHE1znJqHOYfVLT3G0TRX1xaryiw4YmZmVom12yhODS8FjqY5jObQZvlimsvgpcIxtKWgOXaOpjlOzaEtBc115Gia4/OVrFlwxMzMzMzMzMzMzMzMzMzMrNbW120CZmZmZmZmZmZmZmZmZmZmZt00C46YmZmZmZmZmZmZmZmZmZnV2iw4YmZmZmZmZmZmZmZmZmZmVmuz4IiZmZmZmZmZmZmZmZmZmVmtzYIjZmZmZmZmZmZmZmZmZmZmtTYLjpiZmZmZmZmZmZmZmZmZmdXa/j8ncOXfxbzq6wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1332x756 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp/e0\n",
      "Area under surface (rectangular approx) =  169.34850753288674\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.2372213562207848\n",
      "MSE =  1.0183695859015682\n",
      "temp/e1\n",
      "Area under surface (rectangular approx) =  186.34618691923475\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.179326358755108\n",
      "MSE =  1.0225596945520907\n",
      "temp/e2\n",
      "Area under surface (rectangular approx) =  173.48398062177495\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.208571398859526\n",
      "MSE =  1.0207502821974928\n",
      "temp/e3\n",
      "Area under surface (rectangular approx) =  190.16692374763036\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.175574725180831\n",
      "MSE =  1.0297381697492531\n",
      "temp/e4\n",
      "Area under surface (rectangular approx) =  171.03829852047332\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.2384767552930778\n",
      "MSE =  1.0276469498592895\n",
      "temp/e5\n",
      "Area under surface (rectangular approx) =  174.07000820473394\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.1544459794268613\n",
      "MSE =  1.0165690660513158\n",
      "temp/e6\n",
      "Area under surface (rectangular approx) =  174.02094790585915\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.1963463866900885\n",
      "MSE =  1.020355097925651\n",
      "temp/e7\n",
      "Area under surface (rectangular approx) =  165.77002022453604\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.1693510004657846\n",
      "MSE =  1.02841957948096\n",
      "temp/e8\n",
      "Area under surface (rectangular approx) =  181.04803659905676\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.172078332648811\n",
      "MSE =  1.02406164500183\n",
      "temp/e9\n",
      "Area under surface (rectangular approx) =  177.01148154971406\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.209352696911997\n",
      "MSE =  1.0198641002663889\n",
      "temp/e10\n",
      "Area under surface (rectangular approx) =  182.74545010492665\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.2306051986963884\n",
      "MSE =  1.015833541365144\n",
      "temp/e11\n",
      "Area under surface (rectangular approx) =  181.45846336206975\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.2174159923927284\n",
      "MSE =  1.0228881280790285\n",
      "temp/e12\n",
      "Area under surface (rectangular approx) =  184.23222741686664\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.2360195522706827\n",
      "MSE =  1.022176494137974\n",
      "temp/e13\n",
      "Area under surface (rectangular approx) =  178.21027693189836\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.182999757233769\n",
      "MSE =  1.0252807120169822\n",
      "temp/e14\n",
      "Area under surface (rectangular approx) =  169.52104578136337\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.125333234608336\n",
      "MSE =  1.0191538505113653\n",
      "temp/e15\n",
      "Area under surface (rectangular approx) =  194.08791344376493\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.1578907847266433\n",
      "MSE =  1.019685151884861\n",
      "temp/e16\n",
      "Area under surface (rectangular approx) =  174.71617279051497\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.2639612379759115\n",
      "MSE =  1.0096089679610316\n",
      "temp/e17\n",
      "Area under surface (rectangular approx) =  180.65704998384672\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.2307387310608586\n",
      "MSE =  1.0210902034415297\n",
      "temp/e18\n",
      "Area under surface (rectangular approx) =  177.4683902715863\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.2612234389897683\n",
      "MSE =  1.0215017752491613\n",
      "temp/e19\n",
      "Area under surface (rectangular approx) =  184.14720288037432\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.2135078797107113\n",
      "MSE =  1.0108701832679738\n",
      "temp/e20\n",
      "Area under surface (rectangular approx) =  183.31875786480222\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.2473188581411874\n",
      "MSE =  1.012057597882592\n",
      "temp/e21\n",
      "Area under surface (rectangular approx) =  188.33510816341072\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.2528951508354726\n",
      "MSE =  1.0141548542717436\n",
      "temp/e22\n",
      "Area under surface (rectangular approx) =  190.36025898333764\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.246418340427277\n",
      "MSE =  1.017071866992989\n",
      "temp/e23\n",
      "Area under surface (rectangular approx) =  176.83864122714934\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.222639122511261\n",
      "MSE =  1.0268484618126574\n",
      "temp/e24\n",
      "Area under surface (rectangular approx) =  177.83733362610803\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.204858834990136\n",
      "MSE =  1.018698571639427\n",
      "temp/e25\n",
      "Area under surface (rectangular approx) =  172.06899566742504\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.171704288677721\n",
      "MSE =  1.023702034651491\n",
      "temp/e26\n",
      "Area under surface (rectangular approx) =  176.98273017735465\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.1486686745003927\n",
      "MSE =  1.0211334358622766\n",
      "temp/e27\n",
      "Area under surface (rectangular approx) =  174.98266925080577\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.1526653916047693\n",
      "MSE =  1.0190655938619653\n",
      "temp/e28\n",
      "Area under surface (rectangular approx) =  178.50613315611486\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.2225854678117574\n",
      "MSE =  1.0222138613268044\n",
      "temp/e29\n",
      "Area under surface (rectangular approx) =  183.7043371342271\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.224828957787148\n",
      "MSE =  1.0222475309742844\n",
      "temp/e30\n",
      "Area under surface (rectangular approx) =  173.09827420769304\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.1314504061348134\n",
      "MSE =  1.0158669381686682\n",
      "temp/e31\n",
      "Area under surface (rectangular approx) =  180.5069149050623\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.26790010213814\n",
      "MSE =  1.0209519048536382\n",
      "temp/e32\n",
      "Area under surface (rectangular approx) =  201.1575015231775\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.224090013637821\n",
      "MSE =  1.030324250671738\n",
      "temp/e33\n",
      "Area under surface (rectangular approx) =  180.2923772841688\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.0949944779574277\n",
      "MSE =  1.0213257259648967\n",
      "temp/e34\n",
      "Area under surface (rectangular approx) =  179.19038745567758\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.1303929235803847\n",
      "MSE =  1.0192642995866497\n",
      "temp/e35\n",
      "Area under surface (rectangular approx) =  180.3235153328317\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.117910613044549\n",
      "MSE =  1.0232492539826257\n",
      "temp/e36\n",
      "Area under surface (rectangular approx) =  167.8242299104489\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.215489100462113\n",
      "MSE =  1.0261976229644894\n",
      "temp/e37\n",
      "Area under surface (rectangular approx) =  174.49039517093922\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.2440312024366253\n",
      "MSE =  1.0156631659080266\n",
      "temp/e38\n",
      "Area under surface (rectangular approx) =  179.8734315035608\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.2405250485708206\n",
      "MSE =  1.0171062244375655\n",
      "temp/e39\n",
      "Area under surface (rectangular approx) =  175.3686166359246\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.2190909485552974\n",
      "MSE =  1.0222912216272766\n",
      "temp/e40\n",
      "Area under surface (rectangular approx) =  188.0771765358296\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.167250700651094\n",
      "MSE =  1.018855625721344\n",
      "temp/e41\n",
      "Area under surface (rectangular approx) =  170.02751508323396\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.196321670988308\n",
      "MSE =  1.0180152540517973\n",
      "temp/e42\n",
      "Area under surface (rectangular approx) =  180.30556729825008\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.1617175110086233\n",
      "MSE =  1.022334884281449\n",
      "temp/e43\n",
      "Area under surface (rectangular approx) =  168.3794612630236\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.185949665536935\n",
      "MSE =  1.0297131880124812\n",
      "temp/e44\n",
      "Area under surface (rectangular approx) =  173.87313286907943\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.1738143718727128\n",
      "MSE =  1.0220097531218382\n",
      "temp/e45\n",
      "Area under surface (rectangular approx) =  173.56680497295142\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.2264640230101786\n",
      "MSE =  1.030390640235223\n",
      "temp/e46\n",
      "Area under surface (rectangular approx) =  184.37179914140023\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.1714778986961254\n",
      "MSE =  1.0304456986301591\n",
      "temp/e47\n",
      "Area under surface (rectangular approx) =  180.5807549594589\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.0543141747018985\n",
      "MSE =  1.0251976328761592\n",
      "temp/e48\n",
      "Area under surface (rectangular approx) =  174.85358142732645\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.200791155825041\n",
      "MSE =  1.020537873514739\n",
      "temp/e49\n",
      "Area under surface (rectangular approx) =  185.8079353428218\n",
      "Violations =  0.0\n",
      "Average_violations =  -2.2705045810890243\n",
      "MSE =  1.0193874919291583\n"
     ]
    }
   ],
   "source": [
    "metric = []\n",
    "metric_err = []\n",
    "viol = []\n",
    "viol_err = []\n",
    "\n",
    "#normalize the violations for prettier graphing.\n",
    "#also violations are always positive, so just divide by max.\n",
    "\n",
    "#TMK\n",
    "#violation_mean = violation_mean / np.max(violation_mean)\n",
    "\n",
    "for i in range(len(violations)):\n",
    "    print(\"Model_name = \", model_names[i], \"Violations = \", violations[i])\n",
    "    print(\"Average_violations = \", np.mean(violation_mean[i]), np.std(violation_mean[i]))\n",
    "    print(\"MSE = \", np.mean(mean[i]), np.std(mean[i]))\n",
    "    #print(\"mean = \", mean[i])\n",
    "    metric.append(np.mean(mean[i]))\n",
    "    metric_err.append(np.std(mean[i]))\n",
    "    viol.append(np.mean(violation_mean[i]))\n",
    "    #viol.append(violations[i]/times)\n",
    "    viol_err.append(np.std(violation_mean[i]))\n",
    "print(np.array(metric), \n",
    "         np.array(metric_err), \n",
    "         np.array(viol), \n",
    "         np.array(viol_err))    \n",
    "\n",
    "bar_plot(model_names, \n",
    "         np.array(metric), \n",
    "         np.array(metric_err), \n",
    "         np.array(viol), \n",
    "         np.array(viol_err))\n",
    "\n",
    "\n",
    "def heat_plot(x,y,z, xlab = 'Mean', ylab = 'Variance', clim_low = 0, clim_high = 1):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    cax = ax.scatter(x, y, c=z, s=450, edgecolor='')\n",
    "    cax.set_clim(clim_low, clim_high)\n",
    "    ax.set_xlabel(xlab)\n",
    "    ax.set_ylabel(ylab)\n",
    "    plt.colorbar(cax)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "MSE = []\n",
    "VIO = []\n",
    "VIO2 = []\n",
    "AUS = []\n",
    "for i, m in enumerate(models):\n",
    "    print(model_names[i])\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "    \n",
    "    \n",
    "    rectangular_approx = 0\n",
    "    for k, v in metrics_dicts[i].items():\n",
    "        x.append(float(k.split('_')[0]))\n",
    "        y.append(float(k.split('_')[-1]))\n",
    "        z.append(np.mean(v))\n",
    "        rectangular_approx += np.mean(v)\n",
    "    print(\"Area under surface (rectangular approx) = \", rectangular_approx)\n",
    "    print(\"Violations = \", violations[i])\n",
    "    print(\"Average_violations = \", np.mean(violation_mean[i]))\n",
    "    print(\"MSE = \", np.mean(mean[i]))   \n",
    "    MSE.append(np.mean(mean[i]))\n",
    "    VIO.append(np.mean(violation_mean[i]))\n",
    "    VIO2.append(np.mean(violation_mean2[i]))\n",
    "    #VIO.append(violations[i]/times)\n",
    "    AUS.append(rectangular_approx)\n",
    "    \n",
    "    #heat_plot(x,y,z, clim_low = 0, clim_high = 10)\n",
    "    \n",
    "#heat_plot(MSE,VIO,AUS, xlab = 'MSE', ylab='Violations', clim_low = np.min(AUS), clim_high = np.max(AUS))\n",
    "    \n",
    "#VIO = np.abs(VIO)\n",
    "#VIO2 = np.abs(VIO2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1016710089917603\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.10183310075656228\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145.29687221918806 -15.290094570111648\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0701267567273836 0.02224490869871441\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177.46710462358323 -29.50299311513749\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.09699243967425872"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.polynomial.polynomial import polyfit  \n",
    "from scipy.stats import pearsonr\n",
    "from pylab import text\n",
    "\n",
    "\n",
    "print(pearsonr(VIO,AUS)[0])\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(VIO,AUS, 1)\n",
    "ax.plot(VIO,AUS, '.')\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(VIO,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "plt.plot(VIO, b + m * np.array(VIO), '-')\n",
    "ax.set_xlabel(\"BIC\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "fig.savefig('Ex4VIOVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "METRIC = -(VIO/np.max(VIO)) + np.array(MSE)\n",
    "print(pearsonr(METRIC,AUS)[0])\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(METRIC,AUS, 1)\n",
    "ax.plot(METRIC,AUS, '.')\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(METRIC,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "plt.plot(METRIC, b + m * np.array(METRIC), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"BIC (normalized) + MSE\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "fig.savefig('Ex4ProposedVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(MSE,AUS, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(MSE,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(MSE,AUS, '.')\n",
    "plt.plot(MSE, b + m * np.array(MSE), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(VIO2,AUS, 1)\n",
    "print(b,m)\n",
    "ax.plot(VIO2,AUS, '.')\n",
    "plt.plot(VIO2, b + m * np.array(VIO2), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"Violations2\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(VIO,MSE, 1)\n",
    "print(b,m)\n",
    "ax.plot(VIO,MSE, '.')\n",
    "plt.plot(VIO, b + m * np.array(VIO), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"Violations\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(METRIC,AUS, 1)\n",
    "print(b,m)\n",
    "ax.plot(METRIC,AUS, '.')\n",
    "plt.plot(METRIC, b + m * np.array(METRIC), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"MSExVIO\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "plt.show()\n",
    "\n",
    "MSE = np.array(MSE)\n",
    "METRIC = VIO/np.max(VIO)+ MSE\n",
    "x = []\n",
    "y1 = []\n",
    "y2 = []\n",
    "y3 = []\n",
    "for split in range(10, len(AUS), 5):\n",
    "    #print(\"******\", split, \"*******\")\n",
    "    sorted_aus = [AUS for _,AUS in sorted(zip(VIO,AUS))]\n",
    "    sorted_mse = [MSE for _,MSE in sorted(zip(VIO,MSE))]\n",
    "\n",
    "    low = []\n",
    "    high = []\n",
    "    low = sorted_aus[:split]\n",
    "    high = sorted_aus[split:]\n",
    "\n",
    "    x.append(split)\n",
    "    \n",
    "    \n",
    "    #print(\"Low Violations = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High Violations = \", np.mean(high), \"for\", len(high))\n",
    "    y1.append(np.mean(low))\n",
    "    sorted_aus_by_mse = [AUS for _,AUS in sorted(zip(MSE,AUS))]\n",
    "    low = sorted_aus_by_mse[:split]\n",
    "    high = sorted_aus_by_mse[split:]\n",
    "    #print(\"Low AUS by MSE = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High AUS by MSE = \", np.mean(high), \"for\", len(high))\n",
    "    y2.append(np.mean(low))\n",
    "    sorted_aus = [AUS for _,AUS in sorted(zip(METRIC,AUS))]\n",
    "    sorted_mse = [MSE for _,MSE in sorted(zip(METRIC,MSE))]\n",
    "\n",
    "    low = []\n",
    "    high = []\n",
    "    low = sorted_aus[:split]\n",
    "    high = sorted_aus[split:]\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"Low Metric = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High Metric = \", np.mean(high), \"for\", len(high))\n",
    "    y3.append(np.mean(low))\n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x,y1, '-', label = 'Violations')\n",
    "ax.plot(x,y2, '-', label = 'MSE')\n",
    "ax.plot(x,y3, '-', label = 'METRIC')\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "plt.show()  \n",
    "pearsonr(METRIC,AUS)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
