{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes \n",
    "### Required installing Oracle JAVA 8 to get javabridge installed\n",
    "### Then, I was able to install py-causal from https://bd2kccd.github.io/docs/py-causal/\n",
    "### GFCI is slower than RFCI, but more accurate (SPIRTES), GFCI and RFCI account for unobserved variables, FGES assumes no unobserved variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure Learning Performance Guarantees If the assumptions in the previous section hold, then in the large sample limit, the CBN structure output by GFCId will contain an edge of one of four kinds between Xand Y   if and only if Xand Yare not independent conditional on any subset of the other measured variables of less than or equal to a specified size. In addition, there is (1) an arc X->Y   if and only if Xdirectly or indirectly causes Y, and Y   does not directly or indirectly cause X; (2) an edge X <-->Y   if and only if X   is not a direct or indirect cause of Yand Y   is not a direct or indirect cause of X(which can only occur if there are latent confounders of Xand some other variable or Yand some other variable; (3) an edge Xo->Y   only if Yis not a direct or indirect cause of X, but Xmay or may not be an indirect cause of Y; (4) an edge X oâ€“o Y   indicates that Xand Y   are dependent no matter what subset of observed variables is conditioned on, but contains no orientation information (X   may be a direct or indirect cause of Y, and Ymay be an indirect cause of X, or there may be a latent common cause of Xand Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying some various ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import configparser\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, \\\n",
    "                        Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "import keras.optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "import glob, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "# select your GPU Here\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" #Comment this line out if you want all GPUS (2 hehe)\n",
    "\n",
    "# python full-display web browser\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "def get_model(dense, dropouts, inputs):\n",
    "    # dense is an ordered list of the number of dense neurons like [1024, 2048, 1024]\n",
    "    # dropouts is an ordered list of the dropout masks like [0.2, 0.3, 0.4]\n",
    "    inputs = keras.Input(shape = (inputs,))\n",
    "\n",
    "    x = keras.layers.Dense(dense[0], activation = 'relu')(inputs)\n",
    "    x = keras.layers.Dropout(dropouts[0])(x, training=True)\n",
    "    for den, drop in zip(dense[1:], dropouts[1:]):\n",
    "        x = keras.layers.Dense(den, activation = 'relu')(x)\n",
    "        x = keras.layers.Dropout(drop)(x, training=True)\n",
    "    outputs = keras.layers.Dense(1, activation = 'linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512], [2048, 2048, 512]] ['temp/e0', 'temp/e1', 'temp/e2', 'temp/e3', 'temp/e4', 'temp/e5', 'temp/e6', 'temp/e7', 'temp/e8', 'temp/e9', 'temp/e10', 'temp/e11', 'temp/e12', 'temp/e13', 'temp/e14', 'temp/e15', 'temp/e16', 'temp/e17', 'temp/e18', 'temp/e19', 'temp/e20', 'temp/e21', 'temp/e22', 'temp/e23', 'temp/e24', 'temp/e25', 'temp/e26', 'temp/e27', 'temp/e28', 'temp/e29', 'temp/e30', 'temp/e31', 'temp/e32', 'temp/e33', 'temp/e34', 'temp/e35', 'temp/e36', 'temp/e37', 'temp/e38', 'temp/e39', 'temp/e40', 'temp/e41', 'temp/e42', 'temp/e43', 'temp/e44', 'temp/e45', 'temp/e46', 'temp/e47', 'temp/e48', 'temp/e49', 'temp/e50', 'temp/e51', 'temp/e52', 'temp/e53', 'temp/e54', 'temp/e55', 'temp/e56', 'temp/e57', 'temp/e58', 'temp/e59', 'temp/e60', 'temp/e61', 'temp/e62', 'temp/e63', 'temp/e64', 'temp/e65', 'temp/e66', 'temp/e67', 'temp/e68', 'temp/e69', 'temp/e70', 'temp/e71', 'temp/e72', 'temp/e73', 'temp/e74', 'temp/e75', 'temp/e76', 'temp/e77', 'temp/e78', 'temp/e79', 'temp/e80', 'temp/e81', 'temp/e82', 'temp/e83', 'temp/e84', 'temp/e85', 'temp/e86', 'temp/e87', 'temp/e88', 'temp/e89', 'temp/e90', 'temp/e91', 'temp/e92', 'temp/e93', 'temp/e94', 'temp/e95', 'temp/e96', 'temp/e97', 'temp/e98', 'temp/e99', 'temp/e100', 'temp/e101', 'temp/e102', 'temp/e103', 'temp/e104', 'temp/e105', 'temp/e106', 'temp/e107', 'temp/e108', 'temp/e109', 'temp/e110', 'temp/e111', 'temp/e112', 'temp/e113', 'temp/e114', 'temp/e115', 'temp/e116', 'temp/e117', 'temp/e118', 'temp/e119', 'temp/e120', 'temp/e121', 'temp/e122', 'temp/e123', 'temp/e124', 'temp/e125', 'temp/e126', 'temp/e127', 'temp/e128', 'temp/e129', 'temp/e130', 'temp/e131', 'temp/e132', 'temp/e133', 'temp/e134', 'temp/e135', 'temp/e136', 'temp/e137', 'temp/e138', 'temp/e139', 'temp/e140', 'temp/e141', 'temp/e142', 'temp/e143', 'temp/e144', 'temp/e145', 'temp/e146', 'temp/e147', 'temp/e148', 'temp/e149', 'temp/e150', 'temp/e151', 'temp/e152', 'temp/e153', 'temp/e154', 'temp/e155', 'temp/e156', 'temp/e157', 'temp/e158', 'temp/e159', 'temp/e160', 'temp/e161', 'temp/e162', 'temp/e163', 'temp/e164', 'temp/e165', 'temp/e166', 'temp/e167', 'temp/e168', 'temp/e169', 'temp/e170', 'temp/e171', 'temp/e172', 'temp/e173', 'temp/e174', 'temp/e175', 'temp/e176', 'temp/e177', 'temp/e178', 'temp/e179', 'temp/e180', 'temp/e181', 'temp/e182', 'temp/e183', 'temp/e184', 'temp/e185', 'temp/e186', 'temp/e187', 'temp/e188', 'temp/e189', 'temp/e190', 'temp/e191', 'temp/e192', 'temp/e193', 'temp/e194', 'temp/e195', 'temp/e196', 'temp/e197', 'temp/e198', 'temp/e199', 'temp/e200', 'temp/e201', 'temp/e202', 'temp/e203', 'temp/e204', 'temp/e205', 'temp/e206', 'temp/e207', 'temp/e208', 'temp/e209', 'temp/e210', 'temp/e211', 'temp/e212', 'temp/e213', 'temp/e214', 'temp/e215', 'temp/e216', 'temp/e217', 'temp/e218', 'temp/e219', 'temp/e220', 'temp/e221', 'temp/e222', 'temp/e223', 'temp/e224', 'temp/e225', 'temp/e226', 'temp/e227', 'temp/e228', 'temp/e229', 'temp/e230', 'temp/e231', 'temp/e232', 'temp/e233', 'temp/e234', 'temp/e235', 'temp/e236', 'temp/e237', 'temp/e238', 'temp/e239', 'temp/e240', 'temp/e241', 'temp/e242', 'temp/e243', 'temp/e244', 'temp/e245', 'temp/e246', 'temp/e247', 'temp/e248', 'temp/e249', 'temp/e250', 'temp/e251', 'temp/e252', 'temp/e253', 'temp/e254', 'temp/e255', 'temp/e256', 'temp/e257', 'temp/e258', 'temp/e259', 'temp/e260', 'temp/e261', 'temp/e262', 'temp/e263', 'temp/e264', 'temp/e265', 'temp/e266', 'temp/e267', 'temp/e268', 'temp/e269', 'temp/e270', 'temp/e271', 'temp/e272', 'temp/e273', 'temp/e274', 'temp/e275', 'temp/e276', 'temp/e277', 'temp/e278', 'temp/e279', 'temp/e280', 'temp/e281', 'temp/e282', 'temp/e283', 'temp/e284', 'temp/e285', 'temp/e286', 'temp/e287', 'temp/e288', 'temp/e289', 'temp/e290', 'temp/e291', 'temp/e292', 'temp/e293', 'temp/e294', 'temp/e295', 'temp/e296', 'temp/e297', 'temp/e298', 'temp/e299', 'temp/e300', 'temp/e301', 'temp/e302', 'temp/e303', 'temp/e304', 'temp/e305', 'temp/e306', 'temp/e307', 'temp/e308', 'temp/e309', 'temp/e310', 'temp/e311', 'temp/e312', 'temp/e313', 'temp/e314', 'temp/e315', 'temp/e316', 'temp/e317', 'temp/e318', 'temp/e319', 'temp/e320', 'temp/e321', 'temp/e322', 'temp/e323', 'temp/e324', 'temp/e325', 'temp/e326', 'temp/e327', 'temp/e328', 'temp/e329', 'temp/e330', 'temp/e331', 'temp/e332', 'temp/e333', 'temp/e334', 'temp/e335', 'temp/e336', 'temp/e337', 'temp/e338', 'temp/e339', 'temp/e340', 'temp/e341', 'temp/e342', 'temp/e343', 'temp/e344', 'temp/e345', 'temp/e346', 'temp/e347', 'temp/e348', 'temp/e349', 'temp/e350', 'temp/e351', 'temp/e352', 'temp/e353', 'temp/e354', 'temp/e355', 'temp/e356', 'temp/e357', 'temp/e358', 'temp/e359', 'temp/e360', 'temp/e361', 'temp/e362', 'temp/e363', 'temp/e364', 'temp/e365', 'temp/e366', 'temp/e367', 'temp/e368', 'temp/e369', 'temp/e370', 'temp/e371', 'temp/e372', 'temp/e373', 'temp/e374', 'temp/e375', 'temp/e376', 'temp/e377', 'temp/e378', 'temp/e379', 'temp/e380', 'temp/e381', 'temp/e382', 'temp/e383', 'temp/e384', 'temp/e385', 'temp/e386', 'temp/e387', 'temp/e388', 'temp/e389', 'temp/e390', 'temp/e391', 'temp/e392', 'temp/e393', 'temp/e394', 'temp/e395', 'temp/e396', 'temp/e397', 'temp/e398', 'temp/e399', 'temp/e400', 'temp/e401', 'temp/e402', 'temp/e403', 'temp/e404', 'temp/e405', 'temp/e406', 'temp/e407', 'temp/e408', 'temp/e409', 'temp/e410', 'temp/e411', 'temp/e412', 'temp/e413', 'temp/e414', 'temp/e415', 'temp/e416', 'temp/e417', 'temp/e418', 'temp/e419', 'temp/e420', 'temp/e421', 'temp/e422', 'temp/e423', 'temp/e424', 'temp/e425', 'temp/e426', 'temp/e427', 'temp/e428', 'temp/e429', 'temp/e430', 'temp/e431', 'temp/e432', 'temp/e433', 'temp/e434', 'temp/e435', 'temp/e436', 'temp/e437', 'temp/e438', 'temp/e439', 'temp/e440', 'temp/e441', 'temp/e442', 'temp/e443', 'temp/e444', 'temp/e445', 'temp/e446', 'temp/e447', 'temp/e448', 'temp/e449', 'temp/e450', 'temp/e451', 'temp/e452', 'temp/e453', 'temp/e454', 'temp/e455', 'temp/e456', 'temp/e457', 'temp/e458', 'temp/e459', 'temp/e460', 'temp/e461', 'temp/e462', 'temp/e463', 'temp/e464', 'temp/e465', 'temp/e466', 'temp/e467', 'temp/e468', 'temp/e469', 'temp/e470', 'temp/e471', 'temp/e472', 'temp/e473', 'temp/e474', 'temp/e475', 'temp/e476', 'temp/e477', 'temp/e478', 'temp/e479', 'temp/e480', 'temp/e481', 'temp/e482', 'temp/e483', 'temp/e484', 'temp/e485', 'temp/e486', 'temp/e487', 'temp/e488', 'temp/e489', 'temp/e490', 'temp/e491', 'temp/e492', 'temp/e493', 'temp/e494', 'temp/e495', 'temp/e496', 'temp/e497', 'temp/e498', 'temp/e499']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from pycausal import search as s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def discrete_gauss(low, high, samples, std = 20):\n",
    "    x = np.arange(low, high)\n",
    "    xU, xL = x + 0.5, x - 0.5 \n",
    "    prob = ss.norm.cdf(xU, scale = std) - ss.norm.cdf(xL, scale = std)\n",
    "    prob = prob / prob.sum() #normalize the probabilities so their sum is 1\n",
    "    nums = np.random.choice(x, size = samples, p = prob)\n",
    "    return nums\n",
    "\n",
    "\n",
    "\n",
    "def bar_plot(x_ax, val1, val1std, val2, val2std):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ## the data\n",
    "    N = len(x_ax)\n",
    "\n",
    "    ## necessary variables\n",
    "    ind = np.arange(N)                # the x locations for the groups\n",
    "    width = 0.35                      # the width of the bars\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    ## the bars\n",
    "    rects1 = ax.bar(ind, val1, width,\n",
    "                    color='gray',\n",
    "                    yerr=val1std,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='blue'))\n",
    "\n",
    "    rects2 = ax.bar(ind+width, val2, width,\n",
    "                        color='blue',\n",
    "                        #yerr=val2std,\n",
    "                        error_kw=dict(elinewidth=2,ecolor='gray'))\n",
    "\n",
    "    # axes and labels\n",
    "    ax.set_xlim(-width,len(ind)+width)\n",
    "    #ax.set_ylim(0,45)\n",
    "    ax.set_ylabel('Percentage')\n",
    "    ax.set_title('')\n",
    "    plt.xticks(ind + width / 2, x_ax, rotation=75, size = 14)\n",
    "    ## add a legend\n",
    "    ax.legend( (rects1[0], rects2[0]), ('Accuracy', '% Violations') )\n",
    "    fig.savefig(\"violations.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 5000):\n",
    "    a = np.random.gumbel(mean, var, SIZE)\n",
    "    b = np.random.gumbel(mean, var, SIZE)\n",
    "    c = np.random.gumbel(mean, var, SIZE)\n",
    "    d = np.random.gumbel(mean, var, SIZE)\n",
    "    e = np.random.gumbel(mean, var, SIZE)\n",
    "    f= a + b + c + d + e + np.random.gumbel(mean, var, SIZE)\n",
    "    g = f + np.random.gumbel(mean,var, SIZE)\n",
    "    g = np.rint(g)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 5000):\n",
    "    a = np.random.gumbel(mean, var, SIZE)\n",
    "    b = np.random.gumbel(mean, var, SIZE)\n",
    "    c = np.random.gumbel(mean, var, SIZE)\n",
    "    d = np.random.gumbel(mean, var, SIZE)\n",
    "\n",
    "    f= a + b + c + d + np.random.gumbel(mean, var, SIZE)\n",
    "    g = f + np.random.gumbel(mean,var, SIZE)\n",
    "    \n",
    "    \n",
    "    g = np.rint(g)\n",
    "    e = g + np.random.gumbel(mean,var,SIZE)\n",
    "    \n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 400000):\n",
    "    f = np.random.normal(mean, var, SIZE)\n",
    "    a = f + np.random.normal(mean, var, SIZE)\n",
    "    b = f + np.random.normal(mean, var, SIZE)\n",
    "    c = f + np.random.normal(mean, var, SIZE)\n",
    "    d = f + np.random.normal(mean, var, SIZE)\n",
    "    e = f + np.random.normal(mean, var, SIZE)\n",
    "    g = a + b + c + d  + e + np.random.normal(mean, var, SIZE)\n",
    "\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 20000):\n",
    "    a = np.random.normal(mean, var, SIZE)\n",
    "    b = np.random.normal(mean, var, SIZE)\n",
    "    c = np.random.normal(mean, var, SIZE)\n",
    "    d = np.random.normal(mean, var, SIZE)\n",
    "    e = np.random.normal(mean, var, SIZE)\n",
    "    f= a + b + c + d + e + np.random.normal(mean, var, SIZE)\n",
    "    g = f + np.random.normal(mean,var, SIZE)\n",
    "    #g = np.rint(g)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 20000):\n",
    "    a = np.random.normal(mean, var, SIZE)\n",
    "    b = a + np.random.normal(mean, var, SIZE)\n",
    "    c = a + np.random.normal(mean, var, SIZE)\n",
    "    d = a + np.random.normal(mean, var, SIZE)\n",
    "    e = a + np.random.normal(mean, var, SIZE)\n",
    "    f= a + np.random.normal(mean, var, SIZE)\n",
    "    g = f + np.random.normal(mean,var, SIZE)\n",
    "    #g = np.rint(g)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def get_CG(df, tetrad):\n",
    "    tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "    #tetrad.run(algoId = 'fges-mb', targetName = 'g', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "    #       structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "    #       completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "def get_MB(graph, var, pc):\n",
    "    parents = set()\n",
    "    children = set()\n",
    "    for i in pc.extractTetradGraphEdges(graph):\n",
    "        if i[-1] == var and i[3:5] == '->':\n",
    "            parents.add(i[0])\n",
    "        if i[0] == var and i[3:5] == '->':\n",
    "            children.add(i[-1])\n",
    "    return parents, children\n",
    "\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from collections import defaultdict\n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '5000M')\n",
    "tetrad = s.tetradrunner()\n",
    "\n",
    "\n",
    "verbosity = 1\n",
    "\n",
    "\n",
    "\n",
    "models = []\n",
    "model_names = []\n",
    "\n",
    "num_models = 500\n",
    "model_layers = [2048, 2048, 512]\n",
    "for i in range(num_models):\n",
    "    models.append(model_layers)\n",
    "    model_names.append('temp/e' + str(i))\n",
    "\n",
    "print(models, model_names)\n",
    "\n",
    "\n",
    "df = gen_data()\n",
    "X = df[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "y = df['g'].values\n",
    "\n",
    "val_df = gen_data(SIZE = 2000)\n",
    "x_val = df[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "y_val = df['g'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp/e0\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 4s 183us/step - loss: 1.1087 - mean_squared_error: 1.1087 - val_loss: 1.0126 - val_mean_squared_error: 1.0126\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01262, saving model to temp/e0\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0213 - mean_squared_error: 1.0213 - val_loss: 1.0133 - val_mean_squared_error: 1.0133\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01262\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0164 - mean_squared_error: 1.0164 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01262 to 1.00605, saving model to temp/e0\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0136 - mean_squared_error: 1.0136 - val_loss: 1.0096 - val_mean_squared_error: 1.0096\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00605\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0073 - mean_squared_error: 1.0073 - val_loss: 1.0107 - val_mean_squared_error: 1.0107\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00605\n",
      "Epoch 00005: early stopping\n",
      "temp/e1\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0949 - mean_squared_error: 1.0949 - val_loss: 1.0223 - val_mean_squared_error: 1.0223\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02225, saving model to temp/e1\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0214 - mean_squared_error: 1.0214 - val_loss: 1.0063 - val_mean_squared_error: 1.0063\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02225 to 1.00635, saving model to temp/e1\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00635\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0134 - mean_squared_error: 1.0134 - val_loss: 1.0177 - val_mean_squared_error: 1.0177\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00635\n",
      "Epoch 00004: early stopping\n",
      "temp/e2\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.0951 - mean_squared_error: 1.0951 - val_loss: 1.0129 - val_mean_squared_error: 1.0129\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01290, saving model to temp/e2\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0204 - mean_squared_error: 1.0204 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01290 to 1.00884, saving model to temp/e2\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0152 - mean_squared_error: 1.0152 - val_loss: 1.0105 - val_mean_squared_error: 1.0105\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00884\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 1.0096 - val_mean_squared_error: 1.0096\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00884\n",
      "Epoch 00004: early stopping\n",
      "temp/e3\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1004 - mean_squared_error: 1.1004 - val_loss: 1.0179 - val_mean_squared_error: 1.0179\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01792, saving model to temp/e3\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 1.0164 - val_mean_squared_error: 1.0164\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01792 to 1.01641, saving model to temp/e3\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0168 - mean_squared_error: 1.0168 - val_loss: 1.0122 - val_mean_squared_error: 1.0122\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01641 to 1.01220, saving model to temp/e3\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0123 - mean_squared_error: 1.0123 - val_loss: 1.0019 - val_mean_squared_error: 1.0019\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01220 to 1.00192, saving model to temp/e3\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0125 - mean_squared_error: 1.0125 - val_loss: 1.0025 - val_mean_squared_error: 1.0025\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00192\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 1.0006 - val_mean_squared_error: 1.0006\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00192 to 1.00057, saving model to temp/e3\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0007 - val_mean_squared_error: 1.0007\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00057\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 0.9966 - val_mean_squared_error: 0.9966\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00057 to 0.99656, saving model to temp/e3\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0049 - mean_squared_error: 1.0049 - val_loss: 1.0222 - val_mean_squared_error: 1.0222\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99656\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0047 - mean_squared_error: 1.0047 - val_loss: 1.0003 - val_mean_squared_error: 1.0003\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99656\n",
      "Epoch 00010: early stopping\n",
      "temp/e4\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 1.1006 - mean_squared_error: 1.1006 - val_loss: 1.0153 - val_mean_squared_error: 1.0153\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01528, saving model to temp/e4\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0193 - mean_squared_error: 1.0193 - val_loss: 1.0115 - val_mean_squared_error: 1.0115\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01528 to 1.01145, saving model to temp/e4\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0149 - mean_squared_error: 1.0149 - val_loss: 1.0094 - val_mean_squared_error: 1.0094\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01145 to 1.00940, saving model to temp/e4\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0123 - mean_squared_error: 1.0123 - val_loss: 1.0083 - val_mean_squared_error: 1.0083\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00940 to 1.00834, saving model to temp/e4\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0128 - mean_squared_error: 1.0128 - val_loss: 1.0041 - val_mean_squared_error: 1.0041\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00834 to 1.00405, saving model to temp/e4\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00405\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.0043 - val_mean_squared_error: 1.0043\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00405\n",
      "Epoch 00007: early stopping\n",
      "temp/e5\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1054 - mean_squared_error: 1.1054 - val_loss: 1.0192 - val_mean_squared_error: 1.0192\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01916, saving model to temp/e5\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0195 - mean_squared_error: 1.0195 - val_loss: 1.0131 - val_mean_squared_error: 1.0131\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01916 to 1.01307, saving model to temp/e5\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0165 - mean_squared_error: 1.0165 - val_loss: 1.0094 - val_mean_squared_error: 1.0094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss improved from 1.01307 to 1.00941, saving model to temp/e5\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0045 - val_mean_squared_error: 1.0045\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00941 to 1.00453, saving model to temp/e5\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0112 - mean_squared_error: 1.0112 - val_loss: 1.0032 - val_mean_squared_error: 1.0032\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00453 to 1.00315, saving model to temp/e5\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00315\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0017 - val_mean_squared_error: 1.0017\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00315 to 1.00172, saving model to temp/e5\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0052 - mean_squared_error: 1.0052 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00172\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 0.9955 - val_mean_squared_error: 0.9955\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00172 to 0.99550, saving model to temp/e5\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0039 - mean_squared_error: 1.0039 - val_loss: 1.0006 - val_mean_squared_error: 1.0006\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99550\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0053 - mean_squared_error: 1.0053 - val_loss: 1.0021 - val_mean_squared_error: 1.0021\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99550\n",
      "Epoch 00011: early stopping\n",
      "temp/e6\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.1007 - mean_squared_error: 1.1007 - val_loss: 1.0267 - val_mean_squared_error: 1.0267\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02672, saving model to temp/e6\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0186 - mean_squared_error: 1.0186 - val_loss: 1.0118 - val_mean_squared_error: 1.0118\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02672 to 1.01181, saving model to temp/e6\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0184 - mean_squared_error: 1.0184 - val_loss: 1.0130 - val_mean_squared_error: 1.0130\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01181\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0039 - val_mean_squared_error: 1.0039\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01181 to 1.00390, saving model to temp/e6\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0112 - mean_squared_error: 1.0112 - val_loss: 1.0045 - val_mean_squared_error: 1.0045\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00390\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0063 - mean_squared_error: 1.0063 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00390\n",
      "Epoch 00006: early stopping\n",
      "temp/e7\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1093 - mean_squared_error: 1.1093 - val_loss: 1.0176 - val_mean_squared_error: 1.0176\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01757, saving model to temp/e7\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0177 - mean_squared_error: 1.0177 - val_loss: 1.0128 - val_mean_squared_error: 1.0128\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01757 to 1.01282, saving model to temp/e7\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0148 - mean_squared_error: 1.0148 - val_loss: 1.0060 - val_mean_squared_error: 1.0060\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01282 to 1.00596, saving model to temp/e7\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0141 - mean_squared_error: 1.0141 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00596 to 1.00229, saving model to temp/e7\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0103 - mean_squared_error: 1.0103 - val_loss: 1.0100 - val_mean_squared_error: 1.0100\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00229\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0073 - val_mean_squared_error: 1.0073\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00229\n",
      "Epoch 00006: early stopping\n",
      "temp/e8\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1057 - mean_squared_error: 1.1057 - val_loss: 1.0218 - val_mean_squared_error: 1.0218\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02182, saving model to temp/e8\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0187 - mean_squared_error: 1.0187 - val_loss: 1.0076 - val_mean_squared_error: 1.0076\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02182 to 1.00764, saving model to temp/e8\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0174 - mean_squared_error: 1.0174 - val_loss: 1.0044 - val_mean_squared_error: 1.0044\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00764 to 1.00441, saving model to temp/e8\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0159 - mean_squared_error: 1.0159 - val_loss: 1.0079 - val_mean_squared_error: 1.0079\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00441\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0086 - mean_squared_error: 1.0086 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00441\n",
      "Epoch 00005: early stopping\n",
      "temp/e9\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0952 - mean_squared_error: 1.0952 - val_loss: 1.0210 - val_mean_squared_error: 1.0210\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02097, saving model to temp/e9\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0229 - mean_squared_error: 1.0229 - val_loss: 1.0128 - val_mean_squared_error: 1.0128\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02097 to 1.01277, saving model to temp/e9\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0156 - mean_squared_error: 1.0156 - val_loss: 1.0164 - val_mean_squared_error: 1.0164\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01277\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0134 - mean_squared_error: 1.0134 - val_loss: 1.0129 - val_mean_squared_error: 1.0129\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.01277\n",
      "Epoch 00004: early stopping\n",
      "temp/e10\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1035 - mean_squared_error: 1.1035 - val_loss: 1.0141 - val_mean_squared_error: 1.0141\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01414, saving model to temp/e10\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0180 - mean_squared_error: 1.0180 - val_loss: 1.0095 - val_mean_squared_error: 1.0095\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01414 to 1.00952, saving model to temp/e10\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0136 - mean_squared_error: 1.0136 - val_loss: 1.0097 - val_mean_squared_error: 1.0097\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00952\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0114 - mean_squared_error: 1.0114 - val_loss: 1.0129 - val_mean_squared_error: 1.0129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00952\n",
      "Epoch 00004: early stopping\n",
      "temp/e11\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1036 - mean_squared_error: 1.1036 - val_loss: 1.0204 - val_mean_squared_error: 1.0204\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02044, saving model to temp/e11\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0220 - mean_squared_error: 1.0220 - val_loss: 1.0116 - val_mean_squared_error: 1.0116\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02044 to 1.01158, saving model to temp/e11\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0070 - val_mean_squared_error: 1.0070\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01158 to 1.00700, saving model to temp/e11\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0164 - mean_squared_error: 1.0164 - val_loss: 1.0092 - val_mean_squared_error: 1.0092\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00700\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0106 - mean_squared_error: 1.0106 - val_loss: 1.0012 - val_mean_squared_error: 1.0012\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00700 to 1.00119, saving model to temp/e11\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0086 - mean_squared_error: 1.0086 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00119\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0027 - val_mean_squared_error: 1.0027\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00119\n",
      "Epoch 00007: early stopping\n",
      "temp/e12\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0948 - mean_squared_error: 1.0948 - val_loss: 1.0131 - val_mean_squared_error: 1.0131\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01306, saving model to temp/e12\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0237 - mean_squared_error: 1.0237 - val_loss: 1.0118 - val_mean_squared_error: 1.0118\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01306 to 1.01176, saving model to temp/e12\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0147 - mean_squared_error: 1.0147 - val_loss: 1.0102 - val_mean_squared_error: 1.0102\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01176 to 1.01016, saving model to temp/e12\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0120 - val_mean_squared_error: 1.0120\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.01016\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01016 to 1.00397, saving model to temp/e12\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0082 - val_mean_squared_error: 1.0082\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00397\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0064 - mean_squared_error: 1.0064 - val_loss: 1.0001 - val_mean_squared_error: 1.0001\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00397 to 1.00009, saving model to temp/e12\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0041 - mean_squared_error: 1.0041 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00009\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00009\n",
      "Epoch 00009: early stopping\n",
      "temp/e13\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.1050 - mean_squared_error: 1.1050 - val_loss: 1.0165 - val_mean_squared_error: 1.0165\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01648, saving model to temp/e13\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0189 - mean_squared_error: 1.0189 - val_loss: 1.0112 - val_mean_squared_error: 1.0112\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01648 to 1.01123, saving model to temp/e13\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0098 - mean_squared_error: 1.0098 - val_loss: 1.0072 - val_mean_squared_error: 1.0072\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01123 to 1.00719, saving model to temp/e13\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0115 - val_mean_squared_error: 1.0115\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00719\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0116 - mean_squared_error: 1.0116 - val_loss: 1.0099 - val_mean_squared_error: 1.0099\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00719\n",
      "Epoch 00005: early stopping\n",
      "temp/e14\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.1079 - mean_squared_error: 1.1079 - val_loss: 1.0382 - val_mean_squared_error: 1.0382\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03816, saving model to temp/e14\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0223 - mean_squared_error: 1.0223 - val_loss: 1.0120 - val_mean_squared_error: 1.0120\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03816 to 1.01201, saving model to temp/e14\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0160 - mean_squared_error: 1.0160 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01201 to 1.00883, saving model to temp/e14\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0131 - mean_squared_error: 1.0131 - val_loss: 1.0060 - val_mean_squared_error: 1.0060\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00883 to 1.00600, saving model to temp/e14\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0119 - mean_squared_error: 1.0119 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00600 to 1.00301, saving model to temp/e14\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0103 - mean_squared_error: 1.0103 - val_loss: 1.0050 - val_mean_squared_error: 1.0050\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00301\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0054 - mean_squared_error: 1.0054 - val_loss: 1.0055 - val_mean_squared_error: 1.0055\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00301\n",
      "Epoch 00007: early stopping\n",
      "temp/e15\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.0975 - mean_squared_error: 1.0975 - val_loss: 1.0161 - val_mean_squared_error: 1.0161\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01607, saving model to temp/e15\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0192 - mean_squared_error: 1.0192 - val_loss: 1.0144 - val_mean_squared_error: 1.0144\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01607 to 1.01439, saving model to temp/e15\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0115 - val_mean_squared_error: 1.0115\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01439 to 1.01154, saving model to temp/e15\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 1.0106 - val_mean_squared_error: 1.0106\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01154 to 1.01063, saving model to temp/e15\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0073 - mean_squared_error: 1.0073 - val_loss: 1.0204 - val_mean_squared_error: 1.0204\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.01063\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.0031 - val_mean_squared_error: 1.0031\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.01063 to 1.00306, saving model to temp/e15\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 0.9984 - val_mean_squared_error: 0.9984\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00306 to 0.99845, saving model to temp/e15\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0039 - val_mean_squared_error: 1.0039\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99845\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0051 - mean_squared_error: 1.0051 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99845\n",
      "Epoch 00009: early stopping\n",
      "temp/e16\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1138 - mean_squared_error: 1.1138 - val_loss: 1.0109 - val_mean_squared_error: 1.0109\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01094, saving model to temp/e16\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0217 - mean_squared_error: 1.0217 - val_loss: 1.0134 - val_mean_squared_error: 1.0134\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01094\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0178 - mean_squared_error: 1.0178 - val_loss: 1.0070 - val_mean_squared_error: 1.0070\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01094 to 1.00698, saving model to temp/e16\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0107 - mean_squared_error: 1.0107 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00698 to 1.00336, saving model to temp/e16\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0114 - mean_squared_error: 1.0114 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00336\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00336\n",
      "Epoch 00006: early stopping\n",
      "temp/e17\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1041 - mean_squared_error: 1.1041 - val_loss: 1.0147 - val_mean_squared_error: 1.0147\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01467, saving model to temp/e17\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0172 - mean_squared_error: 1.0172 - val_loss: 1.0121 - val_mean_squared_error: 1.0121\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01467 to 1.01207, saving model to temp/e17\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0192 - mean_squared_error: 1.0192 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01207 to 1.00593, saving model to temp/e17\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0139 - mean_squared_error: 1.0139 - val_loss: 1.0074 - val_mean_squared_error: 1.0074\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00593\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00593 to 1.00495, saving model to temp/e17\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00495\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0039 - val_mean_squared_error: 1.0039\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00495 to 1.00392, saving model to temp/e17\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0058 - mean_squared_error: 1.0058 - val_loss: 1.0063 - val_mean_squared_error: 1.0063\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00392\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0068 - mean_squared_error: 1.0068 - val_loss: 0.9972 - val_mean_squared_error: 0.9972\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00392 to 0.99721, saving model to temp/e17\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0048 - mean_squared_error: 1.0048 - val_loss: 0.9975 - val_mean_squared_error: 0.9975\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99721\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0009 - mean_squared_error: 1.0009 - val_loss: 1.0107 - val_mean_squared_error: 1.0107\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99721\n",
      "Epoch 00011: early stopping\n",
      "temp/e18\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.1096 - mean_squared_error: 1.1096 - val_loss: 1.0189 - val_mean_squared_error: 1.0189\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01895, saving model to temp/e18\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0237 - mean_squared_error: 1.0237 - val_loss: 1.0247 - val_mean_squared_error: 1.0247\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01895\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0158 - mean_squared_error: 1.0158 - val_loss: 0.9993 - val_mean_squared_error: 0.9993\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01895 to 0.99926, saving model to temp/e18\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0118 - val_mean_squared_error: 1.0118\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.99926\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0129 - mean_squared_error: 1.0129 - val_loss: 1.0064 - val_mean_squared_error: 1.0064\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.99926\n",
      "Epoch 00005: early stopping\n",
      "temp/e19\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1002 - mean_squared_error: 1.1002 - val_loss: 1.0151 - val_mean_squared_error: 1.0151\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01512, saving model to temp/e19\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0259 - mean_squared_error: 1.0259 - val_loss: 1.0217 - val_mean_squared_error: 1.0217\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01512\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0160 - mean_squared_error: 1.0160 - val_loss: 1.0079 - val_mean_squared_error: 1.0079\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01512 to 1.00788, saving model to temp/e19\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0111 - mean_squared_error: 1.0111 - val_loss: 1.0077 - val_mean_squared_error: 1.0077\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00788 to 1.00775, saving model to temp/e19\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0106 - mean_squared_error: 1.0106 - val_loss: 1.0274 - val_mean_squared_error: 1.0274\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00775\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 1.0032 - val_mean_squared_error: 1.0032\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00775 to 1.00323, saving model to temp/e19\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00323 to 1.00302, saving model to temp/e19\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 1.0029 - val_mean_squared_error: 1.0029\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00302 to 1.00295, saving model to temp/e19\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0039 - mean_squared_error: 1.0039 - val_loss: 1.0018 - val_mean_squared_error: 1.0018\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00295 to 1.00175, saving model to temp/e19\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0064 - mean_squared_error: 1.0064 - val_loss: 1.0036 - val_mean_squared_error: 1.0036\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.00175\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0062 - mean_squared_error: 1.0062 - val_loss: 1.0010 - val_mean_squared_error: 1.0010\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.00175 to 1.00097, saving model to temp/e19\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0019 - mean_squared_error: 1.0019 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.00097\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0047 - mean_squared_error: 1.0047 - val_loss: 0.9972 - val_mean_squared_error: 0.9972\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.00097 to 0.99717, saving model to temp/e19\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0013 - mean_squared_error: 1.0013 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.99717\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0010 - mean_squared_error: 1.0010 - val_loss: 0.9965 - val_mean_squared_error: 0.9965\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.99717 to 0.99646, saving model to temp/e19\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0040 - mean_squared_error: 1.0040 - val_loss: 0.9973 - val_mean_squared_error: 0.9973\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.99646\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0039 - mean_squared_error: 1.0039 - val_loss: 1.0044 - val_mean_squared_error: 1.0044\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.99646\n",
      "Epoch 00017: early stopping\n",
      "temp/e20\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.0997 - mean_squared_error: 1.0997 - val_loss: 1.0136 - val_mean_squared_error: 1.0136\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01359, saving model to temp/e20\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0233 - mean_squared_error: 1.0233 - val_loss: 1.0066 - val_mean_squared_error: 1.0066\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01359 to 1.00665, saving model to temp/e20\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0172 - mean_squared_error: 1.0172 - val_loss: 1.0037 - val_mean_squared_error: 1.0037\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00665 to 1.00370, saving model to temp/e20\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 1.0157 - val_mean_squared_error: 1.0157\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00370\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0142 - mean_squared_error: 1.0142 - val_loss: 1.0077 - val_mean_squared_error: 1.0077\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00370\n",
      "Epoch 00005: early stopping\n",
      "temp/e21\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1149 - mean_squared_error: 1.1149 - val_loss: 1.0206 - val_mean_squared_error: 1.0206\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02060, saving model to temp/e21\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0174 - mean_squared_error: 1.0174 - val_loss: 1.0082 - val_mean_squared_error: 1.0082\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02060 to 1.00818, saving model to temp/e21\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0172 - mean_squared_error: 1.0172 - val_loss: 1.0146 - val_mean_squared_error: 1.0146\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00818\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0140 - mean_squared_error: 1.0140 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00818 to 1.00467, saving model to temp/e21\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0111 - mean_squared_error: 1.0111 - val_loss: 1.0004 - val_mean_squared_error: 1.0004\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00467 to 1.00040, saving model to temp/e21\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0134 - mean_squared_error: 1.0134 - val_loss: 1.0063 - val_mean_squared_error: 1.0063\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00040\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0082 - mean_squared_error: 1.0082 - val_loss: 1.0000 - val_mean_squared_error: 1.0000\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00040 to 0.99997, saving model to temp/e21\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0079 - mean_squared_error: 1.0079 - val_loss: 1.0078 - val_mean_squared_error: 1.0078\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99997\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0073 - mean_squared_error: 1.0073 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99997\n",
      "Epoch 00009: early stopping\n",
      "temp/e22\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1041 - mean_squared_error: 1.1041 - val_loss: 1.0265 - val_mean_squared_error: 1.0265\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02652, saving model to temp/e22\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0217 - mean_squared_error: 1.0217 - val_loss: 1.0071 - val_mean_squared_error: 1.0071\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02652 to 1.00715, saving model to temp/e22\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0193 - mean_squared_error: 1.0193 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00715\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0134 - mean_squared_error: 1.0134 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00715\n",
      "Epoch 00004: early stopping\n",
      "temp/e23\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.1052 - mean_squared_error: 1.1052 - val_loss: 1.0160 - val_mean_squared_error: 1.0160\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01598, saving model to temp/e23\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0156 - mean_squared_error: 1.0156 - val_loss: 1.0156 - val_mean_squared_error: 1.0156\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01598 to 1.01565, saving model to temp/e23\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0148 - mean_squared_error: 1.0148 - val_loss: 1.0157 - val_mean_squared_error: 1.0157\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01565\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01565 to 1.00403, saving model to temp/e23\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0075 - mean_squared_error: 1.0075 - val_loss: 1.0237 - val_mean_squared_error: 1.0237\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00403\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 1.0100 - val_mean_squared_error: 1.0100\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00403\n",
      "Epoch 00006: early stopping\n",
      "temp/e24\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1051 - mean_squared_error: 1.1051 - val_loss: 1.0197 - val_mean_squared_error: 1.0197\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01970, saving model to temp/e24\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0198 - mean_squared_error: 1.0198 - val_loss: 1.0101 - val_mean_squared_error: 1.0101\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01970 to 1.01012, saving model to temp/e24\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0100 - val_mean_squared_error: 1.0100\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01012 to 1.01002, saving model to temp/e24\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0129 - mean_squared_error: 1.0129 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01002 to 1.00691, saving model to temp/e24\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00691 to 1.00467, saving model to temp/e24\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0063 - mean_squared_error: 1.0063 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00467 to 1.00261, saving model to temp/e24\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0053 - mean_squared_error: 1.0053 - val_loss: 1.0028 - val_mean_squared_error: 1.0028\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00261\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0116 - mean_squared_error: 1.0116 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00261\n",
      "Epoch 00008: early stopping\n",
      "temp/e25\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.1004 - mean_squared_error: 1.1004 - val_loss: 1.0157 - val_mean_squared_error: 1.0157\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01566, saving model to temp/e25\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0226 - mean_squared_error: 1.0226 - val_loss: 1.0102 - val_mean_squared_error: 1.0102\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01566 to 1.01025, saving model to temp/e25\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0160 - mean_squared_error: 1.0160 - val_loss: 1.0084 - val_mean_squared_error: 1.0084\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01025 to 1.00844, saving model to temp/e25\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0128 - mean_squared_error: 1.0128 - val_loss: 1.0060 - val_mean_squared_error: 1.0060\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00844 to 1.00601, saving model to temp/e25\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00601 to 1.00532, saving model to temp/e25\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0020 - val_mean_squared_error: 1.0020\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00532 to 1.00198, saving model to temp/e25\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0114 - mean_squared_error: 1.0114 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00198\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0078 - mean_squared_error: 1.0078 - val_loss: 1.0027 - val_mean_squared_error: 1.0027\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00198\n",
      "Epoch 00008: early stopping\n",
      "temp/e26\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1046 - mean_squared_error: 1.1046 - val_loss: 1.0189 - val_mean_squared_error: 1.0189\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01893, saving model to temp/e26\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0240 - mean_squared_error: 1.0240 - val_loss: 1.0162 - val_mean_squared_error: 1.0162\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01893 to 1.01625, saving model to temp/e26\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0157 - mean_squared_error: 1.0157 - val_loss: 1.0072 - val_mean_squared_error: 1.0072\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01625 to 1.00716, saving model to temp/e26\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0160 - mean_squared_error: 1.0160 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00716 to 1.00225, saving model to temp/e26\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00225\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 1.0046 - val_mean_squared_error: 1.0046\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00225\n",
      "Epoch 00006: early stopping\n",
      "temp/e27\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0972 - mean_squared_error: 1.0972 - val_loss: 1.0195 - val_mean_squared_error: 1.0195\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01948, saving model to temp/e27\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0179 - mean_squared_error: 1.0179 - val_loss: 1.0215 - val_mean_squared_error: 1.0215\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01948\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0153 - mean_squared_error: 1.0153 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01948 to 1.00687, saving model to temp/e27\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0128 - mean_squared_error: 1.0128 - val_loss: 1.0060 - val_mean_squared_error: 1.0060\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00687 to 1.00604, saving model to temp/e27\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 1.0056 - val_mean_squared_error: 1.0056\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00604 to 1.00559, saving model to temp/e27\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00559\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00559 to 1.00260, saving model to temp/e27\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0057 - mean_squared_error: 1.0057 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00260\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0062 - mean_squared_error: 1.0062 - val_loss: 1.0021 - val_mean_squared_error: 1.0021\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00260 to 1.00208, saving model to temp/e27\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0064 - mean_squared_error: 1.0064 - val_loss: 0.9998 - val_mean_squared_error: 0.9998\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.00208 to 0.99977, saving model to temp/e27\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0060 - mean_squared_error: 1.0060 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99977\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0035 - mean_squared_error: 1.0035 - val_loss: 0.9979 - val_mean_squared_error: 0.9979\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.99977 to 0.99786, saving model to temp/e27\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0013 - mean_squared_error: 1.0013 - val_loss: 1.0004 - val_mean_squared_error: 1.0004\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.99786\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0017 - mean_squared_error: 1.0017 - val_loss: 0.9992 - val_mean_squared_error: 0.9992\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.99786\n",
      "Epoch 00014: early stopping\n",
      "temp/e28\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.0986 - mean_squared_error: 1.0986 - val_loss: 1.0175 - val_mean_squared_error: 1.0175\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01749, saving model to temp/e28\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0194 - mean_squared_error: 1.0194 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01749 to 1.00861, saving model to temp/e28\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0095 - val_mean_squared_error: 1.0095\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00861\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0093 - mean_squared_error: 1.0093 - val_loss: 1.0094 - val_mean_squared_error: 1.0094\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00861\n",
      "Epoch 00004: early stopping\n",
      "temp/e29\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0971 - mean_squared_error: 1.0971 - val_loss: 1.0111 - val_mean_squared_error: 1.0111\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01111, saving model to temp/e29\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0195 - mean_squared_error: 1.0195 - val_loss: 1.0246 - val_mean_squared_error: 1.0246\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01111\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0148 - mean_squared_error: 1.0148 - val_loss: 1.0174 - val_mean_squared_error: 1.0174\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01111\n",
      "Epoch 00003: early stopping\n",
      "temp/e30\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0993 - mean_squared_error: 1.0993 - val_loss: 1.0210 - val_mean_squared_error: 1.0210\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02097, saving model to temp/e30\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0195 - mean_squared_error: 1.0195 - val_loss: 1.0120 - val_mean_squared_error: 1.0120\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02097 to 1.01203, saving model to temp/e30\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0153 - mean_squared_error: 1.0153 - val_loss: 1.0095 - val_mean_squared_error: 1.0095\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01203 to 1.00947, saving model to temp/e30\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0158 - mean_squared_error: 1.0158 - val_loss: 1.0028 - val_mean_squared_error: 1.0028\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00947 to 1.00285, saving model to temp/e30\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0056 - mean_squared_error: 1.0056 - val_loss: 1.0068 - val_mean_squared_error: 1.0068\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00285\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0111 - mean_squared_error: 1.0111 - val_loss: 1.0004 - val_mean_squared_error: 1.0004\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00285 to 1.00040, saving model to temp/e30\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0054 - mean_squared_error: 1.0054 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00040\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0072 - mean_squared_error: 1.0072 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00040\n",
      "Epoch 00008: early stopping\n",
      "temp/e31\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.1061 - mean_squared_error: 1.1061 - val_loss: 1.0113 - val_mean_squared_error: 1.0113\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01129, saving model to temp/e31\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0231 - mean_squared_error: 1.0231 - val_loss: 1.0116 - val_mean_squared_error: 1.0116\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01129\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0134 - mean_squared_error: 1.0134 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01129 to 1.00746, saving model to temp/e31\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0125 - mean_squared_error: 1.0125 - val_loss: 1.0055 - val_mean_squared_error: 1.0055\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00746 to 1.00545, saving model to temp/e31\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 1.0102 - val_mean_squared_error: 1.0102\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00545\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00545 to 1.00243, saving model to temp/e31\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 1.0009 - val_mean_squared_error: 1.0009\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00243 to 1.00089, saving model to temp/e31\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0183 - val_mean_squared_error: 1.0183\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00089\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0081 - mean_squared_error: 1.0081 - val_loss: 1.0060 - val_mean_squared_error: 1.0060\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00089\n",
      "Epoch 00009: early stopping\n",
      "temp/e32\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1049 - mean_squared_error: 1.1049 - val_loss: 1.0102 - val_mean_squared_error: 1.0102\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01021, saving model to temp/e32\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0195 - mean_squared_error: 1.0195 - val_loss: 1.0193 - val_mean_squared_error: 1.0193\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01021\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0170 - mean_squared_error: 1.0170 - val_loss: 1.0297 - val_mean_squared_error: 1.0297\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01021\n",
      "Epoch 00003: early stopping\n",
      "temp/e33\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0957 - mean_squared_error: 1.0957 - val_loss: 1.0109 - val_mean_squared_error: 1.0109\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01090, saving model to temp/e33\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0184 - mean_squared_error: 1.0184 - val_loss: 1.0105 - val_mean_squared_error: 1.0105\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01090 to 1.01051, saving model to temp/e33\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01051 to 1.00606, saving model to temp/e33\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00606 to 1.00593, saving model to temp/e33\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0129 - mean_squared_error: 1.0129 - val_loss: 1.0045 - val_mean_squared_error: 1.0045\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00593 to 1.00450, saving model to temp/e33\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0123 - mean_squared_error: 1.0123 - val_loss: 1.0066 - val_mean_squared_error: 1.0066\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00450\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0022 - val_mean_squared_error: 1.0022\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00450 to 1.00215, saving model to temp/e33\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0045 - val_mean_squared_error: 1.0045\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00215\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0064 - mean_squared_error: 1.0064 - val_loss: 1.0006 - val_mean_squared_error: 1.0006\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00215 to 1.00060, saving model to temp/e33\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0089 - mean_squared_error: 1.0089 - val_loss: 1.0135 - val_mean_squared_error: 1.0135\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.00060\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.00060\n",
      "Epoch 00011: early stopping\n",
      "temp/e34\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0950 - mean_squared_error: 1.0950 - val_loss: 1.0310 - val_mean_squared_error: 1.0310\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03101, saving model to temp/e34\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0173 - mean_squared_error: 1.0173 - val_loss: 1.0159 - val_mean_squared_error: 1.0159\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03101 to 1.01590, saving model to temp/e34\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0131 - mean_squared_error: 1.0131 - val_loss: 1.0108 - val_mean_squared_error: 1.0108\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01590 to 1.01082, saving model to temp/e34\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0125 - mean_squared_error: 1.0125 - val_loss: 1.0214 - val_mean_squared_error: 1.0214\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.01082\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0136 - mean_squared_error: 1.0136 - val_loss: 0.9997 - val_mean_squared_error: 0.9997\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01082 to 0.99968, saving model to temp/e34\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0072 - mean_squared_error: 1.0072 - val_loss: 1.0014 - val_mean_squared_error: 1.0014\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99968\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0098 - mean_squared_error: 1.0098 - val_loss: 1.0043 - val_mean_squared_error: 1.0043\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99968\n",
      "Epoch 00007: early stopping\n",
      "temp/e35\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0966 - mean_squared_error: 1.0966 - val_loss: 1.0221 - val_mean_squared_error: 1.0221\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02206, saving model to temp/e35\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0167 - mean_squared_error: 1.0167 - val_loss: 1.0158 - val_mean_squared_error: 1.0158\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02206 to 1.01580, saving model to temp/e35\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0199 - mean_squared_error: 1.0199 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01580 to 1.00688, saving model to temp/e35\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0058 - val_mean_squared_error: 1.0058\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00688 to 1.00576, saving model to temp/e35\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0016 - val_mean_squared_error: 1.0016\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00576 to 1.00157, saving model to temp/e35\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0029 - val_mean_squared_error: 1.0029\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00157\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0064 - mean_squared_error: 1.0064 - val_loss: 1.0139 - val_mean_squared_error: 1.0139\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00157\n",
      "Epoch 00007: early stopping\n",
      "temp/e36\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1057 - mean_squared_error: 1.1057 - val_loss: 1.0289 - val_mean_squared_error: 1.0289\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02887, saving model to temp/e36\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0170 - mean_squared_error: 1.0170 - val_loss: 1.0114 - val_mean_squared_error: 1.0114\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02887 to 1.01143, saving model to temp/e36\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01143 to 1.00933, saving model to temp/e36\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00933 to 1.00797, saving model to temp/e36\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00797\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 0.9994 - val_mean_squared_error: 0.9994\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00797 to 0.99936, saving model to temp/e36\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0182 - val_mean_squared_error: 1.0182\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99936\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0011 - val_mean_squared_error: 1.0011\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99936\n",
      "Epoch 00008: early stopping\n",
      "temp/e37\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1035 - mean_squared_error: 1.1035 - val_loss: 1.0268 - val_mean_squared_error: 1.0268\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02685, saving model to temp/e37\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0194 - mean_squared_error: 1.0194 - val_loss: 1.0119 - val_mean_squared_error: 1.0119\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02685 to 1.01194, saving model to temp/e37\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0192 - mean_squared_error: 1.0192 - val_loss: 1.0142 - val_mean_squared_error: 1.0142\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01194\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0142 - mean_squared_error: 1.0142 - val_loss: 1.0064 - val_mean_squared_error: 1.0064\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01194 to 1.00639, saving model to temp/e37\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0127 - mean_squared_error: 1.0127 - val_loss: 1.0163 - val_mean_squared_error: 1.0163\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00639\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0014 - val_mean_squared_error: 1.0014\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00639 to 1.00136, saving model to temp/e37\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0108 - val_mean_squared_error: 1.0108\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00136\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0047 - mean_squared_error: 1.0047 - val_loss: 1.0077 - val_mean_squared_error: 1.0077\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00136\n",
      "Epoch 00008: early stopping\n",
      "temp/e38\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0984 - mean_squared_error: 1.0984 - val_loss: 1.0108 - val_mean_squared_error: 1.0108\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01083, saving model to temp/e38\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0225 - mean_squared_error: 1.0225 - val_loss: 1.0146 - val_mean_squared_error: 1.0146\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01083\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0150 - mean_squared_error: 1.0150 - val_loss: 1.0066 - val_mean_squared_error: 1.0066\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01083 to 1.00664, saving model to temp/e38\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0114 - mean_squared_error: 1.0114 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00664 to 1.00398, saving model to temp/e38\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 1.0130 - val_mean_squared_error: 1.0130\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00398\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0071 - mean_squared_error: 1.0071 - val_loss: 1.0017 - val_mean_squared_error: 1.0017\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00398 to 1.00170, saving model to temp/e38\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 1.0060 - val_mean_squared_error: 1.0060\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00170\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0054 - mean_squared_error: 1.0054 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00170\n",
      "Epoch 00008: early stopping\n",
      "temp/e39\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.0998 - mean_squared_error: 1.0998 - val_loss: 1.0216 - val_mean_squared_error: 1.0216\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02156, saving model to temp/e39\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0237 - mean_squared_error: 1.0237 - val_loss: 1.0094 - val_mean_squared_error: 1.0094\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02156 to 1.00940, saving model to temp/e39\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0190 - mean_squared_error: 1.0190 - val_loss: 1.0120 - val_mean_squared_error: 1.0120\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00940\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0105 - mean_squared_error: 1.0105 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00940 to 1.00334, saving model to temp/e39\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00334 to 1.00300, saving model to temp/e39\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0076 - val_mean_squared_error: 1.0076\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00300\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0072 - mean_squared_error: 1.0072 - val_loss: 1.0010 - val_mean_squared_error: 1.0010\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00300 to 1.00097, saving model to temp/e39\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 1.0065 - val_mean_squared_error: 1.0065\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00097\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0048 - mean_squared_error: 1.0048 - val_loss: 0.9983 - val_mean_squared_error: 0.9983\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00097 to 0.99834, saving model to temp/e39\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0057 - mean_squared_error: 1.0057 - val_loss: 0.9999 - val_mean_squared_error: 0.9999\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99834\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0034 - mean_squared_error: 1.0034 - val_loss: 0.9980 - val_mean_squared_error: 0.9980\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.99834 to 0.99796, saving model to temp/e39\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0030 - mean_squared_error: 1.0030 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.99796\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0018 - mean_squared_error: 1.0018 - val_loss: 0.9983 - val_mean_squared_error: 0.9983\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.99796\n",
      "Epoch 00013: early stopping\n",
      "temp/e40\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1045 - mean_squared_error: 1.1045 - val_loss: 1.0178 - val_mean_squared_error: 1.0178\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01779, saving model to temp/e40\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0191 - mean_squared_error: 1.0191 - val_loss: 1.0133 - val_mean_squared_error: 1.0133\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01779 to 1.01325, saving model to temp/e40\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0159 - mean_squared_error: 1.0159 - val_loss: 1.0226 - val_mean_squared_error: 1.0226\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01325\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01325 to 1.00506, saving model to temp/e40\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0160 - mean_squared_error: 1.0160 - val_loss: 1.0133 - val_mean_squared_error: 1.0133\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00506\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0010 - val_mean_squared_error: 1.0010\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00506 to 1.00104, saving model to temp/e40\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0036 - val_mean_squared_error: 1.0036\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00104\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 1.0100 - val_mean_squared_error: 1.0100\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00104\n",
      "Epoch 00008: early stopping\n",
      "temp/e41\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0934 - mean_squared_error: 1.0934 - val_loss: 1.0168 - val_mean_squared_error: 1.0168\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01685, saving model to temp/e41\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0201 - mean_squared_error: 1.0201 - val_loss: 1.0134 - val_mean_squared_error: 1.0134\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01685 to 1.01341, saving model to temp/e41\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0165 - mean_squared_error: 1.0165 - val_loss: 1.0101 - val_mean_squared_error: 1.0101\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01341 to 1.01007, saving model to temp/e41\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0125 - mean_squared_error: 1.0125 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01007 to 1.00749, saving model to temp/e41\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0125 - mean_squared_error: 1.0125 - val_loss: 1.0095 - val_mean_squared_error: 1.0095\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00749\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0058 - mean_squared_error: 1.0058 - val_loss: 1.0158 - val_mean_squared_error: 1.0158\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00749\n",
      "Epoch 00006: early stopping\n",
      "temp/e42\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0985 - mean_squared_error: 1.0985 - val_loss: 1.0237 - val_mean_squared_error: 1.0237\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02365, saving model to temp/e42\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0215 - mean_squared_error: 1.0215 - val_loss: 1.0118 - val_mean_squared_error: 1.0118\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02365 to 1.01182, saving model to temp/e42\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0140 - mean_squared_error: 1.0140 - val_loss: 1.0133 - val_mean_squared_error: 1.0133\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01182\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01182 to 1.00529, saving model to temp/e42\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 1.0019 - val_mean_squared_error: 1.0019\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00529 to 1.00193, saving model to temp/e42\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 1.0101 - val_mean_squared_error: 1.0101\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00193\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00193\n",
      "Epoch 00007: early stopping\n",
      "temp/e43\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.1012 - mean_squared_error: 1.1012 - val_loss: 1.0198 - val_mean_squared_error: 1.0198\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01979, saving model to temp/e43\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0217 - mean_squared_error: 1.0217 - val_loss: 1.0189 - val_mean_squared_error: 1.0189\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01979 to 1.01888, saving model to temp/e43\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0168 - mean_squared_error: 1.0168 - val_loss: 1.0127 - val_mean_squared_error: 1.0127\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01888 to 1.01265, saving model to temp/e43\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0162 - mean_squared_error: 1.0162 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01265 to 1.00934, saving model to temp/e43\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0131 - mean_squared_error: 1.0131 - val_loss: 1.0239 - val_mean_squared_error: 1.0239\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00934\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.0075 - mean_squared_error: 1.0075 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00934 to 1.00491, saving model to temp/e43\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0037 - mean_squared_error: 1.0037 - val_loss: 0.9992 - val_mean_squared_error: 0.9992\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00491 to 0.99924, saving model to temp/e43\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0081 - mean_squared_error: 1.0081 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99924\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0041 - mean_squared_error: 1.0041 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99924\n",
      "Epoch 00009: early stopping\n",
      "temp/e44\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0977 - mean_squared_error: 1.0977 - val_loss: 1.0237 - val_mean_squared_error: 1.0237\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02369, saving model to temp/e44\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0231 - mean_squared_error: 1.0231 - val_loss: 1.0132 - val_mean_squared_error: 1.0132\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02369 to 1.01318, saving model to temp/e44\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0116 - mean_squared_error: 1.0116 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01318 to 1.00859, saving model to temp/e44\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0153 - mean_squared_error: 1.0153 - val_loss: 1.0058 - val_mean_squared_error: 1.0058\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00859 to 1.00579, saving model to temp/e44\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0011 - val_mean_squared_error: 1.0011\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00579 to 1.00106, saving model to temp/e44\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0078 - mean_squared_error: 1.0078 - val_loss: 1.0161 - val_mean_squared_error: 1.0161\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00106\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0064 - mean_squared_error: 1.0064 - val_loss: 1.0027 - val_mean_squared_error: 1.0027\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00106\n",
      "Epoch 00007: early stopping\n",
      "temp/e45\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1045 - mean_squared_error: 1.1045 - val_loss: 1.0071 - val_mean_squared_error: 1.0071\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00714, saving model to temp/e45\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0203 - mean_squared_error: 1.0203 - val_loss: 1.0230 - val_mean_squared_error: 1.0230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_loss did not improve from 1.00714\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0167 - mean_squared_error: 1.0167 - val_loss: 1.0078 - val_mean_squared_error: 1.0078\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00714\n",
      "Epoch 00003: early stopping\n",
      "temp/e46\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0993 - mean_squared_error: 1.0993 - val_loss: 1.0171 - val_mean_squared_error: 1.0171\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01715, saving model to temp/e46\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0165 - mean_squared_error: 1.0165 - val_loss: 1.0079 - val_mean_squared_error: 1.0079\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01715 to 1.00792, saving model to temp/e46\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0161 - mean_squared_error: 1.0161 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00792 to 1.00509, saving model to temp/e46\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0129 - mean_squared_error: 1.0129 - val_loss: 1.0101 - val_mean_squared_error: 1.0101\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00509\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0111 - mean_squared_error: 1.0111 - val_loss: 1.0101 - val_mean_squared_error: 1.0101\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00509\n",
      "Epoch 00005: early stopping\n",
      "temp/e47\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1064 - mean_squared_error: 1.1064 - val_loss: 1.0110 - val_mean_squared_error: 1.0110\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01098, saving model to temp/e47\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0217 - mean_squared_error: 1.0217 - val_loss: 1.0174 - val_mean_squared_error: 1.0174\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01098\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0150 - mean_squared_error: 1.0150 - val_loss: 1.0166 - val_mean_squared_error: 1.0166\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01098\n",
      "Epoch 00003: early stopping\n",
      "temp/e48\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.0964 - mean_squared_error: 1.0964 - val_loss: 1.0181 - val_mean_squared_error: 1.0181\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01814, saving model to temp/e48\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0203 - mean_squared_error: 1.0203 - val_loss: 1.0111 - val_mean_squared_error: 1.0111\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01814 to 1.01107, saving model to temp/e48\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0176 - mean_squared_error: 1.0176 - val_loss: 1.0092 - val_mean_squared_error: 1.0092\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01107 to 1.00918, saving model to temp/e48\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0138 - mean_squared_error: 1.0138 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00918\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00918 to 1.00378, saving model to temp/e48\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00378\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 1.0014 - val_mean_squared_error: 1.0014\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00378 to 1.00140, saving model to temp/e48\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0058 - mean_squared_error: 1.0058 - val_loss: 1.0129 - val_mean_squared_error: 1.0129\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00140\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0071 - mean_squared_error: 1.0071 - val_loss: 0.9977 - val_mean_squared_error: 0.9977\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00140 to 0.99773, saving model to temp/e48\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0064 - mean_squared_error: 1.0064 - val_loss: 0.9990 - val_mean_squared_error: 0.9990\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99773\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0065 - mean_squared_error: 1.0065 - val_loss: 1.0097 - val_mean_squared_error: 1.0097\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99773\n",
      "Epoch 00011: early stopping\n",
      "temp/e49\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.0989 - mean_squared_error: 1.0989 - val_loss: 1.0157 - val_mean_squared_error: 1.0157\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01567, saving model to temp/e49\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0213 - mean_squared_error: 1.0213 - val_loss: 1.0313 - val_mean_squared_error: 1.0313\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01567\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0157 - mean_squared_error: 1.0157 - val_loss: 1.0198 - val_mean_squared_error: 1.0198\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01567\n",
      "Epoch 00003: early stopping\n",
      "temp/e50\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0931 - mean_squared_error: 1.0931 - val_loss: 1.0289 - val_mean_squared_error: 1.0289\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02886, saving model to temp/e50\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0219 - mean_squared_error: 1.0219 - val_loss: 1.0089 - val_mean_squared_error: 1.0089\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02886 to 1.00892, saving model to temp/e50\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0154 - mean_squared_error: 1.0154 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00892 to 1.00527, saving model to temp/e50\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0036 - val_mean_squared_error: 1.0036\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00527 to 1.00364, saving model to temp/e50\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0019 - val_mean_squared_error: 1.0019\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00364 to 1.00192, saving model to temp/e50\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0072 - mean_squared_error: 1.0072 - val_loss: 1.0043 - val_mean_squared_error: 1.0043\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00192\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0105 - mean_squared_error: 1.0105 - val_loss: 1.0006 - val_mean_squared_error: 1.0006\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00192 to 1.00063, saving model to temp/e50\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0079 - mean_squared_error: 1.0079 - val_loss: 0.9987 - val_mean_squared_error: 0.9987\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00063 to 0.99868, saving model to temp/e50\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0042 - mean_squared_error: 1.0042 - val_loss: 1.0005 - val_mean_squared_error: 1.0005\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99868\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 1.0076 - val_mean_squared_error: 1.0076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99868\n",
      "Epoch 00010: early stopping\n",
      "temp/e51\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.1067 - mean_squared_error: 1.1067 - val_loss: 1.0175 - val_mean_squared_error: 1.0175\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01746, saving model to temp/e51\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0184 - mean_squared_error: 1.0184 - val_loss: 1.0135 - val_mean_squared_error: 1.0135\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01746 to 1.01353, saving model to temp/e51\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0129 - mean_squared_error: 1.0129 - val_loss: 1.0119 - val_mean_squared_error: 1.0119\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01353 to 1.01194, saving model to temp/e51\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0128 - mean_squared_error: 1.0128 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01194 to 1.00377, saving model to temp/e51\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0111 - mean_squared_error: 1.0111 - val_loss: 1.0089 - val_mean_squared_error: 1.0089\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00377\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0105 - mean_squared_error: 1.0105 - val_loss: 1.0138 - val_mean_squared_error: 1.0138\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00377\n",
      "Epoch 00006: early stopping\n",
      "temp/e52\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0955 - mean_squared_error: 1.0955 - val_loss: 1.0212 - val_mean_squared_error: 1.0212\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02124, saving model to temp/e52\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0189 - mean_squared_error: 1.0189 - val_loss: 1.0096 - val_mean_squared_error: 1.0096\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02124 to 1.00964, saving model to temp/e52\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0170 - mean_squared_error: 1.0170 - val_loss: 1.0136 - val_mean_squared_error: 1.0136\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00964\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0095 - val_mean_squared_error: 1.0095\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00964 to 1.00946, saving model to temp/e52\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 0.9988 - val_mean_squared_error: 0.9988\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00946 to 0.99881, saving model to temp/e52\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 1.0013 - val_mean_squared_error: 1.0013\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99881\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0103 - mean_squared_error: 1.0103 - val_loss: 1.0004 - val_mean_squared_error: 1.0004\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99881\n",
      "Epoch 00007: early stopping\n",
      "temp/e53\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0980 - mean_squared_error: 1.0980 - val_loss: 1.0266 - val_mean_squared_error: 1.0266\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02658, saving model to temp/e53\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0175 - mean_squared_error: 1.0175 - val_loss: 1.0190 - val_mean_squared_error: 1.0190\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02658 to 1.01897, saving model to temp/e53\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0157 - mean_squared_error: 1.0157 - val_loss: 1.0123 - val_mean_squared_error: 1.0123\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01897 to 1.01226, saving model to temp/e53\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0112 - mean_squared_error: 1.0112 - val_loss: 1.0085 - val_mean_squared_error: 1.0085\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01226 to 1.00852, saving model to temp/e53\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0097 - val_mean_squared_error: 1.0097\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00852\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00852 to 1.00611, saving model to temp/e53\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 1.0000 - val_mean_squared_error: 1.0000\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00611 to 1.00000, saving model to temp/e53\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.0035 - val_mean_squared_error: 1.0035\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00000\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0070 - mean_squared_error: 1.0070 - val_loss: 1.0022 - val_mean_squared_error: 1.0022\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00000\n",
      "Epoch 00009: early stopping\n",
      "temp/e54\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1021 - mean_squared_error: 1.1021 - val_loss: 1.0224 - val_mean_squared_error: 1.0224\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02243, saving model to temp/e54\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0205 - mean_squared_error: 1.0205 - val_loss: 1.0105 - val_mean_squared_error: 1.0105\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02243 to 1.01048, saving model to temp/e54\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0118 - mean_squared_error: 1.0118 - val_loss: 1.0125 - val_mean_squared_error: 1.0125\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01048\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0141 - mean_squared_error: 1.0141 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01048 to 1.00621, saving model to temp/e54\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0098 - mean_squared_error: 1.0098 - val_loss: 1.0015 - val_mean_squared_error: 1.0015\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00621 to 1.00151, saving model to temp/e54\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 0.9995 - val_mean_squared_error: 0.9995\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00151 to 0.99953, saving model to temp/e54\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0072 - mean_squared_error: 1.0072 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99953\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0069 - mean_squared_error: 1.0069 - val_loss: 0.9995 - val_mean_squared_error: 0.9995\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.99953 to 0.99951, saving model to temp/e54\n",
      "Epoch 00008: early stopping\n",
      "temp/e55\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.0972 - mean_squared_error: 1.0972 - val_loss: 1.0271 - val_mean_squared_error: 1.0271\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02714, saving model to temp/e55\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0240 - mean_squared_error: 1.0240 - val_loss: 1.0120 - val_mean_squared_error: 1.0120\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02714 to 1.01197, saving model to temp/e55\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.0163 - mean_squared_error: 1.0163 - val_loss: 1.0143 - val_mean_squared_error: 1.0143\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01197\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0158 - mean_squared_error: 1.0158 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01197 to 1.00750, saving model to temp/e55\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00750 to 1.00616, saving model to temp/e55\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00616 to 1.00544, saving model to temp/e55\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00544 to 1.00518, saving model to temp/e55\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0069 - mean_squared_error: 1.0069 - val_loss: 1.0021 - val_mean_squared_error: 1.0021\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00518 to 1.00214, saving model to temp/e55\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0081 - mean_squared_error: 1.0081 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00214\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0068 - mean_squared_error: 1.0068 - val_loss: 1.0111 - val_mean_squared_error: 1.0111\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.00214\n",
      "Epoch 00010: early stopping\n",
      "temp/e56\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.1075 - mean_squared_error: 1.1075 - val_loss: 1.0140 - val_mean_squared_error: 1.0140\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01399, saving model to temp/e56\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0204 - mean_squared_error: 1.0204 - val_loss: 1.0135 - val_mean_squared_error: 1.0135\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01399 to 1.01349, saving model to temp/e56\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0116 - mean_squared_error: 1.0116 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01349 to 1.00607, saving model to temp/e56\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0119 - val_mean_squared_error: 1.0119\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00607\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0106 - mean_squared_error: 1.0106 - val_loss: 1.0123 - val_mean_squared_error: 1.0123\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00607\n",
      "Epoch 00005: early stopping\n",
      "temp/e57\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0988 - mean_squared_error: 1.0988 - val_loss: 1.0203 - val_mean_squared_error: 1.0203\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02029, saving model to temp/e57\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0218 - mean_squared_error: 1.0218 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02029 to 1.00862, saving model to temp/e57\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00862\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0085 - val_mean_squared_error: 1.0085\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00862 to 1.00854, saving model to temp/e57\n",
      "Epoch 00004: early stopping\n",
      "temp/e58\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1092 - mean_squared_error: 1.1092 - val_loss: 1.0351 - val_mean_squared_error: 1.0351\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03506, saving model to temp/e58\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0199 - mean_squared_error: 1.0199 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03506 to 1.00933, saving model to temp/e58\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0109 - mean_squared_error: 1.0109 - val_loss: 1.0048 - val_mean_squared_error: 1.0048\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00933 to 1.00485, saving model to temp/e58\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0004 - val_mean_squared_error: 1.0004\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00485 to 1.00038, saving model to temp/e58\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0114 - mean_squared_error: 1.0114 - val_loss: 1.0104 - val_mean_squared_error: 1.0104\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00038\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 1.0102 - val_mean_squared_error: 1.0102\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00038\n",
      "Epoch 00006: early stopping\n",
      "temp/e59\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.1066 - mean_squared_error: 1.1066 - val_loss: 1.0307 - val_mean_squared_error: 1.0307\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03070, saving model to temp/e59\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0186 - mean_squared_error: 1.0186 - val_loss: 1.0128 - val_mean_squared_error: 1.0128\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03070 to 1.01276, saving model to temp/e59\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0177 - mean_squared_error: 1.0177 - val_loss: 1.0102 - val_mean_squared_error: 1.0102\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01276 to 1.01022, saving model to temp/e59\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0076 - val_mean_squared_error: 1.0076\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01022 to 1.00756, saving model to temp/e59\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 0.9990 - val_mean_squared_error: 0.9990\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00756 to 0.99903, saving model to temp/e59\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99903\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99903\n",
      "Epoch 00007: early stopping\n",
      "temp/e60\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.1064 - mean_squared_error: 1.1064 - val_loss: 1.0107 - val_mean_squared_error: 1.0107\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01071, saving model to temp/e60\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0174 - mean_squared_error: 1.0174 - val_loss: 1.0252 - val_mean_squared_error: 1.0252\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01071\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0092 - val_mean_squared_error: 1.0092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss improved from 1.01071 to 1.00917, saving model to temp/e60\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 1.0090 - val_mean_squared_error: 1.0090\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00917 to 1.00904, saving model to temp/e60\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0107 - mean_squared_error: 1.0107 - val_loss: 1.0039 - val_mean_squared_error: 1.0039\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00904 to 1.00393, saving model to temp/e60\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00393 to 1.00295, saving model to temp/e60\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0093 - mean_squared_error: 1.0093 - val_loss: 1.0029 - val_mean_squared_error: 1.0029\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00295 to 1.00289, saving model to temp/e60\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0066 - mean_squared_error: 1.0066 - val_loss: 0.9998 - val_mean_squared_error: 0.9998\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00289 to 0.99980, saving model to temp/e60\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0063 - mean_squared_error: 1.0063 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99980\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0059 - mean_squared_error: 1.0059 - val_loss: 1.0132 - val_mean_squared_error: 1.0132\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99980\n",
      "Epoch 00010: early stopping\n",
      "temp/e61\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.1129 - mean_squared_error: 1.1129 - val_loss: 1.0162 - val_mean_squared_error: 1.0162\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01620, saving model to temp/e61\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0217 - mean_squared_error: 1.0217 - val_loss: 1.0106 - val_mean_squared_error: 1.0106\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01620 to 1.01058, saving model to temp/e61\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0149 - mean_squared_error: 1.0149 - val_loss: 1.0067 - val_mean_squared_error: 1.0067\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01058 to 1.00672, saving model to temp/e61\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 1.0160 - val_mean_squared_error: 1.0160\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00672\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0159 - mean_squared_error: 1.0159 - val_loss: 1.0067 - val_mean_squared_error: 1.0067\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00672 to 1.00666, saving model to temp/e61\n",
      "Epoch 00005: early stopping\n",
      "temp/e62\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.1016 - mean_squared_error: 1.1016 - val_loss: 1.0159 - val_mean_squared_error: 1.0159\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01591, saving model to temp/e62\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0164 - mean_squared_error: 1.0164 - val_loss: 1.0123 - val_mean_squared_error: 1.0123\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01591 to 1.01226, saving model to temp/e62\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0163 - mean_squared_error: 1.0163 - val_loss: 1.0161 - val_mean_squared_error: 1.0161\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01226\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0105 - mean_squared_error: 1.0105 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01226 to 1.00978, saving model to temp/e62\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0043 - val_mean_squared_error: 1.0043\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00978 to 1.00429, saving model to temp/e62\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00429\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 0.9984 - val_mean_squared_error: 0.9984\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00429 to 0.99840, saving model to temp/e62\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0015 - val_mean_squared_error: 1.0015\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99840\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0072 - mean_squared_error: 1.0072 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99840\n",
      "Epoch 00009: early stopping\n",
      "temp/e63\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.1078 - mean_squared_error: 1.1078 - val_loss: 1.0164 - val_mean_squared_error: 1.0164\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01637, saving model to temp/e63\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0208 - mean_squared_error: 1.0208 - val_loss: 1.0130 - val_mean_squared_error: 1.0130\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01637 to 1.01296, saving model to temp/e63\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0123 - mean_squared_error: 1.0123 - val_loss: 1.0073 - val_mean_squared_error: 1.0073\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01296 to 1.00728, saving model to temp/e63\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0140 - mean_squared_error: 1.0140 - val_loss: 1.0217 - val_mean_squared_error: 1.0217\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00728\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00728\n",
      "Epoch 00005: early stopping\n",
      "temp/e64\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.1041 - mean_squared_error: 1.1041 - val_loss: 1.0117 - val_mean_squared_error: 1.0117\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01174, saving model to temp/e64\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0170 - mean_squared_error: 1.0170 - val_loss: 1.0127 - val_mean_squared_error: 1.0127\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01174\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0139 - mean_squared_error: 1.0139 - val_loss: 1.0092 - val_mean_squared_error: 1.0092\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01174 to 1.00923, saving model to temp/e64\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0086 - mean_squared_error: 1.0086 - val_loss: 1.0013 - val_mean_squared_error: 1.0013\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00923 to 1.00130, saving model to temp/e64\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 1.0090 - val_mean_squared_error: 1.0090\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00130\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 0.9998 - val_mean_squared_error: 0.9998\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00130 to 0.99982, saving model to temp/e64\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0075 - mean_squared_error: 1.0075 - val_loss: 1.0170 - val_mean_squared_error: 1.0170\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99982\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 1.0012 - val_mean_squared_error: 1.0012\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99982\n",
      "Epoch 00008: early stopping\n",
      "temp/e65\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.1016 - mean_squared_error: 1.1016 - val_loss: 1.0195 - val_mean_squared_error: 1.0195\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01947, saving model to temp/e65\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0211 - mean_squared_error: 1.0211 - val_loss: 1.0071 - val_mean_squared_error: 1.0071\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01947 to 1.00714, saving model to temp/e65\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0149 - mean_squared_error: 1.0149 - val_loss: 1.0143 - val_mean_squared_error: 1.0143\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00714\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00714 to 1.00486, saving model to temp/e65\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00486\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0072 - mean_squared_error: 1.0072 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00486 to 1.00236, saving model to temp/e65\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0098 - mean_squared_error: 1.0098 - val_loss: 1.0004 - val_mean_squared_error: 1.0004\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00236 to 1.00042, saving model to temp/e65\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0071 - mean_squared_error: 1.0071 - val_loss: 1.0083 - val_mean_squared_error: 1.0083\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00042\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00042\n",
      "Epoch 00009: early stopping\n",
      "temp/e66\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1014 - mean_squared_error: 1.1014 - val_loss: 1.0164 - val_mean_squared_error: 1.0164\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01640, saving model to temp/e66\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0182 - mean_squared_error: 1.0182 - val_loss: 1.0103 - val_mean_squared_error: 1.0103\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01640 to 1.01029, saving model to temp/e66\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0164 - mean_squared_error: 1.0164 - val_loss: 1.0068 - val_mean_squared_error: 1.0068\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01029 to 1.00681, saving model to temp/e66\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0154 - mean_squared_error: 1.0154 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00681 to 1.00301, saving model to temp/e66\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 1.0181 - val_mean_squared_error: 1.0181\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00301\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0003 - val_mean_squared_error: 1.0003\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00301 to 1.00032, saving model to temp/e66\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0082 - mean_squared_error: 1.0082 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00032\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00032\n",
      "Epoch 00008: early stopping\n",
      "temp/e67\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1042 - mean_squared_error: 1.1042 - val_loss: 1.0181 - val_mean_squared_error: 1.0181\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01811, saving model to temp/e67\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0200 - mean_squared_error: 1.0200 - val_loss: 1.0168 - val_mean_squared_error: 1.0168\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01811 to 1.01683, saving model to temp/e67\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0176 - mean_squared_error: 1.0176 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01683 to 1.00926, saving model to temp/e67\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0106 - mean_squared_error: 1.0106 - val_loss: 1.0153 - val_mean_squared_error: 1.0153\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00926\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0093 - mean_squared_error: 1.0093 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00926 to 1.00343, saving model to temp/e67\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0086 - mean_squared_error: 1.0086 - val_loss: 1.0218 - val_mean_squared_error: 1.0218\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00343\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0063 - mean_squared_error: 1.0063 - val_loss: 1.0014 - val_mean_squared_error: 1.0014\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00343 to 1.00140, saving model to temp/e67\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0048 - mean_squared_error: 1.0048 - val_loss: 1.0048 - val_mean_squared_error: 1.0048\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00140\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0053 - mean_squared_error: 1.0053 - val_loss: 1.0027 - val_mean_squared_error: 1.0027\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00140\n",
      "Epoch 00009: early stopping\n",
      "temp/e68\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0995 - mean_squared_error: 1.0995 - val_loss: 1.0223 - val_mean_squared_error: 1.0223\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02235, saving model to temp/e68\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0187 - mean_squared_error: 1.0187 - val_loss: 1.0099 - val_mean_squared_error: 1.0099\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02235 to 1.00993, saving model to temp/e68\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 1.0189 - val_mean_squared_error: 1.0189\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00993\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0137 - mean_squared_error: 1.0137 - val_loss: 1.0064 - val_mean_squared_error: 1.0064\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00993 to 1.00638, saving model to temp/e68\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0116 - mean_squared_error: 1.0116 - val_loss: 1.0084 - val_mean_squared_error: 1.0084\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00638\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 0.9983 - val_mean_squared_error: 0.9983\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00638 to 0.99834, saving model to temp/e68\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0059 - mean_squared_error: 1.0059 - val_loss: 1.0004 - val_mean_squared_error: 1.0004\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99834\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0071 - mean_squared_error: 1.0071 - val_loss: 1.0002 - val_mean_squared_error: 1.0002\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99834\n",
      "Epoch 00008: early stopping\n",
      "temp/e69\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.1013 - mean_squared_error: 1.1013 - val_loss: 1.0283 - val_mean_squared_error: 1.0283\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02832, saving model to temp/e69\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0199 - mean_squared_error: 1.0199 - val_loss: 1.0140 - val_mean_squared_error: 1.0140\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02832 to 1.01396, saving model to temp/e69\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0150 - mean_squared_error: 1.0150 - val_loss: 1.0032 - val_mean_squared_error: 1.0032\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01396 to 1.00321, saving model to temp/e69\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0105 - mean_squared_error: 1.0105 - val_loss: 1.0085 - val_mean_squared_error: 1.0085\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00321\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0156 - mean_squared_error: 1.0156 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00321\n",
      "Epoch 00005: early stopping\n",
      "temp/e70\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0987 - mean_squared_error: 1.0987 - val_loss: 1.0202 - val_mean_squared_error: 1.0202\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02021, saving model to temp/e70\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0191 - mean_squared_error: 1.0191 - val_loss: 1.0295 - val_mean_squared_error: 1.0295\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.02021\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0185 - mean_squared_error: 1.0185 - val_loss: 1.0204 - val_mean_squared_error: 1.0204\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.02021\n",
      "Epoch 00003: early stopping\n",
      "temp/e71\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.1005 - mean_squared_error: 1.1005 - val_loss: 1.0172 - val_mean_squared_error: 1.0172\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01717, saving model to temp/e71\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0203 - mean_squared_error: 1.0203 - val_loss: 1.0176 - val_mean_squared_error: 1.0176\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01717\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0140 - val_mean_squared_error: 1.0140\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01717 to 1.01403, saving model to temp/e71\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0105 - val_mean_squared_error: 1.0105\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01403 to 1.01049, saving model to temp/e71\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 1.0042 - val_mean_squared_error: 1.0042\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01049 to 1.00422, saving model to temp/e71\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0073 - mean_squared_error: 1.0073 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00422\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0100 - val_mean_squared_error: 1.0100\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00422\n",
      "Epoch 00007: early stopping\n",
      "temp/e72\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1084 - mean_squared_error: 1.1084 - val_loss: 1.0195 - val_mean_squared_error: 1.0195\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01945, saving model to temp/e72\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0182 - mean_squared_error: 1.0182 - val_loss: 1.0190 - val_mean_squared_error: 1.0190\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01945 to 1.01902, saving model to temp/e72\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0123 - mean_squared_error: 1.0123 - val_loss: 1.0077 - val_mean_squared_error: 1.0077\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01902 to 1.00768, saving model to temp/e72\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0082 - mean_squared_error: 1.0082 - val_loss: 1.0211 - val_mean_squared_error: 1.0211\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00768\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0114 - mean_squared_error: 1.0114 - val_loss: 1.0068 - val_mean_squared_error: 1.0068\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00768 to 1.00678, saving model to temp/e72\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 0.9990 - val_mean_squared_error: 0.9990\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00678 to 0.99905, saving model to temp/e72\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0076 - mean_squared_error: 1.0076 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99905\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0081 - mean_squared_error: 1.0081 - val_loss: 1.0082 - val_mean_squared_error: 1.0082\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99905\n",
      "Epoch 00008: early stopping\n",
      "temp/e73\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1042 - mean_squared_error: 1.1042 - val_loss: 1.0177 - val_mean_squared_error: 1.0177\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01773, saving model to temp/e73\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0211 - mean_squared_error: 1.0211 - val_loss: 1.0115 - val_mean_squared_error: 1.0115\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01773 to 1.01153, saving model to temp/e73\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0164 - mean_squared_error: 1.0164 - val_loss: 1.0110 - val_mean_squared_error: 1.0110\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01153 to 1.01102, saving model to temp/e73\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0172 - mean_squared_error: 1.0172 - val_loss: 1.0029 - val_mean_squared_error: 1.0029\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01102 to 1.00292, saving model to temp/e73\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00292\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0106 - mean_squared_error: 1.0106 - val_loss: 1.0021 - val_mean_squared_error: 1.0021\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00292 to 1.00214, saving model to temp/e73\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0062 - mean_squared_error: 1.0062 - val_loss: 1.0173 - val_mean_squared_error: 1.0173\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00214\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0004 - val_mean_squared_error: 1.0004\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00214 to 1.00044, saving model to temp/e73\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0051 - mean_squared_error: 1.0051 - val_loss: 1.0020 - val_mean_squared_error: 1.0020\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00044\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0069 - mean_squared_error: 1.0069 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.00044\n",
      "Epoch 00010: early stopping\n",
      "temp/e74\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0987 - mean_squared_error: 1.0987 - val_loss: 1.0233 - val_mean_squared_error: 1.0233\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02327, saving model to temp/e74\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0177 - mean_squared_error: 1.0177 - val_loss: 1.0083 - val_mean_squared_error: 1.0083\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02327 to 1.00831, saving model to temp/e74\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0129 - mean_squared_error: 1.0129 - val_loss: 1.0101 - val_mean_squared_error: 1.0101\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00831\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00831 to 1.00692, saving model to temp/e74\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0167 - val_mean_squared_error: 1.0167\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00692\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0074 - val_mean_squared_error: 1.0074\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00692\n",
      "Epoch 00006: early stopping\n",
      "temp/e75\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1135 - mean_squared_error: 1.1135 - val_loss: 1.0159 - val_mean_squared_error: 1.0159\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01593, saving model to temp/e75\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0202 - mean_squared_error: 1.0202 - val_loss: 1.0172 - val_mean_squared_error: 1.0172\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01593\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0147 - mean_squared_error: 1.0147 - val_loss: 1.0094 - val_mean_squared_error: 1.0094\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01593 to 1.00935, saving model to temp/e75\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0128 - mean_squared_error: 1.0128 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00935 to 1.00377, saving model to temp/e75\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0086 - mean_squared_error: 1.0086 - val_loss: 1.0003 - val_mean_squared_error: 1.0003\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00377 to 1.00034, saving model to temp/e75\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0010 - val_mean_squared_error: 1.0010\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00034\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0048 - mean_squared_error: 1.0048 - val_loss: 1.0152 - val_mean_squared_error: 1.0152\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00034\n",
      "Epoch 00007: early stopping\n",
      "temp/e76\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1015 - mean_squared_error: 1.1015 - val_loss: 1.0119 - val_mean_squared_error: 1.0119\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01188, saving model to temp/e76\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0197 - mean_squared_error: 1.0197 - val_loss: 1.0064 - val_mean_squared_error: 1.0064\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01188 to 1.00642, saving model to temp/e76\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.0176 - mean_squared_error: 1.0176 - val_loss: 1.0101 - val_mean_squared_error: 1.0101\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00642\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0157 - mean_squared_error: 1.0157 - val_loss: 1.0124 - val_mean_squared_error: 1.0124\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00642\n",
      "Epoch 00004: early stopping\n",
      "temp/e77\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1087 - mean_squared_error: 1.1087 - val_loss: 1.0179 - val_mean_squared_error: 1.0179\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01788, saving model to temp/e77\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0214 - mean_squared_error: 1.0214 - val_loss: 1.0132 - val_mean_squared_error: 1.0132\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01788 to 1.01318, saving model to temp/e77\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0119 - mean_squared_error: 1.0119 - val_loss: 1.0097 - val_mean_squared_error: 1.0097\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01318 to 1.00965, saving model to temp/e77\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0109 - mean_squared_error: 1.0109 - val_loss: 1.0213 - val_mean_squared_error: 1.0213\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00965\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0099 - val_mean_squared_error: 1.0099\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00965\n",
      "Epoch 00005: early stopping\n",
      "temp/e78\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1022 - mean_squared_error: 1.1022 - val_loss: 1.0146 - val_mean_squared_error: 1.0146\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01458, saving model to temp/e78\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0185 - mean_squared_error: 1.0185 - val_loss: 1.0309 - val_mean_squared_error: 1.0309\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01458\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0151 - mean_squared_error: 1.0151 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01458 to 1.00518, saving model to temp/e78\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00518 to 1.00397, saving model to temp/e78\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00397\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0107 - mean_squared_error: 1.0107 - val_loss: 0.9990 - val_mean_squared_error: 0.9990\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00397 to 0.99898, saving model to temp/e78\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99898\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 1.0031 - val_mean_squared_error: 1.0031\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99898\n",
      "Epoch 00008: early stopping\n",
      "temp/e79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.1029 - mean_squared_error: 1.1029 - val_loss: 1.0219 - val_mean_squared_error: 1.0219\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02191, saving model to temp/e79\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0172 - mean_squared_error: 1.0172 - val_loss: 1.0271 - val_mean_squared_error: 1.0271\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.02191\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0164 - mean_squared_error: 1.0164 - val_loss: 1.0071 - val_mean_squared_error: 1.0071\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02191 to 1.00713, saving model to temp/e79\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0129 - mean_squared_error: 1.0129 - val_loss: 1.0067 - val_mean_squared_error: 1.0067\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00713 to 1.00673, saving model to temp/e79\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 1.0036 - val_mean_squared_error: 1.0036\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00673 to 1.00363, saving model to temp/e79\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.0072 - mean_squared_error: 1.0072 - val_loss: 1.0009 - val_mean_squared_error: 1.0009\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00363 to 1.00091, saving model to temp/e79\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 1.0031 - val_mean_squared_error: 1.0031\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00091\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0082 - mean_squared_error: 1.0082 - val_loss: 0.9999 - val_mean_squared_error: 0.9999\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00091 to 0.99991, saving model to temp/e79\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0055 - mean_squared_error: 1.0055 - val_loss: 1.0027 - val_mean_squared_error: 1.0027\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99991\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0031 - mean_squared_error: 1.0031 - val_loss: 0.9965 - val_mean_squared_error: 0.9965\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.99991 to 0.99645, saving model to temp/e79\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0050 - mean_squared_error: 1.0050 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99645\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0048 - mean_squared_error: 1.0048 - val_loss: 0.9993 - val_mean_squared_error: 0.9993\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.99645\n",
      "Epoch 00012: early stopping\n",
      "temp/e80\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1001 - mean_squared_error: 1.1001 - val_loss: 1.0132 - val_mean_squared_error: 1.0132\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01318, saving model to temp/e80\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0175 - mean_squared_error: 1.0175 - val_loss: 1.0078 - val_mean_squared_error: 1.0078\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01318 to 1.00778, saving model to temp/e80\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0119 - mean_squared_error: 1.0119 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00778\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 1.0074 - val_mean_squared_error: 1.0074\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00778 to 1.00745, saving model to temp/e80\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0118 - mean_squared_error: 1.0118 - val_loss: 1.0017 - val_mean_squared_error: 1.0017\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00745 to 1.00167, saving model to temp/e80\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0125 - mean_squared_error: 1.0125 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00167\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 0.9982 - val_mean_squared_error: 0.9982\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00167 to 0.99818, saving model to temp/e80\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0063 - mean_squared_error: 1.0063 - val_loss: 1.0043 - val_mean_squared_error: 1.0043\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99818\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0058 - mean_squared_error: 1.0058 - val_loss: 0.9984 - val_mean_squared_error: 0.9984\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99818\n",
      "Epoch 00009: early stopping\n",
      "temp/e81\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1085 - mean_squared_error: 1.1085 - val_loss: 1.0229 - val_mean_squared_error: 1.0229\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02287, saving model to temp/e81\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0196 - mean_squared_error: 1.0196 - val_loss: 1.0129 - val_mean_squared_error: 1.0129\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02287 to 1.01286, saving model to temp/e81\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0073 - val_mean_squared_error: 1.0073\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01286 to 1.00725, saving model to temp/e81\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00725 to 1.00475, saving model to temp/e81\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0103 - mean_squared_error: 1.0103 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00475 to 1.00328, saving model to temp/e81\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 0.9984 - val_mean_squared_error: 0.9984\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00328 to 0.99839, saving model to temp/e81\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 1.0073 - val_mean_squared_error: 1.0073\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99839\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0075 - mean_squared_error: 1.0075 - val_loss: 1.0108 - val_mean_squared_error: 1.0108\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99839\n",
      "Epoch 00008: early stopping\n",
      "temp/e82\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.1061 - mean_squared_error: 1.1061 - val_loss: 1.0153 - val_mean_squared_error: 1.0153\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01532, saving model to temp/e82\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0176 - mean_squared_error: 1.0176 - val_loss: 1.0232 - val_mean_squared_error: 1.0232\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01532\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0153 - mean_squared_error: 1.0153 - val_loss: 1.0071 - val_mean_squared_error: 1.0071\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01532 to 1.00706, saving model to temp/e82\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0056 - val_mean_squared_error: 1.0056\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00706 to 1.00560, saving model to temp/e82\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00560 to 1.00326, saving model to temp/e82\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0018 - val_mean_squared_error: 1.0018\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00326 to 1.00181, saving model to temp/e82\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 0.9994 - val_mean_squared_error: 0.9994\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00181 to 0.99942, saving model to temp/e82\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 0.9989 - val_mean_squared_error: 0.9989\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.99942 to 0.99887, saving model to temp/e82\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0063 - mean_squared_error: 1.0063 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99887\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0034 - mean_squared_error: 1.0034 - val_loss: 1.0001 - val_mean_squared_error: 1.0001\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99887\n",
      "Epoch 00010: early stopping\n",
      "temp/e83\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.1031 - mean_squared_error: 1.1031 - val_loss: 1.0327 - val_mean_squared_error: 1.0327\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03275, saving model to temp/e83\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0181 - val_mean_squared_error: 1.0181\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03275 to 1.01811, saving model to temp/e83\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0173 - mean_squared_error: 1.0173 - val_loss: 1.0160 - val_mean_squared_error: 1.0160\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01811 to 1.01597, saving model to temp/e83\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01597 to 1.00494, saving model to temp/e83\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0129 - mean_squared_error: 1.0129 - val_loss: 1.0025 - val_mean_squared_error: 1.0025\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00494 to 1.00255, saving model to temp/e83\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 0.9997 - val_mean_squared_error: 0.9997\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00255 to 0.99973, saving model to temp/e83\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0071 - mean_squared_error: 1.0071 - val_loss: 1.0141 - val_mean_squared_error: 1.0141\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99973\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0059 - mean_squared_error: 1.0059 - val_loss: 1.0004 - val_mean_squared_error: 1.0004\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99973\n",
      "Epoch 00008: early stopping\n",
      "temp/e84\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1049 - mean_squared_error: 1.1049 - val_loss: 1.0122 - val_mean_squared_error: 1.0122\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01222, saving model to temp/e84\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0210 - mean_squared_error: 1.0210 - val_loss: 1.0115 - val_mean_squared_error: 1.0115\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01222 to 1.01150, saving model to temp/e84\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0156 - mean_squared_error: 1.0156 - val_loss: 1.0018 - val_mean_squared_error: 1.0018\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01150 to 1.00179, saving model to temp/e84\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00179\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0173 - val_mean_squared_error: 1.0173\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00179\n",
      "Epoch 00005: early stopping\n",
      "temp/e85\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.1052 - mean_squared_error: 1.1052 - val_loss: 1.0189 - val_mean_squared_error: 1.0189\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01889, saving model to temp/e85\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0215 - mean_squared_error: 1.0215 - val_loss: 1.0068 - val_mean_squared_error: 1.0068\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01889 to 1.00681, saving model to temp/e85\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0099 - val_mean_squared_error: 1.0099\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00681\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0129 - mean_squared_error: 1.0129 - val_loss: 1.0136 - val_mean_squared_error: 1.0136\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00681\n",
      "Epoch 00004: early stopping\n",
      "temp/e86\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1057 - mean_squared_error: 1.1057 - val_loss: 1.0133 - val_mean_squared_error: 1.0133\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01327, saving model to temp/e86\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0184 - mean_squared_error: 1.0184 - val_loss: 1.0266 - val_mean_squared_error: 1.0266\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01327\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0149 - mean_squared_error: 1.0149 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01327 to 1.00304, saving model to temp/e86\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0155 - mean_squared_error: 1.0155 - val_loss: 1.0042 - val_mean_squared_error: 1.0042\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00304\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 1.0068 - val_mean_squared_error: 1.0068\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00304\n",
      "Epoch 00005: early stopping\n",
      "temp/e87\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1016 - mean_squared_error: 1.1016 - val_loss: 1.0145 - val_mean_squared_error: 1.0145\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01451, saving model to temp/e87\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0168 - mean_squared_error: 1.0168 - val_loss: 1.0105 - val_mean_squared_error: 1.0105\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01451 to 1.01054, saving model to temp/e87\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0153 - mean_squared_error: 1.0153 - val_loss: 1.0121 - val_mean_squared_error: 1.0121\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01054\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0154 - mean_squared_error: 1.0154 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01054 to 1.00263, saving model to temp/e87\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00263\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00263\n",
      "Epoch 00006: early stopping\n",
      "temp/e88\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.1134 - mean_squared_error: 1.1134 - val_loss: 1.0130 - val_mean_squared_error: 1.0130\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01298, saving model to temp/e88\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0207 - mean_squared_error: 1.0207 - val_loss: 1.0074 - val_mean_squared_error: 1.0074\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01298 to 1.00738, saving model to temp/e88\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0161 - mean_squared_error: 1.0161 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00738 to 1.00490, saving model to temp/e88\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0144 - mean_squared_error: 1.0144 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00490\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0093 - mean_squared_error: 1.0093 - val_loss: 1.0092 - val_mean_squared_error: 1.0092\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00490\n",
      "Epoch 00005: early stopping\n",
      "temp/e89\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.0993 - mean_squared_error: 1.0993 - val_loss: 1.0152 - val_mean_squared_error: 1.0152\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01521, saving model to temp/e89\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 1.0134 - val_mean_squared_error: 1.0134\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01521 to 1.01340, saving model to temp/e89\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0136 - mean_squared_error: 1.0136 - val_loss: 1.0131 - val_mean_squared_error: 1.0131\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01340 to 1.01306, saving model to temp/e89\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0125 - mean_squared_error: 1.0125 - val_loss: 1.0065 - val_mean_squared_error: 1.0065\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01306 to 1.00650, saving model to temp/e89\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0138 - mean_squared_error: 1.0138 - val_loss: 1.0014 - val_mean_squared_error: 1.0014\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00650 to 1.00143, saving model to temp/e89\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0063 - mean_squared_error: 1.0063 - val_loss: 1.0017 - val_mean_squared_error: 1.0017\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00143\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0046 - mean_squared_error: 1.0046 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00143\n",
      "Epoch 00007: early stopping\n",
      "temp/e90\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1052 - mean_squared_error: 1.1052 - val_loss: 1.0259 - val_mean_squared_error: 1.0259\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02586, saving model to temp/e90\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0223 - mean_squared_error: 1.0223 - val_loss: 1.0105 - val_mean_squared_error: 1.0105\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02586 to 1.01049, saving model to temp/e90\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 1.0107 - val_mean_squared_error: 1.0107\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01049\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 1.0035 - val_mean_squared_error: 1.0035\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01049 to 1.00347, saving model to temp/e90\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0035 - val_mean_squared_error: 1.0035\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00347\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00347\n",
      "Epoch 00006: early stopping\n",
      "temp/e91\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1084 - mean_squared_error: 1.1084 - val_loss: 1.0234 - val_mean_squared_error: 1.0234\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02341, saving model to temp/e91\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0212 - mean_squared_error: 1.0212 - val_loss: 1.0114 - val_mean_squared_error: 1.0114\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02341 to 1.01140, saving model to temp/e91\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0201 - mean_squared_error: 1.0201 - val_loss: 1.0132 - val_mean_squared_error: 1.0132\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01140\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0157 - mean_squared_error: 1.0157 - val_loss: 1.0074 - val_mean_squared_error: 1.0074\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01140 to 1.00744, saving model to temp/e91\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00744 to 1.00567, saving model to temp/e91\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0066 - mean_squared_error: 1.0066 - val_loss: 0.9998 - val_mean_squared_error: 0.9998\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00567 to 0.99982, saving model to temp/e91\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0068 - mean_squared_error: 1.0068 - val_loss: 1.0017 - val_mean_squared_error: 1.0017\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99982\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0106 - val_mean_squared_error: 1.0106\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99982\n",
      "Epoch 00008: early stopping\n",
      "temp/e92\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1008 - mean_squared_error: 1.1008 - val_loss: 1.0208 - val_mean_squared_error: 1.0208\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02077, saving model to temp/e92\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0170 - mean_squared_error: 1.0170 - val_loss: 1.0108 - val_mean_squared_error: 1.0108\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02077 to 1.01084, saving model to temp/e92\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0175 - mean_squared_error: 1.0175 - val_loss: 1.0064 - val_mean_squared_error: 1.0064\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01084 to 1.00644, saving model to temp/e92\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0229 - val_mean_squared_error: 1.0229\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00644\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0106 - mean_squared_error: 1.0106 - val_loss: 1.0117 - val_mean_squared_error: 1.0117\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00644\n",
      "Epoch 00005: early stopping\n",
      "temp/e93\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.0966 - mean_squared_error: 1.0966 - val_loss: 1.0201 - val_mean_squared_error: 1.0201\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02007, saving model to temp/e93\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0186 - mean_squared_error: 1.0186 - val_loss: 1.0128 - val_mean_squared_error: 1.0128\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02007 to 1.01283, saving model to temp/e93\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0163 - mean_squared_error: 1.0163 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01283 to 1.00338, saving model to temp/e93\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0107 - mean_squared_error: 1.0107 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00338\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0103 - mean_squared_error: 1.0103 - val_loss: 1.0031 - val_mean_squared_error: 1.0031\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00338 to 1.00315, saving model to temp/e93\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0076 - mean_squared_error: 1.0076 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00315 to 1.00255, saving model to temp/e93\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0049 - mean_squared_error: 1.0049 - val_loss: 1.0003 - val_mean_squared_error: 1.0003\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00255 to 1.00034, saving model to temp/e93\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 0.9986 - val_mean_squared_error: 0.9986\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00034 to 0.99861, saving model to temp/e93\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0058 - mean_squared_error: 1.0058 - val_loss: 0.9965 - val_mean_squared_error: 0.9965\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.99861 to 0.99647, saving model to temp/e93\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0035 - mean_squared_error: 1.0035 - val_loss: 0.9986 - val_mean_squared_error: 0.9986\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99647\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0040 - mean_squared_error: 1.0040 - val_loss: 1.0003 - val_mean_squared_error: 1.0003\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99647\n",
      "Epoch 00011: early stopping\n",
      "temp/e94\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.0955 - mean_squared_error: 1.0955 - val_loss: 1.0128 - val_mean_squared_error: 1.0128\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01279, saving model to temp/e94\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0163 - mean_squared_error: 1.0163 - val_loss: 1.0028 - val_mean_squared_error: 1.0028\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01279 to 1.00283, saving model to temp/e94\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0170 - mean_squared_error: 1.0170 - val_loss: 1.0116 - val_mean_squared_error: 1.0116\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00283\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0152 - mean_squared_error: 1.0152 - val_loss: 1.0068 - val_mean_squared_error: 1.0068\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00283\n",
      "Epoch 00004: early stopping\n",
      "temp/e95\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1032 - mean_squared_error: 1.1032 - val_loss: 1.0289 - val_mean_squared_error: 1.0289\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02887, saving model to temp/e95\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0171 - mean_squared_error: 1.0171 - val_loss: 1.0090 - val_mean_squared_error: 1.0090\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02887 to 1.00905, saving model to temp/e95\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0170 - mean_squared_error: 1.0170 - val_loss: 1.0045 - val_mean_squared_error: 1.0045\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00905 to 1.00447, saving model to temp/e95\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0134 - mean_squared_error: 1.0134 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00447 to 1.00265, saving model to temp/e95\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0075 - mean_squared_error: 1.0075 - val_loss: 1.0206 - val_mean_squared_error: 1.0206\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00265\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 142us/step - loss: 1.0086 - mean_squared_error: 1.0086 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00265\n",
      "Epoch 00006: early stopping\n",
      "temp/e96\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.1019 - mean_squared_error: 1.1019 - val_loss: 1.0353 - val_mean_squared_error: 1.0353\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03534, saving model to temp/e96\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0238 - mean_squared_error: 1.0238 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03534 to 1.00624, saving model to temp/e96\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0175 - mean_squared_error: 1.0175 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00624 to 1.00333, saving model to temp/e96\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0162 - mean_squared_error: 1.0162 - val_loss: 1.0126 - val_mean_squared_error: 1.0126\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00333\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0044 - val_mean_squared_error: 1.0044\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00333\n",
      "Epoch 00005: early stopping\n",
      "temp/e97\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1019 - mean_squared_error: 1.1019 - val_loss: 1.0114 - val_mean_squared_error: 1.0114\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01138, saving model to temp/e97\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0206 - mean_squared_error: 1.0206 - val_loss: 1.0114 - val_mean_squared_error: 1.0114\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01138 to 1.01138, saving model to temp/e97\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0154 - mean_squared_error: 1.0154 - val_loss: 1.0146 - val_mean_squared_error: 1.0146\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01138\n",
      "Epoch 00003: early stopping\n",
      "temp/e98\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.1002 - mean_squared_error: 1.1002 - val_loss: 1.0191 - val_mean_squared_error: 1.0191\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01909, saving model to temp/e98\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0213 - mean_squared_error: 1.0213 - val_loss: 1.0147 - val_mean_squared_error: 1.0147\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01909 to 1.01475, saving model to temp/e98\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 1.0099 - val_mean_squared_error: 1.0099\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01475 to 1.00992, saving model to temp/e98\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0094 - val_mean_squared_error: 1.0094\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00992 to 1.00945, saving model to temp/e98\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00945 to 1.00570, saving model to temp/e98\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00570 to 1.00232, saving model to temp/e98\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 1.0014 - val_mean_squared_error: 1.0014\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00232 to 1.00142, saving model to temp/e98\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0050 - mean_squared_error: 1.0050 - val_loss: 1.0177 - val_mean_squared_error: 1.0177\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00142\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0070 - mean_squared_error: 1.0070 - val_loss: 1.0068 - val_mean_squared_error: 1.0068\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00142\n",
      "Epoch 00009: early stopping\n",
      "temp/e99\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1079 - mean_squared_error: 1.1079 - val_loss: 1.0187 - val_mean_squared_error: 1.0187\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01872, saving model to temp/e99\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0190 - mean_squared_error: 1.0190 - val_loss: 1.0120 - val_mean_squared_error: 1.0120\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01872 to 1.01199, saving model to temp/e99\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0144 - val_mean_squared_error: 1.0144\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01199\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0142 - mean_squared_error: 1.0142 - val_loss: 1.0029 - val_mean_squared_error: 1.0029\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01199 to 1.00293, saving model to temp/e99\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0076 - val_mean_squared_error: 1.0076\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00293\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00293\n",
      "Epoch 00006: early stopping\n",
      "temp/e100\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.1001 - mean_squared_error: 1.1001 - val_loss: 1.0204 - val_mean_squared_error: 1.0204\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02044, saving model to temp/e100\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0188 - mean_squared_error: 1.0188 - val_loss: 1.0138 - val_mean_squared_error: 1.0138\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02044 to 1.01381, saving model to temp/e100\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0153 - mean_squared_error: 1.0153 - val_loss: 1.0108 - val_mean_squared_error: 1.0108\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01381 to 1.01085, saving model to temp/e100\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0102 - val_mean_squared_error: 1.0102\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01085 to 1.01024, saving model to temp/e100\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 1.0001 - val_mean_squared_error: 1.0001\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01024 to 1.00009, saving model to temp/e100\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0089 - mean_squared_error: 1.0089 - val_loss: 1.0008 - val_mean_squared_error: 1.0008\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00009\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0065 - mean_squared_error: 1.0065 - val_loss: 0.9996 - val_mean_squared_error: 0.9996\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00009 to 0.99963, saving model to temp/e100\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0071 - mean_squared_error: 1.0071 - val_loss: 1.0006 - val_mean_squared_error: 1.0006\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99963\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0011 - val_mean_squared_error: 1.0011\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99963\n",
      "Epoch 00009: early stopping\n",
      "temp/e101\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.0984 - mean_squared_error: 1.0984 - val_loss: 1.0272 - val_mean_squared_error: 1.0272\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02721, saving model to temp/e101\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0207 - mean_squared_error: 1.0207 - val_loss: 1.0133 - val_mean_squared_error: 1.0133\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02721 to 1.01325, saving model to temp/e101\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01325 to 1.00925, saving model to temp/e101\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0144 - mean_squared_error: 1.0144 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00925 to 1.00333, saving model to temp/e101\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00333\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0109 - mean_squared_error: 1.0109 - val_loss: 1.0020 - val_mean_squared_error: 1.0020\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00333 to 1.00200, saving model to temp/e101\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0065 - mean_squared_error: 1.0065 - val_loss: 0.9988 - val_mean_squared_error: 0.9988\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00200 to 0.99881, saving model to temp/e101\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99881\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 1.0016 - val_mean_squared_error: 1.0016\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99881\n",
      "Epoch 00009: early stopping\n",
      "temp/e102\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.1043 - mean_squared_error: 1.1043 - val_loss: 1.0104 - val_mean_squared_error: 1.0104\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01043, saving model to temp/e102\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0166 - mean_squared_error: 1.0166 - val_loss: 1.0343 - val_mean_squared_error: 1.0343\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01043\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0170 - mean_squared_error: 1.0170 - val_loss: 1.0151 - val_mean_squared_error: 1.0151\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01043\n",
      "Epoch 00003: early stopping\n",
      "temp/e103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.0991 - mean_squared_error: 1.0991 - val_loss: 1.0148 - val_mean_squared_error: 1.0148\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01479, saving model to temp/e103\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.0233 - mean_squared_error: 1.0233 - val_loss: 1.0233 - val_mean_squared_error: 1.0233\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01479\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0165 - mean_squared_error: 1.0165 - val_loss: 1.0214 - val_mean_squared_error: 1.0214\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01479\n",
      "Epoch 00003: early stopping\n",
      "temp/e104\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1035 - mean_squared_error: 1.1035 - val_loss: 1.0222 - val_mean_squared_error: 1.0222\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02220, saving model to temp/e104\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0194 - mean_squared_error: 1.0194 - val_loss: 1.0271 - val_mean_squared_error: 1.0271\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.02220\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0137 - mean_squared_error: 1.0137 - val_loss: 1.0087 - val_mean_squared_error: 1.0087\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02220 to 1.00874, saving model to temp/e104\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0138 - mean_squared_error: 1.0138 - val_loss: 1.0031 - val_mean_squared_error: 1.0031\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00874 to 1.00309, saving model to temp/e104\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 1.0022 - val_mean_squared_error: 1.0022\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00309 to 1.00220, saving model to temp/e104\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0070 - val_mean_squared_error: 1.0070\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00220\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 0.9996 - val_mean_squared_error: 0.9996\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00220 to 0.99957, saving model to temp/e104\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0041 - mean_squared_error: 1.0041 - val_loss: 1.0031 - val_mean_squared_error: 1.0031\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99957\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0055 - mean_squared_error: 1.0055 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99957\n",
      "Epoch 00009: early stopping\n",
      "temp/e105\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1029 - mean_squared_error: 1.1029 - val_loss: 1.0313 - val_mean_squared_error: 1.0313\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03133, saving model to temp/e105\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0213 - mean_squared_error: 1.0213 - val_loss: 1.0184 - val_mean_squared_error: 1.0184\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03133 to 1.01842, saving model to temp/e105\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0157 - mean_squared_error: 1.0157 - val_loss: 1.0035 - val_mean_squared_error: 1.0035\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01842 to 1.00354, saving model to temp/e105\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 1.0109 - val_mean_squared_error: 1.0109\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00354\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0111 - mean_squared_error: 1.0111 - val_loss: 1.0008 - val_mean_squared_error: 1.0008\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00354 to 1.00081, saving model to temp/e105\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 1.0019 - val_mean_squared_error: 1.0019\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00081\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0111 - mean_squared_error: 1.0111 - val_loss: 1.0043 - val_mean_squared_error: 1.0043\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00081\n",
      "Epoch 00007: early stopping\n",
      "temp/e106\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.0916 - mean_squared_error: 1.0916 - val_loss: 1.0210 - val_mean_squared_error: 1.0210\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02096, saving model to temp/e106\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0196 - mean_squared_error: 1.0196 - val_loss: 1.0233 - val_mean_squared_error: 1.0233\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.02096\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0145 - mean_squared_error: 1.0145 - val_loss: 1.0082 - val_mean_squared_error: 1.0082\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02096 to 1.00817, saving model to temp/e106\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0119 - mean_squared_error: 1.0119 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00817 to 1.00468, saving model to temp/e106\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0105 - mean_squared_error: 1.0105 - val_loss: 1.0058 - val_mean_squared_error: 1.0058\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00468\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 1.0022 - val_mean_squared_error: 1.0022\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00468 to 1.00219, saving model to temp/e106\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0042 - val_mean_squared_error: 1.0042\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00219\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0050 - mean_squared_error: 1.0050 - val_loss: 1.0019 - val_mean_squared_error: 1.0019\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00219 to 1.00188, saving model to temp/e106\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0051 - mean_squared_error: 1.0051 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00188\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0064 - mean_squared_error: 1.0064 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.00188\n",
      "Epoch 00010: early stopping\n",
      "temp/e107\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.1054 - mean_squared_error: 1.1054 - val_loss: 1.0212 - val_mean_squared_error: 1.0212\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02123, saving model to temp/e107\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0157 - mean_squared_error: 1.0157 - val_loss: 1.0094 - val_mean_squared_error: 1.0094\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02123 to 1.00938, saving model to temp/e107\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0160 - mean_squared_error: 1.0160 - val_loss: 1.0137 - val_mean_squared_error: 1.0137\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00938\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0134 - mean_squared_error: 1.0134 - val_loss: 0.9993 - val_mean_squared_error: 0.9993\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00938 to 0.99933, saving model to temp/e107\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0134 - mean_squared_error: 1.0134 - val_loss: 0.9986 - val_mean_squared_error: 0.9986\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.99933 to 0.99855, saving model to temp/e107\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0105 - mean_squared_error: 1.0105 - val_loss: 0.9994 - val_mean_squared_error: 0.9994\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99855\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0005 - val_mean_squared_error: 1.0005\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99855\n",
      "Epoch 00007: early stopping\n",
      "temp/e108\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.1035 - mean_squared_error: 1.1035 - val_loss: 1.0226 - val_mean_squared_error: 1.0226\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02256, saving model to temp/e108\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0194 - mean_squared_error: 1.0194 - val_loss: 1.0204 - val_mean_squared_error: 1.0204\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02256 to 1.02040, saving model to temp/e108\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0145 - mean_squared_error: 1.0145 - val_loss: 1.0114 - val_mean_squared_error: 1.0114\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02040 to 1.01142, saving model to temp/e108\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0114 - mean_squared_error: 1.0114 - val_loss: 1.0028 - val_mean_squared_error: 1.0028\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01142 to 1.00277, saving model to temp/e108\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0076 - mean_squared_error: 1.0076 - val_loss: 1.0008 - val_mean_squared_error: 1.0008\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00277 to 1.00075, saving model to temp/e108\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0032 - val_mean_squared_error: 1.0032\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00075\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0081 - mean_squared_error: 1.0081 - val_loss: 1.0099 - val_mean_squared_error: 1.0099\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00075\n",
      "Epoch 00007: early stopping\n",
      "temp/e109\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1141 - mean_squared_error: 1.1141 - val_loss: 1.0188 - val_mean_squared_error: 1.0188\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01884, saving model to temp/e109\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0189 - mean_squared_error: 1.0189 - val_loss: 1.0103 - val_mean_squared_error: 1.0103\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01884 to 1.01034, saving model to temp/e109\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0162 - mean_squared_error: 1.0162 - val_loss: 1.0046 - val_mean_squared_error: 1.0046\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01034 to 1.00455, saving model to temp/e109\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 1.0133 - val_mean_squared_error: 1.0133\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00455\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0168 - val_mean_squared_error: 1.0168\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00455\n",
      "Epoch 00005: early stopping\n",
      "temp/e110\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1041 - mean_squared_error: 1.1041 - val_loss: 1.0357 - val_mean_squared_error: 1.0357\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03572, saving model to temp/e110\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0230 - mean_squared_error: 1.0230 - val_loss: 1.0092 - val_mean_squared_error: 1.0092\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03572 to 1.00919, saving model to temp/e110\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0161 - mean_squared_error: 1.0161 - val_loss: 1.0110 - val_mean_squared_error: 1.0110\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00919\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0129 - mean_squared_error: 1.0129 - val_loss: 1.0196 - val_mean_squared_error: 1.0196\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00919\n",
      "Epoch 00004: early stopping\n",
      "temp/e111\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0980 - mean_squared_error: 1.0980 - val_loss: 1.0224 - val_mean_squared_error: 1.0224\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02239, saving model to temp/e111\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0208 - mean_squared_error: 1.0208 - val_loss: 1.0139 - val_mean_squared_error: 1.0139\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02239 to 1.01391, saving model to temp/e111\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0166 - mean_squared_error: 1.0166 - val_loss: 1.0005 - val_mean_squared_error: 1.0005\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01391 to 1.00045, saving model to temp/e111\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0129 - val_mean_squared_error: 1.0129\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00045\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0134 - mean_squared_error: 1.0134 - val_loss: 1.0013 - val_mean_squared_error: 1.0013\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00045\n",
      "Epoch 00005: early stopping\n",
      "temp/e112\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1009 - mean_squared_error: 1.1009 - val_loss: 1.0234 - val_mean_squared_error: 1.0234\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02336, saving model to temp/e112\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0180 - mean_squared_error: 1.0180 - val_loss: 1.0106 - val_mean_squared_error: 1.0106\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02336 to 1.01061, saving model to temp/e112\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0156 - mean_squared_error: 1.0156 - val_loss: 1.0188 - val_mean_squared_error: 1.0188\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01061\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0107 - mean_squared_error: 1.0107 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01061 to 1.00694, saving model to temp/e112\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0151 - mean_squared_error: 1.0151 - val_loss: 1.0028 - val_mean_squared_error: 1.0028\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00694 to 1.00280, saving model to temp/e112\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0070 - mean_squared_error: 1.0070 - val_loss: 1.0127 - val_mean_squared_error: 1.0127\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00280\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0093 - mean_squared_error: 1.0093 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00280\n",
      "Epoch 00007: early stopping\n",
      "temp/e113\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.1064 - mean_squared_error: 1.1064 - val_loss: 1.0207 - val_mean_squared_error: 1.0207\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02068, saving model to temp/e113\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0221 - mean_squared_error: 1.0221 - val_loss: 1.0166 - val_mean_squared_error: 1.0166\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02068 to 1.01661, saving model to temp/e113\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0138 - mean_squared_error: 1.0138 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01661 to 1.00261, saving model to temp/e113\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0157 - mean_squared_error: 1.0157 - val_loss: 1.0112 - val_mean_squared_error: 1.0112\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00261\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0035 - val_mean_squared_error: 1.0035\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00261\n",
      "Epoch 00005: early stopping\n",
      "temp/e114\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1066 - mean_squared_error: 1.1066 - val_loss: 1.0192 - val_mean_squared_error: 1.0192\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01917, saving model to temp/e114\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01917 to 1.00929, saving model to temp/e114\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 123us/step - loss: 1.0163 - mean_squared_error: 1.0163 - val_loss: 1.0056 - val_mean_squared_error: 1.0056\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00929 to 1.00560, saving model to temp/e114\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0248 - val_mean_squared_error: 1.0248\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00560\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 0.9993 - val_mean_squared_error: 0.9993\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00560 to 0.99930, saving model to temp/e114\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0070 - mean_squared_error: 1.0070 - val_loss: 1.0105 - val_mean_squared_error: 1.0105\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99930\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0082 - mean_squared_error: 1.0082 - val_loss: 0.9996 - val_mean_squared_error: 0.9996\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99930\n",
      "Epoch 00007: early stopping\n",
      "temp/e115\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0971 - mean_squared_error: 1.0971 - val_loss: 1.0150 - val_mean_squared_error: 1.0150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01497, saving model to temp/e115\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0192 - mean_squared_error: 1.0192 - val_loss: 1.0128 - val_mean_squared_error: 1.0128\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01497 to 1.01282, saving model to temp/e115\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.0150 - mean_squared_error: 1.0150 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01282 to 1.00341, saving model to temp/e115\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0118 - mean_squared_error: 1.0118 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00341\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0096 - val_mean_squared_error: 1.0096\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00341\n",
      "Epoch 00005: early stopping\n",
      "temp/e116\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 166us/step - loss: 1.1007 - mean_squared_error: 1.1007 - val_loss: 1.0114 - val_mean_squared_error: 1.0114\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01141, saving model to temp/e116\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0225 - mean_squared_error: 1.0225 - val_loss: 1.0118 - val_mean_squared_error: 1.0118\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01141\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.0161 - mean_squared_error: 1.0161 - val_loss: 1.0221 - val_mean_squared_error: 1.0221\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01141\n",
      "Epoch 00003: early stopping\n",
      "temp/e117\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 160us/step - loss: 1.0980 - mean_squared_error: 1.0980 - val_loss: 1.0158 - val_mean_squared_error: 1.0158\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01579, saving model to temp/e117\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0190 - mean_squared_error: 1.0190 - val_loss: 1.0170 - val_mean_squared_error: 1.0170\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01579\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0198 - mean_squared_error: 1.0198 - val_loss: 1.0090 - val_mean_squared_error: 1.0090\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01579 to 1.00898, saving model to temp/e117\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0145 - mean_squared_error: 1.0145 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00898 to 1.00586, saving model to temp/e117\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0006 - val_mean_squared_error: 1.0006\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00586 to 1.00063, saving model to temp/e117\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00063\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0078 - mean_squared_error: 1.0078 - val_loss: 0.9991 - val_mean_squared_error: 0.9991\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00063 to 0.99912, saving model to temp/e117\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.0068 - mean_squared_error: 1.0068 - val_loss: 1.0043 - val_mean_squared_error: 1.0043\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99912\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 1.0046 - mean_squared_error: 1.0046 - val_loss: 0.9974 - val_mean_squared_error: 0.9974\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.99912 to 0.99736, saving model to temp/e117\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0048 - mean_squared_error: 1.0048 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99736\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 1.0047 - mean_squared_error: 1.0047 - val_loss: 0.9992 - val_mean_squared_error: 0.9992\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99736\n",
      "Epoch 00011: early stopping\n",
      "temp/e118\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.1004 - mean_squared_error: 1.1004 - val_loss: 1.0159 - val_mean_squared_error: 1.0159\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01593, saving model to temp/e118\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0192 - mean_squared_error: 1.0192 - val_loss: 1.0099 - val_mean_squared_error: 1.0099\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01593 to 1.00994, saving model to temp/e118\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0129 - mean_squared_error: 1.0129 - val_loss: 1.0063 - val_mean_squared_error: 1.0063\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00994 to 1.00629, saving model to temp/e118\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0138 - mean_squared_error: 1.0138 - val_loss: 1.0174 - val_mean_squared_error: 1.0174\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00629\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00629\n",
      "Epoch 00005: early stopping\n",
      "temp/e119\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1007 - mean_squared_error: 1.1007 - val_loss: 1.0292 - val_mean_squared_error: 1.0292\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02917, saving model to temp/e119\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0169 - mean_squared_error: 1.0169 - val_loss: 1.0100 - val_mean_squared_error: 1.0100\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02917 to 1.01001, saving model to temp/e119\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0158 - mean_squared_error: 1.0158 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01001 to 1.00588, saving model to temp/e119\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0263 - val_mean_squared_error: 1.0263\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00588\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0025 - val_mean_squared_error: 1.0025\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00588 to 1.00249, saving model to temp/e119\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00249 to 1.00244, saving model to temp/e119\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0063 - mean_squared_error: 1.0063 - val_loss: 1.0105 - val_mean_squared_error: 1.0105\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00244\n",
      "Epoch 00007: early stopping\n",
      "temp/e120\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 166us/step - loss: 1.0976 - mean_squared_error: 1.0976 - val_loss: 1.0138 - val_mean_squared_error: 1.0138\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01384, saving model to temp/e120\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0207 - mean_squared_error: 1.0207 - val_loss: 1.0121 - val_mean_squared_error: 1.0121\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01384 to 1.01214, saving model to temp/e120\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0177 - mean_squared_error: 1.0177 - val_loss: 1.0204 - val_mean_squared_error: 1.0204\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01214\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 1.0161 - val_mean_squared_error: 1.0161\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.01214\n",
      "Epoch 00004: early stopping\n",
      "temp/e121\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 166us/step - loss: 1.0974 - mean_squared_error: 1.0974 - val_loss: 1.0216 - val_mean_squared_error: 1.0216\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02163, saving model to temp/e121\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0197 - mean_squared_error: 1.0197 - val_loss: 1.0063 - val_mean_squared_error: 1.0063\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02163 to 1.00634, saving model to temp/e121\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 142us/step - loss: 1.0138 - mean_squared_error: 1.0138 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00634 to 1.00586, saving model to temp/e121\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00586\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 0.9997 - val_mean_squared_error: 0.9997\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00586 to 0.99969, saving model to temp/e121\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0119 - mean_squared_error: 1.0119 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99969\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 1.0002 - val_mean_squared_error: 1.0002\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99969\n",
      "Epoch 00007: early stopping\n",
      "temp/e122\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.1037 - mean_squared_error: 1.1037 - val_loss: 1.0269 - val_mean_squared_error: 1.0269\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02694, saving model to temp/e122\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0209 - mean_squared_error: 1.0209 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02694 to 1.00863, saving model to temp/e122\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0149 - mean_squared_error: 1.0149 - val_loss: 1.0150 - val_mean_squared_error: 1.0150\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00863\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0127 - mean_squared_error: 1.0127 - val_loss: 1.0095 - val_mean_squared_error: 1.0095\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00863\n",
      "Epoch 00004: early stopping\n",
      "temp/e123\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1004 - mean_squared_error: 1.1004 - val_loss: 1.0263 - val_mean_squared_error: 1.0263\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02633, saving model to temp/e123\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0203 - mean_squared_error: 1.0203 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02633 to 1.00881, saving model to temp/e123\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0148 - mean_squared_error: 1.0148 - val_loss: 1.0104 - val_mean_squared_error: 1.0104\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00881\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0148 - mean_squared_error: 1.0148 - val_loss: 1.0027 - val_mean_squared_error: 1.0027\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00881 to 1.00265, saving model to temp/e123\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0081 - val_mean_squared_error: 1.0081\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00265\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0041 - val_mean_squared_error: 1.0041\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00265\n",
      "Epoch 00006: early stopping\n",
      "temp/e124\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0967 - mean_squared_error: 1.0967 - val_loss: 1.0141 - val_mean_squared_error: 1.0141\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01411, saving model to temp/e124\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0228 - mean_squared_error: 1.0228 - val_loss: 1.0095 - val_mean_squared_error: 1.0095\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01411 to 1.00954, saving model to temp/e124\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0166 - mean_squared_error: 1.0166 - val_loss: 1.0306 - val_mean_squared_error: 1.0306\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00954\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0145 - mean_squared_error: 1.0145 - val_loss: 1.0084 - val_mean_squared_error: 1.0084\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00954 to 1.00841, saving model to temp/e124\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0116 - mean_squared_error: 1.0116 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00841\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0086 - mean_squared_error: 1.0086 - val_loss: 0.9988 - val_mean_squared_error: 0.9988\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00841 to 0.99881, saving model to temp/e124\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0062 - mean_squared_error: 1.0062 - val_loss: 1.0415 - val_mean_squared_error: 1.0415\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99881\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0055 - mean_squared_error: 1.0055 - val_loss: 1.0037 - val_mean_squared_error: 1.0037\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99881\n",
      "Epoch 00008: early stopping\n",
      "temp/e125\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0934 - mean_squared_error: 1.0934 - val_loss: 1.0218 - val_mean_squared_error: 1.0218\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02177, saving model to temp/e125\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0186 - mean_squared_error: 1.0186 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02177 to 1.00618, saving model to temp/e125\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0158 - mean_squared_error: 1.0158 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00618\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0145 - mean_squared_error: 1.0145 - val_loss: 1.0221 - val_mean_squared_error: 1.0221\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00618\n",
      "Epoch 00004: early stopping\n",
      "temp/e126\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1034 - mean_squared_error: 1.1034 - val_loss: 1.0591 - val_mean_squared_error: 1.0591\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.05906, saving model to temp/e126\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0176 - mean_squared_error: 1.0176 - val_loss: 1.0074 - val_mean_squared_error: 1.0074\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.05906 to 1.00737, saving model to temp/e126\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0168 - mean_squared_error: 1.0168 - val_loss: 1.0193 - val_mean_squared_error: 1.0193\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00737\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0137 - mean_squared_error: 1.0137 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00737\n",
      "Epoch 00004: early stopping\n",
      "temp/e127\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0880 - mean_squared_error: 1.0880 - val_loss: 1.0164 - val_mean_squared_error: 1.0164\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01644, saving model to temp/e127\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0222 - mean_squared_error: 1.0222 - val_loss: 1.0102 - val_mean_squared_error: 1.0102\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01644 to 1.01025, saving model to temp/e127\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0176 - mean_squared_error: 1.0176 - val_loss: 1.0168 - val_mean_squared_error: 1.0168\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01025\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0089 - mean_squared_error: 1.0089 - val_loss: 1.0037 - val_mean_squared_error: 1.0037\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01025 to 1.00366, saving model to temp/e127\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0082 - val_mean_squared_error: 1.0082\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00366\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0118 - mean_squared_error: 1.0118 - val_loss: 1.0239 - val_mean_squared_error: 1.0239\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00366\n",
      "Epoch 00006: early stopping\n",
      "temp/e128\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0997 - mean_squared_error: 1.0997 - val_loss: 1.0134 - val_mean_squared_error: 1.0134\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01335, saving model to temp/e128\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0217 - mean_squared_error: 1.0217 - val_loss: 1.0158 - val_mean_squared_error: 1.0158\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01335\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0162 - mean_squared_error: 1.0162 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01335 to 1.00622, saving model to temp/e128\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0127 - val_mean_squared_error: 1.0127\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00622\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0114 - mean_squared_error: 1.0114 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00622\n",
      "Epoch 00005: early stopping\n",
      "temp/e129\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.1027 - mean_squared_error: 1.1027 - val_loss: 1.0194 - val_mean_squared_error: 1.0194\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01937, saving model to temp/e129\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0159 - mean_squared_error: 1.0159 - val_loss: 1.0271 - val_mean_squared_error: 1.0271\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01937\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0171 - mean_squared_error: 1.0171 - val_loss: 1.0149 - val_mean_squared_error: 1.0149\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01937 to 1.01494, saving model to temp/e129\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0005 - val_mean_squared_error: 1.0005\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01494 to 1.00050, saving model to temp/e129\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0079 - mean_squared_error: 1.0079 - val_loss: 1.0013 - val_mean_squared_error: 1.0013\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00050\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0106 - mean_squared_error: 1.0106 - val_loss: 1.0060 - val_mean_squared_error: 1.0060\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00050\n",
      "Epoch 00006: early stopping\n",
      "temp/e130\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0916 - mean_squared_error: 1.0916 - val_loss: 1.0157 - val_mean_squared_error: 1.0157\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01575, saving model to temp/e130\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0224 - mean_squared_error: 1.0224 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01575 to 1.00609, saving model to temp/e130\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0127 - val_mean_squared_error: 1.0127\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00609\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0140 - mean_squared_error: 1.0140 - val_loss: 1.0019 - val_mean_squared_error: 1.0019\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00609 to 1.00192, saving model to temp/e130\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00192\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00192\n",
      "Epoch 00006: early stopping\n",
      "temp/e131\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.0978 - mean_squared_error: 1.0978 - val_loss: 1.0200 - val_mean_squared_error: 1.0200\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01997, saving model to temp/e131\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0196 - mean_squared_error: 1.0196 - val_loss: 1.0087 - val_mean_squared_error: 1.0087\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01997 to 1.00866, saving model to temp/e131\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0191 - val_mean_squared_error: 1.0191\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00866\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0112 - mean_squared_error: 1.0112 - val_loss: 1.0008 - val_mean_squared_error: 1.0008\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00866 to 1.00075, saving model to temp/e131\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0048 - val_mean_squared_error: 1.0048\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00075\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 1.0083 - val_mean_squared_error: 1.0083\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00075\n",
      "Epoch 00006: early stopping\n",
      "temp/e132\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1093 - mean_squared_error: 1.1093 - val_loss: 1.0175 - val_mean_squared_error: 1.0175\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01746, saving model to temp/e132\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0211 - mean_squared_error: 1.0211 - val_loss: 1.0212 - val_mean_squared_error: 1.0212\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01746\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0166 - mean_squared_error: 1.0166 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01746 to 1.00801, saving model to temp/e132\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0163 - val_mean_squared_error: 1.0163\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00801\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0150 - val_mean_squared_error: 1.0150\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00801\n",
      "Epoch 00005: early stopping\n",
      "temp/e133\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1006 - mean_squared_error: 1.1006 - val_loss: 1.0249 - val_mean_squared_error: 1.0249\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02487, saving model to temp/e133\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.0198 - mean_squared_error: 1.0198 - val_loss: 1.0133 - val_mean_squared_error: 1.0133\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02487 to 1.01328, saving model to temp/e133\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0154 - mean_squared_error: 1.0154 - val_loss: 1.0142 - val_mean_squared_error: 1.0142\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01328\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0036 - val_mean_squared_error: 1.0036\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01328 to 1.00363, saving model to temp/e133\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 0.9985 - val_mean_squared_error: 0.9985\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00363 to 0.99855, saving model to temp/e133\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 1.0012 - val_mean_squared_error: 1.0012\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99855\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0059 - mean_squared_error: 1.0059 - val_loss: 1.0015 - val_mean_squared_error: 1.0015\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99855\n",
      "Epoch 00007: early stopping\n",
      "temp/e134\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0919 - mean_squared_error: 1.0919 - val_loss: 1.0172 - val_mean_squared_error: 1.0172\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01718, saving model to temp/e134\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0171 - mean_squared_error: 1.0171 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01718 to 1.00525, saving model to temp/e134\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0162 - mean_squared_error: 1.0162 - val_loss: 1.0056 - val_mean_squared_error: 1.0056\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00525\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0166 - mean_squared_error: 1.0166 - val_loss: 1.0041 - val_mean_squared_error: 1.0041\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00525 to 1.00409, saving model to temp/e134\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00409\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0079 - mean_squared_error: 1.0079 - val_loss: 1.0099 - val_mean_squared_error: 1.0099\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00409\n",
      "Epoch 00006: early stopping\n",
      "temp/e135\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1137 - mean_squared_error: 1.1137 - val_loss: 1.0180 - val_mean_squared_error: 1.0180\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01799, saving model to temp/e135\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0208 - mean_squared_error: 1.0208 - val_loss: 1.0079 - val_mean_squared_error: 1.0079\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01799 to 1.00787, saving model to temp/e135\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0148 - mean_squared_error: 1.0148 - val_loss: 1.0130 - val_mean_squared_error: 1.0130\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00787\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0123 - mean_squared_error: 1.0123 - val_loss: 1.0068 - val_mean_squared_error: 1.0068\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00787 to 1.00682, saving model to temp/e135\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0067 - val_mean_squared_error: 1.0067\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00682 to 1.00669, saving model to temp/e135\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0147 - val_mean_squared_error: 1.0147\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00669\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0111 - mean_squared_error: 1.0111 - val_loss: 1.0009 - val_mean_squared_error: 1.0009\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00669 to 1.00094, saving model to temp/e135\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0055 - val_mean_squared_error: 1.0055\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00094\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0063 - mean_squared_error: 1.0063 - val_loss: 0.9982 - val_mean_squared_error: 0.9982\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00094 to 0.99817, saving model to temp/e135\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0055 - mean_squared_error: 1.0055 - val_loss: 0.9997 - val_mean_squared_error: 0.9997\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99817\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0055 - mean_squared_error: 1.0055 - val_loss: 0.9987 - val_mean_squared_error: 0.9987\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99817\n",
      "Epoch 00011: early stopping\n",
      "temp/e136\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0995 - mean_squared_error: 1.0995 - val_loss: 1.0309 - val_mean_squared_error: 1.0309\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03094, saving model to temp/e136\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0220 - mean_squared_error: 1.0220 - val_loss: 1.0128 - val_mean_squared_error: 1.0128\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03094 to 1.01281, saving model to temp/e136\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0185 - mean_squared_error: 1.0185 - val_loss: 1.0074 - val_mean_squared_error: 1.0074\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01281 to 1.00738, saving model to temp/e136\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 1.0097 - val_mean_squared_error: 1.0097\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00738\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 1.0108 - val_mean_squared_error: 1.0108\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00738\n",
      "Epoch 00005: early stopping\n",
      "temp/e137\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1090 - mean_squared_error: 1.1090 - val_loss: 1.0199 - val_mean_squared_error: 1.0199\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01994, saving model to temp/e137\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0231 - mean_squared_error: 1.0231 - val_loss: 1.0188 - val_mean_squared_error: 1.0188\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01994 to 1.01883, saving model to temp/e137\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0151 - mean_squared_error: 1.0151 - val_loss: 1.0045 - val_mean_squared_error: 1.0045\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01883 to 1.00454, saving model to temp/e137\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0118 - mean_squared_error: 1.0118 - val_loss: 1.0373 - val_mean_squared_error: 1.0373\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00454\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0007 - val_mean_squared_error: 1.0007\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00454 to 1.00070, saving model to temp/e137\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00070\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0027 - val_mean_squared_error: 1.0027\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00070\n",
      "Epoch 00007: early stopping\n",
      "temp/e138\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.1010 - mean_squared_error: 1.1010 - val_loss: 1.0117 - val_mean_squared_error: 1.0117\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01165, saving model to temp/e138\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0183 - mean_squared_error: 1.0183 - val_loss: 1.0167 - val_mean_squared_error: 1.0167\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01165\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0178 - mean_squared_error: 1.0178 - val_loss: 1.0077 - val_mean_squared_error: 1.0077\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01165 to 1.00774, saving model to temp/e138\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0111 - val_mean_squared_error: 1.0111\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00774\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00774 to 1.00505, saving model to temp/e138\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0029 - val_mean_squared_error: 1.0029\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00505 to 1.00294, saving model to temp/e138\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0096 - val_mean_squared_error: 1.0096\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00294\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0062 - mean_squared_error: 1.0062 - val_loss: 1.0019 - val_mean_squared_error: 1.0019\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00294 to 1.00189, saving model to temp/e138\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0047 - mean_squared_error: 1.0047 - val_loss: 1.0028 - val_mean_squared_error: 1.0028\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00189\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0042 - mean_squared_error: 1.0042 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.00189\n",
      "Epoch 00010: early stopping\n",
      "temp/e139\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.1012 - mean_squared_error: 1.1012 - val_loss: 1.0142 - val_mean_squared_error: 1.0142\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01423, saving model to temp/e139\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0217 - mean_squared_error: 1.0217 - val_loss: 1.0044 - val_mean_squared_error: 1.0044\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01423 to 1.00436, saving model to temp/e139\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0172 - mean_squared_error: 1.0172 - val_loss: 1.0123 - val_mean_squared_error: 1.0123\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00436\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0162 - mean_squared_error: 1.0162 - val_loss: 1.0201 - val_mean_squared_error: 1.0201\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00436\n",
      "Epoch 00004: early stopping\n",
      "temp/e140\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1005 - mean_squared_error: 1.1005 - val_loss: 1.0275 - val_mean_squared_error: 1.0275\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02745, saving model to temp/e140\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0238 - mean_squared_error: 1.0238 - val_loss: 1.0266 - val_mean_squared_error: 1.0266\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02745 to 1.02660, saving model to temp/e140\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0175 - mean_squared_error: 1.0175 - val_loss: 1.0090 - val_mean_squared_error: 1.0090\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02660 to 1.00898, saving model to temp/e140\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 1.0129 - val_mean_squared_error: 1.0129\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00898\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0064 - val_mean_squared_error: 1.0064\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00898 to 1.00641, saving model to temp/e140\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0043 - val_mean_squared_error: 1.0043\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00641 to 1.00427, saving model to temp/e140\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0047 - mean_squared_error: 1.0047 - val_loss: 1.0131 - val_mean_squared_error: 1.0131\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00427\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0114 - mean_squared_error: 1.0114 - val_loss: 1.0036 - val_mean_squared_error: 1.0036\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00427 to 1.00358, saving model to temp/e140\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0066 - mean_squared_error: 1.0066 - val_loss: 1.0015 - val_mean_squared_error: 1.0015\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00358 to 1.00145, saving model to temp/e140\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0049 - mean_squared_error: 1.0049 - val_loss: 0.9999 - val_mean_squared_error: 0.9999\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.00145 to 0.99992, saving model to temp/e140\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0061 - mean_squared_error: 1.0061 - val_loss: 0.9977 - val_mean_squared_error: 0.9977\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.99992 to 0.99773, saving model to temp/e140\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0056 - mean_squared_error: 1.0056 - val_loss: 0.9995 - val_mean_squared_error: 0.9995\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.99773\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0047 - mean_squared_error: 1.0047 - val_loss: 0.9967 - val_mean_squared_error: 0.9967\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.99773 to 0.99670, saving model to temp/e140\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0040 - mean_squared_error: 1.0040 - val_loss: 0.9933 - val_mean_squared_error: 0.9933\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.99670 to 0.99326, saving model to temp/e140\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0031 - mean_squared_error: 1.0031 - val_loss: 0.9982 - val_mean_squared_error: 0.9982\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.99326\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0029 - mean_squared_error: 1.0029 - val_loss: 0.9991 - val_mean_squared_error: 0.9991\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.99326\n",
      "Epoch 00016: early stopping\n",
      "temp/e141\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1011 - mean_squared_error: 1.1011 - val_loss: 1.0141 - val_mean_squared_error: 1.0141\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01406, saving model to temp/e141\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0204 - mean_squared_error: 1.0204 - val_loss: 1.0237 - val_mean_squared_error: 1.0237\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01406\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0150 - mean_squared_error: 1.0150 - val_loss: 1.0099 - val_mean_squared_error: 1.0099\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01406 to 1.00989, saving model to temp/e141\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0151 - mean_squared_error: 1.0151 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00989 to 1.00506, saving model to temp/e141\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0025 - val_mean_squared_error: 1.0025\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00506 to 1.00249, saving model to temp/e141\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0114 - mean_squared_error: 1.0114 - val_loss: 0.9987 - val_mean_squared_error: 0.9987\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00249 to 0.99867, saving model to temp/e141\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99867\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0076 - mean_squared_error: 1.0076 - val_loss: 1.0144 - val_mean_squared_error: 1.0144\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99867\n",
      "Epoch 00008: early stopping\n",
      "temp/e142\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0978 - mean_squared_error: 1.0978 - val_loss: 1.0199 - val_mean_squared_error: 1.0199\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01993, saving model to temp/e142\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0180 - mean_squared_error: 1.0180 - val_loss: 1.0168 - val_mean_squared_error: 1.0168\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01993 to 1.01677, saving model to temp/e142\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0183 - mean_squared_error: 1.0183 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01677 to 1.00380, saving model to temp/e142\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 1.0243 - val_mean_squared_error: 1.0243\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00380\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0093 - mean_squared_error: 1.0093 - val_loss: 1.0041 - val_mean_squared_error: 1.0041\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00380\n",
      "Epoch 00005: early stopping\n",
      "temp/e143\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.1000 - mean_squared_error: 1.1000 - val_loss: 1.0108 - val_mean_squared_error: 1.0108\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01082, saving model to temp/e143\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0205 - mean_squared_error: 1.0205 - val_loss: 1.0095 - val_mean_squared_error: 1.0095\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01082 to 1.00946, saving model to temp/e143\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0139 - mean_squared_error: 1.0139 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00946 to 1.00749, saving model to temp/e143\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0112 - mean_squared_error: 1.0112 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00749\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0137 - mean_squared_error: 1.0137 - val_loss: 1.0002 - val_mean_squared_error: 1.0002\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00749 to 1.00016, saving model to temp/e143\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0028 - val_mean_squared_error: 1.0028\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00016\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0069 - mean_squared_error: 1.0069 - val_loss: 1.0020 - val_mean_squared_error: 1.0020\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00016\n",
      "Epoch 00007: early stopping\n",
      "temp/e144\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1027 - mean_squared_error: 1.1027 - val_loss: 1.0207 - val_mean_squared_error: 1.0207\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02065, saving model to temp/e144\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0198 - mean_squared_error: 1.0198 - val_loss: 1.0099 - val_mean_squared_error: 1.0099\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02065 to 1.00989, saving model to temp/e144\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00989 to 1.00613, saving model to temp/e144\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0148 - mean_squared_error: 1.0148 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00613 to 1.00331, saving model to temp/e144\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0111 - mean_squared_error: 1.0111 - val_loss: 1.0039 - val_mean_squared_error: 1.0039\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00331\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0114 - mean_squared_error: 1.0114 - val_loss: 1.0011 - val_mean_squared_error: 1.0011\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00331 to 1.00113, saving model to temp/e144\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0086 - mean_squared_error: 1.0086 - val_loss: 0.9994 - val_mean_squared_error: 0.9994\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00113 to 0.99936, saving model to temp/e144\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0048 - mean_squared_error: 1.0048 - val_loss: 1.0154 - val_mean_squared_error: 1.0154\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99936\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0066 - mean_squared_error: 1.0066 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99936\n",
      "Epoch 00009: early stopping\n",
      "temp/e145\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.1072 - mean_squared_error: 1.1072 - val_loss: 1.0178 - val_mean_squared_error: 1.0178\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01776, saving model to temp/e145\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0189 - mean_squared_error: 1.0189 - val_loss: 1.0256 - val_mean_squared_error: 1.0256\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01776\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0166 - mean_squared_error: 1.0166 - val_loss: 1.0065 - val_mean_squared_error: 1.0065\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01776 to 1.00655, saving model to temp/e145\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0109 - mean_squared_error: 1.0109 - val_loss: 1.0083 - val_mean_squared_error: 1.0083\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00655\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0098 - mean_squared_error: 1.0098 - val_loss: 0.9992 - val_mean_squared_error: 0.9992\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00655 to 0.99919, saving model to temp/e145\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 1.0017 - val_mean_squared_error: 1.0017\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99919\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0069 - mean_squared_error: 1.0069 - val_loss: 0.9989 - val_mean_squared_error: 0.9989\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.99919 to 0.99889, saving model to temp/e145\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0086 - mean_squared_error: 1.0086 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99889\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 0.9956 - val_mean_squared_error: 0.9956\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.99889 to 0.99564, saving model to temp/e145\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0046 - mean_squared_error: 1.0046 - val_loss: 1.0035 - val_mean_squared_error: 1.0035\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99564\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0060 - mean_squared_error: 1.0060 - val_loss: 0.9957 - val_mean_squared_error: 0.9957\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99564\n",
      "Epoch 00011: early stopping\n",
      "temp/e146\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1059 - mean_squared_error: 1.1059 - val_loss: 1.0172 - val_mean_squared_error: 1.0172\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01715, saving model to temp/e146\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 1.0183 - val_mean_squared_error: 1.0183\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01715\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0155 - mean_squared_error: 1.0155 - val_loss: 1.0151 - val_mean_squared_error: 1.0151\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01715 to 1.01506, saving model to temp/e146\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0048 - val_mean_squared_error: 1.0048\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01506 to 1.00479, saving model to temp/e146\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0015 - val_mean_squared_error: 1.0015\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00479 to 1.00152, saving model to temp/e146\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0125 - mean_squared_error: 1.0125 - val_loss: 1.0114 - val_mean_squared_error: 1.0114\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00152\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 1.0130 - val_mean_squared_error: 1.0130\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00152\n",
      "Epoch 00007: early stopping\n",
      "temp/e147\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0962 - mean_squared_error: 1.0962 - val_loss: 1.0152 - val_mean_squared_error: 1.0152\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01520, saving model to temp/e147\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0206 - mean_squared_error: 1.0206 - val_loss: 1.0065 - val_mean_squared_error: 1.0065\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01520 to 1.00647, saving model to temp/e147\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0153 - mean_squared_error: 1.0153 - val_loss: 1.0078 - val_mean_squared_error: 1.0078\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00647\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0022 - val_mean_squared_error: 1.0022\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00647 to 1.00222, saving model to temp/e147\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0014 - val_mean_squared_error: 1.0014\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00222 to 1.00141, saving model to temp/e147\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0076 - mean_squared_error: 1.0076 - val_loss: 1.0036 - val_mean_squared_error: 1.0036\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00141\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 1.0022 - val_mean_squared_error: 1.0022\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00141\n",
      "Epoch 00007: early stopping\n",
      "temp/e148\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.1024 - mean_squared_error: 1.1024 - val_loss: 1.0174 - val_mean_squared_error: 1.0174\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01742, saving model to temp/e148\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0186 - mean_squared_error: 1.0186 - val_loss: 1.0240 - val_mean_squared_error: 1.0240\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01742\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0162 - mean_squared_error: 1.0162 - val_loss: 1.0255 - val_mean_squared_error: 1.0255\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01742\n",
      "Epoch 00003: early stopping\n",
      "temp/e149\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1003 - mean_squared_error: 1.1003 - val_loss: 1.0199 - val_mean_squared_error: 1.0199\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01995, saving model to temp/e149\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0204 - mean_squared_error: 1.0204 - val_loss: 1.0081 - val_mean_squared_error: 1.0081\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01995 to 1.00813, saving model to temp/e149\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0152 - mean_squared_error: 1.0152 - val_loss: 1.0084 - val_mean_squared_error: 1.0084\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00813\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0107 - mean_squared_error: 1.0107 - val_loss: 1.0122 - val_mean_squared_error: 1.0122\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00813\n",
      "Epoch 00004: early stopping\n",
      "temp/e150\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.1072 - mean_squared_error: 1.1072 - val_loss: 1.0156 - val_mean_squared_error: 1.0156\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01556, saving model to temp/e150\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0203 - mean_squared_error: 1.0203 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01556 to 1.00884, saving model to temp/e150\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0159 - mean_squared_error: 1.0159 - val_loss: 1.0113 - val_mean_squared_error: 1.0113\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00884\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0105 - mean_squared_error: 1.0105 - val_loss: 1.0021 - val_mean_squared_error: 1.0021\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00884 to 1.00209, saving model to temp/e150\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 1.0120 - val_mean_squared_error: 1.0120\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00209\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0089 - mean_squared_error: 1.0089 - val_loss: 1.0029 - val_mean_squared_error: 1.0029\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00209\n",
      "Epoch 00006: early stopping\n",
      "temp/e151\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1109 - mean_squared_error: 1.1109 - val_loss: 1.0105 - val_mean_squared_error: 1.0105\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01054, saving model to temp/e151\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0194 - mean_squared_error: 1.0194 - val_loss: 1.0165 - val_mean_squared_error: 1.0165\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01054\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0150 - mean_squared_error: 1.0150 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01054 to 1.00749, saving model to temp/e151\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0155 - mean_squared_error: 1.0155 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00749 to 1.00747, saving model to temp/e151\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0138 - mean_squared_error: 1.0138 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00747 to 1.00402, saving model to temp/e151\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.0073 - mean_squared_error: 1.0073 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00402\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 1.0007 - val_mean_squared_error: 1.0007\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00402 to 1.00069, saving model to temp/e151\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 1.0113 - val_mean_squared_error: 1.0113\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00069\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0027 - mean_squared_error: 1.0027 - val_loss: 0.9962 - val_mean_squared_error: 0.9962\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00069 to 0.99624, saving model to temp/e151\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0052 - mean_squared_error: 1.0052 - val_loss: 1.0018 - val_mean_squared_error: 1.0018\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99624\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0016 - mean_squared_error: 1.0016 - val_loss: 0.9975 - val_mean_squared_error: 0.9975\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99624\n",
      "Epoch 00011: early stopping\n",
      "temp/e152\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.0974 - mean_squared_error: 1.0974 - val_loss: 1.0295 - val_mean_squared_error: 1.0295\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02946, saving model to temp/e152\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0171 - mean_squared_error: 1.0171 - val_loss: 1.0359 - val_mean_squared_error: 1.0359\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.02946\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0141 - mean_squared_error: 1.0141 - val_loss: 1.0169 - val_mean_squared_error: 1.0169\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02946 to 1.01689, saving model to temp/e152\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01689 to 1.00331, saving model to temp/e152\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0109 - val_mean_squared_error: 1.0109\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00331\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0076 - mean_squared_error: 1.0076 - val_loss: 1.0006 - val_mean_squared_error: 1.0006\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00331 to 1.00060, saving model to temp/e152\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0081 - mean_squared_error: 1.0081 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00060\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0046 - mean_squared_error: 1.0046 - val_loss: 1.0083 - val_mean_squared_error: 1.0083\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00060\n",
      "Epoch 00008: early stopping\n",
      "temp/e153\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.1030 - mean_squared_error: 1.1030 - val_loss: 1.0116 - val_mean_squared_error: 1.0116\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01159, saving model to temp/e153\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0183 - mean_squared_error: 1.0183 - val_loss: 1.0334 - val_mean_squared_error: 1.0334\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01159\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0203 - mean_squared_error: 1.0203 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01159 to 1.00566, saving model to temp/e153\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0151 - mean_squared_error: 1.0151 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00566 to 1.00486, saving model to temp/e153\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0079 - mean_squared_error: 1.0079 - val_loss: 1.0045 - val_mean_squared_error: 1.0045\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00486 to 1.00445, saving model to temp/e153\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0073 - mean_squared_error: 1.0073 - val_loss: 1.0032 - val_mean_squared_error: 1.0032\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00445 to 1.00318, saving model to temp/e153\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00318\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0093 - mean_squared_error: 1.0093 - val_loss: 0.9997 - val_mean_squared_error: 0.9997\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00318 to 0.99967, saving model to temp/e153\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 1.0020 - val_mean_squared_error: 1.0020\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99967\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0071 - mean_squared_error: 1.0071 - val_loss: 0.9981 - val_mean_squared_error: 0.9981\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.99967 to 0.99806, saving model to temp/e153\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0050 - mean_squared_error: 1.0050 - val_loss: 1.0084 - val_mean_squared_error: 1.0084\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99806\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0043 - mean_squared_error: 1.0043 - val_loss: 0.9987 - val_mean_squared_error: 0.9987\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.99806\n",
      "Epoch 00012: early stopping\n",
      "temp/e154\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1115 - mean_squared_error: 1.1115 - val_loss: 1.0419 - val_mean_squared_error: 1.0419\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.04186, saving model to temp/e154\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0152 - val_mean_squared_error: 1.0152\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.04186 to 1.01518, saving model to temp/e154\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0137 - mean_squared_error: 1.0137 - val_loss: 1.0136 - val_mean_squared_error: 1.0136\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01518 to 1.01363, saving model to temp/e154\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01363 to 1.00518, saving model to temp/e154\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0103 - mean_squared_error: 1.0103 - val_loss: 1.0020 - val_mean_squared_error: 1.0020\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00518 to 1.00196, saving model to temp/e154\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 1.0094 - val_mean_squared_error: 1.0094\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00196\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0071 - mean_squared_error: 1.0071 - val_loss: 0.9995 - val_mean_squared_error: 0.9995\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00196 to 0.99952, saving model to temp/e154\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0093 - mean_squared_error: 1.0093 - val_loss: 1.0021 - val_mean_squared_error: 1.0021\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99952\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99952\n",
      "Epoch 00009: early stopping\n",
      "temp/e155\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1039 - mean_squared_error: 1.1039 - val_loss: 1.0137 - val_mean_squared_error: 1.0137\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01367, saving model to temp/e155\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0177 - mean_squared_error: 1.0177 - val_loss: 1.0085 - val_mean_squared_error: 1.0085\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01367 to 1.00849, saving model to temp/e155\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0183 - mean_squared_error: 1.0183 - val_loss: 1.0087 - val_mean_squared_error: 1.0087\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00849\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0142 - mean_squared_error: 1.0142 - val_loss: 1.0037 - val_mean_squared_error: 1.0037\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00849 to 1.00372, saving model to temp/e155\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0043 - val_mean_squared_error: 1.0043\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00372\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 0.9987 - val_mean_squared_error: 0.9987\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00372 to 0.99865, saving model to temp/e155\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0035 - mean_squared_error: 1.0035 - val_loss: 1.0056 - val_mean_squared_error: 1.0056\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99865\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0052 - mean_squared_error: 1.0052 - val_loss: 1.0032 - val_mean_squared_error: 1.0032\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99865\n",
      "Epoch 00008: early stopping\n",
      "temp/e156\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0994 - mean_squared_error: 1.0994 - val_loss: 1.0252 - val_mean_squared_error: 1.0252\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02523, saving model to temp/e156\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0226 - mean_squared_error: 1.0226 - val_loss: 1.0117 - val_mean_squared_error: 1.0117\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02523 to 1.01175, saving model to temp/e156\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0153 - mean_squared_error: 1.0153 - val_loss: 1.0189 - val_mean_squared_error: 1.0189\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01175\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0136 - mean_squared_error: 1.0136 - val_loss: 1.0021 - val_mean_squared_error: 1.0021\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01175 to 1.00209, saving model to temp/e156\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0148 - mean_squared_error: 1.0148 - val_loss: 0.9960 - val_mean_squared_error: 0.9960\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00209 to 0.99604, saving model to temp/e156\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0114 - mean_squared_error: 1.0114 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99604\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0109 - mean_squared_error: 1.0109 - val_loss: 0.9972 - val_mean_squared_error: 0.9972\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99604\n",
      "Epoch 00007: early stopping\n",
      "temp/e157\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1051 - mean_squared_error: 1.1051 - val_loss: 1.0274 - val_mean_squared_error: 1.0274\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02744, saving model to temp/e157\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0227 - mean_squared_error: 1.0227 - val_loss: 1.0113 - val_mean_squared_error: 1.0113\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02744 to 1.01126, saving model to temp/e157\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 1.0099 - val_mean_squared_error: 1.0099\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01126 to 1.00990, saving model to temp/e157\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00990 to 1.00236, saving model to temp/e157\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0068 - mean_squared_error: 1.0068 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00236\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0103 - val_mean_squared_error: 1.0103\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00236\n",
      "Epoch 00006: early stopping\n",
      "temp/e158\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0966 - mean_squared_error: 1.0966 - val_loss: 1.0179 - val_mean_squared_error: 1.0179\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01787, saving model to temp/e158\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0190 - mean_squared_error: 1.0190 - val_loss: 1.0101 - val_mean_squared_error: 1.0101\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01787 to 1.01012, saving model to temp/e158\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 1.0071 - val_mean_squared_error: 1.0071\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01012 to 1.00710, saving model to temp/e158\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0156 - val_mean_squared_error: 1.0156\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00710\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0116 - mean_squared_error: 1.0116 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00710 to 1.00572, saving model to temp/e158\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0081 - mean_squared_error: 1.0081 - val_loss: 1.0036 - val_mean_squared_error: 1.0036\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00572 to 1.00362, saving model to temp/e158\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0079 - mean_squared_error: 1.0079 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00362\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0068 - mean_squared_error: 1.0068 - val_loss: 0.9970 - val_mean_squared_error: 0.9970\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00362 to 0.99697, saving model to temp/e158\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0063 - mean_squared_error: 1.0063 - val_loss: 0.9983 - val_mean_squared_error: 0.9983\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99697\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0051 - mean_squared_error: 1.0051 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99697\n",
      "Epoch 00010: early stopping\n",
      "temp/e159\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1042 - mean_squared_error: 1.1042 - val_loss: 1.0105 - val_mean_squared_error: 1.0105\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01048, saving model to temp/e159\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.0184 - mean_squared_error: 1.0184 - val_loss: 1.0085 - val_mean_squared_error: 1.0085\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01048 to 1.00850, saving model to temp/e159\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0199 - mean_squared_error: 1.0199 - val_loss: 1.0055 - val_mean_squared_error: 1.0055\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00850 to 1.00548, saving model to temp/e159\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0136 - mean_squared_error: 1.0136 - val_loss: 1.0187 - val_mean_squared_error: 1.0187\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00548\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0112 - mean_squared_error: 1.0112 - val_loss: 1.0005 - val_mean_squared_error: 1.0005\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00548 to 1.00055, saving model to temp/e159\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0129 - mean_squared_error: 1.0129 - val_loss: 1.0084 - val_mean_squared_error: 1.0084\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00055\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 1.0048 - val_mean_squared_error: 1.0048\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00055\n",
      "Epoch 00007: early stopping\n",
      "temp/e160\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1062 - mean_squared_error: 1.1062 - val_loss: 1.0158 - val_mean_squared_error: 1.0158\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01581, saving model to temp/e160\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0191 - mean_squared_error: 1.0191 - val_loss: 1.0072 - val_mean_squared_error: 1.0072\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01581 to 1.00719, saving model to temp/e160\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0171 - mean_squared_error: 1.0171 - val_loss: 1.0133 - val_mean_squared_error: 1.0133\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00719\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0125 - mean_squared_error: 1.0125 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00719 to 1.00587, saving model to temp/e160\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0128 - mean_squared_error: 1.0128 - val_loss: 1.0025 - val_mean_squared_error: 1.0025\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00587 to 1.00247, saving model to temp/e160\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 1.0073 - val_mean_squared_error: 1.0073\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00247\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.0165 - val_mean_squared_error: 1.0165\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00247\n",
      "Epoch 00007: early stopping\n",
      "temp/e161\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1040 - mean_squared_error: 1.1040 - val_loss: 1.0172 - val_mean_squared_error: 1.0172\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01718, saving model to temp/e161\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0211 - mean_squared_error: 1.0211 - val_loss: 1.0186 - val_mean_squared_error: 1.0186\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01718\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0141 - mean_squared_error: 1.0141 - val_loss: 1.0113 - val_mean_squared_error: 1.0113\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01718 to 1.01126, saving model to temp/e161\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 1.0126 - val_mean_squared_error: 1.0126\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.01126\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01126 to 1.00864, saving model to temp/e161\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0068 - mean_squared_error: 1.0068 - val_loss: 1.0066 - val_mean_squared_error: 1.0066\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00864 to 1.00658, saving model to temp/e161\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0098 - mean_squared_error: 1.0098 - val_loss: 1.0259 - val_mean_squared_error: 1.0259\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00658\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0044 - mean_squared_error: 1.0044 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00658\n",
      "Epoch 00008: early stopping\n",
      "temp/e162\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.1064 - mean_squared_error: 1.1064 - val_loss: 1.0371 - val_mean_squared_error: 1.0371\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03711, saving model to temp/e162\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0192 - mean_squared_error: 1.0192 - val_loss: 1.0085 - val_mean_squared_error: 1.0085\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03711 to 1.00846, saving model to temp/e162\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0089 - val_mean_squared_error: 1.0089\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00846\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0152 - mean_squared_error: 1.0152 - val_loss: 1.0029 - val_mean_squared_error: 1.0029\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00846 to 1.00295, saving model to temp/e162\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00295\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00295\n",
      "Epoch 00006: early stopping\n",
      "temp/e163\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1046 - mean_squared_error: 1.1046 - val_loss: 1.0117 - val_mean_squared_error: 1.0117\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01165, saving model to temp/e163\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0214 - mean_squared_error: 1.0214 - val_loss: 1.0179 - val_mean_squared_error: 1.0179\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01165\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0169 - mean_squared_error: 1.0169 - val_loss: 1.0056 - val_mean_squared_error: 1.0056\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01165 to 1.00564, saving model to temp/e163\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 1.0112 - val_mean_squared_error: 1.0112\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00564\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0073 - val_mean_squared_error: 1.0073\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00564\n",
      "Epoch 00005: early stopping\n",
      "temp/e164\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.1085 - mean_squared_error: 1.1085 - val_loss: 1.0233 - val_mean_squared_error: 1.0233\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02330, saving model to temp/e164\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0196 - mean_squared_error: 1.0196 - val_loss: 1.0309 - val_mean_squared_error: 1.0309\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.02330\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0164 - mean_squared_error: 1.0164 - val_loss: 1.0268 - val_mean_squared_error: 1.0268\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.02330\n",
      "Epoch 00003: early stopping\n",
      "temp/e165\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1064 - mean_squared_error: 1.1064 - val_loss: 1.0138 - val_mean_squared_error: 1.0138\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01378, saving model to temp/e165\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0209 - mean_squared_error: 1.0209 - val_loss: 1.0112 - val_mean_squared_error: 1.0112\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01378 to 1.01124, saving model to temp/e165\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0148 - mean_squared_error: 1.0148 - val_loss: 1.0048 - val_mean_squared_error: 1.0048\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01124 to 1.00481, saving model to temp/e165\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0123 - mean_squared_error: 1.0123 - val_loss: 1.0071 - val_mean_squared_error: 1.0071\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00481\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0123 - mean_squared_error: 1.0123 - val_loss: 1.0029 - val_mean_squared_error: 1.0029\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00481 to 1.00287, saving model to temp/e165\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0112 - mean_squared_error: 1.0112 - val_loss: 1.0129 - val_mean_squared_error: 1.0129\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00287\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0066 - mean_squared_error: 1.0066 - val_loss: 1.0079 - val_mean_squared_error: 1.0079\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00287\n",
      "Epoch 00007: early stopping\n",
      "temp/e166\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1013 - mean_squared_error: 1.1013 - val_loss: 1.0113 - val_mean_squared_error: 1.0113\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01127, saving model to temp/e166\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0194 - mean_squared_error: 1.0194 - val_loss: 1.0284 - val_mean_squared_error: 1.0284\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01127\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0142 - mean_squared_error: 1.0142 - val_loss: 1.0149 - val_mean_squared_error: 1.0149\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01127\n",
      "Epoch 00003: early stopping\n",
      "temp/e167\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1100 - mean_squared_error: 1.1100 - val_loss: 1.0133 - val_mean_squared_error: 1.0133\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01331, saving model to temp/e167\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0172 - mean_squared_error: 1.0172 - val_loss: 1.0191 - val_mean_squared_error: 1.0191\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01331\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.0188 - mean_squared_error: 1.0188 - val_loss: 1.0145 - val_mean_squared_error: 1.0145\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01331\n",
      "Epoch 00003: early stopping\n",
      "temp/e168\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1052 - mean_squared_error: 1.1052 - val_loss: 1.0137 - val_mean_squared_error: 1.0137\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01373, saving model to temp/e168\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0197 - mean_squared_error: 1.0197 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01373 to 1.00877, saving model to temp/e168\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0169 - mean_squared_error: 1.0169 - val_loss: 1.0079 - val_mean_squared_error: 1.0079\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00877 to 1.00790, saving model to temp/e168\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0119 - mean_squared_error: 1.0119 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00790 to 1.00615, saving model to temp/e168\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0104 - val_mean_squared_error: 1.0104\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00615\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0074 - val_mean_squared_error: 1.0074\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00615\n",
      "Epoch 00006: early stopping\n",
      "temp/e169\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0969 - mean_squared_error: 1.0969 - val_loss: 1.0142 - val_mean_squared_error: 1.0142\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01422, saving model to temp/e169\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.0230 - mean_squared_error: 1.0230 - val_loss: 1.0097 - val_mean_squared_error: 1.0097\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01422 to 1.00972, saving model to temp/e169\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0158 - mean_squared_error: 1.0158 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00972 to 1.00928, saving model to temp/e169\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0064 - val_mean_squared_error: 1.0064\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00928 to 1.00640, saving model to temp/e169\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0125 - mean_squared_error: 1.0125 - val_loss: 1.0146 - val_mean_squared_error: 1.0146\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00640\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0076 - mean_squared_error: 1.0076 - val_loss: 1.0029 - val_mean_squared_error: 1.0029\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00640 to 1.00289, saving model to temp/e169\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 0.9974 - val_mean_squared_error: 0.9974\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00289 to 0.99743, saving model to temp/e169\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0057 - mean_squared_error: 1.0057 - val_loss: 1.0002 - val_mean_squared_error: 1.0002\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99743\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0070 - mean_squared_error: 1.0070 - val_loss: 1.0162 - val_mean_squared_error: 1.0162\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99743\n",
      "Epoch 00009: early stopping\n",
      "temp/e170\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1019 - mean_squared_error: 1.1019 - val_loss: 1.0137 - val_mean_squared_error: 1.0137\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01370, saving model to temp/e170\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0192 - mean_squared_error: 1.0192 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01370 to 1.00494, saving model to temp/e170\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0131 - mean_squared_error: 1.0131 - val_loss: 1.0058 - val_mean_squared_error: 1.0058\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00494\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00494 to 1.00228, saving model to temp/e170\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0060 - val_mean_squared_error: 1.0060\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00228\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0107 - mean_squared_error: 1.0107 - val_loss: 1.0029 - val_mean_squared_error: 1.0029\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00228\n",
      "Epoch 00006: early stopping\n",
      "temp/e171\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1052 - mean_squared_error: 1.1052 - val_loss: 1.0143 - val_mean_squared_error: 1.0143\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01432, saving model to temp/e171\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01432 to 1.00618, saving model to temp/e171\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0166 - mean_squared_error: 1.0166 - val_loss: 1.0140 - val_mean_squared_error: 1.0140\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00618\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0137 - mean_squared_error: 1.0137 - val_loss: 1.0122 - val_mean_squared_error: 1.0122\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00618\n",
      "Epoch 00004: early stopping\n",
      "temp/e172\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1002 - mean_squared_error: 1.1002 - val_loss: 1.0168 - val_mean_squared_error: 1.0168\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01685, saving model to temp/e172\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0187 - mean_squared_error: 1.0187 - val_loss: 1.0177 - val_mean_squared_error: 1.0177\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01685\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0144 - val_mean_squared_error: 1.0144\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01685 to 1.01439, saving model to temp/e172\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0031 - val_mean_squared_error: 1.0031\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01439 to 1.00311, saving model to temp/e172\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0089 - mean_squared_error: 1.0089 - val_loss: 1.0065 - val_mean_squared_error: 1.0065\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00311\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0009 - val_mean_squared_error: 1.0009\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00311 to 1.00088, saving model to temp/e172\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 1.0004 - val_mean_squared_error: 1.0004\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00088 to 1.00036, saving model to temp/e172\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0064 - mean_squared_error: 1.0064 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00036\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0098 - mean_squared_error: 1.0098 - val_loss: 1.0117 - val_mean_squared_error: 1.0117\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00036\n",
      "Epoch 00009: early stopping\n",
      "temp/e173\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.1004 - mean_squared_error: 1.1004 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00979, saving model to temp/e173\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0171 - mean_squared_error: 1.0171 - val_loss: 1.0152 - val_mean_squared_error: 1.0152\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.00979\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0123 - mean_squared_error: 1.0123 - val_loss: 1.0215 - val_mean_squared_error: 1.0215\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00979\n",
      "Epoch 00003: early stopping\n",
      "temp/e174\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1117 - mean_squared_error: 1.1117 - val_loss: 1.0230 - val_mean_squared_error: 1.0230\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02297, saving model to temp/e174\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0190 - mean_squared_error: 1.0190 - val_loss: 1.0136 - val_mean_squared_error: 1.0136\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02297 to 1.01361, saving model to temp/e174\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0150 - mean_squared_error: 1.0150 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01361 to 1.00686, saving model to temp/e174\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00686 to 1.00468, saving model to temp/e174\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0111 - mean_squared_error: 1.0111 - val_loss: 1.0018 - val_mean_squared_error: 1.0018\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00468 to 1.00176, saving model to temp/e174\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0042 - val_mean_squared_error: 1.0042\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00176\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.0082 - mean_squared_error: 1.0082 - val_loss: 0.9995 - val_mean_squared_error: 0.9995\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00176 to 0.99949, saving model to temp/e174\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0078 - mean_squared_error: 1.0078 - val_loss: 1.0065 - val_mean_squared_error: 1.0065\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99949\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.0066 - mean_squared_error: 1.0066 - val_loss: 1.0009 - val_mean_squared_error: 1.0009\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99949\n",
      "Epoch 00009: early stopping\n",
      "temp/e175\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.1001 - mean_squared_error: 1.1001 - val_loss: 1.0102 - val_mean_squared_error: 1.0102\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01016, saving model to temp/e175\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0262 - mean_squared_error: 1.0262 - val_loss: 1.0130 - val_mean_squared_error: 1.0130\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01016\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0156 - mean_squared_error: 1.0156 - val_loss: 1.0108 - val_mean_squared_error: 1.0108\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01016\n",
      "Epoch 00003: early stopping\n",
      "temp/e176\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1023 - mean_squared_error: 1.1023 - val_loss: 1.0118 - val_mean_squared_error: 1.0118\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01180, saving model to temp/e176\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0194 - mean_squared_error: 1.0194 - val_loss: 1.0133 - val_mean_squared_error: 1.0133\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01180\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0123 - mean_squared_error: 1.0123 - val_loss: 1.0184 - val_mean_squared_error: 1.0184\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01180\n",
      "Epoch 00003: early stopping\n",
      "temp/e177\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.0907 - mean_squared_error: 1.0907 - val_loss: 1.0126 - val_mean_squared_error: 1.0126\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01263, saving model to temp/e177\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0250 - mean_squared_error: 1.0250 - val_loss: 1.0235 - val_mean_squared_error: 1.0235\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01263\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0168 - mean_squared_error: 1.0168 - val_loss: 1.0073 - val_mean_squared_error: 1.0073\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01263 to 1.00727, saving model to temp/e177\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0149 - mean_squared_error: 1.0149 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00727 to 1.00259, saving model to temp/e177\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0028 - val_mean_squared_error: 1.0028\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00259\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0027 - val_mean_squared_error: 1.0027\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00259\n",
      "Epoch 00006: early stopping\n",
      "temp/e178\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.1056 - mean_squared_error: 1.1056 - val_loss: 1.0335 - val_mean_squared_error: 1.0335\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03351, saving model to temp/e178\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.0162 - mean_squared_error: 1.0162 - val_loss: 1.0147 - val_mean_squared_error: 1.0147\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03351 to 1.01472, saving model to temp/e178\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0189 - mean_squared_error: 1.0189 - val_loss: 1.0117 - val_mean_squared_error: 1.0117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss improved from 1.01472 to 1.01169, saving model to temp/e178\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0147 - mean_squared_error: 1.0147 - val_loss: 1.0237 - val_mean_squared_error: 1.0237\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.01169\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0109 - mean_squared_error: 1.0109 - val_loss: 1.0003 - val_mean_squared_error: 1.0003\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01169 to 1.00035, saving model to temp/e178\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00035\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0086 - mean_squared_error: 1.0086 - val_loss: 0.9998 - val_mean_squared_error: 0.9998\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00035 to 0.99980, saving model to temp/e178\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0065 - mean_squared_error: 1.0065 - val_loss: 1.0066 - val_mean_squared_error: 1.0066\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99980\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0047 - mean_squared_error: 1.0047 - val_loss: 1.0294 - val_mean_squared_error: 1.0294\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99980\n",
      "Epoch 00009: early stopping\n",
      "temp/e179\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1036 - mean_squared_error: 1.1036 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00745, saving model to temp/e179\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0202 - mean_squared_error: 1.0202 - val_loss: 1.0126 - val_mean_squared_error: 1.0126\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.00745\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0160 - mean_squared_error: 1.0160 - val_loss: 1.0048 - val_mean_squared_error: 1.0048\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00745 to 1.00484, saving model to temp/e179\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0107 - mean_squared_error: 1.0107 - val_loss: 1.0035 - val_mean_squared_error: 1.0035\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00484 to 1.00350, saving model to temp/e179\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 0.9999 - val_mean_squared_error: 0.9999\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00350 to 0.99995, saving model to temp/e179\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0098 - mean_squared_error: 1.0098 - val_loss: 0.9991 - val_mean_squared_error: 0.9991\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.99995 to 0.99910, saving model to temp/e179\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99910\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0070 - val_mean_squared_error: 1.0070\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99910\n",
      "Epoch 00008: early stopping\n",
      "temp/e180\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0979 - mean_squared_error: 1.0979 - val_loss: 1.0286 - val_mean_squared_error: 1.0286\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02860, saving model to temp/e180\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 1.0253 - val_mean_squared_error: 1.0253\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02860 to 1.02528, saving model to temp/e180\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0146 - mean_squared_error: 1.0146 - val_loss: 1.0077 - val_mean_squared_error: 1.0077\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02528 to 1.00772, saving model to temp/e180\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0140 - mean_squared_error: 1.0140 - val_loss: 1.0144 - val_mean_squared_error: 1.0144\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00772\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0111 - mean_squared_error: 1.0111 - val_loss: 1.0043 - val_mean_squared_error: 1.0043\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00772 to 1.00428, saving model to temp/e180\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0112 - mean_squared_error: 1.0112 - val_loss: 1.0019 - val_mean_squared_error: 1.0019\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00428 to 1.00191, saving model to temp/e180\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0054 - mean_squared_error: 1.0054 - val_loss: 1.0058 - val_mean_squared_error: 1.0058\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00191\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0071 - mean_squared_error: 1.0071 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00191\n",
      "Epoch 00008: early stopping\n",
      "temp/e181\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1021 - mean_squared_error: 1.1021 - val_loss: 1.0287 - val_mean_squared_error: 1.0287\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02866, saving model to temp/e181\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0198 - mean_squared_error: 1.0198 - val_loss: 1.0120 - val_mean_squared_error: 1.0120\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02866 to 1.01198, saving model to temp/e181\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0160 - mean_squared_error: 1.0160 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01198 to 1.00338, saving model to temp/e181\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0037 - val_mean_squared_error: 1.0037\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00338\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 1.0083 - val_mean_squared_error: 1.0083\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00338\n",
      "Epoch 00005: early stopping\n",
      "temp/e182\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1046 - mean_squared_error: 1.1046 - val_loss: 1.0243 - val_mean_squared_error: 1.0243\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02433, saving model to temp/e182\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0207 - mean_squared_error: 1.0207 - val_loss: 1.0207 - val_mean_squared_error: 1.0207\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02433 to 1.02075, saving model to temp/e182\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0174 - mean_squared_error: 1.0174 - val_loss: 1.0068 - val_mean_squared_error: 1.0068\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02075 to 1.00676, saving model to temp/e182\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0125 - mean_squared_error: 1.0125 - val_loss: 1.0089 - val_mean_squared_error: 1.0089\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00676\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0069 - mean_squared_error: 1.0069 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00676 to 1.00261, saving model to temp/e182\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00261\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0060 - mean_squared_error: 1.0060 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00261\n",
      "Epoch 00007: early stopping\n",
      "temp/e183\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.1032 - mean_squared_error: 1.1032 - val_loss: 1.0143 - val_mean_squared_error: 1.0143\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01431, saving model to temp/e183\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0141 - mean_squared_error: 1.0141 - val_loss: 1.0161 - val_mean_squared_error: 1.0161\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01431\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0168 - mean_squared_error: 1.0168 - val_loss: 1.0151 - val_mean_squared_error: 1.0151\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01431\n",
      "Epoch 00003: early stopping\n",
      "temp/e184\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1043 - mean_squared_error: 1.1043 - val_loss: 1.0094 - val_mean_squared_error: 1.0094\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00944, saving model to temp/e184\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0194 - mean_squared_error: 1.0194 - val_loss: 1.0379 - val_mean_squared_error: 1.0379\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.00944\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 1.0029 - val_mean_squared_error: 1.0029\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00944 to 1.00288, saving model to temp/e184\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0105 - mean_squared_error: 1.0105 - val_loss: 1.0089 - val_mean_squared_error: 1.0089\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00288\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0027 - val_mean_squared_error: 1.0027\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00288 to 1.00273, saving model to temp/e184\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0066 - mean_squared_error: 1.0066 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00273 to 1.00231, saving model to temp/e184\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0044 - mean_squared_error: 1.0044 - val_loss: 1.0136 - val_mean_squared_error: 1.0136\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00231\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0071 - mean_squared_error: 1.0071 - val_loss: 1.0066 - val_mean_squared_error: 1.0066\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00231\n",
      "Epoch 00008: early stopping\n",
      "temp/e185\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0976 - mean_squared_error: 1.0976 - val_loss: 1.0155 - val_mean_squared_error: 1.0155\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01550, saving model to temp/e185\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0178 - mean_squared_error: 1.0178 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01550 to 1.00863, saving model to temp/e185\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0159 - mean_squared_error: 1.0159 - val_loss: 1.0114 - val_mean_squared_error: 1.0114\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00863\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0147 - mean_squared_error: 1.0147 - val_loss: 1.0094 - val_mean_squared_error: 1.0094\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00863\n",
      "Epoch 00004: early stopping\n",
      "temp/e186\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.1015 - mean_squared_error: 1.1015 - val_loss: 1.0140 - val_mean_squared_error: 1.0140\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01404, saving model to temp/e186\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0193 - mean_squared_error: 1.0193 - val_loss: 1.0107 - val_mean_squared_error: 1.0107\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01404 to 1.01068, saving model to temp/e186\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0160 - mean_squared_error: 1.0160 - val_loss: 1.0149 - val_mean_squared_error: 1.0149\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01068\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 1.0073 - val_mean_squared_error: 1.0073\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01068 to 1.00733, saving model to temp/e186\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0078 - val_mean_squared_error: 1.0078\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00733\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0055 - val_mean_squared_error: 1.0055\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00733 to 1.00547, saving model to temp/e186\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0075 - mean_squared_error: 1.0075 - val_loss: 1.0119 - val_mean_squared_error: 1.0119\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00547\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0046 - mean_squared_error: 1.0046 - val_loss: 1.0227 - val_mean_squared_error: 1.0227\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00547\n",
      "Epoch 00008: early stopping\n",
      "temp/e187\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0974 - mean_squared_error: 1.0974 - val_loss: 1.0154 - val_mean_squared_error: 1.0154\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01538, saving model to temp/e187\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0196 - mean_squared_error: 1.0196 - val_loss: 1.0113 - val_mean_squared_error: 1.0113\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01538 to 1.01127, saving model to temp/e187\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0156 - mean_squared_error: 1.0156 - val_loss: 1.0081 - val_mean_squared_error: 1.0081\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01127 to 1.00809, saving model to temp/e187\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0136 - mean_squared_error: 1.0136 - val_loss: 1.0007 - val_mean_squared_error: 1.0007\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00809 to 1.00073, saving model to temp/e187\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.0111 - mean_squared_error: 1.0111 - val_loss: 1.0134 - val_mean_squared_error: 1.0134\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00073\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0103 - mean_squared_error: 1.0103 - val_loss: 1.0025 - val_mean_squared_error: 1.0025\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00073\n",
      "Epoch 00006: early stopping\n",
      "temp/e188\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.1011 - mean_squared_error: 1.1011 - val_loss: 1.0221 - val_mean_squared_error: 1.0221\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02209, saving model to temp/e188\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.0224 - mean_squared_error: 1.0224 - val_loss: 1.0136 - val_mean_squared_error: 1.0136\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02209 to 1.01358, saving model to temp/e188\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0144 - mean_squared_error: 1.0144 - val_loss: 1.0068 - val_mean_squared_error: 1.0068\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01358 to 1.00683, saving model to temp/e188\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0102 - val_mean_squared_error: 1.0102\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00683\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0041 - val_mean_squared_error: 1.0041\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00683 to 1.00408, saving model to temp/e188\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 1.0126 - val_mean_squared_error: 1.0126\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00408\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 1.0063 - val_mean_squared_error: 1.0063\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00408\n",
      "Epoch 00007: early stopping\n",
      "temp/e189\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0984 - mean_squared_error: 1.0984 - val_loss: 1.0109 - val_mean_squared_error: 1.0109\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01088, saving model to temp/e189\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0193 - mean_squared_error: 1.0193 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01088 to 1.00983, saving model to temp/e189\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0187 - mean_squared_error: 1.0187 - val_loss: 1.0173 - val_mean_squared_error: 1.0173\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00983\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0073 - val_mean_squared_error: 1.0073\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00983 to 1.00727, saving model to temp/e189\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00727 to 1.00327, saving model to temp/e189\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0044 - val_mean_squared_error: 1.0044\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00327\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0089 - mean_squared_error: 1.0089 - val_loss: 0.9984 - val_mean_squared_error: 0.9984\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00327 to 0.99840, saving model to temp/e189\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0075 - mean_squared_error: 1.0075 - val_loss: 0.9995 - val_mean_squared_error: 0.9995\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99840\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 0.9986 - val_mean_squared_error: 0.9986\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99840\n",
      "Epoch 00009: early stopping\n",
      "temp/e190\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1031 - mean_squared_error: 1.1031 - val_loss: 1.0197 - val_mean_squared_error: 1.0197\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01965, saving model to temp/e190\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0210 - mean_squared_error: 1.0210 - val_loss: 1.0227 - val_mean_squared_error: 1.0227\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01965\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.0138 - mean_squared_error: 1.0138 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01965 to 1.00883, saving model to temp/e190\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00883 to 1.00534, saving model to temp/e190\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0090 - val_mean_squared_error: 1.0090\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00534\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00534 to 1.00489, saving model to temp/e190\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0054 - mean_squared_error: 1.0054 - val_loss: 1.0138 - val_mean_squared_error: 1.0138\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00489\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0058 - mean_squared_error: 1.0058 - val_loss: 0.9976 - val_mean_squared_error: 0.9976\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00489 to 0.99760, saving model to temp/e190\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0063 - mean_squared_error: 1.0063 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99760\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99760\n",
      "Epoch 00010: early stopping\n",
      "temp/e191\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0987 - mean_squared_error: 1.0987 - val_loss: 1.0118 - val_mean_squared_error: 1.0118\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01183, saving model to temp/e191\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0206 - mean_squared_error: 1.0206 - val_loss: 1.0142 - val_mean_squared_error: 1.0142\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01183\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0155 - mean_squared_error: 1.0155 - val_loss: 1.0157 - val_mean_squared_error: 1.0157\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01183\n",
      "Epoch 00003: early stopping\n",
      "temp/e192\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1050 - mean_squared_error: 1.1050 - val_loss: 1.0205 - val_mean_squared_error: 1.0205\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02054, saving model to temp/e192\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0219 - mean_squared_error: 1.0219 - val_loss: 1.0189 - val_mean_squared_error: 1.0189\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02054 to 1.01890, saving model to temp/e192\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0167 - mean_squared_error: 1.0167 - val_loss: 1.0169 - val_mean_squared_error: 1.0169\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01890 to 1.01692, saving model to temp/e192\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0154 - mean_squared_error: 1.0154 - val_loss: 1.0027 - val_mean_squared_error: 1.0027\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01692 to 1.00274, saving model to temp/e192\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0058 - val_mean_squared_error: 1.0058\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00274\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 1.0025 - val_mean_squared_error: 1.0025\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00274 to 1.00255, saving model to temp/e192\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 1.0005 - val_mean_squared_error: 1.0005\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00255 to 1.00049, saving model to temp/e192\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0053 - mean_squared_error: 1.0053 - val_loss: 1.0045 - val_mean_squared_error: 1.0045\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00049\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0075 - mean_squared_error: 1.0075 - val_loss: 0.9990 - val_mean_squared_error: 0.9990\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00049 to 0.99905, saving model to temp/e192\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0071 - mean_squared_error: 1.0071 - val_loss: 0.9984 - val_mean_squared_error: 0.9984\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.99905 to 0.99841, saving model to temp/e192\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0059 - mean_squared_error: 1.0059 - val_loss: 0.9986 - val_mean_squared_error: 0.9986\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99841\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0041 - mean_squared_error: 1.0041 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.99841\n",
      "Epoch 00012: early stopping\n",
      "temp/e193\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.1078 - mean_squared_error: 1.1078 - val_loss: 1.0312 - val_mean_squared_error: 1.0312\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03120, saving model to temp/e193\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0221 - mean_squared_error: 1.0221 - val_loss: 1.0126 - val_mean_squared_error: 1.0126\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03120 to 1.01262, saving model to temp/e193\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0142 - val_mean_squared_error: 1.0142\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01262\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0118 - mean_squared_error: 1.0118 - val_loss: 1.0079 - val_mean_squared_error: 1.0079\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01262 to 1.00787, saving model to temp/e193\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0073 - val_mean_squared_error: 1.0073\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00787 to 1.00727, saving model to temp/e193\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00727 to 1.00533, saving model to temp/e193\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0100 - val_mean_squared_error: 1.0100\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00533\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0070 - mean_squared_error: 1.0070 - val_loss: 1.0134 - val_mean_squared_error: 1.0134\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00533\n",
      "Epoch 00008: early stopping\n",
      "temp/e194\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.1086 - mean_squared_error: 1.1086 - val_loss: 1.0103 - val_mean_squared_error: 1.0103\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01033, saving model to temp/e194\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0188 - mean_squared_error: 1.0188 - val_loss: 1.0111 - val_mean_squared_error: 1.0111\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01033\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0146 - mean_squared_error: 1.0146 - val_loss: 1.0142 - val_mean_squared_error: 1.0142\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01033\n",
      "Epoch 00003: early stopping\n",
      "temp/e195\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1051 - mean_squared_error: 1.1051 - val_loss: 1.0133 - val_mean_squared_error: 1.0133\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01328, saving model to temp/e195\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0178 - mean_squared_error: 1.0178 - val_loss: 1.0124 - val_mean_squared_error: 1.0124\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01328 to 1.01239, saving model to temp/e195\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0161 - mean_squared_error: 1.0161 - val_loss: 1.0143 - val_mean_squared_error: 1.0143\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01239\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0032 - val_mean_squared_error: 1.0032\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01239 to 1.00321, saving model to temp/e195\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0107 - mean_squared_error: 1.0107 - val_loss: 1.0074 - val_mean_squared_error: 1.0074\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00321\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0071 - mean_squared_error: 1.0071 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00321\n",
      "Epoch 00006: early stopping\n",
      "temp/e196\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.0983 - mean_squared_error: 1.0983 - val_loss: 1.0300 - val_mean_squared_error: 1.0300\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03003, saving model to temp/e196\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.0226 - mean_squared_error: 1.0226 - val_loss: 1.0169 - val_mean_squared_error: 1.0169\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03003 to 1.01688, saving model to temp/e196\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0153 - mean_squared_error: 1.0153 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01688 to 1.00915, saving model to temp/e196\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0152 - mean_squared_error: 1.0152 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00915\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 142us/step - loss: 1.0093 - mean_squared_error: 1.0093 - val_loss: 1.0065 - val_mean_squared_error: 1.0065\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00915 to 1.00654, saving model to temp/e196\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0014 - val_mean_squared_error: 1.0014\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00654 to 1.00137, saving model to temp/e196\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0044 - mean_squared_error: 1.0044 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00137\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0042 - mean_squared_error: 1.0042 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00137\n",
      "Epoch 00008: early stopping\n",
      "temp/e197\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.1071 - mean_squared_error: 1.1071 - val_loss: 1.0216 - val_mean_squared_error: 1.0216\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02164, saving model to temp/e197\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0229 - mean_squared_error: 1.0229 - val_loss: 1.0094 - val_mean_squared_error: 1.0094\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02164 to 1.00939, saving model to temp/e197\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0129 - mean_squared_error: 1.0129 - val_loss: 1.0066 - val_mean_squared_error: 1.0066\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00939 to 1.00657, saving model to temp/e197\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0099 - val_mean_squared_error: 1.0099\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00657\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0112 - mean_squared_error: 1.0112 - val_loss: 1.0089 - val_mean_squared_error: 1.0089\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00657\n",
      "Epoch 00005: early stopping\n",
      "temp/e198\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.1020 - mean_squared_error: 1.1020 - val_loss: 1.0119 - val_mean_squared_error: 1.0119\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01194, saving model to temp/e198\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0190 - mean_squared_error: 1.0190 - val_loss: 1.0092 - val_mean_squared_error: 1.0092\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01194 to 1.00918, saving model to temp/e198\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0138 - mean_squared_error: 1.0138 - val_loss: 1.0242 - val_mean_squared_error: 1.0242\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00918\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0163 - mean_squared_error: 1.0163 - val_loss: 1.0083 - val_mean_squared_error: 1.0083\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00918 to 1.00834, saving model to temp/e198\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0093 - mean_squared_error: 1.0093 - val_loss: 1.0016 - val_mean_squared_error: 1.0016\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00834 to 1.00165, saving model to temp/e198\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0064 - mean_squared_error: 1.0064 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00165\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0031 - mean_squared_error: 1.0031 - val_loss: 1.0079 - val_mean_squared_error: 1.0079\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00165\n",
      "Epoch 00007: early stopping\n",
      "temp/e199\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1024 - mean_squared_error: 1.1024 - val_loss: 1.0170 - val_mean_squared_error: 1.0170\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01703, saving model to temp/e199\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0199 - mean_squared_error: 1.0199 - val_loss: 1.0128 - val_mean_squared_error: 1.0128\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01703 to 1.01279, saving model to temp/e199\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0133 - val_mean_squared_error: 1.0133\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01279\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0148 - mean_squared_error: 1.0148 - val_loss: 1.0105 - val_mean_squared_error: 1.0105\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01279 to 1.01050, saving model to temp/e199\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0086 - mean_squared_error: 1.0086 - val_loss: 1.0058 - val_mean_squared_error: 1.0058\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01050 to 1.00584, saving model to temp/e199\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 1.0060 - val_mean_squared_error: 1.0060\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00584\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0056 - val_mean_squared_error: 1.0056\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00584 to 1.00561, saving model to temp/e199\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0065 - mean_squared_error: 1.0065 - val_loss: 1.0042 - val_mean_squared_error: 1.0042\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00561 to 1.00417, saving model to temp/e199\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0058 - mean_squared_error: 1.0058 - val_loss: 0.9997 - val_mean_squared_error: 0.9997\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00417 to 0.99971, saving model to temp/e199\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0055 - mean_squared_error: 1.0055 - val_loss: 1.0018 - val_mean_squared_error: 1.0018\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99971\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0071 - mean_squared_error: 1.0071 - val_loss: 0.9962 - val_mean_squared_error: 0.9962\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.99971 to 0.99618, saving model to temp/e199\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0048 - mean_squared_error: 1.0048 - val_loss: 1.0063 - val_mean_squared_error: 1.0063\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.99618\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0044 - mean_squared_error: 1.0044 - val_loss: 0.9985 - val_mean_squared_error: 0.9985\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.99618\n",
      "Epoch 00013: early stopping\n",
      "temp/e200\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1014 - mean_squared_error: 1.1014 - val_loss: 1.0125 - val_mean_squared_error: 1.0125\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01253, saving model to temp/e200\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0177 - mean_squared_error: 1.0177 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01253 to 1.00857, saving model to temp/e200\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0155 - mean_squared_error: 1.0155 - val_loss: 1.0067 - val_mean_squared_error: 1.0067\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00857 to 1.00670, saving model to temp/e200\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0144 - mean_squared_error: 1.0144 - val_loss: 1.0135 - val_mean_squared_error: 1.0135\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00670\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0112 - val_mean_squared_error: 1.0112\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00670\n",
      "Epoch 00005: early stopping\n",
      "temp/e201\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0988 - mean_squared_error: 1.0988 - val_loss: 1.0251 - val_mean_squared_error: 1.0251\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02507, saving model to temp/e201\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0218 - mean_squared_error: 1.0218 - val_loss: 1.0090 - val_mean_squared_error: 1.0090\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02507 to 1.00900, saving model to temp/e201\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0081 - val_mean_squared_error: 1.0081\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00900 to 1.00814, saving model to temp/e201\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0045 - val_mean_squared_error: 1.0045\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00814 to 1.00451, saving model to temp/e201\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0060 - val_mean_squared_error: 1.0060\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00451\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 1.0126 - val_mean_squared_error: 1.0126\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00451\n",
      "Epoch 00006: early stopping\n",
      "temp/e202\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1044 - mean_squared_error: 1.1044 - val_loss: 1.0174 - val_mean_squared_error: 1.0174\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01740, saving model to temp/e202\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0230 - mean_squared_error: 1.0230 - val_loss: 1.0276 - val_mean_squared_error: 1.0276\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01740\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.0125 - mean_squared_error: 1.0125 - val_loss: 1.0156 - val_mean_squared_error: 1.0156\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01740 to 1.01555, saving model to temp/e202\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0107 - mean_squared_error: 1.0107 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01555 to 1.00466, saving model to temp/e202\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0129 - mean_squared_error: 1.0129 - val_loss: 1.0035 - val_mean_squared_error: 1.0035\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00466 to 1.00352, saving model to temp/e202\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 1.0096 - val_mean_squared_error: 1.0096\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00352\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 0.9990 - val_mean_squared_error: 0.9990\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00352 to 0.99898, saving model to temp/e202\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 0.9973 - val_mean_squared_error: 0.9973\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.99898 to 0.99733, saving model to temp/e202\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0028 - mean_squared_error: 1.0028 - val_loss: 1.0015 - val_mean_squared_error: 1.0015\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99733\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0053 - mean_squared_error: 1.0053 - val_loss: 0.9985 - val_mean_squared_error: 0.9985\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99733\n",
      "Epoch 00010: early stopping\n",
      "temp/e203\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1033 - mean_squared_error: 1.1033 - val_loss: 1.0133 - val_mean_squared_error: 1.0133\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01328, saving model to temp/e203\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0216 - mean_squared_error: 1.0216 - val_loss: 1.0149 - val_mean_squared_error: 1.0149\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01328\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0146 - mean_squared_error: 1.0146 - val_loss: 1.0108 - val_mean_squared_error: 1.0108\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01328 to 1.01081, saving model to temp/e203\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0160 - mean_squared_error: 1.0160 - val_loss: 1.0043 - val_mean_squared_error: 1.0043\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01081 to 1.00428, saving model to temp/e203\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0082 - mean_squared_error: 1.0082 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00428\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0035 - val_mean_squared_error: 1.0035\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00428 to 1.00354, saving model to temp/e203\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0070 - mean_squared_error: 1.0070 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00354\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0070 - mean_squared_error: 1.0070 - val_loss: 1.0037 - val_mean_squared_error: 1.0037\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00354\n",
      "Epoch 00008: early stopping\n",
      "temp/e204\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.0984 - mean_squared_error: 1.0984 - val_loss: 1.0320 - val_mean_squared_error: 1.0320\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03195, saving model to temp/e204\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0191 - mean_squared_error: 1.0191 - val_loss: 1.0241 - val_mean_squared_error: 1.0241\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03195 to 1.02415, saving model to temp/e204\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0161 - val_mean_squared_error: 1.0161\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02415 to 1.01609, saving model to temp/e204\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0114 - mean_squared_error: 1.0114 - val_loss: 1.0111 - val_mean_squared_error: 1.0111\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01609 to 1.01114, saving model to temp/e204\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0020 - val_mean_squared_error: 1.0020\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01114 to 1.00195, saving model to temp/e204\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0112 - mean_squared_error: 1.0112 - val_loss: 1.0042 - val_mean_squared_error: 1.0042\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00195\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00195\n",
      "Epoch 00007: early stopping\n",
      "temp/e205\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.1121 - mean_squared_error: 1.1121 - val_loss: 1.0121 - val_mean_squared_error: 1.0121\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01205, saving model to temp/e205\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0194 - mean_squared_error: 1.0194 - val_loss: 1.0112 - val_mean_squared_error: 1.0112\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01205 to 1.01125, saving model to temp/e205\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0151 - mean_squared_error: 1.0151 - val_loss: 1.0124 - val_mean_squared_error: 1.0124\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01125\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0119 - mean_squared_error: 1.0119 - val_loss: 1.0076 - val_mean_squared_error: 1.0076\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01125 to 1.00762, saving model to temp/e205\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0118 - mean_squared_error: 1.0118 - val_loss: 0.9996 - val_mean_squared_error: 0.9996\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00762 to 0.99958, saving model to temp/e205\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0046 - val_mean_squared_error: 1.0046\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99958\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0053 - mean_squared_error: 1.0053 - val_loss: 1.0079 - val_mean_squared_error: 1.0079\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99958\n",
      "Epoch 00007: early stopping\n",
      "temp/e206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1046 - mean_squared_error: 1.1046 - val_loss: 1.0139 - val_mean_squared_error: 1.0139\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01392, saving model to temp/e206\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.0174 - mean_squared_error: 1.0174 - val_loss: 1.0150 - val_mean_squared_error: 1.0150\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01392\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0074 - val_mean_squared_error: 1.0074\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01392 to 1.00742, saving model to temp/e206\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0086 - mean_squared_error: 1.0086 - val_loss: 1.0112 - val_mean_squared_error: 1.0112\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00742\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0174 - val_mean_squared_error: 1.0174\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00742\n",
      "Epoch 00005: early stopping\n",
      "temp/e207\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1036 - mean_squared_error: 1.1036 - val_loss: 1.0316 - val_mean_squared_error: 1.0316\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03161, saving model to temp/e207\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0188 - mean_squared_error: 1.0188 - val_loss: 1.0250 - val_mean_squared_error: 1.0250\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03161 to 1.02495, saving model to temp/e207\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0165 - mean_squared_error: 1.0165 - val_loss: 1.0144 - val_mean_squared_error: 1.0144\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02495 to 1.01439, saving model to temp/e207\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0159 - mean_squared_error: 1.0159 - val_loss: 1.0055 - val_mean_squared_error: 1.0055\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01439 to 1.00553, saving model to temp/e207\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0147 - val_mean_squared_error: 1.0147\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00553\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0129 - mean_squared_error: 1.0129 - val_loss: 1.0015 - val_mean_squared_error: 1.0015\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00553 to 1.00154, saving model to temp/e207\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 1.0067 - val_mean_squared_error: 1.0067\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00154\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0089 - mean_squared_error: 1.0089 - val_loss: 1.0174 - val_mean_squared_error: 1.0174\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00154\n",
      "Epoch 00008: early stopping\n",
      "temp/e208\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0934 - mean_squared_error: 1.0934 - val_loss: 1.0125 - val_mean_squared_error: 1.0125\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01255, saving model to temp/e208\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0189 - mean_squared_error: 1.0189 - val_loss: 1.0072 - val_mean_squared_error: 1.0072\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01255 to 1.00723, saving model to temp/e208\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0139 - mean_squared_error: 1.0139 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00723 to 1.00524, saving model to temp/e208\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0138 - mean_squared_error: 1.0138 - val_loss: 1.0092 - val_mean_squared_error: 1.0092\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00524\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00524\n",
      "Epoch 00005: early stopping\n",
      "temp/e209\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.1068 - mean_squared_error: 1.1068 - val_loss: 1.0309 - val_mean_squared_error: 1.0309\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03089, saving model to temp/e209\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0202 - mean_squared_error: 1.0202 - val_loss: 1.0148 - val_mean_squared_error: 1.0148\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03089 to 1.01482, saving model to temp/e209\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0174 - mean_squared_error: 1.0174 - val_loss: 1.0208 - val_mean_squared_error: 1.0208\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01482\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01482 to 1.00494, saving model to temp/e209\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0112 - mean_squared_error: 1.0112 - val_loss: 1.0058 - val_mean_squared_error: 1.0058\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00494\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0078 - mean_squared_error: 1.0078 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00494\n",
      "Epoch 00006: early stopping\n",
      "temp/e210\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1034 - mean_squared_error: 1.1034 - val_loss: 1.0210 - val_mean_squared_error: 1.0210\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02099, saving model to temp/e210\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0237 - mean_squared_error: 1.0237 - val_loss: 1.0282 - val_mean_squared_error: 1.0282\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.02099\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0043 - val_mean_squared_error: 1.0043\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02099 to 1.00429, saving model to temp/e210\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0089 - mean_squared_error: 1.0089 - val_loss: 1.0073 - val_mean_squared_error: 1.0073\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00429\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00429\n",
      "Epoch 00005: early stopping\n",
      "temp/e211\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0992 - mean_squared_error: 1.0992 - val_loss: 1.0201 - val_mean_squared_error: 1.0201\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02012, saving model to temp/e211\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0224 - mean_squared_error: 1.0224 - val_loss: 1.0313 - val_mean_squared_error: 1.0313\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.02012\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0155 - mean_squared_error: 1.0155 - val_loss: 1.0056 - val_mean_squared_error: 1.0056\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02012 to 1.00560, saving model to temp/e211\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0131 - mean_squared_error: 1.0131 - val_loss: 1.0094 - val_mean_squared_error: 1.0094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00560\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0118 - mean_squared_error: 1.0118 - val_loss: 1.0087 - val_mean_squared_error: 1.0087\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00560\n",
      "Epoch 00005: early stopping\n",
      "temp/e212\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1020 - mean_squared_error: 1.1020 - val_loss: 1.0259 - val_mean_squared_error: 1.0259\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02586, saving model to temp/e212\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0188 - mean_squared_error: 1.0188 - val_loss: 1.0104 - val_mean_squared_error: 1.0104\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02586 to 1.01041, saving model to temp/e212\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0162 - mean_squared_error: 1.0162 - val_loss: 1.0070 - val_mean_squared_error: 1.0070\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01041 to 1.00704, saving model to temp/e212\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0140 - mean_squared_error: 1.0140 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00704 to 1.00471, saving model to temp/e212\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00471\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 1.0071 - val_mean_squared_error: 1.0071\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00471\n",
      "Epoch 00006: early stopping\n",
      "temp/e213\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1002 - mean_squared_error: 1.1002 - val_loss: 1.0142 - val_mean_squared_error: 1.0142\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01421, saving model to temp/e213\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0162 - mean_squared_error: 1.0162 - val_loss: 1.0219 - val_mean_squared_error: 1.0219\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01421\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0148 - mean_squared_error: 1.0148 - val_loss: 1.0096 - val_mean_squared_error: 1.0096\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01421 to 1.00962, saving model to temp/e213\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0142 - mean_squared_error: 1.0142 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00962 to 1.00533, saving model to temp/e213\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0046 - val_mean_squared_error: 1.0046\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00533 to 1.00458, saving model to temp/e213\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 1.0147 - val_mean_squared_error: 1.0147\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00458\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0046 - mean_squared_error: 1.0046 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00458\n",
      "Epoch 00007: early stopping\n",
      "temp/e214\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.1084 - mean_squared_error: 1.1084 - val_loss: 1.0348 - val_mean_squared_error: 1.0348\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03483, saving model to temp/e214\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0198 - mean_squared_error: 1.0198 - val_loss: 1.0124 - val_mean_squared_error: 1.0124\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03483 to 1.01235, saving model to temp/e214\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0175 - mean_squared_error: 1.0175 - val_loss: 1.0242 - val_mean_squared_error: 1.0242\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01235\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01235 to 1.00378, saving model to temp/e214\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 1.0129 - val_mean_squared_error: 1.0129\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00378\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00378\n",
      "Epoch 00006: early stopping\n",
      "temp/e215\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1020 - mean_squared_error: 1.1020 - val_loss: 1.0163 - val_mean_squared_error: 1.0163\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01635, saving model to temp/e215\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0165 - mean_squared_error: 1.0165 - val_loss: 1.0151 - val_mean_squared_error: 1.0151\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01635 to 1.01509, saving model to temp/e215\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0174 - mean_squared_error: 1.0174 - val_loss: 1.0176 - val_mean_squared_error: 1.0176\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01509\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0126 - val_mean_squared_error: 1.0126\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01509 to 1.01257, saving model to temp/e215\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0125 - mean_squared_error: 1.0125 - val_loss: 1.0082 - val_mean_squared_error: 1.0082\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01257 to 1.00821, saving model to temp/e215\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0022 - val_mean_squared_error: 1.0022\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00821 to 1.00216, saving model to temp/e215\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0052 - mean_squared_error: 1.0052 - val_loss: 1.0000 - val_mean_squared_error: 1.0000\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00216 to 0.99999, saving model to temp/e215\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0066 - mean_squared_error: 1.0066 - val_loss: 1.0001 - val_mean_squared_error: 1.0001\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99999\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0061 - mean_squared_error: 1.0061 - val_loss: 0.9987 - val_mean_squared_error: 0.9987\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.99999 to 0.99871, saving model to temp/e215\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0063 - mean_squared_error: 1.0063 - val_loss: 0.9997 - val_mean_squared_error: 0.9997\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99871\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0036 - mean_squared_error: 1.0036 - val_loss: 0.9975 - val_mean_squared_error: 0.9975\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.99871 to 0.99746, saving model to temp/e215\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0037 - mean_squared_error: 1.0037 - val_loss: 1.0008 - val_mean_squared_error: 1.0008\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.99746\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0013 - mean_squared_error: 1.0013 - val_loss: 0.9952 - val_mean_squared_error: 0.9952\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.99746 to 0.99524, saving model to temp/e215\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0035 - mean_squared_error: 1.0035 - val_loss: 0.9968 - val_mean_squared_error: 0.9968\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.99524\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0009 - mean_squared_error: 1.0009 - val_loss: 1.0027 - val_mean_squared_error: 1.0027\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.99524\n",
      "Epoch 00015: early stopping\n",
      "temp/e216\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0955 - mean_squared_error: 1.0955 - val_loss: 1.0162 - val_mean_squared_error: 1.0162\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01617, saving model to temp/e216\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0237 - mean_squared_error: 1.0237 - val_loss: 1.0109 - val_mean_squared_error: 1.0109\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01617 to 1.01086, saving model to temp/e216\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0140 - mean_squared_error: 1.0140 - val_loss: 1.0077 - val_mean_squared_error: 1.0077\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01086 to 1.00768, saving model to temp/e216\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0141 - mean_squared_error: 1.0141 - val_loss: 1.0015 - val_mean_squared_error: 1.0015\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00768 to 1.00150, saving model to temp/e216\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00150\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00150\n",
      "Epoch 00006: early stopping\n",
      "temp/e217\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0942 - mean_squared_error: 1.0942 - val_loss: 1.0139 - val_mean_squared_error: 1.0139\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01389, saving model to temp/e217\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0227 - mean_squared_error: 1.0227 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01389 to 1.00518, saving model to temp/e217\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0064 - val_mean_squared_error: 1.0064\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00518\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0118 - mean_squared_error: 1.0118 - val_loss: 1.0139 - val_mean_squared_error: 1.0139\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00518\n",
      "Epoch 00004: early stopping\n",
      "temp/e218\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.0993 - mean_squared_error: 1.0993 - val_loss: 1.0151 - val_mean_squared_error: 1.0151\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01510, saving model to temp/e218\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0189 - mean_squared_error: 1.0189 - val_loss: 1.0178 - val_mean_squared_error: 1.0178\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01510\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0154 - mean_squared_error: 1.0154 - val_loss: 1.0176 - val_mean_squared_error: 1.0176\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01510\n",
      "Epoch 00003: early stopping\n",
      "temp/e219\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1142 - mean_squared_error: 1.1142 - val_loss: 1.0290 - val_mean_squared_error: 1.0290\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02900, saving model to temp/e219\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0162 - mean_squared_error: 1.0162 - val_loss: 1.0121 - val_mean_squared_error: 1.0121\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02900 to 1.01206, saving model to temp/e219\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0174 - mean_squared_error: 1.0174 - val_loss: 1.0076 - val_mean_squared_error: 1.0076\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01206 to 1.00758, saving model to temp/e219\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0072 - val_mean_squared_error: 1.0072\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00758 to 1.00721, saving model to temp/e219\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 1.0045 - val_mean_squared_error: 1.0045\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00721 to 1.00449, saving model to temp/e219\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0118 - mean_squared_error: 1.0118 - val_loss: 1.0087 - val_mean_squared_error: 1.0087\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00449\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0070 - mean_squared_error: 1.0070 - val_loss: 1.0008 - val_mean_squared_error: 1.0008\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00449 to 1.00081, saving model to temp/e219\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.0035 - mean_squared_error: 1.0035 - val_loss: 1.0028 - val_mean_squared_error: 1.0028\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00081\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0050 - mean_squared_error: 1.0050 - val_loss: 1.0215 - val_mean_squared_error: 1.0215\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00081\n",
      "Epoch 00009: early stopping\n",
      "temp/e220\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0963 - mean_squared_error: 1.0963 - val_loss: 1.0120 - val_mean_squared_error: 1.0120\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01201, saving model to temp/e220\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0233 - mean_squared_error: 1.0233 - val_loss: 1.0095 - val_mean_squared_error: 1.0095\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01201 to 1.00949, saving model to temp/e220\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0140 - mean_squared_error: 1.0140 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00949 to 1.00327, saving model to temp/e220\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0121 - val_mean_squared_error: 1.0121\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00327\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00327\n",
      "Epoch 00005: early stopping\n",
      "temp/e221\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.1026 - mean_squared_error: 1.1026 - val_loss: 1.0184 - val_mean_squared_error: 1.0184\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01837, saving model to temp/e221\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0193 - mean_squared_error: 1.0193 - val_loss: 1.0526 - val_mean_squared_error: 1.0526\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01837\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0162 - mean_squared_error: 1.0162 - val_loss: 1.0101 - val_mean_squared_error: 1.0101\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01837 to 1.01011, saving model to temp/e221\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 1.0107 - val_mean_squared_error: 1.0107\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.01011\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 1.0066 - val_mean_squared_error: 1.0066\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01011 to 1.00659, saving model to temp/e221\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0116 - mean_squared_error: 1.0116 - val_loss: 1.0118 - val_mean_squared_error: 1.0118\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00659\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0119 - mean_squared_error: 1.0119 - val_loss: 1.0004 - val_mean_squared_error: 1.0004\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00659 to 1.00038, saving model to temp/e221\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0029 - mean_squared_error: 1.0029 - val_loss: 1.0143 - val_mean_squared_error: 1.0143\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00038\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 0.9994 - val_mean_squared_error: 0.9994\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00038 to 0.99943, saving model to temp/e221\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0060 - mean_squared_error: 1.0060 - val_loss: 0.9986 - val_mean_squared_error: 0.9986\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.99943 to 0.99857, saving model to temp/e221\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0055 - mean_squared_error: 1.0055 - val_loss: 0.9975 - val_mean_squared_error: 0.9975\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.99857 to 0.99752, saving model to temp/e221\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0027 - mean_squared_error: 1.0027 - val_loss: 1.0039 - val_mean_squared_error: 1.0039\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.99752\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0046 - mean_squared_error: 1.0046 - val_loss: 0.9954 - val_mean_squared_error: 0.9954\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.99752 to 0.99545, saving model to temp/e221\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0039 - mean_squared_error: 1.0039 - val_loss: 0.9993 - val_mean_squared_error: 0.9993\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.99545\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0024 - mean_squared_error: 1.0024 - val_loss: 1.0031 - val_mean_squared_error: 1.0031\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.99545\n",
      "Epoch 00015: early stopping\n",
      "temp/e222\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.0991 - mean_squared_error: 1.0991 - val_loss: 1.0248 - val_mean_squared_error: 1.0248\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02477, saving model to temp/e222\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0249 - mean_squared_error: 1.0249 - val_loss: 1.0123 - val_mean_squared_error: 1.0123\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02477 to 1.01233, saving model to temp/e222\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0151 - mean_squared_error: 1.0151 - val_loss: 1.0095 - val_mean_squared_error: 1.0095\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01233 to 1.00952, saving model to temp/e222\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0048 - val_mean_squared_error: 1.0048\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00952 to 1.00478, saving model to temp/e222\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0129 - mean_squared_error: 1.0129 - val_loss: 1.0028 - val_mean_squared_error: 1.0028\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00478 to 1.00278, saving model to temp/e222\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00278\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 0.9981 - val_mean_squared_error: 0.9981\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00278 to 0.99814, saving model to temp/e222\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0066 - mean_squared_error: 1.0066 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99814\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0054 - mean_squared_error: 1.0054 - val_loss: 1.0000 - val_mean_squared_error: 1.0000\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99814\n",
      "Epoch 00009: early stopping\n",
      "temp/e223\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.0938 - mean_squared_error: 1.0938 - val_loss: 1.0212 - val_mean_squared_error: 1.0212\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02119, saving model to temp/e223\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0187 - mean_squared_error: 1.0187 - val_loss: 1.0193 - val_mean_squared_error: 1.0193\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02119 to 1.01928, saving model to temp/e223\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0163 - mean_squared_error: 1.0163 - val_loss: 1.0115 - val_mean_squared_error: 1.0115\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01928 to 1.01154, saving model to temp/e223\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01154 to 1.00616, saving model to temp/e223\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 1.0083 - val_mean_squared_error: 1.0083\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00616\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0082 - mean_squared_error: 1.0082 - val_loss: 1.0014 - val_mean_squared_error: 1.0014\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00616 to 1.00140, saving model to temp/e223\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00140\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 0.9991 - val_mean_squared_error: 0.9991\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00140 to 0.99908, saving model to temp/e223\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0052 - mean_squared_error: 1.0052 - val_loss: 1.0095 - val_mean_squared_error: 1.0095\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99908\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99908\n",
      "Epoch 00010: early stopping\n",
      "temp/e224\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.1027 - mean_squared_error: 1.1027 - val_loss: 1.0292 - val_mean_squared_error: 1.0292\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02916, saving model to temp/e224\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0204 - mean_squared_error: 1.0204 - val_loss: 1.0226 - val_mean_squared_error: 1.0226\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02916 to 1.02262, saving model to temp/e224\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0136 - mean_squared_error: 1.0136 - val_loss: 1.0081 - val_mean_squared_error: 1.0081\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02262 to 1.00813, saving model to temp/e224\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0116 - mean_squared_error: 1.0116 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00813 to 1.00617, saving model to temp/e224\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0048 - val_mean_squared_error: 1.0048\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00617 to 1.00480, saving model to temp/e224\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0066 - mean_squared_error: 1.0066 - val_loss: 1.0132 - val_mean_squared_error: 1.0132\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00480\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0118 - mean_squared_error: 1.0118 - val_loss: 1.0055 - val_mean_squared_error: 1.0055\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00480\n",
      "Epoch 00007: early stopping\n",
      "temp/e225\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.0987 - mean_squared_error: 1.0987 - val_loss: 1.0315 - val_mean_squared_error: 1.0315\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03146, saving model to temp/e225\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0197 - mean_squared_error: 1.0197 - val_loss: 1.0172 - val_mean_squared_error: 1.0172\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03146 to 1.01720, saving model to temp/e225\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0145 - mean_squared_error: 1.0145 - val_loss: 1.0131 - val_mean_squared_error: 1.0131\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01720 to 1.01310, saving model to temp/e225\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0138 - mean_squared_error: 1.0138 - val_loss: 1.0179 - val_mean_squared_error: 1.0179\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.01310\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 1.0058 - val_mean_squared_error: 1.0058\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01310 to 1.00584, saving model to temp/e225\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.0076 - val_mean_squared_error: 1.0076\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00584\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 1.0070 - val_mean_squared_error: 1.0070\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00584\n",
      "Epoch 00007: early stopping\n",
      "temp/e226\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0998 - mean_squared_error: 1.0998 - val_loss: 1.0468 - val_mean_squared_error: 1.0468\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.04681, saving model to temp/e226\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0218 - mean_squared_error: 1.0218 - val_loss: 1.0130 - val_mean_squared_error: 1.0130\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.04681 to 1.01300, saving model to temp/e226\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0169 - mean_squared_error: 1.0169 - val_loss: 1.0136 - val_mean_squared_error: 1.0136\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01300\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0111 - mean_squared_error: 1.0111 - val_loss: 1.0199 - val_mean_squared_error: 1.0199\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.01300\n",
      "Epoch 00004: early stopping\n",
      "temp/e227\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.0947 - mean_squared_error: 1.0947 - val_loss: 1.0282 - val_mean_squared_error: 1.0282\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02823, saving model to temp/e227\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0170 - mean_squared_error: 1.0170 - val_loss: 1.0141 - val_mean_squared_error: 1.0141\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02823 to 1.01410, saving model to temp/e227\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0146 - mean_squared_error: 1.0146 - val_loss: 1.0177 - val_mean_squared_error: 1.0177\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01410\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0165 - mean_squared_error: 1.0165 - val_loss: 1.0144 - val_mean_squared_error: 1.0144\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.01410\n",
      "Epoch 00004: early stopping\n",
      "temp/e228\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.1073 - mean_squared_error: 1.1073 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00798, saving model to temp/e228\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0179 - mean_squared_error: 1.0179 - val_loss: 1.0214 - val_mean_squared_error: 1.0214\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.00798\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0162 - mean_squared_error: 1.0162 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00798 to 1.00565, saving model to temp/e228\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0105 - mean_squared_error: 1.0105 - val_loss: 1.0013 - val_mean_squared_error: 1.0013\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00565 to 1.00132, saving model to temp/e228\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0131 - mean_squared_error: 1.0131 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00132\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0070 - mean_squared_error: 1.0070 - val_loss: 1.0099 - val_mean_squared_error: 1.0099\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00132\n",
      "Epoch 00006: early stopping\n",
      "temp/e229\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.0973 - mean_squared_error: 1.0973 - val_loss: 1.0282 - val_mean_squared_error: 1.0282\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02820, saving model to temp/e229\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0174 - mean_squared_error: 1.0174 - val_loss: 1.0434 - val_mean_squared_error: 1.0434\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.02820\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0182 - mean_squared_error: 1.0182 - val_loss: 1.0258 - val_mean_squared_error: 1.0258\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02820 to 1.02581, saving model to temp/e229\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0141 - mean_squared_error: 1.0141 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.02581 to 1.00524, saving model to temp/e229\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0078 - mean_squared_error: 1.0078 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00524\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0012 - val_mean_squared_error: 1.0012\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00524 to 1.00123, saving model to temp/e229\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0046 - val_mean_squared_error: 1.0046\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00123\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0049 - mean_squared_error: 1.0049 - val_loss: 1.0133 - val_mean_squared_error: 1.0133\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00123\n",
      "Epoch 00008: early stopping\n",
      "temp/e230\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0983 - mean_squared_error: 1.0983 - val_loss: 1.0142 - val_mean_squared_error: 1.0142\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01422, saving model to temp/e230\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0190 - mean_squared_error: 1.0190 - val_loss: 1.0130 - val_mean_squared_error: 1.0130\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01422 to 1.01303, saving model to temp/e230\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0134 - mean_squared_error: 1.0134 - val_loss: 1.0060 - val_mean_squared_error: 1.0060\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01303 to 1.00596, saving model to temp/e230\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 1.0132 - val_mean_squared_error: 1.0132\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00596\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0120 - val_mean_squared_error: 1.0120\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00596\n",
      "Epoch 00005: early stopping\n",
      "temp/e231\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.1021 - mean_squared_error: 1.1021 - val_loss: 1.0169 - val_mean_squared_error: 1.0169\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01689, saving model to temp/e231\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0189 - mean_squared_error: 1.0189 - val_loss: 1.0142 - val_mean_squared_error: 1.0142\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01689 to 1.01418, saving model to temp/e231\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0160 - mean_squared_error: 1.0160 - val_loss: 1.0117 - val_mean_squared_error: 1.0117\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01418 to 1.01168, saving model to temp/e231\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0146 - mean_squared_error: 1.0146 - val_loss: 1.0083 - val_mean_squared_error: 1.0083\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01168 to 1.00828, saving model to temp/e231\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0131 - mean_squared_error: 1.0131 - val_loss: 1.0079 - val_mean_squared_error: 1.0079\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00828 to 1.00787, saving model to temp/e231\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00787 to 1.00537, saving model to temp/e231\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0046 - mean_squared_error: 1.0046 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00537 to 1.00510, saving model to temp/e231\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0064 - mean_squared_error: 1.0064 - val_loss: 1.0013 - val_mean_squared_error: 1.0013\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00510 to 1.00128, saving model to temp/e231\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0056 - mean_squared_error: 1.0056 - val_loss: 1.0019 - val_mean_squared_error: 1.0019\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00128\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0042 - mean_squared_error: 1.0042 - val_loss: 1.0067 - val_mean_squared_error: 1.0067\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.00128\n",
      "Epoch 00010: early stopping\n",
      "temp/e232\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.1044 - mean_squared_error: 1.1044 - val_loss: 1.0245 - val_mean_squared_error: 1.0245\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02453, saving model to temp/e232\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0183 - mean_squared_error: 1.0183 - val_loss: 1.0068 - val_mean_squared_error: 1.0068\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02453 to 1.00677, saving model to temp/e232\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0123 - mean_squared_error: 1.0123 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00677 to 1.00338, saving model to temp/e232\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0085 - val_mean_squared_error: 1.0085\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00338\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0146 - mean_squared_error: 1.0146 - val_loss: 1.0056 - val_mean_squared_error: 1.0056\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00338\n",
      "Epoch 00005: early stopping\n",
      "temp/e233\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.1031 - mean_squared_error: 1.1031 - val_loss: 1.0229 - val_mean_squared_error: 1.0229\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02293, saving model to temp/e233\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0200 - mean_squared_error: 1.0200 - val_loss: 1.0193 - val_mean_squared_error: 1.0193\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02293 to 1.01932, saving model to temp/e233\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0142 - mean_squared_error: 1.0142 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01932 to 1.00304, saving model to temp/e233\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 1.0070 - val_mean_squared_error: 1.0070\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00304\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0140 - mean_squared_error: 1.0140 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00304\n",
      "Epoch 00005: early stopping\n",
      "temp/e234\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.1030 - mean_squared_error: 1.1030 - val_loss: 1.0337 - val_mean_squared_error: 1.0337\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03367, saving model to temp/e234\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0194 - mean_squared_error: 1.0194 - val_loss: 1.0120 - val_mean_squared_error: 1.0120\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03367 to 1.01201, saving model to temp/e234\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0168 - mean_squared_error: 1.0168 - val_loss: 1.0110 - val_mean_squared_error: 1.0110\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01201 to 1.01099, saving model to temp/e234\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0172 - mean_squared_error: 1.0172 - val_loss: 1.0220 - val_mean_squared_error: 1.0220\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.01099\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0028 - val_mean_squared_error: 1.0028\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01099 to 1.00275, saving model to temp/e234\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0086 - mean_squared_error: 1.0086 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00275\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0089 - mean_squared_error: 1.0089 - val_loss: 1.0007 - val_mean_squared_error: 1.0007\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00275 to 1.00067, saving model to temp/e234\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0143 - val_mean_squared_error: 1.0143\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00067\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 0.9993 - val_mean_squared_error: 0.9993\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00067 to 0.99927, saving model to temp/e234\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0056 - mean_squared_error: 1.0056 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99927\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 1.0058 - mean_squared_error: 1.0058 - val_loss: 1.0005 - val_mean_squared_error: 1.0005\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99927\n",
      "Epoch 00011: early stopping\n",
      "temp/e235\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.0989 - mean_squared_error: 1.0989 - val_loss: 1.0317 - val_mean_squared_error: 1.0317\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03166, saving model to temp/e235\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0168 - mean_squared_error: 1.0168 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03166 to 1.00983, saving model to temp/e235\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0134 - mean_squared_error: 1.0134 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00983 to 1.00879, saving model to temp/e235\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0136 - mean_squared_error: 1.0136 - val_loss: 1.0031 - val_mean_squared_error: 1.0031\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00879 to 1.00307, saving model to temp/e235\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00307\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.0069 - mean_squared_error: 1.0069 - val_loss: 1.0082 - val_mean_squared_error: 1.0082\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00307\n",
      "Epoch 00006: early stopping\n",
      "temp/e236\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.0918 - mean_squared_error: 1.0918 - val_loss: 1.0204 - val_mean_squared_error: 1.0204\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02042, saving model to temp/e236\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0219 - mean_squared_error: 1.0219 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02042 to 1.00748, saving model to temp/e236\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0028 - val_mean_squared_error: 1.0028\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00748 to 1.00280, saving model to temp/e236\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0127 - mean_squared_error: 1.0127 - val_loss: 1.0003 - val_mean_squared_error: 1.0003\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00280 to 1.00026, saving model to temp/e236\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0146 - mean_squared_error: 1.0146 - val_loss: 1.0020 - val_mean_squared_error: 1.0020\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00026\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00026\n",
      "Epoch 00006: early stopping\n",
      "temp/e237\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1087 - mean_squared_error: 1.1087 - val_loss: 1.0230 - val_mean_squared_error: 1.0230\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02300, saving model to temp/e237\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0193 - mean_squared_error: 1.0193 - val_loss: 1.0134 - val_mean_squared_error: 1.0134\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02300 to 1.01336, saving model to temp/e237\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0146 - mean_squared_error: 1.0146 - val_loss: 1.0254 - val_mean_squared_error: 1.0254\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01336\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0107 - mean_squared_error: 1.0107 - val_loss: 1.0083 - val_mean_squared_error: 1.0083\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01336 to 1.00830, saving model to temp/e237\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0179 - val_mean_squared_error: 1.0179\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00830\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 1.0150 - val_mean_squared_error: 1.0150\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00830\n",
      "Epoch 00006: early stopping\n",
      "temp/e238\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 170us/step - loss: 1.0975 - mean_squared_error: 1.0975 - val_loss: 1.0258 - val_mean_squared_error: 1.0258\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02581, saving model to temp/e238\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0221 - mean_squared_error: 1.0221 - val_loss: 1.0087 - val_mean_squared_error: 1.0087\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02581 to 1.00871, saving model to temp/e238\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0166 - mean_squared_error: 1.0166 - val_loss: 1.0087 - val_mean_squared_error: 1.0087\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00871 to 1.00869, saving model to temp/e238\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0131 - mean_squared_error: 1.0131 - val_loss: 1.0131 - val_mean_squared_error: 1.0131\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00869\n",
      "Epoch 00004: early stopping\n",
      "temp/e239\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 171us/step - loss: 1.1093 - mean_squared_error: 1.1093 - val_loss: 1.0250 - val_mean_squared_error: 1.0250\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02497, saving model to temp/e239\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0175 - mean_squared_error: 1.0175 - val_loss: 1.0048 - val_mean_squared_error: 1.0048\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02497 to 1.00484, saving model to temp/e239\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0152 - mean_squared_error: 1.0152 - val_loss: 1.0027 - val_mean_squared_error: 1.0027\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00484 to 1.00271, saving model to temp/e239\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00271 to 1.00263, saving model to temp/e239\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00263\n",
      "Epoch 00005: early stopping\n",
      "temp/e240\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1031 - mean_squared_error: 1.1031 - val_loss: 1.0153 - val_mean_squared_error: 1.0153\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01530, saving model to temp/e240\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0198 - mean_squared_error: 1.0198 - val_loss: 1.0108 - val_mean_squared_error: 1.0108\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01530 to 1.01084, saving model to temp/e240\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0169 - mean_squared_error: 1.0169 - val_loss: 1.0068 - val_mean_squared_error: 1.0068\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01084 to 1.00678, saving model to temp/e240\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0145 - mean_squared_error: 1.0145 - val_loss: 1.0028 - val_mean_squared_error: 1.0028\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00678 to 1.00283, saving model to temp/e240\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0149 - val_mean_squared_error: 1.0149\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00283\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 0.9999 - val_mean_squared_error: 0.9999\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00283 to 0.99986, saving model to temp/e240\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0067 - val_mean_squared_error: 1.0067\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99986\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.0059 - mean_squared_error: 1.0059 - val_loss: 0.9998 - val_mean_squared_error: 0.9998\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.99986 to 0.99981, saving model to temp/e240\n",
      "Epoch 00008: early stopping\n",
      "temp/e241\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.0976 - mean_squared_error: 1.0976 - val_loss: 1.0289 - val_mean_squared_error: 1.0289\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02893, saving model to temp/e241\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0177 - mean_squared_error: 1.0177 - val_loss: 1.0119 - val_mean_squared_error: 1.0119\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02893 to 1.01193, saving model to temp/e241\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0175 - mean_squared_error: 1.0175 - val_loss: 1.0045 - val_mean_squared_error: 1.0045\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01193 to 1.00448, saving model to temp/e241\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00448 to 1.00340, saving model to temp/e241\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0126 - val_mean_squared_error: 1.0126\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00340\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 0.9977 - val_mean_squared_error: 0.9977\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00340 to 0.99766, saving model to temp/e241\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0078 - mean_squared_error: 1.0078 - val_loss: 1.0032 - val_mean_squared_error: 1.0032\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99766\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0079 - mean_squared_error: 1.0079 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99766\n",
      "Epoch 00008: early stopping\n",
      "temp/e242\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.0974 - mean_squared_error: 1.0974 - val_loss: 1.0176 - val_mean_squared_error: 1.0176\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01762, saving model to temp/e242\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0197 - mean_squared_error: 1.0197 - val_loss: 1.0078 - val_mean_squared_error: 1.0078\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01762 to 1.00781, saving model to temp/e242\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00781 to 1.00300, saving model to temp/e242\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0136 - mean_squared_error: 1.0136 - val_loss: 1.0138 - val_mean_squared_error: 1.0138\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00300\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00300\n",
      "Epoch 00005: early stopping\n",
      "temp/e243\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 169us/step - loss: 1.1078 - mean_squared_error: 1.1078 - val_loss: 1.0089 - val_mean_squared_error: 1.0089\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00892, saving model to temp/e243\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0183 - mean_squared_error: 1.0183 - val_loss: 1.0122 - val_mean_squared_error: 1.0122\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.00892\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.0116 - mean_squared_error: 1.0116 - val_loss: 1.0105 - val_mean_squared_error: 1.0105\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00892\n",
      "Epoch 00003: early stopping\n",
      "temp/e244\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 166us/step - loss: 1.1125 - mean_squared_error: 1.1125 - val_loss: 1.0130 - val_mean_squared_error: 1.0130\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01298, saving model to temp/e244\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0193 - mean_squared_error: 1.0193 - val_loss: 1.0113 - val_mean_squared_error: 1.0113\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01298 to 1.01132, saving model to temp/e244\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0205 - mean_squared_error: 1.0205 - val_loss: 1.0050 - val_mean_squared_error: 1.0050\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01132 to 1.00495, saving model to temp/e244\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00495\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0134 - mean_squared_error: 1.0134 - val_loss: 1.0114 - val_mean_squared_error: 1.0114\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00495\n",
      "Epoch 00005: early stopping\n",
      "temp/e245\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1041 - mean_squared_error: 1.1041 - val_loss: 1.0266 - val_mean_squared_error: 1.0266\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02659, saving model to temp/e245\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0180 - mean_squared_error: 1.0180 - val_loss: 1.0094 - val_mean_squared_error: 1.0094\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02659 to 1.00937, saving model to temp/e245\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 1.0124 - val_mean_squared_error: 1.0124\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00937\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0071 - val_mean_squared_error: 1.0071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss improved from 1.00937 to 1.00713, saving model to temp/e245\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0055 - mean_squared_error: 1.0055 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00713 to 1.00536, saving model to temp/e245\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0084 - val_mean_squared_error: 1.0084\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00536\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 1.0076 - mean_squared_error: 1.0076 - val_loss: 0.9997 - val_mean_squared_error: 0.9997\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00536 to 0.99967, saving model to temp/e245\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.0075 - mean_squared_error: 1.0075 - val_loss: 1.0060 - val_mean_squared_error: 1.0060\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99967\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.0045 - mean_squared_error: 1.0045 - val_loss: 1.0000 - val_mean_squared_error: 1.0000\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99967\n",
      "Epoch 00009: early stopping\n",
      "temp/e246\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.0992 - mean_squared_error: 1.0992 - val_loss: 1.0205 - val_mean_squared_error: 1.0205\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02052, saving model to temp/e246\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0210 - mean_squared_error: 1.0210 - val_loss: 1.0074 - val_mean_squared_error: 1.0074\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02052 to 1.00743, saving model to temp/e246\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.0144 - mean_squared_error: 1.0144 - val_loss: 1.0160 - val_mean_squared_error: 1.0160\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00743\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 1.0039 - val_mean_squared_error: 1.0039\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00743 to 1.00390, saving model to temp/e246\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0112 - val_mean_squared_error: 1.0112\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00390\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0073 - mean_squared_error: 1.0073 - val_loss: 1.0146 - val_mean_squared_error: 1.0146\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00390\n",
      "Epoch 00006: early stopping\n",
      "temp/e247\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.1036 - mean_squared_error: 1.1036 - val_loss: 1.0163 - val_mean_squared_error: 1.0163\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01627, saving model to temp/e247\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0190 - mean_squared_error: 1.0190 - val_loss: 1.0142 - val_mean_squared_error: 1.0142\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01627 to 1.01420, saving model to temp/e247\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0204 - mean_squared_error: 1.0204 - val_loss: 1.0188 - val_mean_squared_error: 1.0188\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01420\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01420 to 1.00530, saving model to temp/e247\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00530 to 1.00489, saving model to temp/e247\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 0.9989 - val_mean_squared_error: 0.9989\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00489 to 0.99893, saving model to temp/e247\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 0.9977 - val_mean_squared_error: 0.9977\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.99893 to 0.99775, saving model to temp/e247\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0064 - mean_squared_error: 1.0064 - val_loss: 0.9992 - val_mean_squared_error: 0.9992\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99775\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0060 - mean_squared_error: 1.0060 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99775\n",
      "Epoch 00009: early stopping\n",
      "temp/e248\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.1126 - mean_squared_error: 1.1126 - val_loss: 1.0125 - val_mean_squared_error: 1.0125\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01250, saving model to temp/e248\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0191 - mean_squared_error: 1.0191 - val_loss: 1.0157 - val_mean_squared_error: 1.0157\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01250\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0153 - mean_squared_error: 1.0153 - val_loss: 1.0097 - val_mean_squared_error: 1.0097\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01250 to 1.00970, saving model to temp/e248\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0107 - mean_squared_error: 1.0107 - val_loss: 1.0078 - val_mean_squared_error: 1.0078\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00970 to 1.00780, saving model to temp/e248\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0105 - mean_squared_error: 1.0105 - val_loss: 1.0097 - val_mean_squared_error: 1.0097\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00780\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0093 - mean_squared_error: 1.0093 - val_loss: 1.0043 - val_mean_squared_error: 1.0043\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00780 to 1.00427, saving model to temp/e248\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0028 - mean_squared_error: 1.0028 - val_loss: 1.0001 - val_mean_squared_error: 1.0001\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00427 to 1.00011, saving model to temp/e248\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0057 - mean_squared_error: 1.0057 - val_loss: 1.0117 - val_mean_squared_error: 1.0117\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00011\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0054 - mean_squared_error: 1.0054 - val_loss: 1.0117 - val_mean_squared_error: 1.0117\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00011\n",
      "Epoch 00009: early stopping\n",
      "temp/e249\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 165us/step - loss: 1.1005 - mean_squared_error: 1.1005 - val_loss: 1.0368 - val_mean_squared_error: 1.0368\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03683, saving model to temp/e249\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0178 - mean_squared_error: 1.0178 - val_loss: 1.0183 - val_mean_squared_error: 1.0183\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03683 to 1.01830, saving model to temp/e249\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0176 - mean_squared_error: 1.0176 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01830 to 1.00932, saving model to temp/e249\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0163 - mean_squared_error: 1.0163 - val_loss: 1.0081 - val_mean_squared_error: 1.0081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss improved from 1.00932 to 1.00815, saving model to temp/e249\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0056 - val_mean_squared_error: 1.0056\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00815 to 1.00564, saving model to temp/e249\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0124 - val_mean_squared_error: 1.0124\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00564\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0046 - mean_squared_error: 1.0046 - val_loss: 1.0007 - val_mean_squared_error: 1.0007\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00564 to 1.00070, saving model to temp/e249\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0073 - mean_squared_error: 1.0073 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00070\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0073 - val_mean_squared_error: 1.0073\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00070\n",
      "Epoch 00009: early stopping\n",
      "temp/e250\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.0879 - mean_squared_error: 1.0879 - val_loss: 1.0299 - val_mean_squared_error: 1.0299\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02990, saving model to temp/e250\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0243 - mean_squared_error: 1.0243 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02990 to 1.00797, saving model to temp/e250\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0131 - mean_squared_error: 1.0131 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00797 to 1.00613, saving model to temp/e250\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0031 - val_mean_squared_error: 1.0031\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00613 to 1.00308, saving model to temp/e250\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0131 - mean_squared_error: 1.0131 - val_loss: 1.0032 - val_mean_squared_error: 1.0032\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00308\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0109 - val_mean_squared_error: 1.0109\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00308\n",
      "Epoch 00006: early stopping\n",
      "temp/e251\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1044 - mean_squared_error: 1.1044 - val_loss: 1.0155 - val_mean_squared_error: 1.0155\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01554, saving model to temp/e251\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0206 - mean_squared_error: 1.0206 - val_loss: 1.0079 - val_mean_squared_error: 1.0079\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01554 to 1.00792, saving model to temp/e251\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0171 - mean_squared_error: 1.0171 - val_loss: 1.0085 - val_mean_squared_error: 1.0085\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00792\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0157 - mean_squared_error: 1.0157 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00792 to 1.00527, saving model to temp/e251\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0131 - mean_squared_error: 1.0131 - val_loss: 1.0125 - val_mean_squared_error: 1.0125\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00527\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 1.0043 - val_mean_squared_error: 1.0043\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00527 to 1.00431, saving model to temp/e251\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.0050 - mean_squared_error: 1.0050 - val_loss: 1.0014 - val_mean_squared_error: 1.0014\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00431 to 1.00143, saving model to temp/e251\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0106 - mean_squared_error: 1.0106 - val_loss: 0.9973 - val_mean_squared_error: 0.9973\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00143 to 0.99733, saving model to temp/e251\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0038 - mean_squared_error: 1.0038 - val_loss: 1.0167 - val_mean_squared_error: 1.0167\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99733\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0066 - mean_squared_error: 1.0066 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99733\n",
      "Epoch 00010: early stopping\n",
      "temp/e252\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 166us/step - loss: 1.0983 - mean_squared_error: 1.0983 - val_loss: 1.0243 - val_mean_squared_error: 1.0243\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02431, saving model to temp/e252\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0183 - mean_squared_error: 1.0183 - val_loss: 1.0109 - val_mean_squared_error: 1.0109\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02431 to 1.01086, saving model to temp/e252\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0146 - mean_squared_error: 1.0146 - val_loss: 1.0067 - val_mean_squared_error: 1.0067\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01086 to 1.00674, saving model to temp/e252\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 1.0106 - val_mean_squared_error: 1.0106\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00674\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0071 - mean_squared_error: 1.0071 - val_loss: 1.0156 - val_mean_squared_error: 1.0156\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00674\n",
      "Epoch 00005: early stopping\n",
      "temp/e253\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.0937 - mean_squared_error: 1.0937 - val_loss: 1.0369 - val_mean_squared_error: 1.0369\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03686, saving model to temp/e253\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 1.0102 - val_mean_squared_error: 1.0102\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03686 to 1.01024, saving model to temp/e253\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0169 - mean_squared_error: 1.0169 - val_loss: 1.0081 - val_mean_squared_error: 1.0081\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01024 to 1.00809, saving model to temp/e253\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0107 - mean_squared_error: 1.0107 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00809 to 1.00298, saving model to temp/e253\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0125 - mean_squared_error: 1.0125 - val_loss: 1.0045 - val_mean_squared_error: 1.0045\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00298\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 0.9997 - val_mean_squared_error: 0.9997\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00298 to 0.99972, saving model to temp/e253\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0058 - mean_squared_error: 1.0058 - val_loss: 1.0010 - val_mean_squared_error: 1.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99972\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0065 - mean_squared_error: 1.0065 - val_loss: 1.0050 - val_mean_squared_error: 1.0050\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99972\n",
      "Epoch 00008: early stopping\n",
      "temp/e254\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1021 - mean_squared_error: 1.1021 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00882, saving model to temp/e254\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0205 - mean_squared_error: 1.0205 - val_loss: 1.0066 - val_mean_squared_error: 1.0066\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.00882 to 1.00660, saving model to temp/e254\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0156 - mean_squared_error: 1.0156 - val_loss: 1.0133 - val_mean_squared_error: 1.0133\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00660\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00660 to 1.00235, saving model to temp/e254\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 1.0220 - val_mean_squared_error: 1.0220\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00235\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0041 - val_mean_squared_error: 1.0041\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00235\n",
      "Epoch 00006: early stopping\n",
      "temp/e255\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.1061 - mean_squared_error: 1.1061 - val_loss: 1.0380 - val_mean_squared_error: 1.0380\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03798, saving model to temp/e255\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0222 - mean_squared_error: 1.0222 - val_loss: 1.0129 - val_mean_squared_error: 1.0129\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03798 to 1.01294, saving model to temp/e255\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0183 - mean_squared_error: 1.0183 - val_loss: 1.0122 - val_mean_squared_error: 1.0122\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01294 to 1.01223, saving model to temp/e255\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0131 - mean_squared_error: 1.0131 - val_loss: 1.0046 - val_mean_squared_error: 1.0046\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01223 to 1.00463, saving model to temp/e255\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0111 - mean_squared_error: 1.0111 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00463\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0011 - val_mean_squared_error: 1.0011\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00463 to 1.00111, saving model to temp/e255\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0056 - mean_squared_error: 1.0056 - val_loss: 1.0100 - val_mean_squared_error: 1.0100\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00111\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0062 - mean_squared_error: 1.0062 - val_loss: 1.0105 - val_mean_squared_error: 1.0105\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00111\n",
      "Epoch 00008: early stopping\n",
      "temp/e256\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.1008 - mean_squared_error: 1.1008 - val_loss: 1.0166 - val_mean_squared_error: 1.0166\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01658, saving model to temp/e256\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0199 - mean_squared_error: 1.0199 - val_loss: 1.0105 - val_mean_squared_error: 1.0105\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01658 to 1.01052, saving model to temp/e256\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0160 - mean_squared_error: 1.0160 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01052 to 1.00229, saving model to temp/e256\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0012 - val_mean_squared_error: 1.0012\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00229 to 1.00117, saving model to temp/e256\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0119 - mean_squared_error: 1.0119 - val_loss: 1.0019 - val_mean_squared_error: 1.0019\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00117\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0093 - mean_squared_error: 1.0093 - val_loss: 1.0012 - val_mean_squared_error: 1.0012\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00117\n",
      "Epoch 00006: early stopping\n",
      "temp/e257\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.1003 - mean_squared_error: 1.1003 - val_loss: 1.0506 - val_mean_squared_error: 1.0506\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.05064, saving model to temp/e257\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0215 - mean_squared_error: 1.0215 - val_loss: 1.0135 - val_mean_squared_error: 1.0135\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.05064 to 1.01351, saving model to temp/e257\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0155 - mean_squared_error: 1.0155 - val_loss: 1.0206 - val_mean_squared_error: 1.0206\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01351\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0155 - mean_squared_error: 1.0155 - val_loss: 1.0184 - val_mean_squared_error: 1.0184\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.01351\n",
      "Epoch 00004: early stopping\n",
      "temp/e258\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 169us/step - loss: 1.1092 - mean_squared_error: 1.1092 - val_loss: 1.0109 - val_mean_squared_error: 1.0109\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01091, saving model to temp/e258\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0198 - mean_squared_error: 1.0198 - val_loss: 1.0258 - val_mean_squared_error: 1.0258\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01091\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0186 - mean_squared_error: 1.0186 - val_loss: 1.0151 - val_mean_squared_error: 1.0151\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01091\n",
      "Epoch 00003: early stopping\n",
      "temp/e259\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.0969 - mean_squared_error: 1.0969 - val_loss: 1.0180 - val_mean_squared_error: 1.0180\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01801, saving model to temp/e259\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0219 - mean_squared_error: 1.0219 - val_loss: 1.0081 - val_mean_squared_error: 1.0081\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01801 to 1.00808, saving model to temp/e259\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0150 - mean_squared_error: 1.0150 - val_loss: 1.0110 - val_mean_squared_error: 1.0110\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00808\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0149 - mean_squared_error: 1.0149 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00808 to 1.00328, saving model to temp/e259\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0068 - mean_squared_error: 1.0068 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00328 to 1.00230, saving model to temp/e259\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0109 - mean_squared_error: 1.0109 - val_loss: 1.0100 - val_mean_squared_error: 1.0100\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00230\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0015 - val_mean_squared_error: 1.0015\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00230 to 1.00148, saving model to temp/e259\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0076 - mean_squared_error: 1.0076 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00148\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0052 - mean_squared_error: 1.0052 - val_loss: 1.0017 - val_mean_squared_error: 1.0017\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00148\n",
      "Epoch 00009: early stopping\n",
      "temp/e260\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1034 - mean_squared_error: 1.1034 - val_loss: 1.0537 - val_mean_squared_error: 1.0537\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.05372, saving model to temp/e260\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0199 - mean_squared_error: 1.0199 - val_loss: 1.0127 - val_mean_squared_error: 1.0127\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.05372 to 1.01266, saving model to temp/e260\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0163 - mean_squared_error: 1.0163 - val_loss: 1.0084 - val_mean_squared_error: 1.0084\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01266 to 1.00836, saving model to temp/e260\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0148 - mean_squared_error: 1.0148 - val_loss: 1.0209 - val_mean_squared_error: 1.0209\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00836\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00836 to 1.00615, saving model to temp/e260\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00615 to 1.00585, saving model to temp/e260\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0076 - mean_squared_error: 1.0076 - val_loss: 1.0045 - val_mean_squared_error: 1.0045\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00585 to 1.00446, saving model to temp/e260\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0037 - mean_squared_error: 1.0037 - val_loss: 1.0002 - val_mean_squared_error: 1.0002\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00446 to 1.00023, saving model to temp/e260\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0053 - mean_squared_error: 1.0053 - val_loss: 1.0041 - val_mean_squared_error: 1.0041\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00023\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0072 - mean_squared_error: 1.0072 - val_loss: 1.0012 - val_mean_squared_error: 1.0012\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.00023\n",
      "Epoch 00010: early stopping\n",
      "temp/e261\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.0980 - mean_squared_error: 1.0980 - val_loss: 1.0197 - val_mean_squared_error: 1.0197\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01971, saving model to temp/e261\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0205 - mean_squared_error: 1.0205 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01971 to 1.00798, saving model to temp/e261\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0158 - mean_squared_error: 1.0158 - val_loss: 1.0168 - val_mean_squared_error: 1.0168\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00798\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0073 - val_mean_squared_error: 1.0073\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00798 to 1.00728, saving model to temp/e261\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0046 - val_mean_squared_error: 1.0046\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00728 to 1.00457, saving model to temp/e261\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0112 - mean_squared_error: 1.0112 - val_loss: 1.0019 - val_mean_squared_error: 1.0019\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00457 to 1.00194, saving model to temp/e261\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0069 - mean_squared_error: 1.0069 - val_loss: 1.0094 - val_mean_squared_error: 1.0094\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00194\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00194\n",
      "Epoch 00008: early stopping\n",
      "temp/e262\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 169us/step - loss: 1.1007 - mean_squared_error: 1.1007 - val_loss: 1.0161 - val_mean_squared_error: 1.0161\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01612, saving model to temp/e262\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0201 - mean_squared_error: 1.0201 - val_loss: 1.0096 - val_mean_squared_error: 1.0096\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01612 to 1.00962, saving model to temp/e262\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0168 - mean_squared_error: 1.0168 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00962 to 1.00752, saving model to temp/e262\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00752\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0127 - mean_squared_error: 1.0127 - val_loss: 1.0027 - val_mean_squared_error: 1.0027\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00752 to 1.00268, saving model to temp/e262\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0050 - val_mean_squared_error: 1.0050\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00268\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0086 - mean_squared_error: 1.0086 - val_loss: 1.0001 - val_mean_squared_error: 1.0001\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00268 to 1.00014, saving model to temp/e262\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0058 - mean_squared_error: 1.0058 - val_loss: 0.9985 - val_mean_squared_error: 0.9985\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00014 to 0.99845, saving model to temp/e262\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0075 - mean_squared_error: 1.0075 - val_loss: 1.0002 - val_mean_squared_error: 1.0002\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99845\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0065 - mean_squared_error: 1.0065 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99845\n",
      "Epoch 00010: early stopping\n",
      "temp/e263\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 169us/step - loss: 1.1043 - mean_squared_error: 1.1043 - val_loss: 1.0132 - val_mean_squared_error: 1.0132\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01317, saving model to temp/e263\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0208 - mean_squared_error: 1.0208 - val_loss: 1.0170 - val_mean_squared_error: 1.0170\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01317\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0056 - val_mean_squared_error: 1.0056\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01317 to 1.00561, saving model to temp/e263\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0083 - val_mean_squared_error: 1.0083\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00561\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0233 - val_mean_squared_error: 1.0233\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00561\n",
      "Epoch 00005: early stopping\n",
      "temp/e264\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.1011 - mean_squared_error: 1.1011 - val_loss: 1.0240 - val_mean_squared_error: 1.0240\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02403, saving model to temp/e264\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0186 - mean_squared_error: 1.0186 - val_loss: 1.0083 - val_mean_squared_error: 1.0083\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02403 to 1.00826, saving model to temp/e264\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0168 - mean_squared_error: 1.0168 - val_loss: 1.0082 - val_mean_squared_error: 1.0082\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00826 to 1.00819, saving model to temp/e264\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 1.0143 - val_mean_squared_error: 1.0143\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00819\n",
      "Epoch 00004: early stopping\n",
      "temp/e265\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.0950 - mean_squared_error: 1.0950 - val_loss: 1.0296 - val_mean_squared_error: 1.0296\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02957, saving model to temp/e265\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0205 - mean_squared_error: 1.0205 - val_loss: 1.0116 - val_mean_squared_error: 1.0116\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02957 to 1.01160, saving model to temp/e265\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0212 - mean_squared_error: 1.0212 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01160 to 1.00617, saving model to temp/e265\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0110 - val_mean_squared_error: 1.0110\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00617\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00617 to 1.00258, saving model to temp/e265\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0106 - mean_squared_error: 1.0106 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00258\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0069 - mean_squared_error: 1.0069 - val_loss: 1.0099 - val_mean_squared_error: 1.0099\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00258\n",
      "Epoch 00007: early stopping\n",
      "temp/e266\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.1068 - mean_squared_error: 1.1068 - val_loss: 1.0294 - val_mean_squared_error: 1.0294\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02940, saving model to temp/e266\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0229 - mean_squared_error: 1.0229 - val_loss: 1.0141 - val_mean_squared_error: 1.0141\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02940 to 1.01412, saving model to temp/e266\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0162 - mean_squared_error: 1.0162 - val_loss: 1.0070 - val_mean_squared_error: 1.0070\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01412 to 1.00696, saving model to temp/e266\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00696\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 1.0122 - val_mean_squared_error: 1.0122\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00696\n",
      "Epoch 00005: early stopping\n",
      "temp/e267\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.0999 - mean_squared_error: 1.0999 - val_loss: 1.0118 - val_mean_squared_error: 1.0118\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01177, saving model to temp/e267\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0224 - mean_squared_error: 1.0224 - val_loss: 1.0095 - val_mean_squared_error: 1.0095\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01177 to 1.00950, saving model to temp/e267\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0146 - mean_squared_error: 1.0146 - val_loss: 1.0092 - val_mean_squared_error: 1.0092\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00950 to 1.00916, saving model to temp/e267\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0116 - mean_squared_error: 1.0116 - val_loss: 1.0067 - val_mean_squared_error: 1.0067\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00916 to 1.00666, saving model to temp/e267\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0042 - val_mean_squared_error: 1.0042\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00666 to 1.00418, saving model to temp/e267\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0105 - mean_squared_error: 1.0105 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00418 to 1.00379, saving model to temp/e267\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0086 - mean_squared_error: 1.0086 - val_loss: 0.9997 - val_mean_squared_error: 0.9997\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00379 to 0.99966, saving model to temp/e267\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0081 - mean_squared_error: 1.0081 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99966\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0049 - mean_squared_error: 1.0049 - val_loss: 1.0028 - val_mean_squared_error: 1.0028\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99966\n",
      "Epoch 00009: early stopping\n",
      "temp/e268\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1038 - mean_squared_error: 1.1038 - val_loss: 1.0308 - val_mean_squared_error: 1.0308\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03084, saving model to temp/e268\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0210 - mean_squared_error: 1.0210 - val_loss: 1.0107 - val_mean_squared_error: 1.0107\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03084 to 1.01069, saving model to temp/e268\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01069 to 1.00797, saving model to temp/e268\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0149 - mean_squared_error: 1.0149 - val_loss: 1.0141 - val_mean_squared_error: 1.0141\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00797\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0131 - mean_squared_error: 1.0131 - val_loss: 1.0071 - val_mean_squared_error: 1.0071\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00797 to 1.00709, saving model to temp/e268\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0098 - mean_squared_error: 1.0098 - val_loss: 1.0124 - val_mean_squared_error: 1.0124\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00709\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 1.0011 - val_mean_squared_error: 1.0011\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00709 to 1.00109, saving model to temp/e268\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0068 - mean_squared_error: 1.0068 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00109\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0045 - mean_squared_error: 1.0045 - val_loss: 1.0032 - val_mean_squared_error: 1.0032\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00109\n",
      "Epoch 00009: early stopping\n",
      "temp/e269\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 170us/step - loss: 1.1084 - mean_squared_error: 1.1084 - val_loss: 1.0177 - val_mean_squared_error: 1.0177\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01767, saving model to temp/e269\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0178 - mean_squared_error: 1.0178 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01767 to 1.00801, saving model to temp/e269\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0246 - val_mean_squared_error: 1.0246\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00801\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0141 - mean_squared_error: 1.0141 - val_loss: 1.0133 - val_mean_squared_error: 1.0133\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00801\n",
      "Epoch 00004: early stopping\n",
      "temp/e270\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.0967 - mean_squared_error: 1.0967 - val_loss: 1.0200 - val_mean_squared_error: 1.0200\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01999, saving model to temp/e270\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0173 - mean_squared_error: 1.0173 - val_loss: 1.0083 - val_mean_squared_error: 1.0083\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01999 to 1.00830, saving model to temp/e270\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0131 - val_mean_squared_error: 1.0131\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00830\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0141 - mean_squared_error: 1.0141 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00830 to 1.00797, saving model to temp/e270\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0032 - val_mean_squared_error: 1.0032\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00797 to 1.00324, saving model to temp/e270\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0022 - val_mean_squared_error: 1.0022\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00324 to 1.00223, saving model to temp/e270\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 0.9972 - val_mean_squared_error: 0.9972\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00223 to 0.99716, saving model to temp/e270\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 1.0001 - val_mean_squared_error: 1.0001\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99716\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0081 - mean_squared_error: 1.0081 - val_loss: 1.0014 - val_mean_squared_error: 1.0014\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99716\n",
      "Epoch 00009: early stopping\n",
      "temp/e271\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.0997 - mean_squared_error: 1.0997 - val_loss: 1.0441 - val_mean_squared_error: 1.0441\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.04405, saving model to temp/e271\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0224 - mean_squared_error: 1.0224 - val_loss: 1.0109 - val_mean_squared_error: 1.0109\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.04405 to 1.01094, saving model to temp/e271\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0145 - mean_squared_error: 1.0145 - val_loss: 1.0162 - val_mean_squared_error: 1.0162\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01094\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0070 - mean_squared_error: 1.0070 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01094 to 1.00525, saving model to temp/e271\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0200 - val_mean_squared_error: 1.0200\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00525\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 0.9974 - val_mean_squared_error: 0.9974\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00525 to 0.99745, saving model to temp/e271\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 1.0017 - val_mean_squared_error: 1.0017\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99745\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 1.0215 - val_mean_squared_error: 1.0215\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99745\n",
      "Epoch 00008: early stopping\n",
      "temp/e272\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.0989 - mean_squared_error: 1.0989 - val_loss: 1.0201 - val_mean_squared_error: 1.0201\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02009, saving model to temp/e272\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0176 - mean_squared_error: 1.0176 - val_loss: 1.0082 - val_mean_squared_error: 1.0082\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02009 to 1.00817, saving model to temp/e272\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0176 - mean_squared_error: 1.0176 - val_loss: 1.0151 - val_mean_squared_error: 1.0151\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00817\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0123 - mean_squared_error: 1.0123 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00817 to 1.00689, saving model to temp/e272\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0146 - mean_squared_error: 1.0146 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00689 to 1.00265, saving model to temp/e272\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0073 - mean_squared_error: 1.0073 - val_loss: 1.0070 - val_mean_squared_error: 1.0070\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00265\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 0.9998 - val_mean_squared_error: 0.9998\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00265 to 0.99983, saving model to temp/e272\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0078 - mean_squared_error: 1.0078 - val_loss: 1.0138 - val_mean_squared_error: 1.0138\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99983\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0053 - mean_squared_error: 1.0053 - val_loss: 1.0037 - val_mean_squared_error: 1.0037\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99983\n",
      "Epoch 00009: early stopping\n",
      "temp/e273\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.0935 - mean_squared_error: 1.0935 - val_loss: 1.0222 - val_mean_squared_error: 1.0222\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02223, saving model to temp/e273\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0190 - mean_squared_error: 1.0190 - val_loss: 1.0103 - val_mean_squared_error: 1.0103\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02223 to 1.01027, saving model to temp/e273\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01027 to 1.00488, saving model to temp/e273\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0109 - mean_squared_error: 1.0109 - val_loss: 1.0045 - val_mean_squared_error: 1.0045\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00488 to 1.00454, saving model to temp/e273\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.0114 - mean_squared_error: 1.0114 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00454\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0070 - mean_squared_error: 1.0070 - val_loss: 1.0281 - val_mean_squared_error: 1.0281\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00454\n",
      "Epoch 00006: early stopping\n",
      "temp/e274\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1028 - mean_squared_error: 1.1028 - val_loss: 1.0235 - val_mean_squared_error: 1.0235\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02345, saving model to temp/e274\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0208 - mean_squared_error: 1.0208 - val_loss: 1.0112 - val_mean_squared_error: 1.0112\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02345 to 1.01123, saving model to temp/e274\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0144 - mean_squared_error: 1.0144 - val_loss: 1.0081 - val_mean_squared_error: 1.0081\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01123 to 1.00813, saving model to temp/e274\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0155 - mean_squared_error: 1.0155 - val_loss: 1.0078 - val_mean_squared_error: 1.0078\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00813 to 1.00782, saving model to temp/e274\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00782 to 1.00591, saving model to temp/e274\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0081 - mean_squared_error: 1.0081 - val_loss: 1.0107 - val_mean_squared_error: 1.0107\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00591\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00591 to 1.00226, saving model to temp/e274\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0079 - mean_squared_error: 1.0079 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00226\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0045 - mean_squared_error: 1.0045 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00226\n",
      "Epoch 00009: early stopping\n",
      "temp/e275\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1019 - mean_squared_error: 1.1019 - val_loss: 1.0221 - val_mean_squared_error: 1.0221\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02211, saving model to temp/e275\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0196 - mean_squared_error: 1.0196 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02211 to 1.00802, saving model to temp/e275\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0170 - mean_squared_error: 1.0170 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00802\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0148 - val_mean_squared_error: 1.0148\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00802\n",
      "Epoch 00004: early stopping\n",
      "temp/e276\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.0956 - mean_squared_error: 1.0956 - val_loss: 1.0132 - val_mean_squared_error: 1.0132\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01322, saving model to temp/e276\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0176 - mean_squared_error: 1.0176 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01322 to 1.00624, saving model to temp/e276\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0179 - mean_squared_error: 1.0179 - val_loss: 1.0110 - val_mean_squared_error: 1.0110\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00624\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0123 - mean_squared_error: 1.0123 - val_loss: 1.0056 - val_mean_squared_error: 1.0056\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00624 to 1.00555, saving model to temp/e276\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.0106 - mean_squared_error: 1.0106 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00555 to 1.00535, saving model to temp/e276\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00535\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0073 - mean_squared_error: 1.0073 - val_loss: 1.0055 - val_mean_squared_error: 1.0055\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00535\n",
      "Epoch 00007: early stopping\n",
      "temp/e277\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.1020 - mean_squared_error: 1.1020 - val_loss: 1.0156 - val_mean_squared_error: 1.0156\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01558, saving model to temp/e277\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0160 - mean_squared_error: 1.0160 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01558 to 1.00797, saving model to temp/e277\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0169 - mean_squared_error: 1.0169 - val_loss: 1.0073 - val_mean_squared_error: 1.0073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss improved from 1.00797 to 1.00731, saving model to temp/e277\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0119 - mean_squared_error: 1.0119 - val_loss: 1.0065 - val_mean_squared_error: 1.0065\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00731 to 1.00652, saving model to temp/e277\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0138 - mean_squared_error: 1.0138 - val_loss: 1.0032 - val_mean_squared_error: 1.0032\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00652 to 1.00321, saving model to temp/e277\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0134 - mean_squared_error: 1.0134 - val_loss: 1.0082 - val_mean_squared_error: 1.0082\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00321\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0081 - mean_squared_error: 1.0081 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00321\n",
      "Epoch 00007: early stopping\n",
      "temp/e278\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1006 - mean_squared_error: 1.1006 - val_loss: 1.0145 - val_mean_squared_error: 1.0145\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01454, saving model to temp/e278\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0242 - mean_squared_error: 1.0242 - val_loss: 1.0129 - val_mean_squared_error: 1.0129\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01454 to 1.01293, saving model to temp/e278\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0145 - mean_squared_error: 1.0145 - val_loss: 1.0108 - val_mean_squared_error: 1.0108\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01293 to 1.01077, saving model to temp/e278\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0111 - mean_squared_error: 1.0111 - val_loss: 1.0071 - val_mean_squared_error: 1.0071\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01077 to 1.00706, saving model to temp/e278\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0107 - val_mean_squared_error: 1.0107\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00706\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00706 to 1.00344, saving model to temp/e278\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.0032 - mean_squared_error: 1.0032 - val_loss: 1.0041 - val_mean_squared_error: 1.0041\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00344\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0076 - mean_squared_error: 1.0076 - val_loss: 1.0068 - val_mean_squared_error: 1.0068\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00344\n",
      "Epoch 00008: early stopping\n",
      "temp/e279\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 169us/step - loss: 1.0995 - mean_squared_error: 1.0995 - val_loss: 1.0174 - val_mean_squared_error: 1.0174\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01745, saving model to temp/e279\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0209 - mean_squared_error: 1.0209 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01745 to 1.00855, saving model to temp/e279\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0165 - mean_squared_error: 1.0165 - val_loss: 1.0103 - val_mean_squared_error: 1.0103\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00855\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0242 - val_mean_squared_error: 1.0242\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00855\n",
      "Epoch 00004: early stopping\n",
      "temp/e280\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 165us/step - loss: 1.0971 - mean_squared_error: 1.0971 - val_loss: 1.0257 - val_mean_squared_error: 1.0257\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02571, saving model to temp/e280\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0224 - mean_squared_error: 1.0224 - val_loss: 1.0067 - val_mean_squared_error: 1.0067\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02571 to 1.00667, saving model to temp/e280\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0146 - mean_squared_error: 1.0146 - val_loss: 1.0380 - val_mean_squared_error: 1.0380\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00667\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0129 - val_mean_squared_error: 1.0129\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00667\n",
      "Epoch 00004: early stopping\n",
      "temp/e281\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1019 - mean_squared_error: 1.1019 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00907, saving model to temp/e281\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0212 - mean_squared_error: 1.0212 - val_loss: 1.0149 - val_mean_squared_error: 1.0149\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.00907\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0096 - val_mean_squared_error: 1.0096\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00907\n",
      "Epoch 00003: early stopping\n",
      "temp/e282\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1032 - mean_squared_error: 1.1032 - val_loss: 1.0174 - val_mean_squared_error: 1.0174\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01737, saving model to temp/e282\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0220 - mean_squared_error: 1.0220 - val_loss: 1.0153 - val_mean_squared_error: 1.0153\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01737 to 1.01535, saving model to temp/e282\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0159 - mean_squared_error: 1.0159 - val_loss: 1.0103 - val_mean_squared_error: 1.0103\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01535 to 1.01030, saving model to temp/e282\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0127 - val_mean_squared_error: 1.0127\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.01030\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0089 - val_mean_squared_error: 1.0089\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01030 to 1.00888, saving model to temp/e282\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0036 - val_mean_squared_error: 1.0036\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00888 to 1.00364, saving model to temp/e282\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0105 - mean_squared_error: 1.0105 - val_loss: 1.0082 - val_mean_squared_error: 1.0082\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00364\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0015 - val_mean_squared_error: 1.0015\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00364 to 1.00149, saving model to temp/e282\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0059 - mean_squared_error: 1.0059 - val_loss: 1.0015 - val_mean_squared_error: 1.0015\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00149\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 1.0130 - val_mean_squared_error: 1.0130\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.00149\n",
      "Epoch 00010: early stopping\n",
      "temp/e283\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1055 - mean_squared_error: 1.1055 - val_loss: 1.0137 - val_mean_squared_error: 1.0137\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01367, saving model to temp/e283\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0192 - mean_squared_error: 1.0192 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01367 to 1.00911, saving model to temp/e283\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0164 - mean_squared_error: 1.0164 - val_loss: 1.0167 - val_mean_squared_error: 1.0167\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00911\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0142 - mean_squared_error: 1.0142 - val_loss: 1.0074 - val_mean_squared_error: 1.0074\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00911 to 1.00736, saving model to temp/e283\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0084 - val_mean_squared_error: 1.0084\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00736\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00736 to 1.00402, saving model to temp/e283\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00402 to 1.00301, saving model to temp/e283\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0070 - mean_squared_error: 1.0070 - val_loss: 1.0101 - val_mean_squared_error: 1.0101\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00301\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0001 - val_mean_squared_error: 1.0001\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00301 to 1.00013, saving model to temp/e283\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0044 - mean_squared_error: 1.0044 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.00013\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0051 - mean_squared_error: 1.0051 - val_loss: 1.0006 - val_mean_squared_error: 1.0006\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.00013\n",
      "Epoch 00011: early stopping\n",
      "temp/e284\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 166us/step - loss: 1.1013 - mean_squared_error: 1.1013 - val_loss: 1.0359 - val_mean_squared_error: 1.0359\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03587, saving model to temp/e284\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0180 - mean_squared_error: 1.0180 - val_loss: 1.0178 - val_mean_squared_error: 1.0178\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03587 to 1.01784, saving model to temp/e284\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0148 - mean_squared_error: 1.0148 - val_loss: 1.0092 - val_mean_squared_error: 1.0092\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01784 to 1.00924, saving model to temp/e284\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0142 - mean_squared_error: 1.0142 - val_loss: 1.0039 - val_mean_squared_error: 1.0039\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00924 to 1.00393, saving model to temp/e284\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 1.0141 - val_mean_squared_error: 1.0141\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00393\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0081 - val_mean_squared_error: 1.0081\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00393\n",
      "Epoch 00006: early stopping\n",
      "temp/e285\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 166us/step - loss: 1.1071 - mean_squared_error: 1.1071 - val_loss: 1.0216 - val_mean_squared_error: 1.0216\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02158, saving model to temp/e285\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0184 - mean_squared_error: 1.0184 - val_loss: 1.0097 - val_mean_squared_error: 1.0097\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02158 to 1.00966, saving model to temp/e285\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0146 - mean_squared_error: 1.0146 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00966 to 1.00692, saving model to temp/e285\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00692 to 1.00473, saving model to temp/e285\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0148 - mean_squared_error: 1.0148 - val_loss: 1.0063 - val_mean_squared_error: 1.0063\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00473\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00473\n",
      "Epoch 00006: early stopping\n",
      "temp/e286\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 169us/step - loss: 1.0937 - mean_squared_error: 1.0937 - val_loss: 1.0275 - val_mean_squared_error: 1.0275\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02750, saving model to temp/e286\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0220 - mean_squared_error: 1.0220 - val_loss: 1.0117 - val_mean_squared_error: 1.0117\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02750 to 1.01167, saving model to temp/e286\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0166 - mean_squared_error: 1.0166 - val_loss: 1.0099 - val_mean_squared_error: 1.0099\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01167 to 1.00990, saving model to temp/e286\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0187 - val_mean_squared_error: 1.0187\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00990\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0140 - mean_squared_error: 1.0140 - val_loss: 1.0167 - val_mean_squared_error: 1.0167\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00990\n",
      "Epoch 00005: early stopping\n",
      "temp/e287\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 169us/step - loss: 1.1047 - mean_squared_error: 1.1047 - val_loss: 1.0175 - val_mean_squared_error: 1.0175\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01750, saving model to temp/e287\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0177 - mean_squared_error: 1.0177 - val_loss: 1.0111 - val_mean_squared_error: 1.0111\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01750 to 1.01107, saving model to temp/e287\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0117 - val_mean_squared_error: 1.0117\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01107\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0119 - mean_squared_error: 1.0119 - val_loss: 1.0272 - val_mean_squared_error: 1.0272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss did not improve from 1.01107\n",
      "Epoch 00004: early stopping\n",
      "temp/e288\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.1019 - mean_squared_error: 1.1019 - val_loss: 1.0251 - val_mean_squared_error: 1.0251\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02511, saving model to temp/e288\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0183 - mean_squared_error: 1.0183 - val_loss: 1.0301 - val_mean_squared_error: 1.0301\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.02511\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02511 to 1.00753, saving model to temp/e288\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 1.0169 - val_mean_squared_error: 1.0169\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00753\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00753 to 1.00342, saving model to temp/e288\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00342\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0092 - val_mean_squared_error: 1.0092\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00342\n",
      "Epoch 00007: early stopping\n",
      "temp/e289\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 166us/step - loss: 1.0939 - mean_squared_error: 1.0939 - val_loss: 1.0153 - val_mean_squared_error: 1.0153\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01532, saving model to temp/e289\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0164 - mean_squared_error: 1.0164 - val_loss: 1.0186 - val_mean_squared_error: 1.0186\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01532\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0156 - mean_squared_error: 1.0156 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01532 to 1.00521, saving model to temp/e289\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0089 - mean_squared_error: 1.0089 - val_loss: 1.0158 - val_mean_squared_error: 1.0158\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00521\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0116 - mean_squared_error: 1.0116 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00521\n",
      "Epoch 00005: early stopping\n",
      "temp/e290\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1004 - mean_squared_error: 1.1004 - val_loss: 1.0176 - val_mean_squared_error: 1.0176\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01757, saving model to temp/e290\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0210 - mean_squared_error: 1.0210 - val_loss: 1.0081 - val_mean_squared_error: 1.0081\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01757 to 1.00815, saving model to temp/e290\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0144 - mean_squared_error: 1.0144 - val_loss: 1.0200 - val_mean_squared_error: 1.0200\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00815\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0106 - mean_squared_error: 1.0106 - val_loss: 1.0065 - val_mean_squared_error: 1.0065\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00815 to 1.00648, saving model to temp/e290\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00648 to 1.00401, saving model to temp/e290\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0076 - val_mean_squared_error: 1.0076\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00401\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00401\n",
      "Epoch 00007: early stopping\n",
      "temp/e291\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.1075 - mean_squared_error: 1.1075 - val_loss: 1.0210 - val_mean_squared_error: 1.0210\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02100, saving model to temp/e291\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0201 - mean_squared_error: 1.0201 - val_loss: 1.0160 - val_mean_squared_error: 1.0160\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02100 to 1.01598, saving model to temp/e291\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0149 - mean_squared_error: 1.0149 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01598 to 1.00905, saving model to temp/e291\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0128 - mean_squared_error: 1.0128 - val_loss: 1.0193 - val_mean_squared_error: 1.0193\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00905\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0055 - val_mean_squared_error: 1.0055\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00905 to 1.00547, saving model to temp/e291\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0069 - mean_squared_error: 1.0069 - val_loss: 0.9991 - val_mean_squared_error: 0.9991\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00547 to 0.99912, saving model to temp/e291\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0009 - val_mean_squared_error: 1.0009\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99912\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0065 - val_mean_squared_error: 1.0065\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99912\n",
      "Epoch 00008: early stopping\n",
      "temp/e292\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 166us/step - loss: 1.1007 - mean_squared_error: 1.1007 - val_loss: 1.0134 - val_mean_squared_error: 1.0134\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01339, saving model to temp/e292\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0182 - mean_squared_error: 1.0182 - val_loss: 1.0123 - val_mean_squared_error: 1.0123\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01339 to 1.01233, saving model to temp/e292\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0157 - mean_squared_error: 1.0157 - val_loss: 1.0121 - val_mean_squared_error: 1.0121\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01233 to 1.01214, saving model to temp/e292\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0079 - val_mean_squared_error: 1.0079\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01214 to 1.00788, saving model to temp/e292\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 0.9980 - val_mean_squared_error: 0.9980\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00788 to 0.99800, saving model to temp/e292\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99800\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 1.0168 - val_mean_squared_error: 1.0168\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99800\n",
      "Epoch 00007: early stopping\n",
      "temp/e293\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0975 - mean_squared_error: 1.0975 - val_loss: 1.0178 - val_mean_squared_error: 1.0178\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01775, saving model to temp/e293\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0198 - mean_squared_error: 1.0198 - val_loss: 1.0267 - val_mean_squared_error: 1.0267\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01775\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0165 - mean_squared_error: 1.0165 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01775 to 1.00512, saving model to temp/e293\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0175 - val_mean_squared_error: 1.0175\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00512\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0098 - mean_squared_error: 1.0098 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00512 to 1.00264, saving model to temp/e293\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0109 - mean_squared_error: 1.0109 - val_loss: 1.0064 - val_mean_squared_error: 1.0064\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00264\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00264 to 1.00243, saving model to temp/e293\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00243\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0038 - mean_squared_error: 1.0038 - val_loss: 1.0008 - val_mean_squared_error: 1.0008\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00243 to 1.00082, saving model to temp/e293\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0054 - mean_squared_error: 1.0054 - val_loss: 0.9979 - val_mean_squared_error: 0.9979\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.00082 to 0.99787, saving model to temp/e293\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 0.9973 - val_mean_squared_error: 0.9973\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.99787 to 0.99728, saving model to temp/e293\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0036 - mean_squared_error: 1.0036 - val_loss: 1.0009 - val_mean_squared_error: 1.0009\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.99728\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0032 - mean_squared_error: 1.0032 - val_loss: 0.9993 - val_mean_squared_error: 0.9993\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.99728\n",
      "Epoch 00013: early stopping\n",
      "temp/e294\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1019 - mean_squared_error: 1.1019 - val_loss: 1.0121 - val_mean_squared_error: 1.0121\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01211, saving model to temp/e294\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0169 - mean_squared_error: 1.0169 - val_loss: 1.0135 - val_mean_squared_error: 1.0135\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01211\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0188 - mean_squared_error: 1.0188 - val_loss: 1.0173 - val_mean_squared_error: 1.0173\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01211\n",
      "Epoch 00003: early stopping\n",
      "temp/e295\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1019 - mean_squared_error: 1.1019 - val_loss: 1.0226 - val_mean_squared_error: 1.0226\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02260, saving model to temp/e295\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0192 - mean_squared_error: 1.0192 - val_loss: 1.0187 - val_mean_squared_error: 1.0187\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02260 to 1.01875, saving model to temp/e295\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0123 - mean_squared_error: 1.0123 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01875 to 1.00686, saving model to temp/e295\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0142 - mean_squared_error: 1.0142 - val_loss: 1.0106 - val_mean_squared_error: 1.0106\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00686\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00686 to 1.00489, saving model to temp/e295\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0076 - mean_squared_error: 1.0076 - val_loss: 0.9996 - val_mean_squared_error: 0.9996\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00489 to 0.99964, saving model to temp/e295\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 1.0149 - val_mean_squared_error: 1.0149\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99964\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 0.9991 - val_mean_squared_error: 0.9991\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.99964 to 0.99913, saving model to temp/e295\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0048 - mean_squared_error: 1.0048 - val_loss: 1.0055 - val_mean_squared_error: 1.0055\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99913\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0068 - mean_squared_error: 1.0068 - val_loss: 0.9976 - val_mean_squared_error: 0.9976\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.99913 to 0.99760, saving model to temp/e295\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0051 - mean_squared_error: 1.0051 - val_loss: 1.0074 - val_mean_squared_error: 1.0074\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99760\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0078 - mean_squared_error: 1.0078 - val_loss: 1.0077 - val_mean_squared_error: 1.0077\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.99760\n",
      "Epoch 00012: early stopping\n",
      "temp/e296\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1098 - mean_squared_error: 1.1098 - val_loss: 1.0214 - val_mean_squared_error: 1.0214\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02136, saving model to temp/e296\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0196 - mean_squared_error: 1.0196 - val_loss: 1.0286 - val_mean_squared_error: 1.0286\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.02136\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0201 - mean_squared_error: 1.0201 - val_loss: 1.0138 - val_mean_squared_error: 1.0138\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02136 to 1.01379, saving model to temp/e296\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0064 - val_mean_squared_error: 1.0064\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01379 to 1.00638, saving model to temp/e296\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0127 - mean_squared_error: 1.0127 - val_loss: 1.0044 - val_mean_squared_error: 1.0044\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00638 to 1.00442, saving model to temp/e296\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00442 to 1.00228, saving model to temp/e296\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0031 - mean_squared_error: 1.0031 - val_loss: 0.9985 - val_mean_squared_error: 0.9985\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00228 to 0.99847, saving model to temp/e296\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0103 - mean_squared_error: 1.0103 - val_loss: 0.9980 - val_mean_squared_error: 0.9980\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.99847 to 0.99804, saving model to temp/e296\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0059 - mean_squared_error: 1.0059 - val_loss: 1.0016 - val_mean_squared_error: 1.0016\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99804\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0071 - mean_squared_error: 1.0071 - val_loss: 1.0192 - val_mean_squared_error: 1.0192\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99804\n",
      "Epoch 00010: early stopping\n",
      "temp/e297\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1086 - mean_squared_error: 1.1086 - val_loss: 1.0164 - val_mean_squared_error: 1.0164\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01644, saving model to temp/e297\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0203 - mean_squared_error: 1.0203 - val_loss: 1.0097 - val_mean_squared_error: 1.0097\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01644 to 1.00970, saving model to temp/e297\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0152 - mean_squared_error: 1.0152 - val_loss: 1.0164 - val_mean_squared_error: 1.0164\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00970\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0114 - mean_squared_error: 1.0114 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00970 to 1.00911, saving model to temp/e297\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 1.0076 - val_mean_squared_error: 1.0076\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00911 to 1.00764, saving model to temp/e297\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 0.9994 - val_mean_squared_error: 0.9994\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00764 to 0.99944, saving model to temp/e297\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0060 - mean_squared_error: 1.0060 - val_loss: 1.0109 - val_mean_squared_error: 1.0109\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99944\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0064 - mean_squared_error: 1.0064 - val_loss: 1.0015 - val_mean_squared_error: 1.0015\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99944\n",
      "Epoch 00008: early stopping\n",
      "temp/e298\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 170us/step - loss: 1.0976 - mean_squared_error: 1.0976 - val_loss: 1.0153 - val_mean_squared_error: 1.0153\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01529, saving model to temp/e298\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0176 - mean_squared_error: 1.0176 - val_loss: 1.0111 - val_mean_squared_error: 1.0111\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01529 to 1.01112, saving model to temp/e298\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0168 - mean_squared_error: 1.0168 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01112 to 1.00802, saving model to temp/e298\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0153 - mean_squared_error: 1.0153 - val_loss: 1.0205 - val_mean_squared_error: 1.0205\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00802\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0041 - val_mean_squared_error: 1.0041\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00802 to 1.00411, saving model to temp/e298\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 1.0037 - val_mean_squared_error: 1.0037\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00411 to 1.00374, saving model to temp/e298\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 0.9985 - val_mean_squared_error: 0.9985\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00374 to 0.99850, saving model to temp/e298\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0081 - mean_squared_error: 1.0081 - val_loss: 1.0064 - val_mean_squared_error: 1.0064\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99850\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0050 - mean_squared_error: 1.0050 - val_loss: 1.0008 - val_mean_squared_error: 1.0008\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99850\n",
      "Epoch 00009: early stopping\n",
      "temp/e299\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 166us/step - loss: 1.0952 - mean_squared_error: 1.0952 - val_loss: 1.0180 - val_mean_squared_error: 1.0180\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01800, saving model to temp/e299\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0217 - mean_squared_error: 1.0217 - val_loss: 1.0188 - val_mean_squared_error: 1.0188\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01800\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0151 - mean_squared_error: 1.0151 - val_loss: 1.0066 - val_mean_squared_error: 1.0066\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01800 to 1.00660, saving model to temp/e299\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0105 - mean_squared_error: 1.0105 - val_loss: 1.0071 - val_mean_squared_error: 1.0071\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00660\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0107 - mean_squared_error: 1.0107 - val_loss: 1.0154 - val_mean_squared_error: 1.0154\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00660\n",
      "Epoch 00005: early stopping\n",
      "temp/e300\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1047 - mean_squared_error: 1.1047 - val_loss: 1.0168 - val_mean_squared_error: 1.0168\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01677, saving model to temp/e300\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0217 - mean_squared_error: 1.0217 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01677 to 1.00803, saving model to temp/e300\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0148 - mean_squared_error: 1.0148 - val_loss: 1.0082 - val_mean_squared_error: 1.0082\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00803\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0106 - mean_squared_error: 1.0106 - val_loss: 1.0112 - val_mean_squared_error: 1.0112\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00803\n",
      "Epoch 00004: early stopping\n",
      "temp/e301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.0976 - mean_squared_error: 1.0976 - val_loss: 1.0197 - val_mean_squared_error: 1.0197\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01972, saving model to temp/e301\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0215 - mean_squared_error: 1.0215 - val_loss: 1.0113 - val_mean_squared_error: 1.0113\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01972 to 1.01130, saving model to temp/e301\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0142 - mean_squared_error: 1.0142 - val_loss: 1.0044 - val_mean_squared_error: 1.0044\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01130 to 1.00443, saving model to temp/e301\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0077 - val_mean_squared_error: 1.0077\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00443\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0105 - mean_squared_error: 1.0105 - val_loss: 1.0050 - val_mean_squared_error: 1.0050\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00443\n",
      "Epoch 00005: early stopping\n",
      "temp/e302\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.1002 - mean_squared_error: 1.1002 - val_loss: 1.0185 - val_mean_squared_error: 1.0185\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01855, saving model to temp/e302\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.0239 - mean_squared_error: 1.0239 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01855 to 1.00587, saving model to temp/e302\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0136 - mean_squared_error: 1.0136 - val_loss: 1.0191 - val_mean_squared_error: 1.0191\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00587\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00587\n",
      "Epoch 00004: early stopping\n",
      "temp/e303\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.0969 - mean_squared_error: 1.0969 - val_loss: 1.0169 - val_mean_squared_error: 1.0169\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01688, saving model to temp/e303\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0194 - mean_squared_error: 1.0194 - val_loss: 1.0115 - val_mean_squared_error: 1.0115\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01688 to 1.01147, saving model to temp/e303\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0150 - mean_squared_error: 1.0150 - val_loss: 1.0244 - val_mean_squared_error: 1.0244\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01147\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01147 to 1.00982, saving model to temp/e303\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 1.0060 - val_mean_squared_error: 1.0060\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00982 to 1.00601, saving model to temp/e303\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0078 - mean_squared_error: 1.0078 - val_loss: 1.0022 - val_mean_squared_error: 1.0022\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00601 to 1.00221, saving model to temp/e303\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0066 - mean_squared_error: 1.0066 - val_loss: 1.0031 - val_mean_squared_error: 1.0031\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00221\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0048 - mean_squared_error: 1.0048 - val_loss: 0.9999 - val_mean_squared_error: 0.9999\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00221 to 0.99987, saving model to temp/e303\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0072 - mean_squared_error: 1.0072 - val_loss: 1.0019 - val_mean_squared_error: 1.0019\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99987\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0064 - mean_squared_error: 1.0064 - val_loss: 0.9995 - val_mean_squared_error: 0.9995\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.99987 to 0.99945, saving model to temp/e303\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0070 - mean_squared_error: 1.0070 - val_loss: 0.9966 - val_mean_squared_error: 0.9966\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.99945 to 0.99661, saving model to temp/e303\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0030 - mean_squared_error: 1.0030 - val_loss: 0.9971 - val_mean_squared_error: 0.9971\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.99661\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0029 - mean_squared_error: 1.0029 - val_loss: 1.0010 - val_mean_squared_error: 1.0010\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.99661\n",
      "Epoch 00013: early stopping\n",
      "temp/e304\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1101 - mean_squared_error: 1.1101 - val_loss: 1.0178 - val_mean_squared_error: 1.0178\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01777, saving model to temp/e304\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0190 - mean_squared_error: 1.0190 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01777 to 1.00692, saving model to temp/e304\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0166 - mean_squared_error: 1.0166 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00692 to 1.00378, saving model to temp/e304\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0114 - mean_squared_error: 1.0114 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00378\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0075 - mean_squared_error: 1.0075 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00378\n",
      "Epoch 00005: early stopping\n",
      "temp/e305\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1014 - mean_squared_error: 1.1014 - val_loss: 1.0421 - val_mean_squared_error: 1.0421\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.04210, saving model to temp/e305\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0227 - mean_squared_error: 1.0227 - val_loss: 1.0074 - val_mean_squared_error: 1.0074\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.04210 to 1.00736, saving model to temp/e305\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00736\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0127 - mean_squared_error: 1.0127 - val_loss: 1.0141 - val_mean_squared_error: 1.0141\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00736\n",
      "Epoch 00004: early stopping\n",
      "temp/e306\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 166us/step - loss: 1.1105 - mean_squared_error: 1.1105 - val_loss: 1.0207 - val_mean_squared_error: 1.0207\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02071, saving model to temp/e306\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0202 - mean_squared_error: 1.0202 - val_loss: 1.0112 - val_mean_squared_error: 1.0112\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02071 to 1.01116, saving model to temp/e306\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0161 - mean_squared_error: 1.0161 - val_loss: 1.0039 - val_mean_squared_error: 1.0039\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01116 to 1.00390, saving model to temp/e306\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0128 - val_mean_squared_error: 1.0128\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00390\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00390\n",
      "Epoch 00005: early stopping\n",
      "temp/e307\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 170us/step - loss: 1.1009 - mean_squared_error: 1.1009 - val_loss: 1.0213 - val_mean_squared_error: 1.0213\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02130, saving model to temp/e307\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0201 - mean_squared_error: 1.0201 - val_loss: 1.0087 - val_mean_squared_error: 1.0087\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02130 to 1.00867, saving model to temp/e307\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0155 - mean_squared_error: 1.0155 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00867 to 1.00689, saving model to temp/e307\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0109 - mean_squared_error: 1.0109 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00689 to 1.00543, saving model to temp/e307\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00543\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0150 - val_mean_squared_error: 1.0150\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00543\n",
      "Epoch 00006: early stopping\n",
      "temp/e308\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1006 - mean_squared_error: 1.1006 - val_loss: 1.0109 - val_mean_squared_error: 1.0109\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01093, saving model to temp/e308\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0193 - mean_squared_error: 1.0193 - val_loss: 1.0156 - val_mean_squared_error: 1.0156\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01093\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0147 - mean_squared_error: 1.0147 - val_loss: 1.0170 - val_mean_squared_error: 1.0170\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01093\n",
      "Epoch 00003: early stopping\n",
      "temp/e309\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.0967 - mean_squared_error: 1.0967 - val_loss: 1.0148 - val_mean_squared_error: 1.0148\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01481, saving model to temp/e309\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0206 - mean_squared_error: 1.0206 - val_loss: 1.0210 - val_mean_squared_error: 1.0210\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01481\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0159 - mean_squared_error: 1.0159 - val_loss: 1.0132 - val_mean_squared_error: 1.0132\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01481 to 1.01319, saving model to temp/e309\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0137 - mean_squared_error: 1.0137 - val_loss: 1.0082 - val_mean_squared_error: 1.0082\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01319 to 1.00822, saving model to temp/e309\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0109 - mean_squared_error: 1.0109 - val_loss: 1.0020 - val_mean_squared_error: 1.0020\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00822 to 1.00203, saving model to temp/e309\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 1.0143 - val_mean_squared_error: 1.0143\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00203\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 1.0129 - val_mean_squared_error: 1.0129\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00203\n",
      "Epoch 00007: early stopping\n",
      "temp/e310\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 170us/step - loss: 1.0942 - mean_squared_error: 1.0942 - val_loss: 1.0140 - val_mean_squared_error: 1.0140\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01402, saving model to temp/e310\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 1.0174 - mean_squared_error: 1.0174 - val_loss: 1.0142 - val_mean_squared_error: 1.0142\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01402\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 1.0081 - val_mean_squared_error: 1.0081\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01402 to 1.00805, saving model to temp/e310\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0125 - mean_squared_error: 1.0125 - val_loss: 1.0108 - val_mean_squared_error: 1.0108\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00805\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0066 - val_mean_squared_error: 1.0066\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00805 to 1.00659, saving model to temp/e310\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00659\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0103 - mean_squared_error: 1.0103 - val_loss: 1.0007 - val_mean_squared_error: 1.0007\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00659 to 1.00069, saving model to temp/e310\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0075 - mean_squared_error: 1.0075 - val_loss: 0.9984 - val_mean_squared_error: 0.9984\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00069 to 0.99840, saving model to temp/e310\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0061 - mean_squared_error: 1.0061 - val_loss: 0.9968 - val_mean_squared_error: 0.9968\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.99840 to 0.99679, saving model to temp/e310\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0069 - mean_squared_error: 1.0069 - val_loss: 1.0005 - val_mean_squared_error: 1.0005\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99679\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0053 - mean_squared_error: 1.0053 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99679\n",
      "Epoch 00011: early stopping\n",
      "temp/e311\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.0956 - mean_squared_error: 1.0956 - val_loss: 1.0272 - val_mean_squared_error: 1.0272\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02722, saving model to temp/e311\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0167 - mean_squared_error: 1.0167 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02722 to 1.00914, saving model to temp/e311\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0150 - mean_squared_error: 1.0150 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00914 to 1.00856, saving model to temp/e311\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0144 - mean_squared_error: 1.0144 - val_loss: 1.0064 - val_mean_squared_error: 1.0064\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00856 to 1.00640, saving model to temp/e311\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 1.0089 - val_mean_squared_error: 1.0089\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00640\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0109 - mean_squared_error: 1.0109 - val_loss: 1.0100 - val_mean_squared_error: 1.0100\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00640\n",
      "Epoch 00006: early stopping\n",
      "temp/e312\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1029 - mean_squared_error: 1.1029 - val_loss: 1.0158 - val_mean_squared_error: 1.0158\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01577, saving model to temp/e312\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0193 - mean_squared_error: 1.0193 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01577 to 1.00542, saving model to temp/e312\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00542 to 1.00399, saving model to temp/e312\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00399\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0101 - val_mean_squared_error: 1.0101\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00399\n",
      "Epoch 00005: early stopping\n",
      "temp/e313\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1007 - mean_squared_error: 1.1007 - val_loss: 1.0147 - val_mean_squared_error: 1.0147\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01467, saving model to temp/e313\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0209 - mean_squared_error: 1.0209 - val_loss: 1.0213 - val_mean_squared_error: 1.0213\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01467\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0170 - mean_squared_error: 1.0170 - val_loss: 1.0083 - val_mean_squared_error: 1.0083\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01467 to 1.00834, saving model to temp/e313\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0136 - mean_squared_error: 1.0136 - val_loss: 1.0183 - val_mean_squared_error: 1.0183\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00834\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00834 to 1.00472, saving model to temp/e313\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0085 - val_mean_squared_error: 1.0085\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00472\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0064 - mean_squared_error: 1.0064 - val_loss: 1.0018 - val_mean_squared_error: 1.0018\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00472 to 1.00178, saving model to temp/e313\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.0056 - mean_squared_error: 1.0056 - val_loss: 1.0056 - val_mean_squared_error: 1.0056\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00178\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0076 - mean_squared_error: 1.0076 - val_loss: 1.0046 - val_mean_squared_error: 1.0046\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00178\n",
      "Epoch 00009: early stopping\n",
      "temp/e314\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 169us/step - loss: 1.1051 - mean_squared_error: 1.1051 - val_loss: 1.0224 - val_mean_squared_error: 1.0224\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02236, saving model to temp/e314\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0196 - mean_squared_error: 1.0196 - val_loss: 1.0077 - val_mean_squared_error: 1.0077\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02236 to 1.00771, saving model to temp/e314\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0163 - mean_squared_error: 1.0163 - val_loss: 1.0169 - val_mean_squared_error: 1.0169\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00771\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0089 - val_mean_squared_error: 1.0089\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00771\n",
      "Epoch 00004: early stopping\n",
      "temp/e315\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.1017 - mean_squared_error: 1.1017 - val_loss: 1.0182 - val_mean_squared_error: 1.0182\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01820, saving model to temp/e315\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0231 - mean_squared_error: 1.0231 - val_loss: 1.0108 - val_mean_squared_error: 1.0108\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01820 to 1.01080, saving model to temp/e315\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0141 - mean_squared_error: 1.0141 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01080 to 1.00978, saving model to temp/e315\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0139 - mean_squared_error: 1.0139 - val_loss: 1.0029 - val_mean_squared_error: 1.0029\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00978 to 1.00285, saving model to temp/e315\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0012 - val_mean_squared_error: 1.0012\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00285 to 1.00122, saving model to temp/e315\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00122\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0062 - mean_squared_error: 1.0062 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00122\n",
      "Epoch 00007: early stopping\n",
      "temp/e316\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 166us/step - loss: 1.0961 - mean_squared_error: 1.0961 - val_loss: 1.0211 - val_mean_squared_error: 1.0211\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02106, saving model to temp/e316\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0202 - mean_squared_error: 1.0202 - val_loss: 1.0083 - val_mean_squared_error: 1.0083\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02106 to 1.00826, saving model to temp/e316\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0150 - mean_squared_error: 1.0150 - val_loss: 1.0209 - val_mean_squared_error: 1.0209\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00826\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0134 - mean_squared_error: 1.0134 - val_loss: 1.0119 - val_mean_squared_error: 1.0119\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00826\n",
      "Epoch 00004: early stopping\n",
      "temp/e317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 169us/step - loss: 1.1007 - mean_squared_error: 1.1007 - val_loss: 1.0144 - val_mean_squared_error: 1.0144\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01438, saving model to temp/e317\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0207 - mean_squared_error: 1.0207 - val_loss: 1.0176 - val_mean_squared_error: 1.0176\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01438\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.0161 - mean_squared_error: 1.0161 - val_loss: 1.0031 - val_mean_squared_error: 1.0031\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01438 to 1.00311, saving model to temp/e317\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 1.0028 - val_mean_squared_error: 1.0028\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00311 to 1.00275, saving model to temp/e317\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 0.9995 - val_mean_squared_error: 0.9995\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00275 to 0.99949, saving model to temp/e317\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0075 - mean_squared_error: 1.0075 - val_loss: 1.0006 - val_mean_squared_error: 1.0006\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99949\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0057 - mean_squared_error: 1.0057 - val_loss: 1.0010 - val_mean_squared_error: 1.0010\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99949\n",
      "Epoch 00007: early stopping\n",
      "temp/e318\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.0969 - mean_squared_error: 1.0969 - val_loss: 1.0374 - val_mean_squared_error: 1.0374\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03742, saving model to temp/e318\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0224 - mean_squared_error: 1.0224 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03742 to 1.00879, saving model to temp/e318\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0137 - mean_squared_error: 1.0137 - val_loss: 1.0099 - val_mean_squared_error: 1.0099\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00879\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00879 to 1.00534, saving model to temp/e318\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0105 - mean_squared_error: 1.0105 - val_loss: 1.0013 - val_mean_squared_error: 1.0013\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00534 to 1.00129, saving model to temp/e318\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0018 - val_mean_squared_error: 1.0018\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00129\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 0.9997 - val_mean_squared_error: 0.9997\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00129 to 0.99966, saving model to temp/e318\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0093 - mean_squared_error: 1.0093 - val_loss: 1.0006 - val_mean_squared_error: 1.0006\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99966\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0058 - mean_squared_error: 1.0058 - val_loss: 1.0019 - val_mean_squared_error: 1.0019\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99966\n",
      "Epoch 00009: early stopping\n",
      "temp/e319\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.0983 - mean_squared_error: 1.0983 - val_loss: 1.0167 - val_mean_squared_error: 1.0167\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01670, saving model to temp/e319\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0196 - mean_squared_error: 1.0196 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01670 to 1.00749, saving model to temp/e319\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0157 - mean_squared_error: 1.0157 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00749 to 1.00749, saving model to temp/e319\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0146 - mean_squared_error: 1.0146 - val_loss: 1.0096 - val_mean_squared_error: 1.0096\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00749\n",
      "Epoch 00004: early stopping\n",
      "temp/e320\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1094 - mean_squared_error: 1.1094 - val_loss: 1.0139 - val_mean_squared_error: 1.0139\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01390, saving model to temp/e320\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0184 - mean_squared_error: 1.0184 - val_loss: 1.0109 - val_mean_squared_error: 1.0109\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01390 to 1.01087, saving model to temp/e320\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 1.0079 - val_mean_squared_error: 1.0079\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01087 to 1.00795, saving model to temp/e320\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0138 - mean_squared_error: 1.0138 - val_loss: 1.0126 - val_mean_squared_error: 1.0126\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00795\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0145 - val_mean_squared_error: 1.0145\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00795\n",
      "Epoch 00005: early stopping\n",
      "temp/e321\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 166us/step - loss: 1.0963 - mean_squared_error: 1.0963 - val_loss: 1.0078 - val_mean_squared_error: 1.0078\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00783, saving model to temp/e321\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0199 - mean_squared_error: 1.0199 - val_loss: 1.0167 - val_mean_squared_error: 1.0167\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.00783\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0168 - mean_squared_error: 1.0168 - val_loss: 1.0149 - val_mean_squared_error: 1.0149\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00783\n",
      "Epoch 00003: early stopping\n",
      "temp/e322\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.1097 - mean_squared_error: 1.1097 - val_loss: 1.0147 - val_mean_squared_error: 1.0147\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01469, saving model to temp/e322\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0156 - mean_squared_error: 1.0156 - val_loss: 1.0152 - val_mean_squared_error: 1.0152\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01469\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0141 - mean_squared_error: 1.0141 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01469 to 1.00543, saving model to temp/e322\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00543 to 1.00530, saving model to temp/e322\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00530\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0182 - val_mean_squared_error: 1.0182\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00530\n",
      "Epoch 00006: early stopping\n",
      "temp/e323\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.0993 - mean_squared_error: 1.0993 - val_loss: 1.0172 - val_mean_squared_error: 1.0172\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01719, saving model to temp/e323\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0200 - mean_squared_error: 1.0200 - val_loss: 1.0101 - val_mean_squared_error: 1.0101\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01719 to 1.01007, saving model to temp/e323\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0167 - mean_squared_error: 1.0167 - val_loss: 1.0128 - val_mean_squared_error: 1.0128\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01007\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.0136 - mean_squared_error: 1.0136 - val_loss: 1.0308 - val_mean_squared_error: 1.0308\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.01007\n",
      "Epoch 00004: early stopping\n",
      "temp/e324\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1087 - mean_squared_error: 1.1087 - val_loss: 1.0276 - val_mean_squared_error: 1.0276\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02762, saving model to temp/e324\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0192 - mean_squared_error: 1.0192 - val_loss: 1.0146 - val_mean_squared_error: 1.0146\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02762 to 1.01461, saving model to temp/e324\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0160 - mean_squared_error: 1.0160 - val_loss: 1.0234 - val_mean_squared_error: 1.0234\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01461\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0042 - val_mean_squared_error: 1.0042\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01461 to 1.00421, saving model to temp/e324\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 0.9997 - val_mean_squared_error: 0.9997\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00421 to 0.99970, saving model to temp/e324\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0089 - mean_squared_error: 1.0089 - val_loss: 1.0028 - val_mean_squared_error: 1.0028\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99970\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99970\n",
      "Epoch 00007: early stopping\n",
      "temp/e325\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.1006 - mean_squared_error: 1.1006 - val_loss: 1.0167 - val_mean_squared_error: 1.0167\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01674, saving model to temp/e325\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0191 - mean_squared_error: 1.0191 - val_loss: 1.0103 - val_mean_squared_error: 1.0103\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01674 to 1.01032, saving model to temp/e325\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0173 - mean_squared_error: 1.0173 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01032 to 1.00466, saving model to temp/e325\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0116 - mean_squared_error: 1.0116 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00466 to 1.00300, saving model to temp/e325\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 1.0171 - val_mean_squared_error: 1.0171\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00300\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0065 - mean_squared_error: 1.0065 - val_loss: 1.0079 - val_mean_squared_error: 1.0079\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00300\n",
      "Epoch 00006: early stopping\n",
      "temp/e326\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 169us/step - loss: 1.0997 - mean_squared_error: 1.0997 - val_loss: 1.0141 - val_mean_squared_error: 1.0141\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01412, saving model to temp/e326\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0193 - mean_squared_error: 1.0193 - val_loss: 1.0111 - val_mean_squared_error: 1.0111\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01412 to 1.01107, saving model to temp/e326\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0131 - mean_squared_error: 1.0131 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01107 to 1.00543, saving model to temp/e326\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0142 - mean_squared_error: 1.0142 - val_loss: 1.0097 - val_mean_squared_error: 1.0097\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00543\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0037 - val_mean_squared_error: 1.0037\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00543 to 1.00369, saving model to temp/e326\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 0.9998 - val_mean_squared_error: 0.9998\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00369 to 0.99984, saving model to temp/e326\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 1.0008 - val_mean_squared_error: 1.0008\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99984\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0071 - mean_squared_error: 1.0071 - val_loss: 1.0114 - val_mean_squared_error: 1.0114\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99984\n",
      "Epoch 00008: early stopping\n",
      "temp/e327\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.1007 - mean_squared_error: 1.1007 - val_loss: 1.0160 - val_mean_squared_error: 1.0160\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01601, saving model to temp/e327\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0193 - mean_squared_error: 1.0193 - val_loss: 1.0099 - val_mean_squared_error: 1.0099\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01601 to 1.00990, saving model to temp/e327\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 1.0108 - val_mean_squared_error: 1.0108\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00990\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0123 - mean_squared_error: 1.0123 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00990 to 1.00539, saving model to temp/e327\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0119 - mean_squared_error: 1.0119 - val_loss: 1.0155 - val_mean_squared_error: 1.0155\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00539\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0017 - val_mean_squared_error: 1.0017\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00539 to 1.00166, saving model to temp/e327\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0082 - mean_squared_error: 1.0082 - val_loss: 1.0050 - val_mean_squared_error: 1.0050\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00166\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 0.9987 - val_mean_squared_error: 0.9987\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00166 to 0.99867, saving model to temp/e327\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0005 - val_mean_squared_error: 1.0005\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99867\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0057 - mean_squared_error: 1.0057 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99867\n",
      "Epoch 00010: early stopping\n",
      "temp/e328\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 165us/step - loss: 1.1148 - mean_squared_error: 1.1148 - val_loss: 1.0202 - val_mean_squared_error: 1.0202\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02019, saving model to temp/e328\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.0194 - mean_squared_error: 1.0194 - val_loss: 1.0130 - val_mean_squared_error: 1.0130\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02019 to 1.01304, saving model to temp/e328\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0141 - mean_squared_error: 1.0141 - val_loss: 1.0046 - val_mean_squared_error: 1.0046\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01304 to 1.00458, saving model to temp/e328\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0104 - val_mean_squared_error: 1.0104\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00458\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0043 - val_mean_squared_error: 1.0043\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00458 to 1.00434, saving model to temp/e328\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 1.0018 - val_mean_squared_error: 1.0018\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00434 to 1.00185, saving model to temp/e328\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 0.9990 - val_mean_squared_error: 0.9990\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00185 to 0.99903, saving model to temp/e328\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0053 - mean_squared_error: 1.0053 - val_loss: 0.9985 - val_mean_squared_error: 0.9985\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.99903 to 0.99850, saving model to temp/e328\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0009 - val_mean_squared_error: 1.0009\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99850\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0069 - mean_squared_error: 1.0069 - val_loss: 0.9986 - val_mean_squared_error: 0.9986\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99850\n",
      "Epoch 00010: early stopping\n",
      "temp/e329\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 169us/step - loss: 1.1037 - mean_squared_error: 1.1037 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00803, saving model to temp/e329\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0166 - mean_squared_error: 1.0166 - val_loss: 1.0205 - val_mean_squared_error: 1.0205\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.00803\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0160 - mean_squared_error: 1.0160 - val_loss: 1.0112 - val_mean_squared_error: 1.0112\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00803\n",
      "Epoch 00003: early stopping\n",
      "temp/e330\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 169us/step - loss: 1.1008 - mean_squared_error: 1.1008 - val_loss: 1.0337 - val_mean_squared_error: 1.0337\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03374, saving model to temp/e330\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0202 - mean_squared_error: 1.0202 - val_loss: 1.0117 - val_mean_squared_error: 1.0117\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03374 to 1.01168, saving model to temp/e330\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0158 - mean_squared_error: 1.0158 - val_loss: 1.0065 - val_mean_squared_error: 1.0065\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01168 to 1.00650, saving model to temp/e330\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 1.0251 - val_mean_squared_error: 1.0251\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00650\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0025 - val_mean_squared_error: 1.0025\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00650 to 1.00253, saving model to temp/e330\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0082 - mean_squared_error: 1.0082 - val_loss: 1.0304 - val_mean_squared_error: 1.0304\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00253\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0105 - mean_squared_error: 1.0105 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00253\n",
      "Epoch 00007: early stopping\n",
      "temp/e331\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.0977 - mean_squared_error: 1.0977 - val_loss: 1.0330 - val_mean_squared_error: 1.0330\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03296, saving model to temp/e331\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.0200 - mean_squared_error: 1.0200 - val_loss: 1.0095 - val_mean_squared_error: 1.0095\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03296 to 1.00954, saving model to temp/e331\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0134 - mean_squared_error: 1.0134 - val_loss: 1.0177 - val_mean_squared_error: 1.0177\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00954\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0039 - val_mean_squared_error: 1.0039\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00954 to 1.00390, saving model to temp/e331\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0144 - mean_squared_error: 1.0144 - val_loss: 1.0070 - val_mean_squared_error: 1.0070\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00390\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00390\n",
      "Epoch 00006: early stopping\n",
      "temp/e332\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1047 - mean_squared_error: 1.1047 - val_loss: 1.0210 - val_mean_squared_error: 1.0210\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02102, saving model to temp/e332\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0187 - mean_squared_error: 1.0187 - val_loss: 1.0055 - val_mean_squared_error: 1.0055\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02102 to 1.00547, saving model to temp/e332\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0167 - mean_squared_error: 1.0167 - val_loss: 1.0090 - val_mean_squared_error: 1.0090\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00547\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0145 - mean_squared_error: 1.0145 - val_loss: 1.0166 - val_mean_squared_error: 1.0166\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00547\n",
      "Epoch 00004: early stopping\n",
      "temp/e333\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 166us/step - loss: 1.1014 - mean_squared_error: 1.1014 - val_loss: 1.0250 - val_mean_squared_error: 1.0250\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02504, saving model to temp/e333\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0198 - mean_squared_error: 1.0198 - val_loss: 1.0178 - val_mean_squared_error: 1.0178\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02504 to 1.01783, saving model to temp/e333\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0173 - mean_squared_error: 1.0173 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01783 to 1.00607, saving model to temp/e333\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0145 - mean_squared_error: 1.0145 - val_loss: 1.0240 - val_mean_squared_error: 1.0240\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00607\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0008 - val_mean_squared_error: 1.0008\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00607 to 1.00079, saving model to temp/e333\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 1.0073 - val_mean_squared_error: 1.0073\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00079\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 0.9989 - val_mean_squared_error: 0.9989\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00079 to 0.99891, saving model to temp/e333\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0075 - mean_squared_error: 1.0075 - val_loss: 1.0163 - val_mean_squared_error: 1.0163\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99891\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0062 - mean_squared_error: 1.0062 - val_loss: 1.0015 - val_mean_squared_error: 1.0015\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99891\n",
      "Epoch 00009: early stopping\n",
      "temp/e334\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1002 - mean_squared_error: 1.1002 - val_loss: 1.0123 - val_mean_squared_error: 1.0123\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01225, saving model to temp/e334\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0179 - mean_squared_error: 1.0179 - val_loss: 1.0169 - val_mean_squared_error: 1.0169\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01225\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0140 - mean_squared_error: 1.0140 - val_loss: 1.0037 - val_mean_squared_error: 1.0037\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01225 to 1.00370, saving model to temp/e334\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0156 - mean_squared_error: 1.0156 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00370\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0098 - mean_squared_error: 1.0098 - val_loss: 1.0072 - val_mean_squared_error: 1.0072\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00370\n",
      "Epoch 00005: early stopping\n",
      "temp/e335\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 165us/step - loss: 1.1066 - mean_squared_error: 1.1066 - val_loss: 1.0247 - val_mean_squared_error: 1.0247\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02468, saving model to temp/e335\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0230 - mean_squared_error: 1.0230 - val_loss: 1.0113 - val_mean_squared_error: 1.0113\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02468 to 1.01125, saving model to temp/e335\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0165 - mean_squared_error: 1.0165 - val_loss: 1.0094 - val_mean_squared_error: 1.0094\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01125 to 1.00943, saving model to temp/e335\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00943 to 1.00326, saving model to temp/e335\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00326\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0106 - mean_squared_error: 1.0106 - val_loss: 1.0107 - val_mean_squared_error: 1.0107\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00326\n",
      "Epoch 00006: early stopping\n",
      "temp/e336\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.0982 - mean_squared_error: 1.0982 - val_loss: 1.0168 - val_mean_squared_error: 1.0168\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01676, saving model to temp/e336\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0186 - mean_squared_error: 1.0186 - val_loss: 1.0103 - val_mean_squared_error: 1.0103\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01676 to 1.01032, saving model to temp/e336\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0152 - mean_squared_error: 1.0152 - val_loss: 1.0189 - val_mean_squared_error: 1.0189\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01032\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0036 - val_mean_squared_error: 1.0036\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01032 to 1.00360, saving model to temp/e336\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0008 - val_mean_squared_error: 1.0008\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00360 to 1.00085, saving model to temp/e336\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0006 - val_mean_squared_error: 1.0006\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00085 to 1.00063, saving model to temp/e336\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00063\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0053 - mean_squared_error: 1.0053 - val_loss: 1.0022 - val_mean_squared_error: 1.0022\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00063\n",
      "Epoch 00008: early stopping\n",
      "temp/e337\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.0968 - mean_squared_error: 1.0968 - val_loss: 1.0264 - val_mean_squared_error: 1.0264\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02642, saving model to temp/e337\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0195 - mean_squared_error: 1.0195 - val_loss: 1.0207 - val_mean_squared_error: 1.0207\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02642 to 1.02074, saving model to temp/e337\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0161 - mean_squared_error: 1.0161 - val_loss: 1.0109 - val_mean_squared_error: 1.0109\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02074 to 1.01090, saving model to temp/e337\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0123 - mean_squared_error: 1.0123 - val_loss: 1.0106 - val_mean_squared_error: 1.0106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss improved from 1.01090 to 1.01057, saving model to temp/e337\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 1.0071 - val_mean_squared_error: 1.0071\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01057 to 1.00714, saving model to temp/e337\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0075 - mean_squared_error: 1.0075 - val_loss: 1.0021 - val_mean_squared_error: 1.0021\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00714 to 1.00207, saving model to temp/e337\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0071 - mean_squared_error: 1.0071 - val_loss: 1.0050 - val_mean_squared_error: 1.0050\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00207\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0078 - mean_squared_error: 1.0078 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00207\n",
      "Epoch 00008: early stopping\n",
      "temp/e338\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1043 - mean_squared_error: 1.1043 - val_loss: 1.0154 - val_mean_squared_error: 1.0154\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01544, saving model to temp/e338\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0193 - mean_squared_error: 1.0193 - val_loss: 1.0169 - val_mean_squared_error: 1.0169\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01544\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0196 - mean_squared_error: 1.0196 - val_loss: 1.0104 - val_mean_squared_error: 1.0104\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01544 to 1.01036, saving model to temp/e338\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01036 to 1.00590, saving model to temp/e338\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0109 - mean_squared_error: 1.0109 - val_loss: 1.0050 - val_mean_squared_error: 1.0050\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00590 to 1.00501, saving model to temp/e338\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0137 - val_mean_squared_error: 1.0137\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00501\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0069 - mean_squared_error: 1.0069 - val_loss: 1.0063 - val_mean_squared_error: 1.0063\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00501\n",
      "Epoch 00007: early stopping\n",
      "temp/e339\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1055 - mean_squared_error: 1.1055 - val_loss: 1.0267 - val_mean_squared_error: 1.0267\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02673, saving model to temp/e339\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0137 - mean_squared_error: 1.0137 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02673 to 1.00927, saving model to temp/e339\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0139 - mean_squared_error: 1.0139 - val_loss: 1.0022 - val_mean_squared_error: 1.0022\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00927 to 1.00219, saving model to temp/e339\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0118 - mean_squared_error: 1.0118 - val_loss: 1.0195 - val_mean_squared_error: 1.0195\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00219\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 1.0105 - val_mean_squared_error: 1.0105\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00219\n",
      "Epoch 00005: early stopping\n",
      "temp/e340\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1033 - mean_squared_error: 1.1033 - val_loss: 1.0227 - val_mean_squared_error: 1.0227\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02266, saving model to temp/e340\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0208 - mean_squared_error: 1.0208 - val_loss: 1.0116 - val_mean_squared_error: 1.0116\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02266 to 1.01158, saving model to temp/e340\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0077 - val_mean_squared_error: 1.0077\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01158 to 1.00774, saving model to temp/e340\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0154 - mean_squared_error: 1.0154 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00774 to 1.00748, saving model to temp/e340\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00748 to 1.00586, saving model to temp/e340\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0068 - mean_squared_error: 1.0068 - val_loss: 1.0324 - val_mean_squared_error: 1.0324\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00586\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 1.0005 - val_mean_squared_error: 1.0005\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00586 to 1.00050, saving model to temp/e340\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00050\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0055 - mean_squared_error: 1.0055 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00050\n",
      "Epoch 00009: early stopping\n",
      "temp/e341\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1058 - mean_squared_error: 1.1058 - val_loss: 1.0187 - val_mean_squared_error: 1.0187\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01868, saving model to temp/e341\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0183 - mean_squared_error: 1.0183 - val_loss: 1.0082 - val_mean_squared_error: 1.0082\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01868 to 1.00820, saving model to temp/e341\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0155 - mean_squared_error: 1.0155 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00820 to 1.00376, saving model to temp/e341\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 1.0012 - val_mean_squared_error: 1.0012\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00376 to 1.00121, saving model to temp/e341\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.0083 - val_mean_squared_error: 1.0083\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00121\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0082 - mean_squared_error: 1.0082 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00121\n",
      "Epoch 00006: early stopping\n",
      "temp/e342\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 169us/step - loss: 1.1014 - mean_squared_error: 1.1014 - val_loss: 1.0191 - val_mean_squared_error: 1.0191\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01911, saving model to temp/e342\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 1.0220 - val_mean_squared_error: 1.0220\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01911\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0153 - mean_squared_error: 1.0153 - val_loss: 1.0104 - val_mean_squared_error: 1.0104\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01911 to 1.01044, saving model to temp/e342\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0141 - mean_squared_error: 1.0141 - val_loss: 1.0077 - val_mean_squared_error: 1.0077\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01044 to 1.00767, saving model to temp/e342\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0106 - mean_squared_error: 1.0106 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00767\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.0019 - val_mean_squared_error: 1.0019\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00767 to 1.00188, saving model to temp/e342\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0063 - mean_squared_error: 1.0063 - val_loss: 1.0012 - val_mean_squared_error: 1.0012\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00188 to 1.00120, saving model to temp/e342\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0031 - mean_squared_error: 1.0031 - val_loss: 1.0163 - val_mean_squared_error: 1.0163\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00120\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 1.0184 - val_mean_squared_error: 1.0184\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00120\n",
      "Epoch 00009: early stopping\n",
      "temp/e343\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.1044 - mean_squared_error: 1.1044 - val_loss: 1.0225 - val_mean_squared_error: 1.0225\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02250, saving model to temp/e343\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0222 - mean_squared_error: 1.0222 - val_loss: 1.0127 - val_mean_squared_error: 1.0127\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02250 to 1.01272, saving model to temp/e343\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0150 - mean_squared_error: 1.0150 - val_loss: 1.0037 - val_mean_squared_error: 1.0037\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01272 to 1.00374, saving model to temp/e343\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0067 - val_mean_squared_error: 1.0067\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00374\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 1.0009 - val_mean_squared_error: 1.0009\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00374 to 1.00092, saving model to temp/e343\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 1.0060 - val_mean_squared_error: 1.0060\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00092\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0107 - mean_squared_error: 1.0107 - val_loss: 1.0097 - val_mean_squared_error: 1.0097\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00092\n",
      "Epoch 00007: early stopping\n",
      "temp/e344\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1003 - mean_squared_error: 1.1003 - val_loss: 1.0274 - val_mean_squared_error: 1.0274\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02741, saving model to temp/e344\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0216 - mean_squared_error: 1.0216 - val_loss: 1.0198 - val_mean_squared_error: 1.0198\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02741 to 1.01980, saving model to temp/e344\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0138 - mean_squared_error: 1.0138 - val_loss: 1.0097 - val_mean_squared_error: 1.0097\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01980 to 1.00970, saving model to temp/e344\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0078 - val_mean_squared_error: 1.0078\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00970 to 1.00776, saving model to temp/e344\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0134 - mean_squared_error: 1.0134 - val_loss: 1.0084 - val_mean_squared_error: 1.0084\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00776\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0106 - mean_squared_error: 1.0106 - val_loss: 1.0101 - val_mean_squared_error: 1.0101\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00776\n",
      "Epoch 00006: early stopping\n",
      "temp/e345\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.0969 - mean_squared_error: 1.0969 - val_loss: 1.0215 - val_mean_squared_error: 1.0215\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02148, saving model to temp/e345\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0172 - mean_squared_error: 1.0172 - val_loss: 1.0142 - val_mean_squared_error: 1.0142\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02148 to 1.01418, saving model to temp/e345\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0139 - val_mean_squared_error: 1.0139\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01418 to 1.01392, saving model to temp/e345\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0156 - mean_squared_error: 1.0156 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01392 to 1.00342, saving model to temp/e345\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 0.9985 - val_mean_squared_error: 0.9985\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00342 to 0.99846, saving model to temp/e345\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99846\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 1.0017 - val_mean_squared_error: 1.0017\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99846\n",
      "Epoch 00007: early stopping\n",
      "temp/e346\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1026 - mean_squared_error: 1.1026 - val_loss: 1.0530 - val_mean_squared_error: 1.0530\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.05297, saving model to temp/e346\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0206 - mean_squared_error: 1.0206 - val_loss: 1.0064 - val_mean_squared_error: 1.0064\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.05297 to 1.00642, saving model to temp/e346\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0140 - mean_squared_error: 1.0140 - val_loss: 1.0193 - val_mean_squared_error: 1.0193\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00642\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0148 - val_mean_squared_error: 1.0148\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00642\n",
      "Epoch 00004: early stopping\n",
      "temp/e347\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1015 - mean_squared_error: 1.1015 - val_loss: 1.0132 - val_mean_squared_error: 1.0132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01319, saving model to temp/e347\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0186 - mean_squared_error: 1.0186 - val_loss: 1.0256 - val_mean_squared_error: 1.0256\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01319\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0164 - mean_squared_error: 1.0164 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01319 to 1.00537, saving model to temp/e347\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00537\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 1.0093 - mean_squared_error: 1.0093 - val_loss: 1.0109 - val_mean_squared_error: 1.0109\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00537\n",
      "Epoch 00005: early stopping\n",
      "temp/e348\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1044 - mean_squared_error: 1.1044 - val_loss: 1.0255 - val_mean_squared_error: 1.0255\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02554, saving model to temp/e348\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0197 - mean_squared_error: 1.0197 - val_loss: 1.0109 - val_mean_squared_error: 1.0109\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02554 to 1.01094, saving model to temp/e348\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0156 - mean_squared_error: 1.0156 - val_loss: 1.0081 - val_mean_squared_error: 1.0081\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01094 to 1.00812, saving model to temp/e348\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0162 - mean_squared_error: 1.0162 - val_loss: 1.0199 - val_mean_squared_error: 1.0199\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00812\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00812 to 1.00239, saving model to temp/e348\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 1.0055 - val_mean_squared_error: 1.0055\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00239\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0071 - mean_squared_error: 1.0071 - val_loss: 1.0031 - val_mean_squared_error: 1.0031\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00239\n",
      "Epoch 00007: early stopping\n",
      "temp/e349\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.0962 - mean_squared_error: 1.0962 - val_loss: 1.0190 - val_mean_squared_error: 1.0190\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01896, saving model to temp/e349\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0191 - mean_squared_error: 1.0191 - val_loss: 1.0200 - val_mean_squared_error: 1.0200\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01896\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0162 - mean_squared_error: 1.0162 - val_loss: 1.0194 - val_mean_squared_error: 1.0194\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01896\n",
      "Epoch 00003: early stopping\n",
      "temp/e350\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 166us/step - loss: 1.1014 - mean_squared_error: 1.1014 - val_loss: 1.0150 - val_mean_squared_error: 1.0150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01503, saving model to temp/e350\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0168 - mean_squared_error: 1.0168 - val_loss: 1.0134 - val_mean_squared_error: 1.0134\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01503 to 1.01337, saving model to temp/e350\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0159 - mean_squared_error: 1.0159 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01337 to 1.00619, saving model to temp/e350\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0106 - mean_squared_error: 1.0106 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00619\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0078 - mean_squared_error: 1.0078 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00619\n",
      "Epoch 00005: early stopping\n",
      "temp/e351\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.0945 - mean_squared_error: 1.0945 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00984, saving model to temp/e351\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0176 - mean_squared_error: 1.0176 - val_loss: 1.0138 - val_mean_squared_error: 1.0138\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.00984\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0207 - mean_squared_error: 1.0207 - val_loss: 1.0082 - val_mean_squared_error: 1.0082\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00984 to 1.00818, saving model to temp/e351\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0097 - val_mean_squared_error: 1.0097\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00818\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0013 - val_mean_squared_error: 1.0013\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00818 to 1.00133, saving model to temp/e351\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0078 - mean_squared_error: 1.0078 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00133\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0065 - mean_squared_error: 1.0065 - val_loss: 1.0133 - val_mean_squared_error: 1.0133\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00133\n",
      "Epoch 00007: early stopping\n",
      "temp/e352\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 166us/step - loss: 1.0952 - mean_squared_error: 1.0952 - val_loss: 1.0229 - val_mean_squared_error: 1.0229\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02289, saving model to temp/e352\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0197 - mean_squared_error: 1.0197 - val_loss: 1.0149 - val_mean_squared_error: 1.0149\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02289 to 1.01488, saving model to temp/e352\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0176 - mean_squared_error: 1.0176 - val_loss: 1.0142 - val_mean_squared_error: 1.0142\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01488 to 1.01424, saving model to temp/e352\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0123 - val_mean_squared_error: 1.0123\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01424 to 1.01226, saving model to temp/e352\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 1.0133 - val_mean_squared_error: 1.0133\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.01226\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0093 - mean_squared_error: 1.0093 - val_loss: 1.0008 - val_mean_squared_error: 1.0008\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.01226 to 1.00079, saving model to temp/e352\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.0068 - mean_squared_error: 1.0068 - val_loss: 1.0044 - val_mean_squared_error: 1.0044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00079\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 0.9992 - val_mean_squared_error: 0.9992\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00079 to 0.99917, saving model to temp/e352\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0061 - mean_squared_error: 1.0061 - val_loss: 1.0003 - val_mean_squared_error: 1.0003\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99917\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0049 - mean_squared_error: 1.0049 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99917\n",
      "Epoch 00010: early stopping\n",
      "temp/e353\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 165us/step - loss: 1.1028 - mean_squared_error: 1.1028 - val_loss: 1.0167 - val_mean_squared_error: 1.0167\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01669, saving model to temp/e353\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0192 - mean_squared_error: 1.0192 - val_loss: 1.0105 - val_mean_squared_error: 1.0105\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01669 to 1.01047, saving model to temp/e353\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.0165 - mean_squared_error: 1.0165 - val_loss: 1.0043 - val_mean_squared_error: 1.0043\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01047 to 1.00430, saving model to temp/e353\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0136 - mean_squared_error: 1.0136 - val_loss: 1.0148 - val_mean_squared_error: 1.0148\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00430\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0125 - mean_squared_error: 1.0125 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00430 to 1.00232, saving model to temp/e353\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0049 - mean_squared_error: 1.0049 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00232\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0069 - mean_squared_error: 1.0069 - val_loss: 1.0037 - val_mean_squared_error: 1.0037\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00232\n",
      "Epoch 00007: early stopping\n",
      "temp/e354\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 165us/step - loss: 1.1011 - mean_squared_error: 1.1011 - val_loss: 1.0212 - val_mean_squared_error: 1.0212\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02125, saving model to temp/e354\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0164 - mean_squared_error: 1.0164 - val_loss: 1.0339 - val_mean_squared_error: 1.0339\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.02125\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02125 to 1.00909, saving model to temp/e354\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00909 to 1.00592, saving model to temp/e354\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00592 to 1.00301, saving model to temp/e354\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0029 - val_mean_squared_error: 1.0029\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00301 to 1.00289, saving model to temp/e354\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0115 - val_mean_squared_error: 1.0115\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00289\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 1.0036 - val_mean_squared_error: 1.0036\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00289\n",
      "Epoch 00008: early stopping\n",
      "temp/e355\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1021 - mean_squared_error: 1.1021 - val_loss: 1.0321 - val_mean_squared_error: 1.0321\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03206, saving model to temp/e355\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0206 - mean_squared_error: 1.0206 - val_loss: 1.0160 - val_mean_squared_error: 1.0160\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03206 to 1.01600, saving model to temp/e355\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.0137 - mean_squared_error: 1.0137 - val_loss: 1.0085 - val_mean_squared_error: 1.0085\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01600 to 1.00848, saving model to temp/e355\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0123 - mean_squared_error: 1.0123 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00848 to 1.00542, saving model to temp/e355\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00542 to 1.00338, saving model to temp/e355\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0107 - mean_squared_error: 1.0107 - val_loss: 1.0197 - val_mean_squared_error: 1.0197\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00338\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0066 - mean_squared_error: 1.0066 - val_loss: 1.0019 - val_mean_squared_error: 1.0019\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00338 to 1.00189, saving model to temp/e355\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 0.9996 - val_mean_squared_error: 0.9996\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00189 to 0.99956, saving model to temp/e355\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0073 - mean_squared_error: 1.0073 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99956\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0057 - mean_squared_error: 1.0057 - val_loss: 0.9962 - val_mean_squared_error: 0.9962\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.99956 to 0.99620, saving model to temp/e355\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0043 - mean_squared_error: 1.0043 - val_loss: 1.0028 - val_mean_squared_error: 1.0028\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99620\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0056 - mean_squared_error: 1.0056 - val_loss: 0.9947 - val_mean_squared_error: 0.9947\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.99620 to 0.99466, saving model to temp/e355\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0042 - mean_squared_error: 1.0042 - val_loss: 0.9986 - val_mean_squared_error: 0.9986\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.99466\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 1.0035 - mean_squared_error: 1.0035 - val_loss: 1.0046 - val_mean_squared_error: 1.0046\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.99466\n",
      "Epoch 00014: early stopping\n",
      "temp/e356\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 161us/step - loss: 1.1113 - mean_squared_error: 1.1113 - val_loss: 1.0228 - val_mean_squared_error: 1.0228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02280, saving model to temp/e356\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.0192 - mean_squared_error: 1.0192 - val_loss: 1.0085 - val_mean_squared_error: 1.0085\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02280 to 1.00849, saving model to temp/e356\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00849\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0140 - mean_squared_error: 1.0140 - val_loss: 1.0025 - val_mean_squared_error: 1.0025\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00849 to 1.00249, saving model to temp/e356\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 1.0089 - val_mean_squared_error: 1.0089\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00249\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0136 - mean_squared_error: 1.0136 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00249\n",
      "Epoch 00006: early stopping\n",
      "temp/e357\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.0964 - mean_squared_error: 1.0964 - val_loss: 1.0146 - val_mean_squared_error: 1.0146\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01463, saving model to temp/e357\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0199 - mean_squared_error: 1.0199 - val_loss: 1.0274 - val_mean_squared_error: 1.0274\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01463\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.0186 - mean_squared_error: 1.0186 - val_loss: 1.0070 - val_mean_squared_error: 1.0070\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01463 to 1.00698, saving model to temp/e357\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0155 - mean_squared_error: 1.0155 - val_loss: 1.0109 - val_mean_squared_error: 1.0109\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00698\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00698 to 1.00378, saving model to temp/e357\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00378 to 1.00230, saving model to temp/e357\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0009 - val_mean_squared_error: 1.0009\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00230 to 1.00092, saving model to temp/e357\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0069 - mean_squared_error: 1.0069 - val_loss: 1.0019 - val_mean_squared_error: 1.0019\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00092\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0045 - mean_squared_error: 1.0045 - val_loss: 0.9988 - val_mean_squared_error: 0.9988\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00092 to 0.99883, saving model to temp/e357\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.0057 - mean_squared_error: 1.0057 - val_loss: 1.0071 - val_mean_squared_error: 1.0071\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99883\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0057 - mean_squared_error: 1.0057 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99883\n",
      "Epoch 00011: early stopping\n",
      "temp/e358\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 1.1093 - mean_squared_error: 1.1093 - val_loss: 1.0160 - val_mean_squared_error: 1.0160\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01601, saving model to temp/e358\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0182 - mean_squared_error: 1.0182 - val_loss: 1.0142 - val_mean_squared_error: 1.0142\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01601 to 1.01419, saving model to temp/e358\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0166 - mean_squared_error: 1.0166 - val_loss: 1.0090 - val_mean_squared_error: 1.0090\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01419 to 1.00899, saving model to temp/e358\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.0144 - mean_squared_error: 1.0144 - val_loss: 1.0104 - val_mean_squared_error: 1.0104\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00899\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0009 - val_mean_squared_error: 1.0009\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00899 to 1.00085, saving model to temp/e358\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00085\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0058 - mean_squared_error: 1.0058 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00085\n",
      "Epoch 00007: early stopping\n",
      "temp/e359\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 169us/step - loss: 1.1035 - mean_squared_error: 1.1035 - val_loss: 1.0297 - val_mean_squared_error: 1.0297\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02971, saving model to temp/e359\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0170 - mean_squared_error: 1.0170 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02971 to 1.00611, saving model to temp/e359\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0174 - mean_squared_error: 1.0174 - val_loss: 1.0094 - val_mean_squared_error: 1.0094\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00611\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0006 - val_mean_squared_error: 1.0006\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00611 to 1.00058, saving model to temp/e359\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0098 - mean_squared_error: 1.0098 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00058\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00058\n",
      "Epoch 00006: early stopping\n",
      "temp/e360\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 164us/step - loss: 1.1064 - mean_squared_error: 1.1064 - val_loss: 1.0156 - val_mean_squared_error: 1.0156\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01561, saving model to temp/e360\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.0203 - mean_squared_error: 1.0203 - val_loss: 1.0161 - val_mean_squared_error: 1.0161\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01561\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.0140 - mean_squared_error: 1.0140 - val_loss: 1.0046 - val_mean_squared_error: 1.0046\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01561 to 1.00456, saving model to temp/e360\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0134 - mean_squared_error: 1.0134 - val_loss: 1.0063 - val_mean_squared_error: 1.0063\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00456\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.0105 - mean_squared_error: 1.0105 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00456\n",
      "Epoch 00005: early stopping\n",
      "temp/e361\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 1.1009 - mean_squared_error: 1.1009 - val_loss: 1.0194 - val_mean_squared_error: 1.0194\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01941, saving model to temp/e361\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 1.0158 - mean_squared_error: 1.0158 - val_loss: 1.0113 - val_mean_squared_error: 1.0113\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01941 to 1.01133, saving model to temp/e361\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.0202 - mean_squared_error: 1.0202 - val_loss: 1.0151 - val_mean_squared_error: 1.0151\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01133\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0157 - mean_squared_error: 1.0157 - val_loss: 1.0116 - val_mean_squared_error: 1.0116\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.01133\n",
      "Epoch 00004: early stopping\n",
      "temp/e362\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 1.0977 - mean_squared_error: 1.0977 - val_loss: 1.0160 - val_mean_squared_error: 1.0160\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01604, saving model to temp/e362\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0195 - mean_squared_error: 1.0195 - val_loss: 1.0119 - val_mean_squared_error: 1.0119\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01604 to 1.01187, saving model to temp/e362\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01187 to 1.00622, saving model to temp/e362\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0068 - val_mean_squared_error: 1.0068\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00622\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.0106 - mean_squared_error: 1.0106 - val_loss: 1.0089 - val_mean_squared_error: 1.0089\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00622\n",
      "Epoch 00005: early stopping\n",
      "temp/e363\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 163us/step - loss: 1.1013 - mean_squared_error: 1.1013 - val_loss: 1.0215 - val_mean_squared_error: 1.0215\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02148, saving model to temp/e363\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 1.0070 - val_mean_squared_error: 1.0070\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02148 to 1.00703, saving model to temp/e363\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0197 - mean_squared_error: 1.0197 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00703 to 1.00488, saving model to temp/e363\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.0112 - mean_squared_error: 1.0112 - val_loss: 1.0083 - val_mean_squared_error: 1.0083\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00488\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 1.0105 - mean_squared_error: 1.0105 - val_loss: 1.0239 - val_mean_squared_error: 1.0239\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00488\n",
      "Epoch 00005: early stopping\n",
      "temp/e364\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1009 - mean_squared_error: 1.1009 - val_loss: 1.0097 - val_mean_squared_error: 1.0097\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00970, saving model to temp/e364\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0207 - mean_squared_error: 1.0207 - val_loss: 1.0272 - val_mean_squared_error: 1.0272\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.00970\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0163 - mean_squared_error: 1.0163 - val_loss: 1.0063 - val_mean_squared_error: 1.0063\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00970 to 1.00630, saving model to temp/e364\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.0111 - mean_squared_error: 1.0111 - val_loss: 1.0056 - val_mean_squared_error: 1.0056\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00630 to 1.00560, saving model to temp/e364\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0050 - val_mean_squared_error: 1.0050\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00560 to 1.00501, saving model to temp/e364\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00501 to 1.00339, saving model to temp/e364\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 1.0071 - mean_squared_error: 1.0071 - val_loss: 1.0085 - val_mean_squared_error: 1.0085\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00339\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.0078 - mean_squared_error: 1.0078 - val_loss: 1.0199 - val_mean_squared_error: 1.0199\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00339\n",
      "Epoch 00008: early stopping\n",
      "temp/e365\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 160us/step - loss: 1.1009 - mean_squared_error: 1.1009 - val_loss: 1.0146 - val_mean_squared_error: 1.0146\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01463, saving model to temp/e365\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0210 - mean_squared_error: 1.0210 - val_loss: 1.0139 - val_mean_squared_error: 1.0139\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01463 to 1.01392, saving model to temp/e365\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01392 to 1.00336, saving model to temp/e365\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0170 - mean_squared_error: 1.0170 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00336\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00336\n",
      "Epoch 00005: early stopping\n",
      "temp/e366\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 162us/step - loss: 1.1007 - mean_squared_error: 1.1007 - val_loss: 1.0158 - val_mean_squared_error: 1.0158\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01577, saving model to temp/e366\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.0187 - mean_squared_error: 1.0187 - val_loss: 1.0122 - val_mean_squared_error: 1.0122\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01577 to 1.01223, saving model to temp/e366\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0107 - mean_squared_error: 1.0107 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01223 to 1.00690, saving model to temp/e366\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0056 - val_mean_squared_error: 1.0056\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00690 to 1.00565, saving model to temp/e366\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0172 - val_mean_squared_error: 1.0172\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00565\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 1.0148 - val_mean_squared_error: 1.0148\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00565\n",
      "Epoch 00006: early stopping\n",
      "temp/e367\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 1.1041 - mean_squared_error: 1.1041 - val_loss: 1.0164 - val_mean_squared_error: 1.0164\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01640, saving model to temp/e367\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0204 - mean_squared_error: 1.0204 - val_loss: 1.0231 - val_mean_squared_error: 1.0231\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01640\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0058 - val_mean_squared_error: 1.0058\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01640 to 1.00576, saving model to temp/e367\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0173 - mean_squared_error: 1.0173 - val_loss: 1.0115 - val_mean_squared_error: 1.0115\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00576\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0111 - mean_squared_error: 1.0111 - val_loss: 1.0018 - val_mean_squared_error: 1.0018\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00576 to 1.00181, saving model to temp/e367\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0082 - mean_squared_error: 1.0082 - val_loss: 1.0039 - val_mean_squared_error: 1.0039\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00181\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0027 - val_mean_squared_error: 1.0027\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00181\n",
      "Epoch 00007: early stopping\n",
      "temp/e368\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 164us/step - loss: 1.1066 - mean_squared_error: 1.1066 - val_loss: 1.0473 - val_mean_squared_error: 1.0473\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.04732, saving model to temp/e368\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.0198 - mean_squared_error: 1.0198 - val_loss: 1.0358 - val_mean_squared_error: 1.0358\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.04732 to 1.03580, saving model to temp/e368\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.0172 - mean_squared_error: 1.0172 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.03580 to 1.00495, saving model to temp/e368\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 1.0112 - val_mean_squared_error: 1.0112\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00495\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0123 - mean_squared_error: 1.0123 - val_loss: 1.0110 - val_mean_squared_error: 1.0110\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00495\n",
      "Epoch 00005: early stopping\n",
      "temp/e369\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 164us/step - loss: 1.1012 - mean_squared_error: 1.1012 - val_loss: 1.0143 - val_mean_squared_error: 1.0143\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01433, saving model to temp/e369\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0209 - mean_squared_error: 1.0209 - val_loss: 1.0157 - val_mean_squared_error: 1.0157\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01433\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0164 - mean_squared_error: 1.0164 - val_loss: 1.0296 - val_mean_squared_error: 1.0296\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01433\n",
      "Epoch 00003: early stopping\n",
      "temp/e370\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 162us/step - loss: 1.1110 - mean_squared_error: 1.1110 - val_loss: 1.0310 - val_mean_squared_error: 1.0310\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03099, saving model to temp/e370\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0200 - mean_squared_error: 1.0200 - val_loss: 1.0285 - val_mean_squared_error: 1.0285\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03099 to 1.02851, saving model to temp/e370\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 1.0060 - val_mean_squared_error: 1.0060\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02851 to 1.00596, saving model to temp/e370\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.0165 - mean_squared_error: 1.0165 - val_loss: 0.9988 - val_mean_squared_error: 0.9988\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00596 to 0.99884, saving model to temp/e370\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0006 - val_mean_squared_error: 1.0006\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.99884\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0097 - val_mean_squared_error: 1.0097\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99884\n",
      "Epoch 00006: early stopping\n",
      "temp/e371\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 164us/step - loss: 1.0960 - mean_squared_error: 1.0960 - val_loss: 1.0176 - val_mean_squared_error: 1.0176\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01762, saving model to temp/e371\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0178 - mean_squared_error: 1.0178 - val_loss: 1.0151 - val_mean_squared_error: 1.0151\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01762 to 1.01509, saving model to temp/e371\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0168 - mean_squared_error: 1.0168 - val_loss: 1.0081 - val_mean_squared_error: 1.0081\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01509 to 1.00813, saving model to temp/e371\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.0151 - mean_squared_error: 1.0151 - val_loss: 1.0066 - val_mean_squared_error: 1.0066\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00813 to 1.00661, saving model to temp/e371\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0010 - val_mean_squared_error: 1.0010\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00661 to 1.00096, saving model to temp/e371\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 1.0120 - val_mean_squared_error: 1.0120\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00096\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 136us/step - loss: 1.0079 - mean_squared_error: 1.0079 - val_loss: 1.0094 - val_mean_squared_error: 1.0094\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00096\n",
      "Epoch 00007: early stopping\n",
      "temp/e372\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 166us/step - loss: 1.1173 - mean_squared_error: 1.1173 - val_loss: 1.0188 - val_mean_squared_error: 1.0188\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01877, saving model to temp/e372\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0196 - mean_squared_error: 1.0196 - val_loss: 1.0215 - val_mean_squared_error: 1.0215\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01877\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 1.0161 - mean_squared_error: 1.0161 - val_loss: 1.0263 - val_mean_squared_error: 1.0263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01877\n",
      "Epoch 00003: early stopping\n",
      "temp/e373\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 171us/step - loss: 1.1023 - mean_squared_error: 1.1023 - val_loss: 1.0265 - val_mean_squared_error: 1.0265\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02647, saving model to temp/e373\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0178 - mean_squared_error: 1.0178 - val_loss: 1.0169 - val_mean_squared_error: 1.0169\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02647 to 1.01690, saving model to temp/e373\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 1.0147 - mean_squared_error: 1.0147 - val_loss: 1.0132 - val_mean_squared_error: 1.0132\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01690 to 1.01322, saving model to temp/e373\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0042 - val_mean_squared_error: 1.0042\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01322 to 1.00424, saving model to temp/e373\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 1.0192 - val_mean_squared_error: 1.0192\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00424\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 1.0070 - mean_squared_error: 1.0070 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00424\n",
      "Epoch 00006: early stopping\n",
      "temp/e374\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 160us/step - loss: 1.0998 - mean_squared_error: 1.0998 - val_loss: 1.0102 - val_mean_squared_error: 1.0102\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01020, saving model to temp/e374\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 1.0176 - mean_squared_error: 1.0176 - val_loss: 1.0220 - val_mean_squared_error: 1.0220\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01020\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.0150 - mean_squared_error: 1.0150 - val_loss: 1.0046 - val_mean_squared_error: 1.0046\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01020 to 1.00457, saving model to temp/e374\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.0141 - mean_squared_error: 1.0141 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00457 to 1.00242, saving model to temp/e374\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0160 - val_mean_squared_error: 1.0160\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00242\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.0131 - mean_squared_error: 1.0131 - val_loss: 0.9987 - val_mean_squared_error: 0.9987\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00242 to 0.99873, saving model to temp/e374\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99873\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0081 - mean_squared_error: 1.0081 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99873\n",
      "Epoch 00008: early stopping\n",
      "temp/e375\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.1052 - mean_squared_error: 1.1052 - val_loss: 1.0182 - val_mean_squared_error: 1.0182\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01824, saving model to temp/e375\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 126us/step - loss: 1.0222 - mean_squared_error: 1.0222 - val_loss: 1.0132 - val_mean_squared_error: 1.0132\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01824 to 1.01322, saving model to temp/e375\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0189 - mean_squared_error: 1.0189 - val_loss: 1.0070 - val_mean_squared_error: 1.0070\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01322 to 1.00702, saving model to temp/e375\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0142 - mean_squared_error: 1.0142 - val_loss: 1.0177 - val_mean_squared_error: 1.0177\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00702\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0089 - mean_squared_error: 1.0089 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00702\n",
      "Epoch 00005: early stopping\n",
      "temp/e376\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.1101 - mean_squared_error: 1.1101 - val_loss: 1.0113 - val_mean_squared_error: 1.0113\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01134, saving model to temp/e376\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0236 - mean_squared_error: 1.0236 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01134 to 1.00800, saving model to temp/e376\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0154 - mean_squared_error: 1.0154 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00800\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0112 - val_mean_squared_error: 1.0112\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00800\n",
      "Epoch 00004: early stopping\n",
      "temp/e377\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.1025 - mean_squared_error: 1.1025 - val_loss: 1.0155 - val_mean_squared_error: 1.0155\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01553, saving model to temp/e377\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0196 - mean_squared_error: 1.0196 - val_loss: 1.0132 - val_mean_squared_error: 1.0132\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01553 to 1.01324, saving model to temp/e377\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0193 - mean_squared_error: 1.0193 - val_loss: 1.0177 - val_mean_squared_error: 1.0177\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01324\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0148 - mean_squared_error: 1.0148 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01324 to 1.00225, saving model to temp/e377\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0106 - mean_squared_error: 1.0106 - val_loss: 1.0126 - val_mean_squared_error: 1.0126\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00225\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0069 - mean_squared_error: 1.0069 - val_loss: 1.0042 - val_mean_squared_error: 1.0042\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00225\n",
      "Epoch 00006: early stopping\n",
      "temp/e378\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 1.0991 - mean_squared_error: 1.0991 - val_loss: 1.0192 - val_mean_squared_error: 1.0192\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01918, saving model to temp/e378\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0193 - mean_squared_error: 1.0193 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01918 to 1.00472, saving model to temp/e378\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0158 - mean_squared_error: 1.0158 - val_loss: 1.0090 - val_mean_squared_error: 1.0090\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00472\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0048 - val_mean_squared_error: 1.0048\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00472\n",
      "Epoch 00004: early stopping\n",
      "temp/e379\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.0972 - mean_squared_error: 1.0972 - val_loss: 1.0347 - val_mean_squared_error: 1.0347\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03472, saving model to temp/e379\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0224 - mean_squared_error: 1.0224 - val_loss: 1.0245 - val_mean_squared_error: 1.0245\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03472 to 1.02447, saving model to temp/e379\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 126us/step - loss: 1.0116 - mean_squared_error: 1.0116 - val_loss: 1.0187 - val_mean_squared_error: 1.0187\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02447 to 1.01865, saving model to temp/e379\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0105 - val_mean_squared_error: 1.0105\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01865 to 1.01046, saving model to temp/e379\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0114 - mean_squared_error: 1.0114 - val_loss: 1.0134 - val_mean_squared_error: 1.0134\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.01046\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0119 - mean_squared_error: 1.0119 - val_loss: 0.9996 - val_mean_squared_error: 0.9996\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.01046 to 0.99962, saving model to temp/e379\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99962\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0062 - mean_squared_error: 1.0062 - val_loss: 1.0060 - val_mean_squared_error: 1.0060\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99962\n",
      "Epoch 00008: early stopping\n",
      "temp/e380\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.1037 - mean_squared_error: 1.1037 - val_loss: 1.0209 - val_mean_squared_error: 1.0209\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02091, saving model to temp/e380\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0202 - mean_squared_error: 1.0202 - val_loss: 1.0116 - val_mean_squared_error: 1.0116\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02091 to 1.01156, saving model to temp/e380\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0168 - mean_squared_error: 1.0168 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01156 to 1.00914, saving model to temp/e380\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0116 - mean_squared_error: 1.0116 - val_loss: 1.0166 - val_mean_squared_error: 1.0166\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00914\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0116 - mean_squared_error: 1.0116 - val_loss: 1.0154 - val_mean_squared_error: 1.0154\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00914\n",
      "Epoch 00005: early stopping\n",
      "temp/e381\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 142us/step - loss: 1.0998 - mean_squared_error: 1.0998 - val_loss: 1.0267 - val_mean_squared_error: 1.0267\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02667, saving model to temp/e381\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0211 - mean_squared_error: 1.0211 - val_loss: 1.0134 - val_mean_squared_error: 1.0134\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02667 to 1.01340, saving model to temp/e381\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01340 to 1.00930, saving model to temp/e381\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0108 - val_mean_squared_error: 1.0108\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00930\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0277 - val_mean_squared_error: 1.0277\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00930\n",
      "Epoch 00005: early stopping\n",
      "temp/e382\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.1011 - mean_squared_error: 1.1011 - val_loss: 1.0326 - val_mean_squared_error: 1.0326\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03263, saving model to temp/e382\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0195 - mean_squared_error: 1.0195 - val_loss: 1.0068 - val_mean_squared_error: 1.0068\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03263 to 1.00681, saving model to temp/e382\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0177 - mean_squared_error: 1.0177 - val_loss: 1.0125 - val_mean_squared_error: 1.0125\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00681\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0131 - val_mean_squared_error: 1.0131\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00681\n",
      "Epoch 00004: early stopping\n",
      "temp/e383\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.0972 - mean_squared_error: 1.0972 - val_loss: 1.0119 - val_mean_squared_error: 1.0119\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01192, saving model to temp/e383\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0189 - mean_squared_error: 1.0189 - val_loss: 1.0109 - val_mean_squared_error: 1.0109\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01192 to 1.01086, saving model to temp/e383\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01086 to 1.00474, saving model to temp/e383\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0145 - mean_squared_error: 1.0145 - val_loss: 1.0097 - val_mean_squared_error: 1.0097\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00474\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0180 - val_mean_squared_error: 1.0180\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00474\n",
      "Epoch 00005: early stopping\n",
      "temp/e384\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.1051 - mean_squared_error: 1.1051 - val_loss: 1.0259 - val_mean_squared_error: 1.0259\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02588, saving model to temp/e384\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0212 - mean_squared_error: 1.0212 - val_loss: 1.0188 - val_mean_squared_error: 1.0188\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02588 to 1.01880, saving model to temp/e384\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01880 to 1.00591, saving model to temp/e384\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0161 - mean_squared_error: 1.0161 - val_loss: 1.0164 - val_mean_squared_error: 1.0164\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00591\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00591 to 1.00335, saving model to temp/e384\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00335 to 1.00238, saving model to temp/e384\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0115 - val_mean_squared_error: 1.0115\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00238\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0044 - mean_squared_error: 1.0044 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00238\n",
      "Epoch 00008: early stopping\n",
      "temp/e385\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.0940 - mean_squared_error: 1.0940 - val_loss: 1.0537 - val_mean_squared_error: 1.0537\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.05369, saving model to temp/e385\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0189 - mean_squared_error: 1.0189 - val_loss: 1.0270 - val_mean_squared_error: 1.0270\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.05369 to 1.02700, saving model to temp/e385\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0168 - mean_squared_error: 1.0168 - val_loss: 1.0196 - val_mean_squared_error: 1.0196\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02700 to 1.01960, saving model to temp/e385\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0127 - mean_squared_error: 1.0127 - val_loss: 1.0056 - val_mean_squared_error: 1.0056\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01960 to 1.00555, saving model to temp/e385\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0151 - val_mean_squared_error: 1.0151\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00555\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 1.0050 - val_mean_squared_error: 1.0050\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00555 to 1.00496, saving model to temp/e385\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0112 - mean_squared_error: 1.0112 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00496 to 1.00260, saving model to temp/e385\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 0.9998 - val_mean_squared_error: 0.9998\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00260 to 0.99978, saving model to temp/e385\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0032 - mean_squared_error: 1.0032 - val_loss: 1.0001 - val_mean_squared_error: 1.0001\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99978\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0017 - mean_squared_error: 1.0017 - val_loss: 1.0020 - val_mean_squared_error: 1.0020\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99978\n",
      "Epoch 00010: early stopping\n",
      "temp/e386\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1020 - mean_squared_error: 1.1020 - val_loss: 1.0127 - val_mean_squared_error: 1.0127\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01269, saving model to temp/e386\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0195 - mean_squared_error: 1.0195 - val_loss: 1.0089 - val_mean_squared_error: 1.0089\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01269 to 1.00888, saving model to temp/e386\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0146 - mean_squared_error: 1.0146 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00888 to 1.00614, saving model to temp/e386\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0144 - mean_squared_error: 1.0144 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00614 to 1.00507, saving model to temp/e386\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00507 to 1.00304, saving model to temp/e386\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0075 - mean_squared_error: 1.0075 - val_loss: 1.0078 - val_mean_squared_error: 1.0078\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00304\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00304\n",
      "Epoch 00007: early stopping\n",
      "temp/e387\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.0989 - mean_squared_error: 1.0989 - val_loss: 1.0194 - val_mean_squared_error: 1.0194\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01936, saving model to temp/e387\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0214 - mean_squared_error: 1.0214 - val_loss: 1.0369 - val_mean_squared_error: 1.0369\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01936\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0142 - mean_squared_error: 1.0142 - val_loss: 1.0115 - val_mean_squared_error: 1.0115\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01936 to 1.01147, saving model to temp/e387\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0101 - val_mean_squared_error: 1.0101\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01147 to 1.01010, saving model to temp/e387\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0036 - val_mean_squared_error: 1.0036\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01010 to 1.00360, saving model to temp/e387\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00360\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 0.9967 - val_mean_squared_error: 0.9967\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00360 to 0.99668, saving model to temp/e387\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0086 - mean_squared_error: 1.0086 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99668\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0055 - mean_squared_error: 1.0055 - val_loss: 1.0028 - val_mean_squared_error: 1.0028\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99668\n",
      "Epoch 00009: early stopping\n",
      "temp/e388\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.0938 - mean_squared_error: 1.0938 - val_loss: 1.0303 - val_mean_squared_error: 1.0303\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03031, saving model to temp/e388\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0201 - mean_squared_error: 1.0201 - val_loss: 1.0137 - val_mean_squared_error: 1.0137\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03031 to 1.01372, saving model to temp/e388\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0036 - val_mean_squared_error: 1.0036\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01372 to 1.00361, saving model to temp/e388\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 1.0144 - val_mean_squared_error: 1.0144\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00361\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0060 - val_mean_squared_error: 1.0060\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00361\n",
      "Epoch 00005: early stopping\n",
      "temp/e389\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.1108 - mean_squared_error: 1.1108 - val_loss: 1.0117 - val_mean_squared_error: 1.0117\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01167, saving model to temp/e389\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0233 - mean_squared_error: 1.0233 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01167 to 1.00540, saving model to temp/e389\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0150 - mean_squared_error: 1.0150 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00540\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0118 - mean_squared_error: 1.0118 - val_loss: 1.0136 - val_mean_squared_error: 1.0136\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00540\n",
      "Epoch 00004: early stopping\n",
      "temp/e390\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 1.1044 - mean_squared_error: 1.1044 - val_loss: 1.0181 - val_mean_squared_error: 1.0181\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01805, saving model to temp/e390\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0191 - mean_squared_error: 1.0191 - val_loss: 1.0156 - val_mean_squared_error: 1.0156\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01805 to 1.01562, saving model to temp/e390\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0171 - mean_squared_error: 1.0171 - val_loss: 1.0139 - val_mean_squared_error: 1.0139\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01562 to 1.01388, saving model to temp/e390\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0156 - mean_squared_error: 1.0156 - val_loss: 1.0129 - val_mean_squared_error: 1.0129\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01388 to 1.01293, saving model to temp/e390\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01293 to 1.00234, saving model to temp/e390\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00234\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0086 - mean_squared_error: 1.0086 - val_loss: 1.0005 - val_mean_squared_error: 1.0005\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00234 to 1.00050, saving model to temp/e390\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0058 - mean_squared_error: 1.0058 - val_loss: 1.0016 - val_mean_squared_error: 1.0016\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00050\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0072 - mean_squared_error: 1.0072 - val_loss: 0.9989 - val_mean_squared_error: 0.9989\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00050 to 0.99889, saving model to temp/e390\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0053 - mean_squared_error: 1.0053 - val_loss: 1.0060 - val_mean_squared_error: 1.0060\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99889\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0070 - mean_squared_error: 1.0070 - val_loss: 0.9969 - val_mean_squared_error: 0.9969\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.99889 to 0.99692, saving model to temp/e390\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0024 - mean_squared_error: 1.0024 - val_loss: 0.9947 - val_mean_squared_error: 0.9947\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.99692 to 0.99470, saving model to temp/e390\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0048 - mean_squared_error: 1.0048 - val_loss: 1.0006 - val_mean_squared_error: 1.0006\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.99470\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0040 - mean_squared_error: 1.0040 - val_loss: 0.9971 - val_mean_squared_error: 0.9971\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.99470\n",
      "Epoch 00014: early stopping\n",
      "temp/e391\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.1028 - mean_squared_error: 1.1028 - val_loss: 1.0151 - val_mean_squared_error: 1.0151\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01509, saving model to temp/e391\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0194 - mean_squared_error: 1.0194 - val_loss: 1.0139 - val_mean_squared_error: 1.0139\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01509 to 1.01387, saving model to temp/e391\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0164 - mean_squared_error: 1.0164 - val_loss: 1.0139 - val_mean_squared_error: 1.0139\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01387\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0139 - mean_squared_error: 1.0139 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01387 to 1.00509, saving model to temp/e391\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00509\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0109 - mean_squared_error: 1.0109 - val_loss: 1.0079 - val_mean_squared_error: 1.0079\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00509\n",
      "Epoch 00006: early stopping\n",
      "temp/e392\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 1.1122 - mean_squared_error: 1.1122 - val_loss: 1.0182 - val_mean_squared_error: 1.0182\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01817, saving model to temp/e392\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0185 - mean_squared_error: 1.0185 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01817 to 1.00748, saving model to temp/e392\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0170 - mean_squared_error: 1.0170 - val_loss: 1.0138 - val_mean_squared_error: 1.0138\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00748\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0071 - val_mean_squared_error: 1.0071\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00748 to 1.00708, saving model to temp/e392\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 0.9980 - val_mean_squared_error: 0.9980\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00708 to 0.99799, saving model to temp/e392\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0021 - val_mean_squared_error: 1.0021\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99799\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 1.0107 - val_mean_squared_error: 1.0107\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99799\n",
      "Epoch 00007: early stopping\n",
      "temp/e393\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 143us/step - loss: 1.0995 - mean_squared_error: 1.0995 - val_loss: 1.0286 - val_mean_squared_error: 1.0286\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02857, saving model to temp/e393\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0186 - mean_squared_error: 1.0186 - val_loss: 1.0106 - val_mean_squared_error: 1.0106\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02857 to 1.01056, saving model to temp/e393\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 1.0044 - val_mean_squared_error: 1.0044\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01056 to 1.00442, saving model to temp/e393\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0157 - mean_squared_error: 1.0157 - val_loss: 1.0085 - val_mean_squared_error: 1.0085\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00442\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0078 - mean_squared_error: 1.0078 - val_loss: 1.0215 - val_mean_squared_error: 1.0215\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00442\n",
      "Epoch 00005: early stopping\n",
      "temp/e394\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.1038 - mean_squared_error: 1.1038 - val_loss: 1.0132 - val_mean_squared_error: 1.0132\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01323, saving model to temp/e394\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0210 - mean_squared_error: 1.0210 - val_loss: 1.0265 - val_mean_squared_error: 1.0265\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01323\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0352 - val_mean_squared_error: 1.0352\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01323\n",
      "Epoch 00003: early stopping\n",
      "temp/e395\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.0988 - mean_squared_error: 1.0988 - val_loss: 1.0111 - val_mean_squared_error: 1.0111\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01114, saving model to temp/e395\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0253 - mean_squared_error: 1.0253 - val_loss: 1.0078 - val_mean_squared_error: 1.0078\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01114 to 1.00778, saving model to temp/e395\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0150 - mean_squared_error: 1.0150 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00778 to 1.00325, saving model to temp/e395\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 1.0042 - val_mean_squared_error: 1.0042\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00325\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0082 - mean_squared_error: 1.0082 - val_loss: 1.0078 - val_mean_squared_error: 1.0078\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00325\n",
      "Epoch 00005: early stopping\n",
      "temp/e396\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.0998 - mean_squared_error: 1.0998 - val_loss: 1.0165 - val_mean_squared_error: 1.0165\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01653, saving model to temp/e396\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0213 - mean_squared_error: 1.0213 - val_loss: 1.0370 - val_mean_squared_error: 1.0370\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01653\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0168 - mean_squared_error: 1.0168 - val_loss: 1.0081 - val_mean_squared_error: 1.0081\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01653 to 1.00811, saving model to temp/e396\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0147 - mean_squared_error: 1.0147 - val_loss: 1.0042 - val_mean_squared_error: 1.0042\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00811 to 1.00417, saving model to temp/e396\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0102 - val_mean_squared_error: 1.0102\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00417\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0085 - val_mean_squared_error: 1.0085\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00417\n",
      "Epoch 00006: early stopping\n",
      "temp/e397\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1012 - mean_squared_error: 1.1012 - val_loss: 1.0253 - val_mean_squared_error: 1.0253\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02533, saving model to temp/e397\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0193 - mean_squared_error: 1.0193 - val_loss: 1.0260 - val_mean_squared_error: 1.0260\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.02533\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0153 - mean_squared_error: 1.0153 - val_loss: 1.0107 - val_mean_squared_error: 1.0107\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02533 to 1.01070, saving model to temp/e397\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01070 to 1.00400, saving model to temp/e397\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00400 to 1.00381, saving model to temp/e397\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 1.0001 - val_mean_squared_error: 1.0001\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00381 to 1.00009, saving model to temp/e397\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0093 - mean_squared_error: 1.0093 - val_loss: 0.9989 - val_mean_squared_error: 0.9989\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00009 to 0.99889, saving model to temp/e397\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0065 - mean_squared_error: 1.0065 - val_loss: 0.9998 - val_mean_squared_error: 0.9998\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99889\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0058 - mean_squared_error: 1.0058 - val_loss: 1.0027 - val_mean_squared_error: 1.0027\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99889\n",
      "Epoch 00009: early stopping\n",
      "temp/e398\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.1003 - mean_squared_error: 1.1003 - val_loss: 1.0189 - val_mean_squared_error: 1.0189\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01889, saving model to temp/e398\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0145 - mean_squared_error: 1.0145 - val_loss: 1.0110 - val_mean_squared_error: 1.0110\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01889 to 1.01098, saving model to temp/e398\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0166 - mean_squared_error: 1.0166 - val_loss: 1.0205 - val_mean_squared_error: 1.0205\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01098\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0159 - mean_squared_error: 1.0159 - val_loss: 1.0157 - val_mean_squared_error: 1.0157\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.01098\n",
      "Epoch 00004: early stopping\n",
      "temp/e399\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.1031 - mean_squared_error: 1.1031 - val_loss: 1.0137 - val_mean_squared_error: 1.0137\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01369, saving model to temp/e399\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0174 - mean_squared_error: 1.0174 - val_loss: 1.0142 - val_mean_squared_error: 1.0142\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01369\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0139 - mean_squared_error: 1.0139 - val_loss: 1.0081 - val_mean_squared_error: 1.0081\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01369 to 1.00813, saving model to temp/e399\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0103 - mean_squared_error: 1.0103 - val_loss: 1.0048 - val_mean_squared_error: 1.0048\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00813 to 1.00484, saving model to temp/e399\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00484 to 1.00472, saving model to temp/e399\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 1.0012 - val_mean_squared_error: 1.0012\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00472 to 1.00120, saving model to temp/e399\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 0.9993 - val_mean_squared_error: 0.9993\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00120 to 0.99929, saving model to temp/e399\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0064 - mean_squared_error: 1.0064 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99929\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0073 - mean_squared_error: 1.0073 - val_loss: 1.0020 - val_mean_squared_error: 1.0020\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99929\n",
      "Epoch 00009: early stopping\n",
      "temp/e400\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1104 - mean_squared_error: 1.1104 - val_loss: 1.0137 - val_mean_squared_error: 1.0137\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01372, saving model to temp/e400\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0196 - mean_squared_error: 1.0196 - val_loss: 1.0192 - val_mean_squared_error: 1.0192\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01372\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0158 - mean_squared_error: 1.0158 - val_loss: 1.0102 - val_mean_squared_error: 1.0102\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01372 to 1.01021, saving model to temp/e400\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0140 - mean_squared_error: 1.0140 - val_loss: 1.0041 - val_mean_squared_error: 1.0041\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01021 to 1.00407, saving model to temp/e400\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 1.0050 - val_mean_squared_error: 1.0050\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00407\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0105 - mean_squared_error: 1.0105 - val_loss: 0.9999 - val_mean_squared_error: 0.9999\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00407 to 0.99985, saving model to temp/e400\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0068 - mean_squared_error: 1.0068 - val_loss: 1.0037 - val_mean_squared_error: 1.0037\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99985\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 1.0152 - val_mean_squared_error: 1.0152\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99985\n",
      "Epoch 00008: early stopping\n",
      "temp/e401\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.1060 - mean_squared_error: 1.1060 - val_loss: 1.0205 - val_mean_squared_error: 1.0205\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02053, saving model to temp/e401\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0173 - mean_squared_error: 1.0173 - val_loss: 1.0184 - val_mean_squared_error: 1.0184\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02053 to 1.01842, saving model to temp/e401\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0012 - val_mean_squared_error: 1.0012\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01842 to 1.00125, saving model to temp/e401\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0118 - mean_squared_error: 1.0118 - val_loss: 1.0185 - val_mean_squared_error: 1.0185\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00125\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0004 - val_mean_squared_error: 1.0004\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00125 to 1.00036, saving model to temp/e401\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0172 - val_mean_squared_error: 1.0172\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00036\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0037 - val_mean_squared_error: 1.0037\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00036\n",
      "Epoch 00007: early stopping\n",
      "temp/e402\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.0956 - mean_squared_error: 1.0956 - val_loss: 1.0230 - val_mean_squared_error: 1.0230\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02295, saving model to temp/e402\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0229 - mean_squared_error: 1.0229 - val_loss: 1.0106 - val_mean_squared_error: 1.0106\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02295 to 1.01057, saving model to temp/e402\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0042 - val_mean_squared_error: 1.0042\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01057 to 1.00423, saving model to temp/e402\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0147 - mean_squared_error: 1.0147 - val_loss: 1.0105 - val_mean_squared_error: 1.0105\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00423\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0098 - mean_squared_error: 1.0098 - val_loss: 1.0042 - val_mean_squared_error: 1.0042\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00423 to 1.00416, saving model to temp/e402\n",
      "Epoch 00005: early stopping\n",
      "temp/e403\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.0988 - mean_squared_error: 1.0988 - val_loss: 1.0395 - val_mean_squared_error: 1.0395\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03945, saving model to temp/e403\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0228 - mean_squared_error: 1.0228 - val_loss: 1.0090 - val_mean_squared_error: 1.0090\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03945 to 1.00904, saving model to temp/e403\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0153 - mean_squared_error: 1.0153 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00904 to 1.00587, saving model to temp/e403\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0072 - val_mean_squared_error: 1.0072\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00587\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00587 to 1.00342, saving model to temp/e403\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00342\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.0084 - val_mean_squared_error: 1.0084\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00342\n",
      "Epoch 00007: early stopping\n",
      "temp/e404\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1017 - mean_squared_error: 1.1017 - val_loss: 1.0147 - val_mean_squared_error: 1.0147\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01466, saving model to temp/e404\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0226 - mean_squared_error: 1.0226 - val_loss: 1.0188 - val_mean_squared_error: 1.0188\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01466\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0164 - mean_squared_error: 1.0164 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01466 to 1.00587, saving model to temp/e404\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0116 - mean_squared_error: 1.0116 - val_loss: 1.0025 - val_mean_squared_error: 1.0025\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00587 to 1.00254, saving model to temp/e404\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0112 - mean_squared_error: 1.0112 - val_loss: 1.0022 - val_mean_squared_error: 1.0022\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00254 to 1.00224, saving model to temp/e404\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0089 - mean_squared_error: 1.0089 - val_loss: 1.0082 - val_mean_squared_error: 1.0082\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00224\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0071 - mean_squared_error: 1.0071 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00224\n",
      "Epoch 00007: early stopping\n",
      "temp/e405\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1069 - mean_squared_error: 1.1069 - val_loss: 1.0137 - val_mean_squared_error: 1.0137\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01366, saving model to temp/e405\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0237 - mean_squared_error: 1.0237 - val_loss: 1.0077 - val_mean_squared_error: 1.0077\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01366 to 1.00767, saving model to temp/e405\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0131 - mean_squared_error: 1.0131 - val_loss: 1.0084 - val_mean_squared_error: 1.0084\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00767\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0116 - mean_squared_error: 1.0116 - val_loss: 1.0211 - val_mean_squared_error: 1.0211\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00767\n",
      "Epoch 00004: early stopping\n",
      "temp/e406\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.0966 - mean_squared_error: 1.0966 - val_loss: 1.0274 - val_mean_squared_error: 1.0274\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02741, saving model to temp/e406\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0232 - mean_squared_error: 1.0232 - val_loss: 1.0272 - val_mean_squared_error: 1.0272\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02741 to 1.02715, saving model to temp/e406\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0171 - mean_squared_error: 1.0171 - val_loss: 1.0120 - val_mean_squared_error: 1.0120\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02715 to 1.01202, saving model to temp/e406\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0079 - val_mean_squared_error: 1.0079\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01202 to 1.00785, saving model to temp/e406\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0096 - val_mean_squared_error: 1.0096\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00785\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0106 - mean_squared_error: 1.0106 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00785 to 1.00623, saving model to temp/e406\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0001 - val_mean_squared_error: 1.0001\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00623 to 1.00012, saving model to temp/e406\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00012\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0056 - mean_squared_error: 1.0056 - val_loss: 1.0008 - val_mean_squared_error: 1.0008\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00012\n",
      "Epoch 00009: early stopping\n",
      "temp/e407\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.1002 - mean_squared_error: 1.1002 - val_loss: 1.0108 - val_mean_squared_error: 1.0108\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01082, saving model to temp/e407\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0200 - mean_squared_error: 1.0200 - val_loss: 1.0074 - val_mean_squared_error: 1.0074\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01082 to 1.00735, saving model to temp/e407\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0128 - mean_squared_error: 1.0128 - val_loss: 1.0076 - val_mean_squared_error: 1.0076\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00735\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0114 - mean_squared_error: 1.0114 - val_loss: 1.0206 - val_mean_squared_error: 1.0206\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00735\n",
      "Epoch 00004: early stopping\n",
      "temp/e408\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.0999 - mean_squared_error: 1.0999 - val_loss: 1.0107 - val_mean_squared_error: 1.0107\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01067, saving model to temp/e408\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0212 - mean_squared_error: 1.0212 - val_loss: 1.0096 - val_mean_squared_error: 1.0096\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01067 to 1.00957, saving model to temp/e408\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0107 - val_mean_squared_error: 1.0107\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00957\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0035 - val_mean_squared_error: 1.0035\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00957 to 1.00352, saving model to temp/e408\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0116 - mean_squared_error: 1.0116 - val_loss: 1.0045 - val_mean_squared_error: 1.0045\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00352\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0046 - val_mean_squared_error: 1.0046\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00352\n",
      "Epoch 00006: early stopping\n",
      "temp/e409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.1004 - mean_squared_error: 1.1004 - val_loss: 1.0134 - val_mean_squared_error: 1.0134\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01339, saving model to temp/e409\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0178 - mean_squared_error: 1.0178 - val_loss: 1.0339 - val_mean_squared_error: 1.0339\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01339\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0166 - mean_squared_error: 1.0166 - val_loss: 1.0109 - val_mean_squared_error: 1.0109\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01339 to 1.01094, saving model to temp/e409\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0179 - mean_squared_error: 1.0179 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01094 to 1.00491, saving model to temp/e409\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00491 to 1.00304, saving model to temp/e409\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0112 - mean_squared_error: 1.0112 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00304\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0082 - mean_squared_error: 1.0082 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00304\n",
      "Epoch 00007: early stopping\n",
      "temp/e410\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.0969 - mean_squared_error: 1.0969 - val_loss: 1.0139 - val_mean_squared_error: 1.0139\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01389, saving model to temp/e410\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0229 - mean_squared_error: 1.0229 - val_loss: 1.0145 - val_mean_squared_error: 1.0145\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01389\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0166 - mean_squared_error: 1.0166 - val_loss: 1.0184 - val_mean_squared_error: 1.0184\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01389\n",
      "Epoch 00003: early stopping\n",
      "temp/e411\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.0988 - mean_squared_error: 1.0988 - val_loss: 1.0207 - val_mean_squared_error: 1.0207\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02068, saving model to temp/e411\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0190 - mean_squared_error: 1.0190 - val_loss: 1.0213 - val_mean_squared_error: 1.0213\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.02068\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0137 - mean_squared_error: 1.0137 - val_loss: 1.0035 - val_mean_squared_error: 1.0035\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02068 to 1.00355, saving model to temp/e411\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0137 - mean_squared_error: 1.0137 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00355 to 1.00295, saving model to temp/e411\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 1.0041 - val_mean_squared_error: 1.0041\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00295\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 1.0035 - val_mean_squared_error: 1.0035\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00295\n",
      "Epoch 00006: early stopping\n",
      "temp/e412\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1102 - mean_squared_error: 1.1102 - val_loss: 1.0173 - val_mean_squared_error: 1.0173\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01733, saving model to temp/e412\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0191 - mean_squared_error: 1.0191 - val_loss: 1.0135 - val_mean_squared_error: 1.0135\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01733 to 1.01345, saving model to temp/e412\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0160 - mean_squared_error: 1.0160 - val_loss: 1.0104 - val_mean_squared_error: 1.0104\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01345 to 1.01039, saving model to temp/e412\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0151 - mean_squared_error: 1.0151 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01039 to 1.00401, saving model to temp/e412\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 1.0201 - val_mean_squared_error: 1.0201\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00401\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0167 - val_mean_squared_error: 1.0167\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00401\n",
      "Epoch 00006: early stopping\n",
      "temp/e413\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1075 - mean_squared_error: 1.1075 - val_loss: 1.0119 - val_mean_squared_error: 1.0119\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01188, saving model to temp/e413\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0192 - mean_squared_error: 1.0192 - val_loss: 1.0190 - val_mean_squared_error: 1.0190\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01188\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0178 - val_mean_squared_error: 1.0178\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01188\n",
      "Epoch 00003: early stopping\n",
      "temp/e414\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.1024 - mean_squared_error: 1.1024 - val_loss: 1.0251 - val_mean_squared_error: 1.0251\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02513, saving model to temp/e414\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0221 - mean_squared_error: 1.0221 - val_loss: 1.0478 - val_mean_squared_error: 1.0478\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.02513\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0155 - mean_squared_error: 1.0155 - val_loss: 1.0121 - val_mean_squared_error: 1.0121\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02513 to 1.01210, saving model to temp/e414\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0118 - mean_squared_error: 1.0118 - val_loss: 1.0106 - val_mean_squared_error: 1.0106\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01210 to 1.01060, saving model to temp/e414\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0048 - val_mean_squared_error: 1.0048\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01060 to 1.00476, saving model to temp/e414\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0098 - mean_squared_error: 1.0098 - val_loss: 1.0000 - val_mean_squared_error: 1.0000\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00476 to 1.00002, saving model to temp/e414\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 0.9996 - val_mean_squared_error: 0.9996\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00002 to 0.99958, saving model to temp/e414\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0068 - mean_squared_error: 1.0068 - val_loss: 1.0019 - val_mean_squared_error: 1.0019\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99958\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99958\n",
      "Epoch 00009: early stopping\n",
      "temp/e415\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.0976 - mean_squared_error: 1.0976 - val_loss: 1.0147 - val_mean_squared_error: 1.0147\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01467, saving model to temp/e415\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0198 - mean_squared_error: 1.0198 - val_loss: 1.0110 - val_mean_squared_error: 1.0110\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01467 to 1.01100, saving model to temp/e415\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0177 - mean_squared_error: 1.0177 - val_loss: 1.0092 - val_mean_squared_error: 1.0092\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01100 to 1.00918, saving model to temp/e415\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0018 - val_mean_squared_error: 1.0018\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00918 to 1.00176, saving model to temp/e415\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0089 - mean_squared_error: 1.0089 - val_loss: 1.0003 - val_mean_squared_error: 1.0003\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00176 to 1.00034, saving model to temp/e415\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 0.9986 - val_mean_squared_error: 0.9986\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00034 to 0.99857, saving model to temp/e415\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0047 - mean_squared_error: 1.0047 - val_loss: 1.0169 - val_mean_squared_error: 1.0169\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99857\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0030 - mean_squared_error: 1.0030 - val_loss: 0.9995 - val_mean_squared_error: 0.9995\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99857\n",
      "Epoch 00008: early stopping\n",
      "temp/e416\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.0999 - mean_squared_error: 1.0999 - val_loss: 1.0268 - val_mean_squared_error: 1.0268\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02677, saving model to temp/e416\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0178 - mean_squared_error: 1.0178 - val_loss: 1.0203 - val_mean_squared_error: 1.0203\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02677 to 1.02027, saving model to temp/e416\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0159 - mean_squared_error: 1.0159 - val_loss: 1.0142 - val_mean_squared_error: 1.0142\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02027 to 1.01416, saving model to temp/e416\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0147 - mean_squared_error: 1.0147 - val_loss: 1.0136 - val_mean_squared_error: 1.0136\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01416 to 1.01360, saving model to temp/e416\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 1.0045 - val_mean_squared_error: 1.0045\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01360 to 1.00446, saving model to temp/e416\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.0123 - val_mean_squared_error: 1.0123\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00446\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0078 - mean_squared_error: 1.0078 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00446 to 1.00241, saving model to temp/e416\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0070 - mean_squared_error: 1.0070 - val_loss: 1.0016 - val_mean_squared_error: 1.0016\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00241 to 1.00157, saving model to temp/e416\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0073 - mean_squared_error: 1.0073 - val_loss: 0.9993 - val_mean_squared_error: 0.9993\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00157 to 0.99926, saving model to temp/e416\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0044 - mean_squared_error: 1.0044 - val_loss: 1.0004 - val_mean_squared_error: 1.0004\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99926\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0048 - mean_squared_error: 1.0048 - val_loss: 1.0056 - val_mean_squared_error: 1.0056\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99926\n",
      "Epoch 00011: early stopping\n",
      "temp/e417\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.0974 - mean_squared_error: 1.0974 - val_loss: 1.0253 - val_mean_squared_error: 1.0253\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02529, saving model to temp/e417\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0173 - mean_squared_error: 1.0173 - val_loss: 1.0280 - val_mean_squared_error: 1.0280\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.02529\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0165 - mean_squared_error: 1.0165 - val_loss: 1.0140 - val_mean_squared_error: 1.0140\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02529 to 1.01396, saving model to temp/e417\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01396 to 1.00404, saving model to temp/e417\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00404\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0062 - mean_squared_error: 1.0062 - val_loss: 1.0083 - val_mean_squared_error: 1.0083\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00404\n",
      "Epoch 00006: early stopping\n",
      "temp/e418\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.0909 - mean_squared_error: 1.0909 - val_loss: 1.0185 - val_mean_squared_error: 1.0185\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01847, saving model to temp/e418\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0174 - mean_squared_error: 1.0174 - val_loss: 1.0141 - val_mean_squared_error: 1.0141\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01847 to 1.01414, saving model to temp/e418\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0158 - mean_squared_error: 1.0158 - val_loss: 1.0160 - val_mean_squared_error: 1.0160\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01414\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01414 to 1.00517, saving model to temp/e418\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00517\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00517\n",
      "Epoch 00006: early stopping\n",
      "temp/e419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 1.0968 - mean_squared_error: 1.0968 - val_loss: 1.0153 - val_mean_squared_error: 1.0153\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01527, saving model to temp/e419\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0198 - mean_squared_error: 1.0198 - val_loss: 1.0065 - val_mean_squared_error: 1.0065\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01527 to 1.00646, saving model to temp/e419\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0179 - mean_squared_error: 1.0179 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00646\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0056 - val_mean_squared_error: 1.0056\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00646 to 1.00564, saving model to temp/e419\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0111 - val_mean_squared_error: 1.0111\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00564\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00564 to 1.00491, saving model to temp/e419\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0089 - mean_squared_error: 1.0089 - val_loss: 0.9992 - val_mean_squared_error: 0.9992\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00491 to 0.99920, saving model to temp/e419\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0064 - mean_squared_error: 1.0064 - val_loss: 1.0056 - val_mean_squared_error: 1.0056\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99920\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0076 - mean_squared_error: 1.0076 - val_loss: 0.9994 - val_mean_squared_error: 0.9994\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99920\n",
      "Epoch 00009: early stopping\n",
      "temp/e420\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1039 - mean_squared_error: 1.1039 - val_loss: 1.0162 - val_mean_squared_error: 1.0162\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01625, saving model to temp/e420\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0194 - mean_squared_error: 1.0194 - val_loss: 1.0135 - val_mean_squared_error: 1.0135\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01625 to 1.01353, saving model to temp/e420\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0164 - val_mean_squared_error: 1.0164\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01353\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0136 - mean_squared_error: 1.0136 - val_loss: 1.0090 - val_mean_squared_error: 1.0090\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01353 to 1.00896, saving model to temp/e420\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 1.0063 - val_mean_squared_error: 1.0063\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00896 to 1.00627, saving model to temp/e420\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0064 - mean_squared_error: 1.0064 - val_loss: 1.0025 - val_mean_squared_error: 1.0025\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00627 to 1.00247, saving model to temp/e420\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0076 - mean_squared_error: 1.0076 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00247\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0062 - mean_squared_error: 1.0062 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00247\n",
      "Epoch 00008: early stopping\n",
      "temp/e421\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.0979 - mean_squared_error: 1.0979 - val_loss: 1.0208 - val_mean_squared_error: 1.0208\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02078, saving model to temp/e421\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0197 - mean_squared_error: 1.0197 - val_loss: 1.0131 - val_mean_squared_error: 1.0131\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02078 to 1.01312, saving model to temp/e421\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0156 - mean_squared_error: 1.0156 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01312 to 1.00542, saving model to temp/e421\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 1.0079 - val_mean_squared_error: 1.0079\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00542\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0072 - mean_squared_error: 1.0072 - val_loss: 1.0179 - val_mean_squared_error: 1.0179\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00542\n",
      "Epoch 00005: early stopping\n",
      "temp/e422\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.1052 - mean_squared_error: 1.1052 - val_loss: 1.0193 - val_mean_squared_error: 1.0193\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01929, saving model to temp/e422\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0184 - mean_squared_error: 1.0184 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01929 to 1.00685, saving model to temp/e422\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0183 - mean_squared_error: 1.0183 - val_loss: 1.0041 - val_mean_squared_error: 1.0041\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00685 to 1.00405, saving model to temp/e422\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00405\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 1.0211 - val_mean_squared_error: 1.0211\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00405\n",
      "Epoch 00005: early stopping\n",
      "temp/e423\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.1029 - mean_squared_error: 1.1029 - val_loss: 1.0153 - val_mean_squared_error: 1.0153\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01526, saving model to temp/e423\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0216 - mean_squared_error: 1.0216 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01526 to 1.00802, saving model to temp/e423\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0141 - mean_squared_error: 1.0141 - val_loss: 1.0079 - val_mean_squared_error: 1.0079\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00802 to 1.00794, saving model to temp/e423\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0127 - mean_squared_error: 1.0127 - val_loss: 1.0056 - val_mean_squared_error: 1.0056\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00794 to 1.00565, saving model to temp/e423\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0093 - mean_squared_error: 1.0093 - val_loss: 1.0070 - val_mean_squared_error: 1.0070\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00565\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0012 - val_mean_squared_error: 1.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_loss improved from 1.00565 to 1.00119, saving model to temp/e423\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0076 - mean_squared_error: 1.0076 - val_loss: 1.0133 - val_mean_squared_error: 1.0133\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00119\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0073 - mean_squared_error: 1.0073 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00119\n",
      "Epoch 00008: early stopping\n",
      "temp/e424\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.1000 - mean_squared_error: 1.1000 - val_loss: 1.0216 - val_mean_squared_error: 1.0216\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02155, saving model to temp/e424\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0186 - mean_squared_error: 1.0186 - val_loss: 1.0127 - val_mean_squared_error: 1.0127\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02155 to 1.01274, saving model to temp/e424\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0128 - mean_squared_error: 1.0128 - val_loss: 1.0164 - val_mean_squared_error: 1.0164\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01274\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0149 - mean_squared_error: 1.0149 - val_loss: 1.0094 - val_mean_squared_error: 1.0094\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01274 to 1.00942, saving model to temp/e424\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0129 - mean_squared_error: 1.0129 - val_loss: 1.0368 - val_mean_squared_error: 1.0368\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00942\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0118 - mean_squared_error: 1.0118 - val_loss: 1.0018 - val_mean_squared_error: 1.0018\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00942 to 1.00180, saving model to temp/e424\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0027 - val_mean_squared_error: 1.0027\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00180\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 1.0022 - val_mean_squared_error: 1.0022\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00180\n",
      "Epoch 00008: early stopping\n",
      "temp/e425\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.0949 - mean_squared_error: 1.0949 - val_loss: 1.0352 - val_mean_squared_error: 1.0352\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03522, saving model to temp/e425\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0179 - mean_squared_error: 1.0179 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03522 to 1.00533, saving model to temp/e425\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0137 - mean_squared_error: 1.0137 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00533\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0123 - mean_squared_error: 1.0123 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00533 to 1.00526, saving model to temp/e425\n",
      "Epoch 00004: early stopping\n",
      "temp/e426\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.0993 - mean_squared_error: 1.0993 - val_loss: 1.0198 - val_mean_squared_error: 1.0198\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01979, saving model to temp/e426\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0194 - mean_squared_error: 1.0194 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01979 to 1.00882, saving model to temp/e426\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0159 - mean_squared_error: 1.0159 - val_loss: 1.0124 - val_mean_squared_error: 1.0124\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00882\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0112 - mean_squared_error: 1.0112 - val_loss: 1.0100 - val_mean_squared_error: 1.0100\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00882\n",
      "Epoch 00004: early stopping\n",
      "temp/e427\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.0952 - mean_squared_error: 1.0952 - val_loss: 1.0099 - val_mean_squared_error: 1.0099\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00987, saving model to temp/e427\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0188 - mean_squared_error: 1.0188 - val_loss: 1.0065 - val_mean_squared_error: 1.0065\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.00987 to 1.00648, saving model to temp/e427\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0148 - mean_squared_error: 1.0148 - val_loss: 1.0090 - val_mean_squared_error: 1.0090\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00648\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0129 - mean_squared_error: 1.0129 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00648 to 1.00513, saving model to temp/e427\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00513 to 1.00239, saving model to temp/e427\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 0.9999 - val_mean_squared_error: 0.9999\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00239 to 0.99989, saving model to temp/e427\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99989\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0079 - mean_squared_error: 1.0079 - val_loss: 1.0039 - val_mean_squared_error: 1.0039\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99989\n",
      "Epoch 00008: early stopping\n",
      "temp/e428\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 1.0950 - mean_squared_error: 1.0950 - val_loss: 1.0316 - val_mean_squared_error: 1.0316\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03159, saving model to temp/e428\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0212 - mean_squared_error: 1.0212 - val_loss: 1.0126 - val_mean_squared_error: 1.0126\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03159 to 1.01256, saving model to temp/e428\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 1.0397 - val_mean_squared_error: 1.0397\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01256\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0169 - mean_squared_error: 1.0169 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01256 to 1.00616, saving model to temp/e428\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00616 to 1.00545, saving model to temp/e428\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00545\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0069 - mean_squared_error: 1.0069 - val_loss: 1.0032 - val_mean_squared_error: 1.0032\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00545 to 1.00319, saving model to temp/e428\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 1.0007 - val_mean_squared_error: 1.0007\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00319 to 1.00074, saving model to temp/e428\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0073 - mean_squared_error: 1.0073 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00074\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0037 - mean_squared_error: 1.0037 - val_loss: 0.9990 - val_mean_squared_error: 0.9990\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.00074 to 0.99902, saving model to temp/e428\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0056 - mean_squared_error: 1.0056 - val_loss: 0.9998 - val_mean_squared_error: 0.9998\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99902\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0051 - mean_squared_error: 1.0051 - val_loss: 1.0097 - val_mean_squared_error: 1.0097\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.99902\n",
      "Epoch 00012: early stopping\n",
      "temp/e429\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1003 - mean_squared_error: 1.1003 - val_loss: 1.0245 - val_mean_squared_error: 1.0245\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02449, saving model to temp/e429\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0188 - mean_squared_error: 1.0188 - val_loss: 1.0184 - val_mean_squared_error: 1.0184\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02449 to 1.01838, saving model to temp/e429\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0179 - mean_squared_error: 1.0179 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01838 to 1.00491, saving model to temp/e429\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00491 to 1.00404, saving model to temp/e429\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0119 - mean_squared_error: 1.0119 - val_loss: 1.0050 - val_mean_squared_error: 1.0050\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00404\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0115 - val_mean_squared_error: 1.0115\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00404\n",
      "Epoch 00006: early stopping\n",
      "temp/e430\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.1007 - mean_squared_error: 1.1007 - val_loss: 1.0278 - val_mean_squared_error: 1.0278\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02781, saving model to temp/e430\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0194 - mean_squared_error: 1.0194 - val_loss: 1.0072 - val_mean_squared_error: 1.0072\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02781 to 1.00723, saving model to temp/e430\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0171 - mean_squared_error: 1.0171 - val_loss: 1.0173 - val_mean_squared_error: 1.0173\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00723\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0149 - mean_squared_error: 1.0149 - val_loss: 0.9990 - val_mean_squared_error: 0.9990\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00723 to 0.99903, saving model to temp/e430\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 1.0135 - val_mean_squared_error: 1.0135\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.99903\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0117 - val_mean_squared_error: 1.0117\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99903\n",
      "Epoch 00006: early stopping\n",
      "temp/e431\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.0949 - mean_squared_error: 1.0949 - val_loss: 1.0103 - val_mean_squared_error: 1.0103\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01030, saving model to temp/e431\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0216 - mean_squared_error: 1.0216 - val_loss: 1.0252 - val_mean_squared_error: 1.0252\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01030\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0172 - mean_squared_error: 1.0172 - val_loss: 1.0142 - val_mean_squared_error: 1.0142\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01030\n",
      "Epoch 00003: early stopping\n",
      "temp/e432\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1033 - mean_squared_error: 1.1033 - val_loss: 1.0096 - val_mean_squared_error: 1.0096\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00956, saving model to temp/e432\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0200 - mean_squared_error: 1.0200 - val_loss: 1.0109 - val_mean_squared_error: 1.0109\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.00956\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0260 - val_mean_squared_error: 1.0260\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00956\n",
      "Epoch 00003: early stopping\n",
      "temp/e433\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.0983 - mean_squared_error: 1.0983 - val_loss: 1.0134 - val_mean_squared_error: 1.0134\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01345, saving model to temp/e433\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0230 - mean_squared_error: 1.0230 - val_loss: 1.0183 - val_mean_squared_error: 1.0183\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01345\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0147 - mean_squared_error: 1.0147 - val_loss: 1.0048 - val_mean_squared_error: 1.0048\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01345 to 1.00482, saving model to temp/e433\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0077 - val_mean_squared_error: 1.0077\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00482\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0081 - mean_squared_error: 1.0081 - val_loss: 1.0101 - val_mean_squared_error: 1.0101\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00482\n",
      "Epoch 00005: early stopping\n",
      "temp/e434\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.1031 - mean_squared_error: 1.1031 - val_loss: 1.0132 - val_mean_squared_error: 1.0132\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01325, saving model to temp/e434\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 126us/step - loss: 1.0216 - mean_squared_error: 1.0216 - val_loss: 1.0172 - val_mean_squared_error: 1.0172\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01325\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s 124us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0064 - val_mean_squared_error: 1.0064\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01325 to 1.00641, saving model to temp/e434\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 126us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 1.0060 - val_mean_squared_error: 1.0060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss improved from 1.00641 to 1.00601, saving model to temp/e434\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0008 - val_mean_squared_error: 1.0008\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00601 to 1.00080, saving model to temp/e434\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 123us/step - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00080\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 1.0071 - mean_squared_error: 1.0071 - val_loss: 1.0025 - val_mean_squared_error: 1.0025\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00080\n",
      "Epoch 00007: early stopping\n",
      "temp/e435\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 1.1021 - mean_squared_error: 1.1021 - val_loss: 1.0171 - val_mean_squared_error: 1.0171\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01713, saving model to temp/e435\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 126us/step - loss: 1.0217 - mean_squared_error: 1.0217 - val_loss: 1.0084 - val_mean_squared_error: 1.0084\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01713 to 1.00839, saving model to temp/e435\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 126us/step - loss: 1.0140 - mean_squared_error: 1.0140 - val_loss: 1.0078 - val_mean_squared_error: 1.0078\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00839 to 1.00776, saving model to temp/e435\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 125us/step - loss: 1.0147 - mean_squared_error: 1.0147 - val_loss: 1.0064 - val_mean_squared_error: 1.0064\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00776 to 1.00635, saving model to temp/e435\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00635\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 125us/step - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 1.0099 - val_mean_squared_error: 1.0099\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00635\n",
      "Epoch 00006: early stopping\n",
      "temp/e436\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 142us/step - loss: 1.1054 - mean_squared_error: 1.1054 - val_loss: 1.0148 - val_mean_squared_error: 1.0148\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01478, saving model to temp/e436\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 125us/step - loss: 1.0209 - mean_squared_error: 1.0209 - val_loss: 1.0146 - val_mean_squared_error: 1.0146\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01478 to 1.01462, saving model to temp/e436\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 1.0146 - mean_squared_error: 1.0146 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01462 to 1.00980, saving model to temp/e436\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0107 - mean_squared_error: 1.0107 - val_loss: 1.0077 - val_mean_squared_error: 1.0077\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00980 to 1.00769, saving model to temp/e436\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 1.0046 - val_mean_squared_error: 1.0046\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00769 to 1.00459, saving model to temp/e436\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0052 - mean_squared_error: 1.0052 - val_loss: 1.0041 - val_mean_squared_error: 1.0041\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00459 to 1.00412, saving model to temp/e436\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00412\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 1.0016 - val_mean_squared_error: 1.0016\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00412 to 1.00157, saving model to temp/e436\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0071 - mean_squared_error: 1.0071 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00157\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0044 - mean_squared_error: 1.0044 - val_loss: 0.9970 - val_mean_squared_error: 0.9970\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.00157 to 0.99703, saving model to temp/e436\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0056 - mean_squared_error: 1.0056 - val_loss: 0.9970 - val_mean_squared_error: 0.9970\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.99703 to 0.99696, saving model to temp/e436\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 1.0034 - mean_squared_error: 1.0034 - val_loss: 1.0025 - val_mean_squared_error: 1.0025\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.99696\n",
      "Epoch 00012: early stopping\n",
      "temp/e437\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 1.1039 - mean_squared_error: 1.1039 - val_loss: 1.0217 - val_mean_squared_error: 1.0217\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02174, saving model to temp/e437\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0202 - mean_squared_error: 1.0202 - val_loss: 1.0295 - val_mean_squared_error: 1.0295\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.02174\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0182 - mean_squared_error: 1.0182 - val_loss: 1.0152 - val_mean_squared_error: 1.0152\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02174 to 1.01524, saving model to temp/e437\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0105 - mean_squared_error: 1.0105 - val_loss: 1.0106 - val_mean_squared_error: 1.0106\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01524 to 1.01061, saving model to temp/e437\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0018 - val_mean_squared_error: 1.0018\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01061 to 1.00176, saving model to temp/e437\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0112 - mean_squared_error: 1.0112 - val_loss: 1.0042 - val_mean_squared_error: 1.0042\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00176\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0005 - val_mean_squared_error: 1.0005\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00176 to 1.00052, saving model to temp/e437\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 1.0066 - mean_squared_error: 1.0066 - val_loss: 1.0154 - val_mean_squared_error: 1.0154\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00052\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00052\n",
      "Epoch 00009: early stopping\n",
      "temp/e438\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 141us/step - loss: 1.1030 - mean_squared_error: 1.1030 - val_loss: 1.0132 - val_mean_squared_error: 1.0132\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01318, saving model to temp/e438\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 1.0185 - mean_squared_error: 1.0185 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01318 to 1.00796, saving model to temp/e438\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0145 - mean_squared_error: 1.0145 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00796 to 1.00400, saving model to temp/e438\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s 123us/step - loss: 1.0127 - mean_squared_error: 1.0127 - val_loss: 1.0025 - val_mean_squared_error: 1.0025\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00400 to 1.00250, saving model to temp/e438\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s 125us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0001 - val_mean_squared_error: 1.0001\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00250 to 1.00011, saving model to temp/e438\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s 125us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0153 - val_mean_squared_error: 1.0153\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00011\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s 123us/step - loss: 1.0109 - mean_squared_error: 1.0109 - val_loss: 1.0043 - val_mean_squared_error: 1.0043\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00011\n",
      "Epoch 00007: early stopping\n",
      "temp/e439\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 1.1030 - mean_squared_error: 1.1030 - val_loss: 1.0150 - val_mean_squared_error: 1.0150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01500, saving model to temp/e439\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s 124us/step - loss: 1.0171 - mean_squared_error: 1.0171 - val_loss: 1.0108 - val_mean_squared_error: 1.0108\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01500 to 1.01082, saving model to temp/e439\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 1.0114 - mean_squared_error: 1.0114 - val_loss: 1.0067 - val_mean_squared_error: 1.0067\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01082 to 1.00671, saving model to temp/e439\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0101 - val_mean_squared_error: 1.0101\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00671\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00671 to 1.00467, saving model to temp/e439\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0096 - val_mean_squared_error: 1.0096\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00467\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.0072 - mean_squared_error: 1.0072 - val_loss: 0.9993 - val_mean_squared_error: 0.9993\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00467 to 0.99929, saving model to temp/e439\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 1.0079 - mean_squared_error: 1.0079 - val_loss: 0.9941 - val_mean_squared_error: 0.9941\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.99929 to 0.99411, saving model to temp/e439\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 1.0054 - mean_squared_error: 1.0054 - val_loss: 1.0094 - val_mean_squared_error: 1.0094\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99411\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 1.0045 - mean_squared_error: 1.0045 - val_loss: 1.0029 - val_mean_squared_error: 1.0029\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99411\n",
      "Epoch 00010: early stopping\n",
      "temp/e440\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1012 - mean_squared_error: 1.1012 - val_loss: 1.0136 - val_mean_squared_error: 1.0136\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01357, saving model to temp/e440\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0182 - mean_squared_error: 1.0182 - val_loss: 1.0163 - val_mean_squared_error: 1.0163\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01357\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0164 - mean_squared_error: 1.0164 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01357 to 1.00403, saving model to temp/e440\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0125 - mean_squared_error: 1.0125 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00403 to 1.00379, saving model to temp/e440\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0142 - val_mean_squared_error: 1.0142\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00379\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0111 - mean_squared_error: 1.0111 - val_loss: 1.0149 - val_mean_squared_error: 1.0149\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00379\n",
      "Epoch 00006: early stopping\n",
      "temp/e441\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.1016 - mean_squared_error: 1.1016 - val_loss: 1.0123 - val_mean_squared_error: 1.0123\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01228, saving model to temp/e441\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0226 - mean_squared_error: 1.0226 - val_loss: 1.0097 - val_mean_squared_error: 1.0097\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01228 to 1.00971, saving model to temp/e441\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0129 - mean_squared_error: 1.0129 - val_loss: 1.0187 - val_mean_squared_error: 1.0187\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00971\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00971 to 1.00616, saving model to temp/e441\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 125us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0058 - val_mean_squared_error: 1.0058\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00616 to 1.00578, saving model to temp/e441\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0123 - val_mean_squared_error: 1.0123\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00578\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0079 - mean_squared_error: 1.0079 - val_loss: 1.0025 - val_mean_squared_error: 1.0025\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00578 to 1.00246, saving model to temp/e441\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s 125us/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00246\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s 124us/step - loss: 1.0073 - mean_squared_error: 1.0073 - val_loss: 1.0005 - val_mean_squared_error: 1.0005\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00246 to 1.00047, saving model to temp/e441\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0059 - mean_squared_error: 1.0059 - val_loss: 1.0048 - val_mean_squared_error: 1.0048\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.00047\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 2s 124us/step - loss: 1.0043 - mean_squared_error: 1.0043 - val_loss: 1.0007 - val_mean_squared_error: 1.0007\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.00047\n",
      "Epoch 00011: early stopping\n",
      "temp/e442\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.0947 - mean_squared_error: 1.0947 - val_loss: 1.0340 - val_mean_squared_error: 1.0340\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03403, saving model to temp/e442\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0211 - mean_squared_error: 1.0211 - val_loss: 1.0074 - val_mean_squared_error: 1.0074\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03403 to 1.00742, saving model to temp/e442\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0139 - mean_squared_error: 1.0139 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00742\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0042 - val_mean_squared_error: 1.0042\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00742 to 1.00418, saving model to temp/e442\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 1.0170 - val_mean_squared_error: 1.0170\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00418\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0089 - mean_squared_error: 1.0089 - val_loss: 1.0020 - val_mean_squared_error: 1.0020\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00418 to 1.00195, saving model to temp/e442\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0065 - mean_squared_error: 1.0065 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00195\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0119 - mean_squared_error: 1.0119 - val_loss: 0.9997 - val_mean_squared_error: 0.9997\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00195 to 0.99968, saving model to temp/e442\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0081 - mean_squared_error: 1.0081 - val_loss: 0.9971 - val_mean_squared_error: 0.9971\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.99968 to 0.99712, saving model to temp/e442\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0042 - mean_squared_error: 1.0042 - val_loss: 0.9999 - val_mean_squared_error: 0.9999\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99712\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0052 - mean_squared_error: 1.0052 - val_loss: 0.9953 - val_mean_squared_error: 0.9953\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.99712 to 0.99530, saving model to temp/e442\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 1.0110 - val_mean_squared_error: 1.0110\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.99530\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0056 - mean_squared_error: 1.0056 - val_loss: 0.9984 - val_mean_squared_error: 0.9984\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.99530\n",
      "Epoch 00013: early stopping\n",
      "temp/e443\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 1.0972 - mean_squared_error: 1.0972 - val_loss: 1.0114 - val_mean_squared_error: 1.0114\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01142, saving model to temp/e443\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0188 - mean_squared_error: 1.0188 - val_loss: 1.0151 - val_mean_squared_error: 1.0151\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01142\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0201 - val_mean_squared_error: 1.0201\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01142\n",
      "Epoch 00003: early stopping\n",
      "temp/e444\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 1.1013 - mean_squared_error: 1.1013 - val_loss: 1.0176 - val_mean_squared_error: 1.0176\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01760, saving model to temp/e444\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0170 - mean_squared_error: 1.0170 - val_loss: 1.0126 - val_mean_squared_error: 1.0126\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01760 to 1.01264, saving model to temp/e444\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0167 - mean_squared_error: 1.0167 - val_loss: 1.0071 - val_mean_squared_error: 1.0071\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01264 to 1.00707, saving model to temp/e444\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0200 - val_mean_squared_error: 1.0200\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00707\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0129 - mean_squared_error: 1.0129 - val_loss: 1.0042 - val_mean_squared_error: 1.0042\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00707 to 1.00424, saving model to temp/e444\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0070 - mean_squared_error: 1.0070 - val_loss: 1.0048 - val_mean_squared_error: 1.0048\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00424\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0098 - mean_squared_error: 1.0098 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00424\n",
      "Epoch 00007: early stopping\n",
      "temp/e445\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1010 - mean_squared_error: 1.1010 - val_loss: 1.0183 - val_mean_squared_error: 1.0183\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01828, saving model to temp/e445\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0206 - mean_squared_error: 1.0206 - val_loss: 1.0111 - val_mean_squared_error: 1.0111\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01828 to 1.01112, saving model to temp/e445\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0177 - mean_squared_error: 1.0177 - val_loss: 1.0162 - val_mean_squared_error: 1.0162\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01112\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0155 - mean_squared_error: 1.0155 - val_loss: 0.9998 - val_mean_squared_error: 0.9998\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01112 to 0.99984, saving model to temp/e445\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0076 - val_mean_squared_error: 1.0076\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.99984\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0098 - mean_squared_error: 1.0098 - val_loss: 1.0045 - val_mean_squared_error: 1.0045\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99984\n",
      "Epoch 00006: early stopping\n",
      "temp/e446\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.0959 - mean_squared_error: 1.0959 - val_loss: 1.0149 - val_mean_squared_error: 1.0149\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01487, saving model to temp/e446\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0211 - mean_squared_error: 1.0211 - val_loss: 1.0103 - val_mean_squared_error: 1.0103\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01487 to 1.01034, saving model to temp/e446\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0160 - mean_squared_error: 1.0160 - val_loss: 1.0110 - val_mean_squared_error: 1.0110\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01034\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01034 to 1.00752, saving model to temp/e446\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0107 - mean_squared_error: 1.0107 - val_loss: 1.0122 - val_mean_squared_error: 1.0122\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00752\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0085 - val_mean_squared_error: 1.0085\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00752\n",
      "Epoch 00006: early stopping\n",
      "temp/e447\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 1.1016 - mean_squared_error: 1.1016 - val_loss: 1.0202 - val_mean_squared_error: 1.0202\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02020, saving model to temp/e447\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0224 - mean_squared_error: 1.0224 - val_loss: 1.0124 - val_mean_squared_error: 1.0124\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02020 to 1.01239, saving model to temp/e447\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0182 - mean_squared_error: 1.0182 - val_loss: 1.0110 - val_mean_squared_error: 1.0110\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01239 to 1.01097, saving model to temp/e447\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0158 - mean_squared_error: 1.0158 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01097 to 1.00880, saving model to temp/e447\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0012 - val_mean_squared_error: 1.0012\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00880 to 1.00121, saving model to temp/e447\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0018 - val_mean_squared_error: 1.0018\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00121\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0072 - mean_squared_error: 1.0072 - val_loss: 1.0029 - val_mean_squared_error: 1.0029\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00121\n",
      "Epoch 00007: early stopping\n",
      "temp/e448\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1071 - mean_squared_error: 1.1071 - val_loss: 1.0124 - val_mean_squared_error: 1.0124\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01243, saving model to temp/e448\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0208 - mean_squared_error: 1.0208 - val_loss: 1.0199 - val_mean_squared_error: 1.0199\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01243\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0190 - mean_squared_error: 1.0190 - val_loss: 1.0036 - val_mean_squared_error: 1.0036\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01243 to 1.00363, saving model to temp/e448\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0082 - mean_squared_error: 1.0082 - val_loss: 1.0105 - val_mean_squared_error: 1.0105\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00363\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 1.0076 - val_mean_squared_error: 1.0076\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00363\n",
      "Epoch 00005: early stopping\n",
      "temp/e449\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.0973 - mean_squared_error: 1.0973 - val_loss: 1.0146 - val_mean_squared_error: 1.0146\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01463, saving model to temp/e449\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0193 - mean_squared_error: 1.0193 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01463 to 1.00926, saving model to temp/e449\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0139 - mean_squared_error: 1.0139 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00926 to 1.00328, saving model to temp/e449\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0178 - mean_squared_error: 1.0178 - val_loss: 1.0029 - val_mean_squared_error: 1.0029\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00328 to 1.00286, saving model to temp/e449\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 1.0164 - val_mean_squared_error: 1.0164\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00286\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.0017 - val_mean_squared_error: 1.0017\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00286 to 1.00174, saving model to temp/e449\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 0.9996 - val_mean_squared_error: 0.9996\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00174 to 0.99963, saving model to temp/e449\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0081 - mean_squared_error: 1.0081 - val_loss: 0.9981 - val_mean_squared_error: 0.9981\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.99963 to 0.99807, saving model to temp/e449\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0073 - mean_squared_error: 1.0073 - val_loss: 0.9975 - val_mean_squared_error: 0.9975\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.99807 to 0.99748, saving model to temp/e449\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0065 - mean_squared_error: 1.0065 - val_loss: 1.0029 - val_mean_squared_error: 1.0029\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99748\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0031 - mean_squared_error: 1.0031 - val_loss: 1.0158 - val_mean_squared_error: 1.0158\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99748\n",
      "Epoch 00011: early stopping\n",
      "temp/e450\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.1048 - mean_squared_error: 1.1048 - val_loss: 1.0283 - val_mean_squared_error: 1.0283\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02828, saving model to temp/e450\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0180 - mean_squared_error: 1.0180 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02828 to 1.00857, saving model to temp/e450\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0182 - mean_squared_error: 1.0182 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00857 to 1.00543, saving model to temp/e450\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00543 to 1.00533, saving model to temp/e450\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00533 to 1.00522, saving model to temp/e450\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0062 - mean_squared_error: 1.0062 - val_loss: 0.9992 - val_mean_squared_error: 0.9992\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00522 to 0.99917, saving model to temp/e450\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.0022 - val_mean_squared_error: 1.0022\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99917\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99917\n",
      "Epoch 00008: early stopping\n",
      "temp/e451\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.0948 - mean_squared_error: 1.0948 - val_loss: 1.0199 - val_mean_squared_error: 1.0199\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01992, saving model to temp/e451\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0212 - mean_squared_error: 1.0212 - val_loss: 1.0079 - val_mean_squared_error: 1.0079\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01992 to 1.00792, saving model to temp/e451\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00792\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0131 - mean_squared_error: 1.0131 - val_loss: 1.0076 - val_mean_squared_error: 1.0076\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00792 to 1.00760, saving model to temp/e451\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 1.0102 - val_mean_squared_error: 1.0102\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00760\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0056 - mean_squared_error: 1.0056 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00760 to 1.00495, saving model to temp/e451\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00495\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0020 - val_mean_squared_error: 1.0020\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00495 to 1.00203, saving model to temp/e451\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0069 - mean_squared_error: 1.0069 - val_loss: 1.0013 - val_mean_squared_error: 1.0013\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00203 to 1.00132, saving model to temp/e451\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0046 - mean_squared_error: 1.0046 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.00132\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0057 - mean_squared_error: 1.0057 - val_loss: 0.9981 - val_mean_squared_error: 0.9981\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.00132 to 0.99806, saving model to temp/e451\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0055 - mean_squared_error: 1.0055 - val_loss: 0.9984 - val_mean_squared_error: 0.9984\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.99806\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0042 - mean_squared_error: 1.0042 - val_loss: 1.0015 - val_mean_squared_error: 1.0015\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.99806\n",
      "Epoch 00013: early stopping\n",
      "temp/e452\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.0941 - mean_squared_error: 1.0941 - val_loss: 1.0162 - val_mean_squared_error: 1.0162\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01619, saving model to temp/e452\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0204 - mean_squared_error: 1.0204 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01619 to 1.00925, saving model to temp/e452\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0165 - mean_squared_error: 1.0165 - val_loss: 1.0077 - val_mean_squared_error: 1.0077\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00925 to 1.00765, saving model to temp/e452\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0134 - mean_squared_error: 1.0134 - val_loss: 1.0043 - val_mean_squared_error: 1.0043\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00765 to 1.00429, saving model to temp/e452\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00429\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 1.0022 - val_mean_squared_error: 1.0022\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00429 to 1.00224, saving model to temp/e452\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 0.9990 - val_mean_squared_error: 0.9990\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00224 to 0.99897, saving model to temp/e452\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 1.0021 - val_mean_squared_error: 1.0021\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99897\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0070 - mean_squared_error: 1.0070 - val_loss: 1.0012 - val_mean_squared_error: 1.0012\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99897\n",
      "Epoch 00009: early stopping\n",
      "temp/e453\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.0979 - mean_squared_error: 1.0979 - val_loss: 1.0174 - val_mean_squared_error: 1.0174\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01740, saving model to temp/e453\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 1.0138 - val_mean_squared_error: 1.0138\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01740 to 1.01379, saving model to temp/e453\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0158 - mean_squared_error: 1.0158 - val_loss: 1.0100 - val_mean_squared_error: 1.0100\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01379 to 1.01001, saving model to temp/e453\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0109 - mean_squared_error: 1.0109 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01001 to 1.00512, saving model to temp/e453\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0111 - mean_squared_error: 1.0111 - val_loss: 1.0029 - val_mean_squared_error: 1.0029\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00512 to 1.00289, saving model to temp/e453\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00289\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00289\n",
      "Epoch 00007: early stopping\n",
      "temp/e454\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.1034 - mean_squared_error: 1.1034 - val_loss: 1.0143 - val_mean_squared_error: 1.0143\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01433, saving model to temp/e454\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0209 - mean_squared_error: 1.0209 - val_loss: 1.0145 - val_mean_squared_error: 1.0145\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01433\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0157 - mean_squared_error: 1.0157 - val_loss: 1.0116 - val_mean_squared_error: 1.0116\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01433 to 1.01160, saving model to temp/e454\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0140 - mean_squared_error: 1.0140 - val_loss: 1.0239 - val_mean_squared_error: 1.0239\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.01160\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0118 - val_mean_squared_error: 1.0118\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.01160\n",
      "Epoch 00005: early stopping\n",
      "temp/e455\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.1125 - mean_squared_error: 1.1125 - val_loss: 1.0258 - val_mean_squared_error: 1.0258\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02580, saving model to temp/e455\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0199 - mean_squared_error: 1.0199 - val_loss: 1.0163 - val_mean_squared_error: 1.0163\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02580 to 1.01630, saving model to temp/e455\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0154 - mean_squared_error: 1.0154 - val_loss: 1.0174 - val_mean_squared_error: 1.0174\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01630\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0027 - val_mean_squared_error: 1.0027\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01630 to 1.00266, saving model to temp/e455\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0079 - val_mean_squared_error: 1.0079\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00266\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0086 - mean_squared_error: 1.0086 - val_loss: 1.0041 - val_mean_squared_error: 1.0041\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00266\n",
      "Epoch 00006: early stopping\n",
      "temp/e456\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.1044 - mean_squared_error: 1.1044 - val_loss: 1.0266 - val_mean_squared_error: 1.0266\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02663, saving model to temp/e456\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0215 - mean_squared_error: 1.0215 - val_loss: 1.0188 - val_mean_squared_error: 1.0188\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02663 to 1.01884, saving model to temp/e456\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0119 - mean_squared_error: 1.0119 - val_loss: 1.0060 - val_mean_squared_error: 1.0060\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01884 to 1.00601, saving model to temp/e456\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0136 - mean_squared_error: 1.0136 - val_loss: 1.0032 - val_mean_squared_error: 1.0032\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00601 to 1.00316, saving model to temp/e456\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0079 - val_mean_squared_error: 1.0079\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00316\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0002 - val_mean_squared_error: 1.0002\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00316 to 1.00019, saving model to temp/e456\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0086 - mean_squared_error: 1.0086 - val_loss: 1.0016 - val_mean_squared_error: 1.0016\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00019\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0033 - mean_squared_error: 1.0033 - val_loss: 1.0083 - val_mean_squared_error: 1.0083\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00019\n",
      "Epoch 00008: early stopping\n",
      "temp/e457\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1028 - mean_squared_error: 1.1028 - val_loss: 1.0135 - val_mean_squared_error: 1.0135\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01353, saving model to temp/e457\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0191 - mean_squared_error: 1.0191 - val_loss: 1.0102 - val_mean_squared_error: 1.0102\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01353 to 1.01015, saving model to temp/e457\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0190 - val_mean_squared_error: 1.0190\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01015\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01015 to 1.00542, saving model to temp/e457\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0021 - val_mean_squared_error: 1.0021\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00542 to 1.00210, saving model to temp/e457\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0079 - mean_squared_error: 1.0079 - val_loss: 1.0220 - val_mean_squared_error: 1.0220\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00210\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0028 - val_mean_squared_error: 1.0028\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00210\n",
      "Epoch 00007: early stopping\n",
      "temp/e458\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.1018 - mean_squared_error: 1.1018 - val_loss: 1.0225 - val_mean_squared_error: 1.0225\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02245, saving model to temp/e458\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0210 - mean_squared_error: 1.0210 - val_loss: 1.0158 - val_mean_squared_error: 1.0158\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02245 to 1.01582, saving model to temp/e458\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0144 - mean_squared_error: 1.0144 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01582 to 1.00978, saving model to temp/e458\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0307 - val_mean_squared_error: 1.0307\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00978\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00978 to 1.00380, saving model to temp/e458\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0016 - val_mean_squared_error: 1.0016\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00380 to 1.00158, saving model to temp/e458\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0083 - mean_squared_error: 1.0083 - val_loss: 1.0114 - val_mean_squared_error: 1.0114\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00158\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 0.9982 - val_mean_squared_error: 0.9982\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00158 to 0.99822, saving model to temp/e458\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0058 - mean_squared_error: 1.0058 - val_loss: 1.0048 - val_mean_squared_error: 1.0048\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99822\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0065 - mean_squared_error: 1.0065 - val_loss: 0.9963 - val_mean_squared_error: 0.9963\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.99822 to 0.99632, saving model to temp/e458\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0070 - mean_squared_error: 1.0070 - val_loss: 0.9973 - val_mean_squared_error: 0.9973\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99632\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0042 - mean_squared_error: 1.0042 - val_loss: 1.0025 - val_mean_squared_error: 1.0025\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.99632\n",
      "Epoch 00012: early stopping\n",
      "temp/e459\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1035 - mean_squared_error: 1.1035 - val_loss: 1.0140 - val_mean_squared_error: 1.0140\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01399, saving model to temp/e459\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0180 - mean_squared_error: 1.0180 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01399 to 1.00927, saving model to temp/e459\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0155 - mean_squared_error: 1.0155 - val_loss: 1.0012 - val_mean_squared_error: 1.0012\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00927 to 1.00116, saving model to temp/e459\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0119 - mean_squared_error: 1.0119 - val_loss: 1.0148 - val_mean_squared_error: 1.0148\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00116\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 0.9994 - val_mean_squared_error: 0.9994\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00116 to 0.99941, saving model to temp/e459\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 1.0039 - val_mean_squared_error: 1.0039\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99941\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0081 - mean_squared_error: 1.0081 - val_loss: 0.9997 - val_mean_squared_error: 0.9997\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99941\n",
      "Epoch 00007: early stopping\n",
      "temp/e460\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1057 - mean_squared_error: 1.1057 - val_loss: 1.0158 - val_mean_squared_error: 1.0158\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01582, saving model to temp/e460\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0213 - mean_squared_error: 1.0213 - val_loss: 1.0181 - val_mean_squared_error: 1.0181\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01582\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0183 - mean_squared_error: 1.0183 - val_loss: 1.0139 - val_mean_squared_error: 1.0139\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01582 to 1.01394, saving model to temp/e460\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0113 - val_mean_squared_error: 1.0113\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01394 to 1.01126, saving model to temp/e460\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0105 - mean_squared_error: 1.0105 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01126 to 1.00572, saving model to temp/e460\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0089 - mean_squared_error: 1.0089 - val_loss: 1.0129 - val_mean_squared_error: 1.0129\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00572\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0065 - mean_squared_error: 1.0065 - val_loss: 0.9984 - val_mean_squared_error: 0.9984\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00572 to 0.99844, saving model to temp/e460\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0078 - mean_squared_error: 1.0078 - val_loss: 0.9962 - val_mean_squared_error: 0.9962\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.99844 to 0.99621, saving model to temp/e460\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0056 - mean_squared_error: 1.0056 - val_loss: 1.0039 - val_mean_squared_error: 1.0039\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99621\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 0.9976 - val_mean_squared_error: 0.9976\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99621\n",
      "Epoch 00010: early stopping\n",
      "temp/e461\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1003 - mean_squared_error: 1.1003 - val_loss: 1.0100 - val_mean_squared_error: 1.0100\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00997, saving model to temp/e461\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0157 - mean_squared_error: 1.0157 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.00997 to 1.00379, saving model to temp/e461\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0178 - mean_squared_error: 1.0178 - val_loss: 1.0021 - val_mean_squared_error: 1.0021\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00379 to 1.00205, saving model to temp/e461\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00205\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0086 - mean_squared_error: 1.0086 - val_loss: 1.0025 - val_mean_squared_error: 1.0025\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00205\n",
      "Epoch 00005: early stopping\n",
      "temp/e462\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.0994 - mean_squared_error: 1.0994 - val_loss: 1.0165 - val_mean_squared_error: 1.0165\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01648, saving model to temp/e462\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0158 - mean_squared_error: 1.0158 - val_loss: 1.0205 - val_mean_squared_error: 1.0205\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01648\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0156 - mean_squared_error: 1.0156 - val_loss: 1.0082 - val_mean_squared_error: 1.0082\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01648 to 1.00821, saving model to temp/e462\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0045 - val_mean_squared_error: 1.0045\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00821 to 1.00446, saving model to temp/e462\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 1.0084 - val_mean_squared_error: 1.0084\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00446\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0064 - mean_squared_error: 1.0064 - val_loss: 1.0070 - val_mean_squared_error: 1.0070\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00446\n",
      "Epoch 00006: early stopping\n",
      "temp/e463\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.0892 - mean_squared_error: 1.0892 - val_loss: 1.0182 - val_mean_squared_error: 1.0182\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01824, saving model to temp/e463\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0156 - mean_squared_error: 1.0156 - val_loss: 1.0240 - val_mean_squared_error: 1.0240\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01824\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0164 - mean_squared_error: 1.0164 - val_loss: 1.0099 - val_mean_squared_error: 1.0099\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01824 to 1.00992, saving model to temp/e463\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0160 - mean_squared_error: 1.0160 - val_loss: 1.0028 - val_mean_squared_error: 1.0028\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00992 to 1.00284, saving model to temp/e463\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00284\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0048 - val_mean_squared_error: 1.0048\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00284\n",
      "Epoch 00006: early stopping\n",
      "temp/e464\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1060 - mean_squared_error: 1.1060 - val_loss: 1.0241 - val_mean_squared_error: 1.0241\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02410, saving model to temp/e464\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0148 - mean_squared_error: 1.0148 - val_loss: 1.0127 - val_mean_squared_error: 1.0127\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02410 to 1.01265, saving model to temp/e464\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0177 - mean_squared_error: 1.0177 - val_loss: 1.0104 - val_mean_squared_error: 1.0104\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01265 to 1.01038, saving model to temp/e464\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0079 - mean_squared_error: 1.0079 - val_loss: 1.0129 - val_mean_squared_error: 1.0129\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.01038\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0110 - val_mean_squared_error: 1.0110\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.01038\n",
      "Epoch 00005: early stopping\n",
      "temp/e465\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.0925 - mean_squared_error: 1.0925 - val_loss: 1.0164 - val_mean_squared_error: 1.0164\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01642, saving model to temp/e465\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0176 - mean_squared_error: 1.0176 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01642 to 1.00911, saving model to temp/e465\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0155 - mean_squared_error: 1.0155 - val_loss: 1.0090 - val_mean_squared_error: 1.0090\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00911 to 1.00902, saving model to temp/e465\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0054 - val_mean_squared_error: 1.0054\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00902 to 1.00537, saving model to temp/e465\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 1.0120 - val_mean_squared_error: 1.0120\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00537\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 1.0048 - val_mean_squared_error: 1.0048\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00537 to 1.00481, saving model to temp/e465\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0073 - mean_squared_error: 1.0073 - val_loss: 1.0015 - val_mean_squared_error: 1.0015\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00481 to 1.00150, saving model to temp/e465\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0072 - val_mean_squared_error: 1.0072\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00150\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0040 - mean_squared_error: 1.0040 - val_loss: 0.9983 - val_mean_squared_error: 0.9983\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.00150 to 0.99827, saving model to temp/e465\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0052 - mean_squared_error: 1.0052 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99827\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0062 - mean_squared_error: 1.0062 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99827\n",
      "Epoch 00011: early stopping\n",
      "temp/e466\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.1010 - mean_squared_error: 1.1010 - val_loss: 1.0144 - val_mean_squared_error: 1.0144\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01438, saving model to temp/e466\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0184 - mean_squared_error: 1.0184 - val_loss: 1.0111 - val_mean_squared_error: 1.0111\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01438 to 1.01111, saving model to temp/e466\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0180 - mean_squared_error: 1.0180 - val_loss: 1.0070 - val_mean_squared_error: 1.0070\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01111 to 1.00703, saving model to temp/e466\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0112 - mean_squared_error: 1.0112 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00703 to 1.00618, saving model to temp/e466\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0147 - mean_squared_error: 1.0147 - val_loss: 1.0149 - val_mean_squared_error: 1.0149\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00618\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0103 - mean_squared_error: 1.0103 - val_loss: 1.0188 - val_mean_squared_error: 1.0188\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00618\n",
      "Epoch 00006: early stopping\n",
      "temp/e467\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.0980 - mean_squared_error: 1.0980 - val_loss: 1.0118 - val_mean_squared_error: 1.0118\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01176, saving model to temp/e467\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0200 - mean_squared_error: 1.0200 - val_loss: 1.0245 - val_mean_squared_error: 1.0245\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01176\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0164 - mean_squared_error: 1.0164 - val_loss: 1.0124 - val_mean_squared_error: 1.0124\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01176\n",
      "Epoch 00003: early stopping\n",
      "temp/e468\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.1016 - mean_squared_error: 1.1016 - val_loss: 1.0160 - val_mean_squared_error: 1.0160\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01597, saving model to temp/e468\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0178 - mean_squared_error: 1.0178 - val_loss: 1.0120 - val_mean_squared_error: 1.0120\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01597 to 1.01200, saving model to temp/e468\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0182 - val_mean_squared_error: 1.0182\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01200\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0129 - mean_squared_error: 1.0129 - val_loss: 1.0051 - val_mean_squared_error: 1.0051\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01200 to 1.00506, saving model to temp/e468\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0005 - val_mean_squared_error: 1.0005\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00506 to 1.00048, saving model to temp/e468\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0068 - mean_squared_error: 1.0068 - val_loss: 1.0017 - val_mean_squared_error: 1.0017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00048\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0096 - mean_squared_error: 1.0096 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00048\n",
      "Epoch 00007: early stopping\n",
      "temp/e469\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.0987 - mean_squared_error: 1.0987 - val_loss: 1.0187 - val_mean_squared_error: 1.0187\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01874, saving model to temp/e469\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0245 - mean_squared_error: 1.0245 - val_loss: 1.0081 - val_mean_squared_error: 1.0081\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01874 to 1.00809, saving model to temp/e469\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0172 - mean_squared_error: 1.0172 - val_loss: 1.0085 - val_mean_squared_error: 1.0085\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00809\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0150 - mean_squared_error: 1.0150 - val_loss: 1.0090 - val_mean_squared_error: 1.0090\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00809\n",
      "Epoch 00004: early stopping\n",
      "temp/e470\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 1.1035 - mean_squared_error: 1.1035 - val_loss: 1.0296 - val_mean_squared_error: 1.0296\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02965, saving model to temp/e470\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0179 - mean_squared_error: 1.0179 - val_loss: 1.0116 - val_mean_squared_error: 1.0116\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02965 to 1.01162, saving model to temp/e470\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0182 - mean_squared_error: 1.0182 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01162 to 1.00589, saving model to temp/e470\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00589 to 1.00469, saving model to temp/e470\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0111 - mean_squared_error: 1.0111 - val_loss: 1.0081 - val_mean_squared_error: 1.0081\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00469\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0089 - mean_squared_error: 1.0089 - val_loss: 1.0130 - val_mean_squared_error: 1.0130\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00469\n",
      "Epoch 00006: early stopping\n",
      "temp/e471\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.0969 - mean_squared_error: 1.0969 - val_loss: 1.0160 - val_mean_squared_error: 1.0160\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01598, saving model to temp/e471\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0205 - mean_squared_error: 1.0205 - val_loss: 1.0236 - val_mean_squared_error: 1.0236\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01598\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0133 - mean_squared_error: 1.0133 - val_loss: 1.0209 - val_mean_squared_error: 1.0209\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01598\n",
      "Epoch 00003: early stopping\n",
      "temp/e472\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.1029 - mean_squared_error: 1.1029 - val_loss: 1.0158 - val_mean_squared_error: 1.0158\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01581, saving model to temp/e472\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0206 - mean_squared_error: 1.0206 - val_loss: 1.0125 - val_mean_squared_error: 1.0125\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01581 to 1.01254, saving model to temp/e472\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0147 - mean_squared_error: 1.0147 - val_loss: 1.0283 - val_mean_squared_error: 1.0283\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01254\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0146 - mean_squared_error: 1.0146 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01254 to 1.00621, saving model to temp/e472\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 1.0049 - val_mean_squared_error: 1.0049\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00621 to 1.00494, saving model to temp/e472\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0109 - mean_squared_error: 1.0109 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00494 to 1.00376, saving model to temp/e472\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.0006 - val_mean_squared_error: 1.0006\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00376 to 1.00060, saving model to temp/e472\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0051 - mean_squared_error: 1.0051 - val_loss: 0.9971 - val_mean_squared_error: 0.9971\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00060 to 0.99715, saving model to temp/e472\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0049 - mean_squared_error: 1.0049 - val_loss: 1.0011 - val_mean_squared_error: 1.0011\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99715\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0052 - mean_squared_error: 1.0052 - val_loss: 0.9995 - val_mean_squared_error: 0.9995\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99715\n",
      "Epoch 00010: early stopping\n",
      "temp/e473\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.1053 - mean_squared_error: 1.1053 - val_loss: 1.0277 - val_mean_squared_error: 1.0277\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02774, saving model to temp/e473\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0159 - mean_squared_error: 1.0159 - val_loss: 1.0117 - val_mean_squared_error: 1.0117\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02774 to 1.01171, saving model to temp/e473\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0185 - mean_squared_error: 1.0185 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01171 to 1.00911, saving model to temp/e473\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0154 - mean_squared_error: 1.0154 - val_loss: 1.0096 - val_mean_squared_error: 1.0096\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00911\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.0137 - val_mean_squared_error: 1.0137\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00911\n",
      "Epoch 00005: early stopping\n",
      "temp/e474\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.1000 - mean_squared_error: 1.1000 - val_loss: 1.0148 - val_mean_squared_error: 1.0148\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01476, saving model to temp/e474\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0223 - mean_squared_error: 1.0223 - val_loss: 1.0176 - val_mean_squared_error: 1.0176\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01476\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0153 - mean_squared_error: 1.0153 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01476 to 1.00804, saving model to temp/e474\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0146 - mean_squared_error: 1.0146 - val_loss: 1.0228 - val_mean_squared_error: 1.0228\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00804\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0118 - mean_squared_error: 1.0118 - val_loss: 1.0078 - val_mean_squared_error: 1.0078\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00804 to 1.00783, saving model to temp/e474\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0091 - mean_squared_error: 1.0091 - val_loss: 1.0319 - val_mean_squared_error: 1.0319\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00783\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0078 - mean_squared_error: 1.0078 - val_loss: 1.0008 - val_mean_squared_error: 1.0008\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00783 to 1.00077, saving model to temp/e474\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0066 - mean_squared_error: 1.0066 - val_loss: 1.0071 - val_mean_squared_error: 1.0071\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00077\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0048 - mean_squared_error: 1.0048 - val_loss: 1.0035 - val_mean_squared_error: 1.0035\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00077\n",
      "Epoch 00009: early stopping\n",
      "temp/e475\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.1062 - mean_squared_error: 1.1062 - val_loss: 1.0114 - val_mean_squared_error: 1.0114\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01139, saving model to temp/e475\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0193 - mean_squared_error: 1.0193 - val_loss: 1.0219 - val_mean_squared_error: 1.0219\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01139\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0182 - mean_squared_error: 1.0182 - val_loss: 1.0115 - val_mean_squared_error: 1.0115\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01139\n",
      "Epoch 00003: early stopping\n",
      "temp/e476\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.1032 - mean_squared_error: 1.1032 - val_loss: 1.0218 - val_mean_squared_error: 1.0218\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02182, saving model to temp/e476\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0182 - mean_squared_error: 1.0182 - val_loss: 1.0065 - val_mean_squared_error: 1.0065\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02182 to 1.00650, saving model to temp/e476\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0167 - mean_squared_error: 1.0167 - val_loss: 1.0053 - val_mean_squared_error: 1.0053\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00650 to 1.00528, saving model to temp/e476\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0297 - val_mean_squared_error: 1.0297\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00528\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00528 to 1.00258, saving model to temp/e476\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0075 - mean_squared_error: 1.0075 - val_loss: 1.0041 - val_mean_squared_error: 1.0041\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00258\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.0019 - val_mean_squared_error: 1.0019\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00258 to 1.00194, saving model to temp/e476\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0052 - mean_squared_error: 1.0052 - val_loss: 1.0001 - val_mean_squared_error: 1.0001\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00194 to 1.00015, saving model to temp/e476\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 1.0071 - val_mean_squared_error: 1.0071\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.00015\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0082 - mean_squared_error: 1.0082 - val_loss: 0.9984 - val_mean_squared_error: 0.9984\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.00015 to 0.99844, saving model to temp/e476\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0069 - mean_squared_error: 1.0069 - val_loss: 1.0072 - val_mean_squared_error: 1.0072\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99844\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0050 - mean_squared_error: 1.0050 - val_loss: 0.9981 - val_mean_squared_error: 0.9981\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.99844 to 0.99813, saving model to temp/e476\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0030 - mean_squared_error: 1.0030 - val_loss: 0.9978 - val_mean_squared_error: 0.9978\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.99813 to 0.99775, saving model to temp/e476\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0032 - mean_squared_error: 1.0032 - val_loss: 1.0037 - val_mean_squared_error: 1.0037\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.99775\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0032 - mean_squared_error: 1.0032 - val_loss: 0.9980 - val_mean_squared_error: 0.9980\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.99775\n",
      "Epoch 00015: early stopping\n",
      "temp/e477\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1017 - mean_squared_error: 1.1017 - val_loss: 1.0212 - val_mean_squared_error: 1.0212\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02115, saving model to temp/e477\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0207 - mean_squared_error: 1.0207 - val_loss: 1.0112 - val_mean_squared_error: 1.0112\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02115 to 1.01124, saving model to temp/e477\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0189 - mean_squared_error: 1.0189 - val_loss: 1.0169 - val_mean_squared_error: 1.0169\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01124\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 1.0088 - val_mean_squared_error: 1.0088\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01124 to 1.00876, saving model to temp/e477\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 1.0002 - val_mean_squared_error: 1.0002\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00876 to 1.00023, saving model to temp/e477\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0061 - mean_squared_error: 1.0061 - val_loss: 1.0032 - val_mean_squared_error: 1.0032\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00023\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0052 - mean_squared_error: 1.0052 - val_loss: 1.0177 - val_mean_squared_error: 1.0177\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00023\n",
      "Epoch 00007: early stopping\n",
      "temp/e478\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 147us/step - loss: 1.1118 - mean_squared_error: 1.1118 - val_loss: 1.0217 - val_mean_squared_error: 1.0217\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02173, saving model to temp/e478\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0220 - mean_squared_error: 1.0220 - val_loss: 1.0092 - val_mean_squared_error: 1.0092\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02173 to 1.00920, saving model to temp/e478\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 2s 122us/step - loss: 1.0145 - mean_squared_error: 1.0145 - val_loss: 1.0177 - val_mean_squared_error: 1.0177\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00920\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0125 - mean_squared_error: 1.0125 - val_loss: 1.0242 - val_mean_squared_error: 1.0242\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00920\n",
      "Epoch 00004: early stopping\n",
      "temp/e479\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.1030 - mean_squared_error: 1.1030 - val_loss: 1.0166 - val_mean_squared_error: 1.0166\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01656, saving model to temp/e479\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0172 - mean_squared_error: 1.0172 - val_loss: 1.0092 - val_mean_squared_error: 1.0092\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01656 to 1.00915, saving model to temp/e479\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0147 - mean_squared_error: 1.0147 - val_loss: 1.0078 - val_mean_squared_error: 1.0078\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00915 to 1.00783, saving model to temp/e479\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0145 - mean_squared_error: 1.0145 - val_loss: 1.0232 - val_mean_squared_error: 1.0232\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00783\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0099 - val_mean_squared_error: 1.0099\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00783\n",
      "Epoch 00005: early stopping\n",
      "temp/e480\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 1.1057 - mean_squared_error: 1.1057 - val_loss: 1.0139 - val_mean_squared_error: 1.0139\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01393, saving model to temp/e480\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0189 - mean_squared_error: 1.0189 - val_loss: 1.0126 - val_mean_squared_error: 1.0126\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01393 to 1.01263, saving model to temp/e480\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0179 - mean_squared_error: 1.0179 - val_loss: 1.0092 - val_mean_squared_error: 1.0092\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01263 to 1.00919, saving model to temp/e480\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0131 - mean_squared_error: 1.0131 - val_loss: 1.0104 - val_mean_squared_error: 1.0104\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00919\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 1.0022 - val_mean_squared_error: 1.0022\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00919 to 1.00224, saving model to temp/e480\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0062 - mean_squared_error: 1.0062 - val_loss: 1.0231 - val_mean_squared_error: 1.0231\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00224\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0072 - mean_squared_error: 1.0072 - val_loss: 1.0004 - val_mean_squared_error: 1.0004\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00224 to 1.00040, saving model to temp/e480\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0093 - mean_squared_error: 1.0093 - val_loss: 0.9993 - val_mean_squared_error: 0.9993\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.00040 to 0.99929, saving model to temp/e480\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0055 - mean_squared_error: 1.0055 - val_loss: 1.0002 - val_mean_squared_error: 1.0002\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99929\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0042 - mean_squared_error: 1.0042 - val_loss: 1.0050 - val_mean_squared_error: 1.0050\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99929\n",
      "Epoch 00010: early stopping\n",
      "temp/e481\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.0995 - mean_squared_error: 1.0995 - val_loss: 1.0167 - val_mean_squared_error: 1.0167\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01674, saving model to temp/e481\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0228 - mean_squared_error: 1.0228 - val_loss: 1.0089 - val_mean_squared_error: 1.0089\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01674 to 1.00888, saving model to temp/e481\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0153 - mean_squared_error: 1.0153 - val_loss: 1.0207 - val_mean_squared_error: 1.0207\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00888\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0123 - mean_squared_error: 1.0123 - val_loss: 1.0138 - val_mean_squared_error: 1.0138\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00888\n",
      "Epoch 00004: early stopping\n",
      "temp/e482\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 1.0974 - mean_squared_error: 1.0974 - val_loss: 1.0356 - val_mean_squared_error: 1.0356\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03556, saving model to temp/e482\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0198 - mean_squared_error: 1.0198 - val_loss: 1.0121 - val_mean_squared_error: 1.0121\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03556 to 1.01209, saving model to temp/e482\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0110 - mean_squared_error: 1.0110 - val_loss: 1.0032 - val_mean_squared_error: 1.0032\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01209 to 1.00323, saving model to temp/e482\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0155 - mean_squared_error: 1.0155 - val_loss: 1.0092 - val_mean_squared_error: 1.0092\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00323\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0098 - mean_squared_error: 1.0098 - val_loss: 1.0139 - val_mean_squared_error: 1.0139\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00323\n",
      "Epoch 00005: early stopping\n",
      "temp/e483\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1060 - mean_squared_error: 1.1060 - val_loss: 1.0153 - val_mean_squared_error: 1.0153\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01531, saving model to temp/e483\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0192 - mean_squared_error: 1.0192 - val_loss: 1.0254 - val_mean_squared_error: 1.0254\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01531\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0154 - mean_squared_error: 1.0154 - val_loss: 1.0065 - val_mean_squared_error: 1.0065\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01531 to 1.00652, saving model to temp/e483\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0102 - val_mean_squared_error: 1.0102\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00652\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0013 - val_mean_squared_error: 1.0013\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00652 to 1.00125, saving model to temp/e483\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 1.0123 - val_mean_squared_error: 1.0123\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00125\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0060 - mean_squared_error: 1.0060 - val_loss: 1.0032 - val_mean_squared_error: 1.0032\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00125\n",
      "Epoch 00007: early stopping\n",
      "temp/e484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 1.0940 - mean_squared_error: 1.0940 - val_loss: 1.0169 - val_mean_squared_error: 1.0169\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01690, saving model to temp/e484\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0196 - mean_squared_error: 1.0196 - val_loss: 1.0076 - val_mean_squared_error: 1.0076\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01690 to 1.00763, saving model to temp/e484\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0117 - mean_squared_error: 1.0117 - val_loss: 1.0221 - val_mean_squared_error: 1.0221\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00763\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0136 - mean_squared_error: 1.0136 - val_loss: 1.0117 - val_mean_squared_error: 1.0117\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00763\n",
      "Epoch 00004: early stopping\n",
      "temp/e485\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 1.1095 - mean_squared_error: 1.1095 - val_loss: 1.0285 - val_mean_squared_error: 1.0285\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02846, saving model to temp/e485\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0166 - mean_squared_error: 1.0166 - val_loss: 1.0098 - val_mean_squared_error: 1.0098\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02846 to 1.00976, saving model to temp/e485\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0177 - mean_squared_error: 1.0177 - val_loss: 1.0153 - val_mean_squared_error: 1.0153\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00976\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 1.0243 - val_mean_squared_error: 1.0243\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00976\n",
      "Epoch 00004: early stopping\n",
      "temp/e486\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.1011 - mean_squared_error: 1.1011 - val_loss: 1.0106 - val_mean_squared_error: 1.0106\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01060, saving model to temp/e486\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0215 - mean_squared_error: 1.0215 - val_loss: 1.0083 - val_mean_squared_error: 1.0083\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01060 to 1.00833, saving model to temp/e486\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0114 - mean_squared_error: 1.0114 - val_loss: 1.0144 - val_mean_squared_error: 1.0144\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00833\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 1.0119 - val_mean_squared_error: 1.0119\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00833\n",
      "Epoch 00004: early stopping\n",
      "temp/e487\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1023 - mean_squared_error: 1.1023 - val_loss: 1.0147 - val_mean_squared_error: 1.0147\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01467, saving model to temp/e487\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0201 - mean_squared_error: 1.0201 - val_loss: 1.0104 - val_mean_squared_error: 1.0104\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01467 to 1.01043, saving model to temp/e487\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0184 - mean_squared_error: 1.0184 - val_loss: 1.0144 - val_mean_squared_error: 1.0144\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01043\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0131 - mean_squared_error: 1.0131 - val_loss: 1.0092 - val_mean_squared_error: 1.0092\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01043 to 1.00920, saving model to temp/e487\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 1.0403 - val_mean_squared_error: 1.0403\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00920\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0125 - mean_squared_error: 1.0125 - val_loss: 1.0062 - val_mean_squared_error: 1.0062\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00920 to 1.00618, saving model to temp/e487\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0065 - mean_squared_error: 1.0065 - val_loss: 0.9992 - val_mean_squared_error: 0.9992\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00618 to 0.99923, saving model to temp/e487\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0034 - mean_squared_error: 1.0034 - val_loss: 1.0071 - val_mean_squared_error: 1.0071\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99923\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 1.0084 - val_mean_squared_error: 1.0084\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99923\n",
      "Epoch 00009: early stopping\n",
      "temp/e488\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 1.0966 - mean_squared_error: 1.0966 - val_loss: 1.0119 - val_mean_squared_error: 1.0119\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01192, saving model to temp/e488\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0218 - mean_squared_error: 1.0218 - val_loss: 1.0076 - val_mean_squared_error: 1.0076\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01192 to 1.00761, saving model to temp/e488\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0161 - mean_squared_error: 1.0161 - val_loss: 1.0207 - val_mean_squared_error: 1.0207\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00761\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0145 - mean_squared_error: 1.0145 - val_loss: 1.0134 - val_mean_squared_error: 1.0134\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00761\n",
      "Epoch 00004: early stopping\n",
      "temp/e489\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.1026 - mean_squared_error: 1.1026 - val_loss: 1.0364 - val_mean_squared_error: 1.0364\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03643, saving model to temp/e489\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0177 - mean_squared_error: 1.0177 - val_loss: 1.0113 - val_mean_squared_error: 1.0113\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03643 to 1.01128, saving model to temp/e489\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0161 - mean_squared_error: 1.0161 - val_loss: 1.0044 - val_mean_squared_error: 1.0044\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01128 to 1.00440, saving model to temp/e489\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0116 - mean_squared_error: 1.0116 - val_loss: 1.0142 - val_mean_squared_error: 1.0142\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00440\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0131 - val_mean_squared_error: 1.0131\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00440\n",
      "Epoch 00005: early stopping\n",
      "temp/e490\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.0965 - mean_squared_error: 1.0965 - val_loss: 1.0152 - val_mean_squared_error: 1.0152\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01522, saving model to temp/e490\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0223 - mean_squared_error: 1.0223 - val_loss: 1.0137 - val_mean_squared_error: 1.0137\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01522 to 1.01374, saving model to temp/e490\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0173 - mean_squared_error: 1.0173 - val_loss: 1.0068 - val_mean_squared_error: 1.0068\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01374 to 1.00680, saving model to temp/e490\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00680 to 1.00397, saving model to temp/e490\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0094 - mean_squared_error: 1.0094 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00397\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0082 - mean_squared_error: 1.0082 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00397 to 1.00341, saving model to temp/e490\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0088 - mean_squared_error: 1.0088 - val_loss: 1.0046 - val_mean_squared_error: 1.0046\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.00341\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 1.0044 - val_mean_squared_error: 1.0044\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.00341\n",
      "Epoch 00008: early stopping\n",
      "temp/e491\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 1.1014 - mean_squared_error: 1.1014 - val_loss: 1.0200 - val_mean_squared_error: 1.0200\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02002, saving model to temp/e491\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0187 - mean_squared_error: 1.0187 - val_loss: 1.0090 - val_mean_squared_error: 1.0090\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02002 to 1.00900, saving model to temp/e491\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0167 - mean_squared_error: 1.0167 - val_loss: 1.0059 - val_mean_squared_error: 1.0059\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00900 to 1.00594, saving model to temp/e491\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0122 - mean_squared_error: 1.0122 - val_loss: 1.0068 - val_mean_squared_error: 1.0068\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00594\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 1.0072 - val_mean_squared_error: 1.0072\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00594\n",
      "Epoch 00005: early stopping\n",
      "temp/e492\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.1073 - mean_squared_error: 1.1073 - val_loss: 1.0102 - val_mean_squared_error: 1.0102\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01016, saving model to temp/e492\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0215 - mean_squared_error: 1.0215 - val_loss: 1.0392 - val_mean_squared_error: 1.0392\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01016\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0155 - mean_squared_error: 1.0155 - val_loss: 1.0115 - val_mean_squared_error: 1.0115\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01016\n",
      "Epoch 00003: early stopping\n",
      "temp/e493\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 1.1054 - mean_squared_error: 1.1054 - val_loss: 1.0163 - val_mean_squared_error: 1.0163\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01633, saving model to temp/e493\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0237 - mean_squared_error: 1.0237 - val_loss: 1.0116 - val_mean_squared_error: 1.0116\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01633 to 1.01161, saving model to temp/e493\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0150 - mean_squared_error: 1.0150 - val_loss: 1.0068 - val_mean_squared_error: 1.0068\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01161 to 1.00682, saving model to temp/e493\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 1.0078 - val_mean_squared_error: 1.0078\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00682\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0107 - mean_squared_error: 1.0107 - val_loss: 1.0118 - val_mean_squared_error: 1.0118\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00682\n",
      "Epoch 00005: early stopping\n",
      "temp/e494\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 1.1064 - mean_squared_error: 1.1064 - val_loss: 1.0216 - val_mean_squared_error: 1.0216\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02164, saving model to temp/e494\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0188 - mean_squared_error: 1.0188 - val_loss: 1.0132 - val_mean_squared_error: 1.0132\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02164 to 1.01316, saving model to temp/e494\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0140 - val_mean_squared_error: 1.0140\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01316\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0153 - mean_squared_error: 1.0153 - val_loss: 1.0040 - val_mean_squared_error: 1.0040\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01316 to 1.00399, saving model to temp/e494\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0127 - mean_squared_error: 1.0127 - val_loss: 1.0132 - val_mean_squared_error: 1.0132\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00399\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0085 - mean_squared_error: 1.0085 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.00399\n",
      "Epoch 00006: early stopping\n",
      "temp/e495\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.1020 - mean_squared_error: 1.1020 - val_loss: 1.0162 - val_mean_squared_error: 1.0162\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01619, saving model to temp/e495\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0167 - mean_squared_error: 1.0167 - val_loss: 1.0067 - val_mean_squared_error: 1.0067\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01619 to 1.00667, saving model to temp/e495\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0153 - mean_squared_error: 1.0153 - val_loss: 1.0032 - val_mean_squared_error: 1.0032\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00667 to 1.00319, saving model to temp/e495\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0093 - mean_squared_error: 1.0093 - val_loss: 1.0151 - val_mean_squared_error: 1.0151\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00319\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0129 - mean_squared_error: 1.0129 - val_loss: 1.0032 - val_mean_squared_error: 1.0032\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00319\n",
      "Epoch 00005: early stopping\n",
      "temp/e496\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 1.1050 - mean_squared_error: 1.1050 - val_loss: 1.0284 - val_mean_squared_error: 1.0284\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02842, saving model to temp/e496\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0211 - mean_squared_error: 1.0211 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02842 to 1.00750, saving model to temp/e496\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 133us/step - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.00750 to 1.00235, saving model to temp/e496\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 1.0074 - val_mean_squared_error: 1.0074\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00235\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0106 - mean_squared_error: 1.0106 - val_loss: 1.0011 - val_mean_squared_error: 1.0011\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00235 to 1.00108, saving model to temp/e496\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0131 - mean_squared_error: 1.0131 - val_loss: 0.9979 - val_mean_squared_error: 0.9979\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.00108 to 0.99788, saving model to temp/e496\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0065 - mean_squared_error: 1.0065 - val_loss: 1.0025 - val_mean_squared_error: 1.0025\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99788\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0043 - mean_squared_error: 1.0043 - val_loss: 1.0105 - val_mean_squared_error: 1.0105\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99788\n",
      "Epoch 00008: early stopping\n",
      "temp/e497\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.1024 - mean_squared_error: 1.1024 - val_loss: 1.0222 - val_mean_squared_error: 1.0222\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02224, saving model to temp/e497\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0194 - mean_squared_error: 1.0194 - val_loss: 1.0076 - val_mean_squared_error: 1.0076\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02224 to 1.00756, saving model to temp/e497\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 1.0135 - val_mean_squared_error: 1.0135\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00756\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0136 - mean_squared_error: 1.0136 - val_loss: 1.0013 - val_mean_squared_error: 1.0013\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00756 to 1.00135, saving model to temp/e497\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 0.9974 - val_mean_squared_error: 0.9974\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00135 to 0.99742, saving model to temp/e497\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99742\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0071 - mean_squared_error: 1.0071 - val_loss: 0.9998 - val_mean_squared_error: 0.9998\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99742\n",
      "Epoch 00007: early stopping\n",
      "temp/e498\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 142us/step - loss: 1.0990 - mean_squared_error: 1.0990 - val_loss: 1.0149 - val_mean_squared_error: 1.0149\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01486, saving model to temp/e498\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0219 - mean_squared_error: 1.0219 - val_loss: 1.0218 - val_mean_squared_error: 1.0218\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01486\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0201 - mean_squared_error: 1.0201 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01486 to 1.00614, saving model to temp/e498\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 1.0150 - mean_squared_error: 1.0150 - val_loss: 1.0129 - val_mean_squared_error: 1.0129\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00614\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0103 - mean_squared_error: 1.0103 - val_loss: 1.0164 - val_mean_squared_error: 1.0164\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.00614\n",
      "Epoch 00005: early stopping\n",
      "temp/e499\n",
      "Train on 20000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 1.1012 - mean_squared_error: 1.1012 - val_loss: 1.0166 - val_mean_squared_error: 1.0166\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01661, saving model to temp/e499\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 1.0216 - mean_squared_error: 1.0216 - val_loss: 1.0075 - val_mean_squared_error: 1.0075\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01661 to 1.00745, saving model to temp/e499\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 1.0160 - val_mean_squared_error: 1.0160\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00745\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 1.0122 - val_mean_squared_error: 1.0122\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00745\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "for idx, model_name in enumerate(model_names):\n",
    "    print(model_name)\n",
    "\n",
    "    if type(models[idx]) is list:\n",
    "        #clear session\n",
    "        keras.backend.clear_session() \n",
    "        #get model according to specification\n",
    "        model = get_model(models[idx], [0.2] * len(models), 6)\n",
    "        callbacks = [ModelCheckpoint(model_name, verbose= verbosity, monitor='val_loss',save_best_only=True), \n",
    "                     EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose= verbosity, mode='auto')]\n",
    "        model.compile(optimizer = optimizers.SGD(lr = 0.001, momentum = 0.9, ), loss='mean_squared_error', metrics = ['mse'])\n",
    "        #print(len(X), len(y))\n",
    "        model.fit(X, y, epochs = 20, validation_data = (x_val, y_val), callbacks = callbacks, batch_size = 32, verbose = verbosity)\n",
    "    else:\n",
    "        models[idx].fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "1 1\n",
      "1 2\n",
      "1 3\n",
      "2 1\n",
      "2 2\n",
      "2 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n            setA = get_MB(get_CG(perturbed_df, tetrad), \\'g\\', pc)\\n            if setA != {\\'f\\'}:\\n                print(\"Error in SETA markov blanket\")\\n                #setA = {\\'f\\'}\\n            setC = get_MB(get_CG(test_df2, tetrad), \\'g\\', pc)\\n\\n            if setA != setC:\\n                causal_dicts[idx][str(m) + \\'_\\' + str(v)].append(1)\\n            else:\\n                causal_dicts[idx][str(m) + \\'_\\' + str(v)].append(0)\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_test = 2000\n",
    "metrics_dicts = []\n",
    "for m in models:\n",
    "    metrics_dicts.append(defaultdict(list))\n",
    "\n",
    "\n",
    "#means = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "#variances = [1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0]\n",
    "means = [0, 1, 2]\n",
    "variances = [1,2,3]\n",
    "\n",
    "\n",
    "# ok at this point we need to check the model on various variances and means\n",
    "for m in means:\n",
    "    for v in variances:\n",
    "        print(m,v)\n",
    "        #t0 = time.time()\n",
    "        perturbed_df = gen_data(mean =m, var = v, SIZE = nb_test)\n",
    "        y_test2 = perturbed_df['g']\n",
    "        x_test2 = perturbed_df[['a', 'b', 'c', 'd', 'e', 'f']]\n",
    "        #t1 = time.time()\n",
    "        #print(\"Time for gen_data = \", t1 - t0)\n",
    "        for idx, model_name in enumerate(model_names):\n",
    "            #t0 = time.time()\n",
    "            if type(models[idx]) is list:\n",
    "                keras.backend.clear_session()\n",
    "                model = load_model(model_name)\n",
    "            else:\n",
    "                model = models[idx]\n",
    "            #t1 = time.time()\n",
    "            #print(\"Time to load model = \", t1 - t0)\n",
    "            \n",
    "            y_pred2 = model.predict(x_test2)\n",
    "            metrics_dicts[idx][str(m) + '_' + str(v)].append(mean_squared_error(y_test2, y_pred2))\n",
    "\n",
    "            test_df2 = pd.DataFrame(x_test2, columns = ['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "            test_targets2 = pd.DataFrame(model.predict(x_test2), columns = ['g'])\n",
    "            test_df2 = test_df2.join(test_targets2)\n",
    "'''\n",
    "            setA = get_MB(get_CG(perturbed_df, tetrad), 'g', pc)\n",
    "            if setA != {'f'}:\n",
    "                print(\"Error in SETA markov blanket\")\n",
    "                #setA = {'f'}\n",
    "            setC = get_MB(get_CG(test_df2, tetrad), 'g', pc)\n",
    "\n",
    "            if setA != setC:\n",
    "                causal_dicts[idx][str(m) + '_' + str(v)].append(1)\n",
    "            else:\n",
    "                causal_dicts[idx][str(m) + '_' + str(v)].append(0)\n",
    "\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times =  0\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18632.714573927016\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18595.103872688487\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18542.286833555372\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18631.978073444745\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18796.397024178023\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18581.264340039656\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18685.46397518372\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18754.91533073672\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18781.06589765241\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18670.640587205773\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18760.046917287098\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18650.46186174568\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18575.18937704827\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18589.9624925579\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18654.44718015627\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18589.393304889327\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18693.11889581182\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18750.59797201033\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18573.212616712055\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18651.705706826073\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18462.144319612074\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18645.570220372967\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18609.420205647155\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18612.62005562244\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18639.478457669626\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18780.46065409177\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18716.546463586663\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18623.9129449062\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18692.871801109457\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18536.652877052402\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18532.845171183944\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18769.82487383203\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18704.781317442164\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18565.443399218348\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18759.36372981929\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18601.768728272662\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18568.941368736465\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18572.260912311995\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18696.215204196174\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18550.940959568412\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18788.454932154724\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18589.921959686217\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18570.871232106594\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18648.629783922755\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18781.60639609168\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18508.604054623192\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18562.567418294773\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18702.92022147084\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18644.85063925037\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18507.65227449589\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18659.64933615539\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18628.889778026198\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18647.750943014624\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18548.71508247762\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18550.29149512295\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18577.967791204566\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18656.88548744153\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18726.531566892372\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18600.676457637474\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18728.568132068434\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18657.76172836411\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18450.254582832848\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18636.24769022784\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18585.405171559047\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18796.722886763106\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18559.83690863241\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18685.708451102277\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18566.39607801304\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18615.8172516776\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18708.03111819717\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18366.883698699865\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18725.116525972746\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18594.123603012376\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18662.92640849809\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18848.172254691344\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18555.9010700525\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18529.494660315606\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18778.00805880076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18768.081350352404\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18666.355172511852\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18536.72249782081\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18672.475496493396\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18565.138487892058\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18686.51458623911\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18683.77016109373\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18666.609942644925\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18601.402982716274\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18655.572866250273\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18754.16931442975\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18552.371435623907\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18614.705442569462\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18659.8608024535\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18796.64072665151\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18646.603162533742\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18648.6618418991\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18700.89206362411\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18714.665494572946\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18492.53365216816\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18543.41944281981\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18603.278141790805\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18685.556384201456\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18689.174894707532\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18693.244547111743\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18469.334923042068\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18670.42746060429\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18738.99882623023\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18514.648968503003\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18709.825009376847\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18593.15584809109\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18632.52649990831\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18738.624080135472\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18670.27398442391\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18642.771929208197\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18670.15902789793\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18601.11062713022\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18557.881031579665\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18632.283317658832\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18732.753821217342\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18780.725662722547\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18499.355657476055\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18743.34455796238\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18635.400531113577\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18626.840259629775\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18658.756536388362\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18594.010494217768\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18721.672445693737\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18700.301930810605\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18618.043641287353\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18771.712534944192\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18649.01791951243\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18667.36331656755\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18643.204779010575\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18664.983483395285\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18578.45458023574\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18590.65922230612\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18587.337101200894\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18500.705678047387\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18546.13963906707\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18538.070265338192\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18684.262185901847\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18581.850801339013\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18720.276439361252\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18629.272385566856\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18650.80925754189\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18738.255369067145\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18667.72864916562\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18714.828796222435\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18628.61508682705\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18662.792943307915\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18646.800882879073\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18607.764024071534\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18656.160836188003\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18600.019864783837\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18703.802541067253\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18656.05820544027\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18570.379160251916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18677.68702430621\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18662.0312485105\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18625.834304387226\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18696.944902138766\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18614.98499423833\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18648.881121677477\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18586.329137558292\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18568.92625714789\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18640.059202046778\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18723.20726476638\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18603.4691930855\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18604.342487287322\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18711.508285839813\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18639.87826416112\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18542.482249219916\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18585.783452618605\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18538.704352871788\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18705.51459706888\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18670.46957826648\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18609.28056550472\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18670.095001221787\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18646.63116243442\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18738.568026204568\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18718.841668724897\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18764.620473064744\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18723.432121602404\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18610.08993712193\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18665.47459304318\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18784.766730108735\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18677.029666541544\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18528.266868570325\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18627.31405638506\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18763.74922943428\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18619.714645242988\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18708.820441795986\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18692.568061887712\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18568.966104320385\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18572.743241598993\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18638.243956067345\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18645.296527082897\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18616.442846642538\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18525.58416952804\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18618.32252403204\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18649.40615725621\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18618.843921934356\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18508.031950558136\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18596.493310091257\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18633.234888299812\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18675.001707006857\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18639.587763581007\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18667.551854152905\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18664.347175148076\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18520.4210341201\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18652.332250750347\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18677.17149254942\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18639.649881660294\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18699.187766231567\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18508.712481137412\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18695.988644133813\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18570.12367510223\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18597.991962310232\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18568.13776161325\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18467.17209624602\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18722.26645293508\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18649.949849618064\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18612.94143637291\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18619.540123906852\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18643.068049649475\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18635.198727553143\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18847.91536108991\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18420.366090522246\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18463.344460294305\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18693.241513048717\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18574.54545348776\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18726.09207172166\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18591.502935698074\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18535.88135295581\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18639.58871383058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18560.076090422266\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18757.06956929893\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18541.983195501874\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18696.985335996793\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18652.472827982252\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18677.238117725577\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18731.661594732752\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18656.267467962578\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18714.823134817627\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18718.2055360654\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18596.506948664683\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18760.353625970653\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18605.899247974023\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18651.211475170414\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18717.297288890008\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18661.274735605974\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18595.81116584643\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18730.78080370064\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18524.066586294593\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18653.366950082258\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18561.707820884854\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18503.471188794363\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18689.103319395756\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18663.70041311242\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18663.891246172225\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18555.372565737707\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18574.992560602168\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18556.673481699458\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18599.507539862687\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18730.103144646266\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18569.650875722564\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18612.847592960712\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18605.316417775197\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18677.176737157628\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18767.737153887745\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18506.7360642472\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18744.181167160707\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18703.862850846912\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18667.818066108666\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18799.40463551874\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18650.9670484366\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18668.284564413938\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18584.000167889644\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18483.842734468522\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18570.548873323198\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18689.779592979452\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18623.96513208081\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18676.898680215323\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18713.158852490404\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18582.46884122007\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18699.93481989294\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18683.912043468903\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18629.869479869107\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18671.128506967594\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18568.28978323054\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18651.573921752708\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18473.67170278814\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18580.64864043475\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18635.875027198974\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18718.05332349361\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18692.278351654353\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18589.13542375371\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18680.502008483363\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18730.29157101019\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18675.893596348993\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18760.469218522703\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18662.525891262096\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18605.41691614624\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18754.344383264273\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18685.019772046395\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18758.619286566118\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18571.59920617156\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18521.6699540036\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18769.044762059697\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18522.312675797904\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18682.187098822575\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18664.26026437578\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18732.72997724181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18724.269253811748\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18699.30358738794\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18611.462349981197\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18644.631686153025\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18494.060618521697\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18606.890774767235\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18722.76763261226\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18685.251671567057\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18448.072254580646\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18576.400361918673\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18734.283328569534\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18797.710001731888\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18738.764482114937\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18577.075719944223\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18749.41455666481\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18663.663856106887\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18690.683678311292\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18740.21320982334\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18673.841907548423\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18645.7874321742\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18625.550437202175\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18534.527238742958\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18614.37390300259\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18614.78780426438\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18562.648301173736\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18535.96267356329\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18747.504167347965\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18674.702434740244\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18601.09172378285\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18584.3689955784\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18789.7942730902\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18643.962995321104\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18769.07125847369\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18671.41726783599\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18533.693197270237\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18700.70417245418\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18638.389477323\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18723.30455904614\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18705.875110570494\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18606.791871252382\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18748.878211461142\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18775.892975781982\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18696.591936797657\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18639.799737288664\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18699.027203629117\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18730.465119977725\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18680.75216640357\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18714.87887752285\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18595.583437795685\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18793.965866469334\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18549.65852877389\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18632.765296620542\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18578.35470001424\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18564.78006813455\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18619.645569799257\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18760.172166524746\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18707.075172550572\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18642.37980158705\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18703.891006986356\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18633.24180693745\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18631.87008575845\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18703.39717317133\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18667.545135187505\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18462.18303417736\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18551.49290623557\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18688.90200369951\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18523.85239318893\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18552.456982735766\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18636.24757160231\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18696.310178797463\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18551.257715198288\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18606.068130050597\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18629.405273645203\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18545.886602884908\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18640.34675194964\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18627.077269319838\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18639.833956620903\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18592.523447965545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18646.187168555058\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18553.84549005451\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18638.174543383968\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18826.855812699752\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18610.07487648706\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18666.652802152104\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18614.586400495955\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18587.886983820128\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18589.568863157845\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18614.02491753493\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18676.641438776103\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18736.547374463695\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18487.97679288261\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18681.928629377086\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18715.395210209866\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18575.111149087446\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18685.585903404066\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18558.235292351255\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18601.916108626985\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18716.466012093526\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18493.613929925268\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18613.48682089247\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18703.865369515326\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18762.71683961987\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18588.547261006122\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18727.5137330698\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18624.619471547085\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18523.553286987564\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18621.896003582875\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18627.45773025024\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18752.940957557337\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18741.753137823824\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18643.58374977262\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18641.714001813136\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18616.5920445222\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18625.424754232194\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18769.244484608822\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18546.252904709643\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18636.437865682776\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18705.8029263661\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18602.157449872262\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18605.823071156312\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18719.954041969173\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18699.789631361902\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18621.181030976073\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18634.99569419306\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18585.41716501632\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18684.627407191423\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18657.12995724046\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18608.421916524047\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18583.209958876025\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18604.580779760083\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18656.665264366824\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18702.014912403494\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18624.486963141655\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18537.84041234428\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18781.536207206445\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18491.020672632818\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18597.702171601293\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18775.978997107795\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18651.705237436538\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18516.699129823926\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18805.029698175927\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18776.05118323837\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18452.323867675128\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18615.65157554559\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18597.702374077682\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18656.157102589335\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18694.46549800411\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18648.820863654626\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18646.327891560013\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18638.744337332\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18507.627980377267\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18665.41914343048\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18435.52297242929\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18559.027984735334\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18606.57889327691\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18734.056502893556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18602.641858893152\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18727.076411035036\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18737.214773896012\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18626.231839407585\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18678.372430975654\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18629.472617671883\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18550.878791962063\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18552.685205373997\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18781.0502280763\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18680.458764342155\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18588.941533393277\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18614.74477994283\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18570.238580667406\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18705.52465249563\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18538.343471799904\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18721.848065766753\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18567.744434316213\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18743.661033162218\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18594.771135841653\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18616.445766598168\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18615.856068903435\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18588.78269961967\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18631.699625219353\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18581.415266707838\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18732.69978541836\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18590.322499941318\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18590.393244824256\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18507.459421093405\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18532.498537729298\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18666.282955457726\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18745.314390049134\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19542.21719710771 -18528.740401332994\n",
      "Times =  1\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19469.66302669438\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19429.611804639622\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19369.501004085858\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19462.894442269928\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19637.471939792333\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19447.682549117468\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19517.81737136683\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19600.44177940223\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19611.524496051257\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19512.560687360467\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19605.343801333118\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19516.149672784344\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19409.184813745753\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19439.925237458032\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19517.94181818507\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19438.422932556306\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19559.011476746793\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19601.62845498608\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19443.866509121533\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19516.302725699614\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19309.379680275008\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19502.226865646327\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19478.32389025282\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19447.20762955126\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19460.519273043057\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19664.01877015903\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19553.34107867966\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19476.28191548679\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19571.77433518474\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19366.55421913267\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19394.631765332117\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19635.123030371746\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19563.925110780718\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19407.737218871163\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19614.924197166474\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19450.362577589844\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19453.29211631744\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19417.498892779688\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19536.024817558446\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19398.77230495454\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19640.170806644892\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19449.08186072415\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19434.726903637475\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19497.7446555606\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19645.48590383007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19327.691185545384\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19419.658149513038\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19562.351699655155\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19485.914219374856\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19367.17766247996\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19510.35154352013\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19445.540770629672\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19502.61984326062\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19384.546933354148\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19405.346165433217\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19432.2928313195\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19532.717687252156\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19575.749632107203\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19439.131707990226\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19572.45349626537\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19512.291051757013\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19319.114763952242\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19479.135353014837\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19439.415711617963\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19652.402793118617\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19421.780378844036\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19532.422191005677\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19419.88400943665\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19490.675748223483\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19578.362196469712\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19234.20523812786\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19574.602358978682\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19443.35730155981\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19544.15845011855\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19693.084784067105\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19419.50579696546\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19406.89246427185\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19652.31846751843\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19618.555871604207\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19531.75986145291\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19368.470287215994\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19519.697405186773\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19426.815846958318\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19545.496809979097\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19534.60911659683\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19534.32812116335\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19448.33980934367\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19487.077485459064\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19601.2810263148\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19401.37488300715\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19479.240678109974\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19507.572936819626\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19657.396341957894\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19519.737048307397\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19519.0471446835\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19571.566247386407\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19566.77346856306\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19339.20146276885\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19396.738140642396\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19440.805808940193\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19538.413337223916\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19539.16845978537\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19546.415161099212\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19308.883012147802\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19539.640585456822\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19583.24134456165\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19353.740949640534\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19555.591314175577\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19469.793604447732\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19488.683099554837\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19607.47149779213\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19514.487473278554\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19475.583143511838\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19508.55777351533\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19466.265677975305\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19421.235027299364\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19469.28836506043\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19611.89175853255\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19635.82597529552\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19350.88206771447\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19608.404277810467\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19515.351481258487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19446.09156475443\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19537.042067661274\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19443.22332115612\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19567.541803751745\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19540.481018236995\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19492.025902860136\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19634.70629085635\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19500.52849909597\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19538.189094760222\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19472.062659968302\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19506.2635043756\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19433.65517371712\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19438.820440913212\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19467.289623297405\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19365.960397190473\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19407.54918217777\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19404.292019259196\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19555.89050621973\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19420.83063486286\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19601.20359225525\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19452.174024150936\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19519.43853020926\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19581.133871237445\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19508.8467887418\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19567.067793039976\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19502.175843288835\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19491.62418820495\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19492.352467925033\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19497.720484050522\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19509.66095054946\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19443.232569372234\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19565.069798908975\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19536.710062065522\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19408.773266797863\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19495.395980779962\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19488.172376595066\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19483.326961019782\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19565.687966940157\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19433.6103528663\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19517.65645608558\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19422.57406037703\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19448.03018375415\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19496.16938823011\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19545.499418545533\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19465.210157191388\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19427.152861058607\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19552.183114595588\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19478.413080491402\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19391.634189779175\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19433.934752289508\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19387.58285240523\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19540.89510356764\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19507.86028659604\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19452.239265849556\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19539.293708443605\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19504.155765990217\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19586.48043043985\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19563.8798337706\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19610.135404855995\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19585.025743960497\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19457.956884920717\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19531.30528557396\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19614.328118130874\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19538.427473041047\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19376.968356154037\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19468.19776923867\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19593.34319983542\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19472.556721573896\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19572.352991090607\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19515.22288143384\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19397.75760797468\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19442.423521664205\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19512.202490158452\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19499.736243120922\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19478.78048703015\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19351.161452852226\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19464.14561321338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19512.393303731908\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19463.42761595892\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19355.90242474846\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19474.95217161844\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19498.174625968124\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19537.64951277212\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19519.518226468368\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19530.31534373108\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19495.40257651204\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19399.328202235774\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19491.611303230206\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19512.712885608802\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19489.056912291977\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19541.386318521065\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19380.236707337728\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19536.084231640772\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19434.27796611637\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19450.392703710037\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19424.20183948802\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19336.57116207633\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19594.47751190835\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19490.52552176481\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19448.861037417046\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19484.339575684986\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19477.41207593065\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19493.079298856916\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19711.789848018816\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19273.578330150394\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19299.688138499365\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19543.10951035112\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19442.700525968987\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19595.6572482766\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19440.344254430296\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19378.46802768765\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19486.645231802337\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19415.074597064187\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19598.99171107803\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19398.63092632129\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19512.71004455334\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19483.986439079832\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19510.73211822832\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19601.76962406448\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19503.145084777745\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19581.150507613034\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19555.34082498336\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19446.433064826007\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19617.08660250441\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19449.40809118747\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19521.62755856921\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19569.88156670415\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19541.003378922724\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19457.165300222943\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19568.833704835233\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19361.472281180075\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19483.971233070748\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19403.268621989362\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19377.79528518347\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19534.131060423897\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19507.891411650337\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19518.58868980226\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19412.496580236188\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19409.94211670034\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19419.861350705498\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19448.511427873018\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19569.047913169405\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19388.465973722166\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19460.7151907943\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19451.802230615343\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19533.28573239349\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19612.76185324096\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19379.431625955232\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19588.891531237092\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19542.733368916186\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19526.250221156944\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19649.905269159703\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19508.40851244997\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19536.012985774847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19408.654699578132\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19314.21914603544\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19422.311238709997\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19553.496544501162\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19525.422711945903\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19529.796786223887\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19581.731925420736\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19436.58357744115\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19561.434819683178\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19505.71459655255\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19488.238776486516\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19548.287356790515\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19435.012453633037\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19516.735650144656\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19332.0647030718\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19441.772329877884\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19490.644649464724\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19567.787467742583\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19537.66811580141\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19436.436349835276\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19528.97395128778\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19601.990816794187\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19544.64262889009\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19603.393825732728\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19498.580421311693\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19422.392652978313\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19598.777199606204\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19512.79701741324\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19592.967541152364\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19422.78479883069\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19371.058892657944\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19628.259320938196\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19381.462030406576\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19540.016461495896\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19534.349969623578\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19605.392857833813\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19561.54800129268\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19543.147838002245\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19453.19227980681\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19490.749206567834\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19319.58200062016\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19436.14841425934\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19578.456143479314\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19513.92847331062\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19297.599215543803\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19397.235788660375\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19604.0819307602\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19654.741956629106\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19581.276153404946\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19445.7649960553\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19626.313625860064\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19503.10184839638\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19539.94414598454\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19586.678277166404\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19512.856233050195\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19494.33641324468\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19466.965463251137\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19409.746603854554\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19471.887566011817\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19460.950627065242\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19415.20755078397\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19400.011022414958\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19569.28613196014\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19541.93498495583\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19487.684467435967\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19441.98124388317\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19643.807299984903\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19472.730722916203\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19615.40122491371\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19505.735752424785\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19412.120988539238\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19558.019542939146\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19504.701362585525\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19610.988524280107\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19535.254122639843\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19479.019056924677\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19593.42435051114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19626.045924336984\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19541.757096466947\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19493.604487604847\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19544.044188315114\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19574.68260138959\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19532.243741006736\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19552.08505601105\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19459.66324144099\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19661.515900279956\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19421.598914985785\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19468.84079798492\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19416.36235252636\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19420.20450146806\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19462.289810305603\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19605.483400916433\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19557.241364058984\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19473.21762803091\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19560.355540134216\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19499.55058627543\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19453.28887042048\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19528.79390638373\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19502.996714982382\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19296.286876233225\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19427.831813958397\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19531.618680023832\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19391.31120121624\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19420.757399363574\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19497.323115514646\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19541.0661874907\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19427.39395999462\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19469.063693941145\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19496.916619150412\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19407.09102435327\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19485.37288234428\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19464.047044176594\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19487.922148661237\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19446.64148164342\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19504.127414594783\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19411.946497365505\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19520.639942848466\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19668.510921765912\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19419.902431868974\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19517.81878002137\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19487.047107685903\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19451.796906066473\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19458.102435923116\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19489.55947232647\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19545.265305979403\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19602.14260114783\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19358.707267082696\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19526.293848724818\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19542.590466101665\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19436.62796181516\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19553.419748530767\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19371.19868824426\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19460.61076734738\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19572.97407056309\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19335.399113836207\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19473.78022237704\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19556.85875555138\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19567.938904736508\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19440.58236736393\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19567.556341218373\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19462.588669567263\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19389.901282094535\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19487.74560206249\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19489.691807843847\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19610.101244576126\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19604.206179895547\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19501.17999879978\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19492.19657808481\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19448.346832077295\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19475.67172285382\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19604.52532989418\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19389.425395248734\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19479.094738797823\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19543.977608660512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19474.60562051102\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19475.47406250439\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19572.687467863245\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19532.58557134485\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19482.244785646835\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19486.995403030283\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19438.435480194297\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19553.12688479083\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19525.233196344925\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19442.383795916572\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19455.1513194342\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19477.57152038804\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19519.943308570397\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19529.67616408553\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19495.51054833571\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19408.444971527737\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19621.81778700681\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19371.720567183467\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19462.363883117952\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19630.161991219466\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19499.451927918573\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19360.996143252636\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19621.969188850482\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19626.228134077544\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19324.611350653162\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19440.89734376657\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19467.93495581639\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19501.312217624483\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19533.24324373699\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19508.5092889879\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19488.681974941155\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19490.27542543468\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19373.01415147259\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19513.442438837123\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19316.749115319937\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19409.829627131134\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19465.405503336195\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19570.332915574476\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19452.8265926528\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19591.82228841178\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19591.221486344763\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19482.498342147\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19552.40767463873\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19492.62892282758\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19405.54340510723\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19389.62625596713\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19646.363006298765\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19516.192796443807\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19458.372955066778\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19476.84036638576\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19426.500842633162\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19502.96253228227\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19366.428763945743\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19567.925798075066\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19448.411134212423\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19588.96209779183\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19472.590397266526\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19490.6054635913\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19482.98168849181\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19454.73712667181\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19472.779260732303\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19423.65653889553\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19540.390719712857\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19453.820674703515\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19415.606436411832\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19393.176691651945\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19411.965759410654\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19522.08584957195\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19608.473770257093\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-20282.695755351288 -19391.387277141097\n",
      "Times =  2\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18844.800954385068\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18803.681219471342\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18726.24927414979\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18811.125078215646\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -19008.58552439463\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18771.01676611564\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18854.82909181597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18960.932093120606\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18994.470908256968\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18865.8713136965\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18980.21741616835\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18889.238996563974\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18748.694134184883\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18768.44729716837\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18872.20805380977\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18806.396049153722\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18922.961600002178\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18947.539690403468\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18789.984744135934\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18858.9360773833\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18680.664753669607\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18851.02678408733\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18826.788640267492\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18831.389777063825\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18839.433171425215\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18996.392061746483\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18906.583079845816\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18832.601690616957\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18919.273109676345\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18711.680801290568\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18759.7493769646\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18999.48758563784\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18914.233145846432\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18775.42155824244\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18992.706909069493\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18821.086986198545\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18758.67762926652\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18783.302179370243\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18895.52858403804\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18744.411528540557\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -19005.036337671605\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18820.17287367858\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18769.692036530178\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18843.61982765374\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18996.820557817002\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18730.834923002214\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18775.087528375476\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18927.8360238885\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18843.82291827664\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18744.488439056586\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18870.763564795838\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18802.58521906169\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18863.467243101288\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18757.495465946202\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18762.32318161599\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18807.16092632239\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18894.745997497943\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18946.749968265765\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18810.66049985863\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18928.862399872793\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18912.26136840433\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18667.195749424187\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18862.004566945932\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18814.317733095937\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -19009.90144192557\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18758.233421981935\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18893.243943021054\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18767.36769140595\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18849.475935784743\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18958.04417413116\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18606.98389628231\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18931.275807532857\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18807.501501310922\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18900.539584205373\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -19041.90922015238\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18768.152991177914\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18760.861630704436\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18978.212011503572\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18987.949218743273\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18890.188181672183\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18730.07792535828\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18898.57994332634\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18800.304729605086\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18878.55337749992\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18888.704602043756\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18885.99921040341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18817.311261820854\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18875.004997641998\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18971.568368631\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18766.02148621824\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18847.024975224642\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18885.234133218095\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18990.978756564455\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18882.758249339804\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18878.597581502196\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18958.424461630762\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18957.286078562138\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18720.353836061517\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18750.22908534752\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18825.344765867874\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18903.175494801195\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18897.738591030848\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18933.661633481555\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18689.779488671018\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18907.223550905368\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18968.701516224944\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18719.883842158975\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18909.01401664556\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18803.250492500352\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18843.559118576974\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18969.041746130348\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18891.31015796819\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18820.681320452684\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18856.373784447813\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18821.578654917812\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18779.68376719923\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18844.906349312405\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18950.862715390984\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -19010.158580870626\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18696.009126702305\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18945.72809077271\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18861.41233148647\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18848.525124744803\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18881.337498963054\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18803.52862291699\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18926.280788583867\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18893.101572897445\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18844.304339754795\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18988.925966141112\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18875.903968276747\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18869.052460287974\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18853.418559019272\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18876.521396813772\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18798.256025618997\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18819.368085881637\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18806.102029869304\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18729.547815661335\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18788.08094522869\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18773.881238071786\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18917.885204341477\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18782.925221311692\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18950.35960883105\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18834.53024034041\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18882.109503876844\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18939.46201042534\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18887.16707548844\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18945.836005240293\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18850.915656122364\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18852.935086188947\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18857.49120138469\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18851.187966540703\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18884.249840446013\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18804.11645945678\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18924.645277435688\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18864.292026861076\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18800.60313259504\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18865.22515903282\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18849.478854811037\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18842.323042049564\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18909.97098253412\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18783.45354752447\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18903.51166622868\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18781.793765760725\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18778.867704266562\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18869.127208551494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18901.77070164381\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18834.46615495926\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18797.57143600007\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18916.509346979507\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18855.57131963371\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18790.053322889034\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18800.56456444308\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18732.855395974613\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18904.976400672647\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18867.846856675867\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18836.708039720732\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18885.583369079413\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18855.93502938039\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18936.53727393018\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18934.455000493515\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18979.693777463945\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18927.76919105081\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18840.681521558927\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18886.823202405372\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -19007.582523387002\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18885.768918520986\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18745.920189025477\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18839.962447100006\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18941.046309575344\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18816.14377919677\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18958.085251691697\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18852.671020318186\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18781.655299866725\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18778.97266569466\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18847.9035619102\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18852.144998134605\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18832.279635309696\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18723.215059303686\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18829.719970102775\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18849.80967070532\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18818.17413777954\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18729.493069419936\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18804.542719233992\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18853.42515997895\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18897.81096080173\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18859.9285046861\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18896.510279626564\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18859.877343095508\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18735.0723870092\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18864.193030405157\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18896.62340855912\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18867.41876829925\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18908.878246156375\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18718.46270493936\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18926.781511113033\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18824.691991666507\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18807.880780119165\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18791.56639252585\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18693.47698087309\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18934.928495128675\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18857.73272697401\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18836.641982196452\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18839.7450408199\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18823.128038877592\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18883.19848990598\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -19045.608581598382\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18637.79893897483\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18656.694431826163\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18911.91970937706\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18766.8304964142\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18952.952004246505\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18805.396794880464\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18734.064031390448\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18864.513431381678\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18787.600234597932\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18950.251567886768\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18770.75448375096\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18886.590513730112\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18855.007353043653\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18877.67103842371\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18933.347058454467\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18868.414796802157\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18914.603950203003\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18913.43055824991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18829.002011944063\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18962.048874438522\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18830.549227025953\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18863.20389740026\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18923.481711838293\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18898.470947010526\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18809.814683222437\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18953.55367448728\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18721.801461421936\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18869.248997589606\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18796.468253666535\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18725.740109084578\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18890.65594634509\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18854.354538090898\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18867.96510802918\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18776.932978402005\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18761.069592371863\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18794.05863302278\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18793.247124395424\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18929.424566140075\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18783.757910525666\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18823.76113095662\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18825.59323952111\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18872.88733170584\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18960.408085517895\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18733.659279699772\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18978.33469860781\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18897.686477678413\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18890.00405398565\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -19015.849439948175\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18896.13615801093\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18886.59146347565\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18783.36643424148\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18714.94394515466\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18795.5286511119\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18909.280620939928\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18874.042277918965\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18893.94033100214\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18937.8371446851\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18797.423725064516\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18915.82169316616\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18881.672938101245\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18846.73759550856\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18925.22543005126\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18811.81933964956\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18881.70544404411\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18688.198040362142\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18811.645224220036\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18860.037849303873\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18927.10184492055\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18929.49002112252\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18787.58829519878\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18903.333627817385\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18984.457375992424\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18899.589437444487\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18982.710462144816\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18858.84765597168\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18784.23138222285\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18978.95393670589\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18870.64696881069\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18958.542883161215\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18797.081477270058\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18731.63857258288\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18981.41416961131\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18743.263390459975\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18905.4636229376\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18903.84776696273\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18976.57030717279\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18906.62368926203\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18913.586855099216\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18809.87602545764\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18876.564057321724\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18699.932485465662\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18822.227646582476\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18940.68772289552\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18892.980688455915\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18666.59876146702\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18772.52631975156\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18963.12426538851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18996.388279248633\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18948.363215335863\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18792.48109619583\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18960.12258497957\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18870.563299288733\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18902.551352263563\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18943.79704606763\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18865.665425103594\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18853.14978635213\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18826.52524616873\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18755.19429458707\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18823.703292138933\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18814.399612652236\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18774.697562415644\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18758.623407385676\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18950.246603990912\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18889.581696268662\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18844.973932110475\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18787.147434412287\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -19022.896557669203\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18871.677664303883\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18975.546905098807\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18889.015735803077\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18767.637109569427\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18928.31311217606\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18866.050569176303\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18928.882298993925\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18903.535787625682\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18837.063445666958\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18967.04851168725\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18993.07599887897\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18893.428716453538\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18851.775027264925\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18891.35714189817\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18917.09179335108\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18899.066551957167\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18920.341741101016\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18826.199449821746\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -19030.53280317287\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18775.55904703055\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18842.74697899803\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18771.731096603027\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18810.434031590208\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18839.61509355819\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18973.779849478276\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18941.31124635708\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18837.856725709855\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18921.090346740104\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18861.92487632629\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18866.729890481336\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18892.196274537288\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18860.480451874042\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18674.484277585914\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18809.03920855154\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18909.41465587262\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18708.11025295371\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18777.205536269998\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18840.563489249944\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18876.94243183259\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18801.56745158483\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18814.69810239862\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18838.4435001362\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18768.188189946515\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18859.608831587913\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18829.65583123259\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18867.046326017116\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18801.965311124746\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18853.759209995853\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18755.66843309388\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18865.251921891784\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -19036.000109530873\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18821.400662551077\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18887.651050966084\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18845.203666428886\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18787.1710829523\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18822.170363372556\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18816.83127558916\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18874.67437291873\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18957.169093017215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18744.175491465383\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18862.48851933043\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18913.426909457827\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18785.636660537704\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18894.02244121637\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18745.965484144923\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18829.72537905184\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18924.42611589541\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18742.81471140523\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18843.772719995344\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18910.217963265964\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18928.12780235385\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18793.465937356552\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18937.231888782553\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18812.109066818237\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18752.678103519564\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18821.691666871342\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18841.10823782687\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18974.914270384405\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18964.672410146683\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18869.9099349936\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18849.522410793463\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18813.93035840445\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18864.50419880607\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18954.92529586674\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18791.681637570182\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18860.020206889898\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18923.668272987445\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18820.48913654385\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18799.433840251604\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18924.913861449557\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18901.162321221753\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18823.461017159556\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18861.771629546514\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18816.840014456488\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18899.597449293127\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18905.03483667824\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18835.03178312659\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18827.331499278764\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18831.969897054663\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18894.247035945475\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18877.153863697615\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18834.827400103037\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18757.349962365148\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18995.926607231762\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18730.645445122424\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18813.54448781878\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18970.713526225107\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18874.00296561086\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18711.03746521711\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18983.270157421633\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18990.771029093514\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18684.490488216532\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18826.452909946674\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18815.87219736094\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18867.93803705603\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18870.01239646075\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18881.1437793368\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18864.199940521106\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18869.134769698063\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18733.70312848053\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18877.390391298242\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18654.844734739694\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18768.249225239662\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18821.446040830488\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18971.703158998404\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18825.407917865734\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18946.357476173813\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18918.010338352917\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18847.71199537308\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18919.76853448605\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18838.030219509386\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18779.287087230085\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18757.83298600004\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18977.057066567693\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18874.07182122776\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18839.616349434\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18828.185485101236\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18794.121548113515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18894.360088599133\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18738.975479300556\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18917.602717282974\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18801.542908235853\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18975.068334180556\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18808.42235944976\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18827.625128421827\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18835.91266739411\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18821.655666442522\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18852.94074485617\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18766.56692126389\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18891.56332191302\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18802.21705398161\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18787.627744338966\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18746.785322600485\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18780.66113906011\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18900.617660841024\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18967.177241373367\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19736.9986232255 -18741.733817438693\n",
      "Times =  3\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18724.17764022525\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18677.539111936137\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18586.117998004895\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18713.311288591947\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18850.61517832106\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18660.80475305909\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18758.472520261224\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18815.62513965264\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18861.113356900463\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18724.509396908343\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18828.321594806494\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18723.454923759982\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18627.28914313386\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18656.706026798398\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18754.033645536016\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18683.171480993322\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18783.308924133813\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18818.855009469553\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18660.41409453423\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18729.61013739953\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18558.812069193824\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18711.042569863796\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18694.130741277248\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18697.188882321203\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18704.170356739593\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18866.330775993054\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18779.768990100296\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18680.796643677222\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18778.426283798537\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18592.196863374353\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18614.304394236024\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18860.78168787879\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18779.11753758871\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18633.424170069\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18848.77108576863\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18688.573789664028\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18633.71155644598\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18643.21152290543\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18728.429988199412\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18619.329424833686\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18852.504605487487\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18672.846364561698\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18629.81495370671\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18721.307656370256\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18847.057755563823\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18576.200726351835\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18651.956566708384\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18792.781352190217\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18720.991809760148\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18587.907287048267\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18728.377271766003\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18675.41626304953\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18718.65635860594\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18626.113911967623\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18612.121122271947\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18643.49989806224\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18730.715102104856\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18813.35018065828\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18671.611552488444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18791.31634023999\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18761.119970074426\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18532.85887115636\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18709.91751471336\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18682.51910531359\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18872.904688867726\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18643.107098612985\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18749.709874306922\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18650.99401785403\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18697.30350396592\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18818.881585116662\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18432.37059563812\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18786.919716259723\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18672.766883307417\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18761.088755593846\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18909.691966149105\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18630.718918951363\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18624.86367076362\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18859.398432274043\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18854.180289339947\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18755.505312861704\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18619.91640249684\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18767.678567054674\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18657.708032220864\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18748.0848492886\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18775.888401852662\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18780.276389492527\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18677.747992786106\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18738.50205686774\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18799.79514393773\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18630.246696615934\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18719.381148845037\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18751.30029320356\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18864.988040603093\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18734.02222963633\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18761.687602189762\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18781.591241200487\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18779.10918172893\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18558.900973680113\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18607.47127758758\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18674.702532766732\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18776.89292421483\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18760.14430142324\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18784.62333069287\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18550.50402543186\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18734.565834958907\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18821.18765563498\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18586.920309636418\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18783.554009681462\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18661.3249360556\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18697.490965646306\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18840.254156226136\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18738.12354134474\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18714.163195647612\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18747.49386153705\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18688.63552949681\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18651.517654224695\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18699.275309078977\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18827.863514985907\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18874.29414716673\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18560.42894949356\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18822.18934295984\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18720.335547354\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18669.73047643444\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18750.57090097094\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18694.7547275197\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18809.062762081376\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18778.032877950718\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18696.083275554418\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18854.916175981867\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18743.490751498386\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18740.34536557955\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18729.068178588957\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18765.46696418088\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18678.606969715125\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18691.31612318708\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18694.995705962232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18569.909474881086\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18652.477915602656\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18631.005729146575\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18783.06376173804\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18673.4706710783\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18808.666482946646\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18683.911809485497\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18705.884500885935\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18817.11080113852\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18752.439214395967\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18803.934978220692\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18733.90369886762\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18751.26901298241\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18733.14770244054\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18703.300243319976\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18760.089864252404\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18672.017584350906\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18804.939880533468\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18727.9695332793\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18666.548986394475\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18766.74390270539\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18714.5487367927\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18724.237452346773\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18784.44763487975\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18673.994514839327\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18747.818881123967\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18661.22283243796\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18679.909526724186\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18728.679500186885\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18766.29222916965\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18663.110709443947\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18683.84830638299\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18792.64587475793\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18694.364377247326\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18646.145769030853\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18659.733079517166\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18636.015399874857\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18775.275570199476\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18731.54214417851\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18676.061649905496\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18757.79469830944\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18715.87836343922\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18814.156575279925\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18793.184595855182\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18847.52429702253\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18796.059081383813\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18694.942994188248\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18735.5178207026\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18878.3003106983\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18750.110683834668\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18587.95367076407\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18723.31894573648\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18803.366051170207\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18682.641055583994\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18817.72979991311\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18739.420133160074\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18644.244660469183\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18656.947873668185\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18693.745655541134\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18716.895691901933\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18704.738677382546\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18591.20298722756\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18698.70713726335\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18724.62937212908\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18684.756848461697\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18592.504569129138\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18671.223175800904\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18735.245367542393\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18752.68128940039\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18730.52334696607\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18749.41099285409\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18746.697147273422\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18598.501495358174\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18716.078701416536\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18755.092310437827\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18723.39770156441\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18793.35774569771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18588.821299613574\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18763.38239252098\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18667.074080910996\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18696.400744505798\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18654.81391127076\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18579.346608427815\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18801.300323069307\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18717.243700032966\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18693.839444437835\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18683.332621108922\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18695.148679040794\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18734.46105174914\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18943.643703771024\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18523.73937992824\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18516.860645573783\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18762.420102832795\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18677.785143403118\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18832.130170936347\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18656.185616704537\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18587.83109452378\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18721.613983862248\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18656.71596424137\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18827.82266586069\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18614.94932509625\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18752.07881512472\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18737.204768194937\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18736.69604904858\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18834.10484522154\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18712.937581592396\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18790.037674431096\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18771.3759140381\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18684.98750423642\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18824.942570898944\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18685.65742402268\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18749.07947195362\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18792.04511038971\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18756.029016624558\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18694.170319953413\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18805.67286721751\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18599.235803713374\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18717.24821951837\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18633.90667750793\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18587.21841344748\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18768.374658497505\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18720.806713845053\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18741.30371869097\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18670.754646758687\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18660.528296174638\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18647.242592696806\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18644.38682218088\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18792.742716010143\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18642.575658608606\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18675.68297547505\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18684.057840839978\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18774.82482040709\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18831.30151570719\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18606.4970305654\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18853.128545610853\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18771.357481778607\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18740.161198031972\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18856.7796997599\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18741.751371668473\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18760.691491344995\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18670.131700544374\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18567.691530908567\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18660.64679055003\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18765.757428300643\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18737.929253050832\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18759.297088543175\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18801.73507798418\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18659.116142161693\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18784.923171276285\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18754.62476657971\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18726.42363848523\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18745.65915807281\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18658.130783755994\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18748.671073575028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18546.052968386604\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18684.210348924233\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18706.25638610762\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18805.213760329407\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18770.8794480758\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18661.41039016518\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18762.981775360662\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18825.71701537779\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18769.287285177044\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18830.157158612434\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18753.769248226203\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18656.003570133063\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18828.749275277358\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18730.896784785422\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18823.654349552708\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18663.741863782936\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18591.697949225294\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18858.384260759718\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18600.316044576473\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18757.390882438107\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18743.08108858341\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18839.780422405383\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18785.57339124499\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18776.790270266047\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18689.574913829645\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18709.294277880406\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18544.2529385079\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18680.2638863673\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18788.816760635877\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18729.636725487897\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18504.773001804977\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18638.953960430674\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18855.556999456203\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18858.57674410795\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18803.48092798805\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18652.423306181983\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18836.956163362163\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18721.848204538182\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18771.507673512235\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18820.51097306701\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18723.978291930827\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18717.139604955082\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18689.24842054965\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18620.294507027953\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18680.804207618727\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18671.459913399707\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18647.504151655667\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18634.615596668416\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18820.99117103638\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18769.845770563923\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18710.775512833465\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18660.28022006509\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18869.49629315867\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18728.90146795981\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18859.229884917942\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18753.962800268768\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18637.14485092197\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18781.960729909588\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18724.681556558964\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18809.682931852567\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18748.805630940322\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18698.57019774979\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18802.08910487187\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18864.284992566194\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18758.1700020118\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18729.047629968012\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18780.57413658699\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18788.002534354444\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18755.81935443511\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18768.74320172304\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18675.214956523127\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18897.505939527422\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18646.329232675183\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18673.048433504948\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18650.843412879087\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18650.396301108012\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18687.104410934502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18818.271887507944\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18778.165085664266\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18708.056123578623\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18777.37516049215\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18719.725225976785\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18721.345319890897\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18749.170739242556\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18752.7118915348\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18548.750297324856\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18640.96679829189\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18788.195645769592\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18572.834338774126\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18647.575965567892\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18729.565780403922\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18760.25557731271\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18640.478022147392\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18712.109898149793\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18702.31665436468\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18613.78642004824\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18699.461806612533\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18735.627992728503\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18727.747734548033\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18666.86590737635\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18708.354177077774\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18635.388723656022\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18753.05097061833\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18902.35295164143\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18678.091611853466\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18730.629186867056\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18708.251720849243\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18656.900282925304\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18662.957047456435\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18679.3863675876\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18736.50352696441\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18813.746246967326\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18608.812793992824\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18730.559002047357\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18788.703752028126\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18685.812646977804\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18764.474966008736\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18619.59011461829\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18658.18506254476\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18790.526139877325\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18603.302359542486\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18689.18292891997\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18778.352088069267\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18791.184223455533\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18670.450187355113\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18799.21166913148\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18684.07380695019\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18608.853195580028\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18692.47896265277\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18711.335792488724\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18819.113117167646\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18828.931567370317\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18724.209100866094\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18702.05305366681\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18695.28325394091\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18729.028077752657\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18818.977234597103\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18637.761182463597\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18710.081981844563\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18791.30220375332\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18674.527437525656\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18702.070260146487\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18760.689926546926\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18762.722618312822\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18698.549973984245\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18736.215993455637\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18657.035886077396\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18776.46349724974\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18737.879609251144\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18693.43167444657\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18688.992467681925\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18692.056024833102\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18735.162208205307\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18737.766378471893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18699.8272885236\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18647.87079541196\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18852.36533699582\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18590.400114916134\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18675.457444174543\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18845.686493751677\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18740.249018404505\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18585.689674823105\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18856.824916408517\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18862.884355118662\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18538.25591234273\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18680.034743696444\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18696.574169472402\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18704.146854941857\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18751.40804618226\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18736.6598744766\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18728.074711837962\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18712.509275036857\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18590.81049174263\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18747.74431846558\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18524.70764152568\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18665.176016021163\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18669.824126854484\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18798.034283717763\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18683.992698843234\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18842.658077396183\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18796.81363621082\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18706.02854027461\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18765.92764508862\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18695.519366156645\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18607.759518149993\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18614.824591364824\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18868.470566955024\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18784.14690620328\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18701.57007977268\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18689.171101654796\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18672.27863025197\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18773.464863811583\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18600.230173588192\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18794.03717378216\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18670.14309156252\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18826.945603656888\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18676.084431907224\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18710.357788153073\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18685.152572542844\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18689.2107185798\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18707.182750834938\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18645.58074450892\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18760.685522359563\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18660.507816340312\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18634.45483988578\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18614.266364598592\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18616.53374110874\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18736.344263525967\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18844.728444499724\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19531.310660373067 -18618.007149976773\n",
      "Times =  4\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19070.86847766713\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19020.941940632383\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18972.014730854036\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19032.22060272739\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19225.142295610116\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19008.99609455889\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19111.040110219907\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19176.66960852276\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19211.162415153998\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19082.14415022148\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19196.70659506772\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19073.87290110898\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18990.053104233368\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18978.837248155767\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19085.672149512335\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19042.627383695522\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19109.29132073894\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19170.68496733615\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19005.031331830724\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19084.87702130664\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18903.82986521606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19075.497365807892\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19050.252021600645\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19028.502289553147\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19055.470231559804\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19217.98095955806\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19147.19498456808\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19047.03874552504\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19155.1946240085\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18953.3146716261\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18954.95226445171\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19198.97338410527\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19150.664888755244\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19002.096077244387\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19212.163407725562\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19047.5156396653\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18993.01400864421\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19009.58732863308\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19116.74652083601\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18975.734849241086\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19220.609096927245\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19049.303788835903\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19014.416835610213\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19100.779965350688\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19207.843535704265\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18929.426410331555\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19010.851813623784\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19155.955950056617\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19072.056270910194\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18963.9497774227\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19087.666439927278\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19039.836703992347\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19069.87402545671\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18985.69418401977\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18981.816877629113\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19012.992973621527\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19083.197284412985\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19171.901062947563\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19020.984541348113\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19145.093834284497\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19079.79616180635\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18877.905940137494\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19078.807714317612\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19026.98964049756\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19223.250055165234\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18992.888242820107\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19119.68985326495\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18984.115271221275\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19083.875842095338\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19159.83286161114\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18814.155376073162\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19159.738273700787\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19034.959569006576\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19096.119586297806\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19280.354136478534\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18991.498955492127\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18984.59957573231\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19198.857987856674\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19188.26277134724\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19109.57569528302\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18968.16934437212\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19114.79447185999\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19012.232104755938\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19108.96305255432\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19126.940756281765\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19127.88605916514\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19018.99145311274\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19099.98355948712\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19171.133816286863\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18962.853062986105\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19062.49254373557\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19115.62127215617\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19233.772832400507\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19095.53242909875\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19078.53997438728\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19147.510586314245\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19150.976507032057\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18927.004804178163\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18956.104569971038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19031.65868351459\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19127.555206944424\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19127.775785351605\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19141.505303691192\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18897.158334427568\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19129.075173503054\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19184.18962182377\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18962.702952533702\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19126.26160957337\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19034.01944415962\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19062.007786959253\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19195.810701259496\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19122.437238791772\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19065.103359656896\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19084.423351476576\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19043.82226707598\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19004.11495675024\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19050.984396967455\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19190.19356956154\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19209.232208405752\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18918.18075201807\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19177.183770773663\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19058.861718304972\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19063.812535630997\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19094.494148270613\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19052.45497724444\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19150.201478976956\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19127.64514498209\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19046.681620938747\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19214.610444595775\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19092.510536136706\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19116.470489949355\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19074.506176165334\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19097.9501008696\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19017.722970730036\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19024.71916849891\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19020.23506191545\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18927.414906874244\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19001.502855060608\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18973.593893240366\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19135.03468182226\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19007.537430215696\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19166.501242484454\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19056.086687136332\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19097.15367868145\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19161.36796538351\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19081.59160278696\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19151.610067806254\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19060.638636002725\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19101.181774631095\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19085.595896359704\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19074.55879762809\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19085.251441222914\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19019.201344005636\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19169.49104932007\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19087.34892313304\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18990.81338455171\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19104.26947044015\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19060.21975253509\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19055.66008376386\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19128.76768167679\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19010.71298461111\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19069.602251019616\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18967.31964051054\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19008.172058182274\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19080.644095013464\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19124.78783062919\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19022.132902793375\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19003.94042084665\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19142.64571261256\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19035.39114914781\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18973.671762535552\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19002.62849842125\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18950.315652962116\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19135.618567850368\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19086.920205053848\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19023.89796499526\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19103.142147591956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19081.300921927155\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19155.100566387642\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19165.20262846465\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19200.338225527343\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19148.56316893121\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19062.290788454553\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19066.188915535316\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19216.882095742214\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19117.22810430869\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18954.37883548268\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19055.969952238138\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19182.65821852731\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19048.580954067627\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19162.127375055312\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19074.167527566642\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18980.491896351392\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19002.73468600125\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19078.913495791603\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19061.838052629853\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19057.42307650305\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18956.56280988052\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19031.705434014966\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19075.332448858266\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19036.697929563023\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18941.237823075193\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19023.154174323186\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19077.22527286297\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19115.29273979912\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19076.079230274623\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19089.9471931209\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19081.65392999923\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18982.122581517036\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19065.245672870355\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19088.70967164092\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19081.476896151944\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19125.223675908645\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18940.387602702656\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19111.873010357864\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19014.38360058009\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19049.988280844427\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19030.0998467911\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18917.836783976665\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19160.416345154787\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19058.862329900818\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19049.964355371176\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19054.969649928436\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19061.840567542822\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19094.94388386996\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19264.51458278259\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18863.653896424377\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18900.39856688372\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19137.70209301615\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18986.030114128785\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19179.00621337638\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19015.48447290085\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18963.120101557415\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19068.302954630675\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19005.75262583852\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19171.485240425427\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18969.91522511507\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19097.155860149913\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19094.865495644888\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19109.554322596756\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19163.89609945581\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19089.365933074\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19141.72403345853\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19131.18598435918\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19034.942502968006\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19187.763466795925\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19034.911686525622\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19088.52637846239\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19147.45595475477\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19118.634175642583\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19014.506431776364\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19135.308489497806\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18929.577682739236\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19081.926749498536\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18969.769182886554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18932.547513728998\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19102.59217648288\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19087.1877890987\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19097.57355350054\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19001.29140896962\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19032.42331570554\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18995.356862049906\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18999.904825058053\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19154.6463769112\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18988.452678967675\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19010.222607419382\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19050.108452737644\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19092.427470563343\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19188.03062899349\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18954.653292092462\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19192.61004385549\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19125.888450406437\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19101.810940104235\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19213.47098437854\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19092.42355242095\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19102.042757654675\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19004.87540465653\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18935.297445470052\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19000.47565364719\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19131.10132173583\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19095.825742507746\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19103.723326195806\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19156.081304338077\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19016.190152038846\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19134.770039726827\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19101.37591038982\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19092.49385251281\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19121.454919505977\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19020.99730191588\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19073.288650826322\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18908.66196958682\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19029.987310818404\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19078.7762675401\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19160.94044011617\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19131.27384255936\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19014.09566620522\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19092.413049410734\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19175.268487519184\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19121.60503210705\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19178.401060932636\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19089.595990966223\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18994.23264764839\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19190.697903084896\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19088.31538059065\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19176.985300396515\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19036.967836732263\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18970.694897364498\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19191.975268356553\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18974.435318998134\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19105.63574594428\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19106.04320782272\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19200.74431179453\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19144.227533347017\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19145.746303635147\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19061.59513948671\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19067.55432064917\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18898.314461440466\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19057.88255312055\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19147.433287151904\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19099.93946683929\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18877.215763486918\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19002.067411483025\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19193.296566931538\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19247.07405875741\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19158.882860485857\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19017.461684759033\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19200.03432240357\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19092.315838308867\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19130.873102124402\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19175.544494118287\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19105.58845842809\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19084.440026813645\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19053.595497263323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18995.891586890724\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19021.18898667994\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19026.39008564761\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18980.473159879657\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18955.769400917205\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19184.421504968042\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19122.164696046275\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19069.003671704817\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19009.058043034467\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19214.231644547028\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19084.137581977415\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19217.76592995088\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19138.53809163234\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19006.46806755055\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19144.87088546924\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19103.83551555859\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19166.73978440394\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19095.710315145163\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19067.008835316916\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19177.59191097708\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19217.17349905819\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19114.54443346231\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19082.442120151405\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19125.387621178932\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19158.04867423289\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19141.320048745314\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19104.565053667306\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19013.076177996434\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19250.057896829272\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18975.97075464046\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19042.629436172007\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19013.60952861431\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19019.949763076333\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19051.3753550994\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19194.62770445682\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19138.287731754124\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19067.104055890035\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19147.277534490215\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19095.858118397875\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19080.586064130155\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19110.16745958708\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19093.132988110337\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18897.753995704625\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18991.620499220553\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19140.8661019812\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18924.31580029754\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18971.707695242563\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19076.423173362033\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19133.11027914151\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19009.503449006283\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19047.585196885768\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19044.43696188724\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18988.954284556967\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19081.98605804328\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19056.24657554012\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19078.32238114874\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19049.90466202901\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19079.852018918707\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19000.92241800781\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19101.04858230122\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19251.986287220516\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19038.085842170927\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19121.491609892288\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19063.597112642965\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19005.67730965448\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19009.025523132797\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19026.35545549829\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19094.760838775466\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19185.74352395581\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18943.525520390504\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19087.141824521193\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19123.663769879626\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19011.103826422925\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19137.85533101115\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18973.29706644668\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19036.90669253898\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19124.489996023305\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18959.852162722524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19052.723150825135\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19114.91031170714\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19172.92841623586\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19023.49677172217\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19134.136417619055\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19029.887315690212\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18955.27178246765\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19046.40968383174\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19066.8359132627\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19202.098132512652\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19203.381367760958\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19092.410246936408\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19066.66460150112\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19040.45452165629\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19073.213979890086\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19177.632430555106\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18982.60290810159\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19066.773276597054\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19144.38240076769\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19027.651978084203\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19029.232743068365\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19127.678492986986\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19128.319386009887\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19058.803923196625\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19062.84957577483\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19019.63964280549\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19140.848721141745\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19096.845602376732\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19041.515613426694\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19030.462648039975\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19043.818257861316\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19108.95745531513\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19109.460823082918\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19066.034574184105\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18992.055622320077\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19197.500781979616\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18959.08336654764\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19043.68732426031\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19200.427913803407\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19069.875163333938\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18913.209025402633\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19199.230989249245\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19215.171810674598\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18901.20766491831\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19031.418650045533\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19036.390157230253\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19097.157097878033\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19113.76320036987\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19092.33079161628\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19067.018633301574\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19092.259349528416\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18955.717613741523\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19099.458719693703\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18903.034425231283\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19002.288336354704\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19028.597840627037\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19171.46804959833\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19048.779411201085\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19196.081653161513\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19155.90273873139\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19072.313060176788\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19119.27306607679\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19064.609409356937\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18991.465701289148\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18984.980091283654\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19219.471671619693\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19100.924139082366\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19029.89972842918\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19038.74932757581\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19013.030242027973\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19132.596053366764\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18949.42338310753\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19136.229862813685\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18997.021657509067\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19163.227637934517\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19028.56306090934\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19040.565649599095\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19069.924515422586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19032.967329248397\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19044.606684907558\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19008.13567382945\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19091.71422475866\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19027.281018117606\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19004.244296941888\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18980.209259768733\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18997.291617824616\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19088.614240863022\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -19188.816628463992\n",
      "['a --> b', 'a --> c', 'a --> d', 'a --> e', 'a --> f', 'f --> g']\n",
      "-19925.74393130834 -18981.441861350075\n"
     ]
    }
   ],
   "source": [
    "#the number of times to sample\n",
    "times = 5\n",
    "\n",
    "## the size of the test set\n",
    "\n",
    "\n",
    "violations = np.zeros(len(models))\n",
    "violation_mean = np.zeros((len(models), times))\n",
    "violation_mean2 = np.zeros((len(models), times))\n",
    "mean = np.zeros((len(models), times))\n",
    "\n",
    "fold = 0\n",
    "\n",
    "from pycausal import prior as p\n",
    "def get_bic(df, prior):\n",
    "\n",
    "    tetrad.run(algoId = 'gfci', dfs = df,  scoreId = 'sem-bic-deterministic', dataType = 'continuous',\n",
    "               structurePrior = 1.0, samplePrior = 1, maxDegree = -1, maxPathLength = -1, priorKnowledge = prior,\n",
    "               completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True,\n",
    "               penaltyDiscount = 200\n",
    "               )\n",
    "    BIC = tetrad.getTetradGraph().getAllAttributes().toString()\n",
    "    BIC = float(BIC.split('=')[-1].split('}')[0])\n",
    "    return BIC #/ len(df)\n",
    "import itertools\n",
    "def get_pairs(lst):\n",
    "    a = set()\n",
    "    for i in itertools.combinations(lst,2):\n",
    "        a.add(i)\n",
    "    return a\n",
    "\n",
    "full_conx = get_pairs(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n",
    "forced_conx = set({('a','b'), ('a','c'),('a','d'),('a','e'),('a','f'),('f','g')})\n",
    "restricted_conx = full_conx.difference(forced_conx)   \n",
    "prior = p.knowledge(requiredirect =  list(map(list, forced_conx)),\n",
    "                       forbiddirect = list(map(list, restricted_conx))\n",
    "                       )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for t in range(times):\n",
    "    print(\"Times = \", t)\n",
    "    df_test = gen_data(SIZE = nb_test)\n",
    "    x_test = df_test[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "    y_test = df_test['g'].values\n",
    "    bic_orig = get_bic(df_test,prior)\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        if type(models[idx]) is list:\n",
    "            keras.backend.clear_session()\n",
    "            model = load_model(model_name)\n",
    "        else:\n",
    "            model = models[idx]\n",
    "        test_df = pd.DataFrame(x_test, columns = ['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "        test_targets = pd.DataFrame(model.predict(x_test), columns = ['g'])\n",
    "        test_df = test_df.join(test_targets)\n",
    "       \n",
    "        mean[idx][t] = mean_squared_error(y_test, model.predict(x_test)) \n",
    "        bic_pred = get_bic(test_df,prior)\n",
    "        print(tetrad.getEdges())\n",
    "        print(bic_orig, bic_pred)\n",
    "        violation_mean[idx][t] = bic_pred\n",
    "        violation_mean2[idx][t] = bic_pred\n",
    "        #print(bic_orig - bic_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_name =  temp/e0 Violations =  0.0\n",
      "Average_violations =  -18948.44493457977 299.12518383956575\n",
      "MSE =  1.040246978564548 0.011303135563931083\n",
      "Model_name =  temp/e1 Violations =  0.0\n",
      "Average_violations =  -18905.375589873594 298.8976194068354\n",
      "MSE =  1.03996163087596 0.009555784547025183\n",
      "Model_name =  temp/e2 Violations =  0.0\n",
      "Average_violations =  -18839.23396812999 304.5733726080612\n",
      "MSE =  1.041794294316458 0.007865244519275555\n",
      "Model_name =  temp/e3 Violations =  0.0\n",
      "Average_violations =  -18930.30589704993 298.10313106296854\n",
      "MSE =  1.0299412499772722 0.01243656435101075\n",
      "Model_name =  temp/e4 Violations =  0.0\n",
      "Average_violations =  -19103.64239245923 305.67777402440277\n",
      "MSE =  1.0355111129865768 0.013689812323026638\n",
      "Model_name =  temp/e5 Violations =  0.0\n",
      "Average_violations =  -18893.952900578144 312.1207932492023\n",
      "MSE =  1.0346664572300797 0.014049923710767642\n",
      "Model_name =  temp/e6 Violations =  0.0\n",
      "Average_violations =  -18985.52461376953 302.57563652530376\n",
      "MSE =  1.0323162452064483 0.01329547600166737\n",
      "Model_name =  temp/e7 Violations =  0.0\n",
      "Average_violations =  -19061.71679028699 306.0341665875224\n",
      "MSE =  1.0361583809651602 0.008868741714269065\n",
      "Model_name =  temp/e8 Violations =  0.0\n",
      "Average_violations =  -19091.86741480302 297.8610001289761\n",
      "MSE =  1.039725780932961 0.009348680625740471\n",
      "Model_name =  temp/e9 Violations =  0.0\n",
      "Average_violations =  -18971.145227078512 305.8320878255764\n",
      "MSE =  1.040496047770921 0.01328568685418294\n",
      "Model_name =  temp/e10 Violations =  0.0\n",
      "Average_violations =  -19074.127264932555 304.9926729152441\n",
      "MSE =  1.04319172345738 0.012546763702742374\n",
      "Model_name =  temp/e11 Violations =  0.0\n",
      "Average_violations =  -18970.635671192595 309.34720311363543\n",
      "MSE =  1.0342726660386057 0.012531903149104443\n",
      "Model_name =  temp/e12 Violations =  0.0\n",
      "Average_violations =  -18870.08211446923 305.17862114611387\n",
      "MSE =  1.0385718805350597 0.012144242021360755\n",
      "Model_name =  temp/e13 Violations =  0.0\n",
      "Average_violations =  -18886.775660427695 306.42502671769677\n",
      "MSE =  1.042160125079373 0.0130535994156671\n",
      "Model_name =  temp/e14 Violations =  0.0\n",
      "Average_violations =  -18976.860569439894 306.32115772013975\n",
      "MSE =  1.0325708205085087 0.016146377164534112\n",
      "Model_name =  temp/e15 Violations =  0.0\n",
      "Average_violations =  -18912.00223025764 303.89945131336196\n",
      "MSE =  1.0365410958863912 0.012047660591182827\n",
      "Model_name =  temp/e16 Violations =  0.0\n",
      "Average_violations =  -19013.53844348671 306.78589478830537\n",
      "MSE =  1.0399825688937818 0.013572129634013705\n",
      "Model_name =  temp/e17 Violations =  0.0\n",
      "Average_violations =  -19057.861218841117 307.284075714047\n",
      "MSE =  1.0369443798328448 0.014716265631501729\n",
      "Model_name =  temp/e18 Violations =  0.0\n",
      "Average_violations =  -18894.501859266893 310.79445591884183\n",
      "MSE =  1.0374956875934764 0.010301509799173937\n",
      "Model_name =  temp/e19 Violations =  0.0\n",
      "Average_violations =  -18968.286333723034 310.8254985955111\n",
      "MSE =  1.0318002143270195 0.012176068585509996\n",
      "Model_name =  temp/e20 Violations =  0.0\n",
      "Average_violations =  -18782.966137593314 301.78022367249076\n",
      "MSE =  1.0368122877414125 0.011716024498608387\n",
      "Model_name =  temp/e21 Violations =  0.0\n",
      "Average_violations =  -18957.072761155665 309.84764702144196\n",
      "MSE =  1.035936324435989 0.012287999510521116\n",
      "Model_name =  temp/e22 Violations =  0.0\n",
      "Average_violations =  -18931.783099809072 311.1790599098694\n",
      "MSE =  1.0411053878331564 0.009824038630747838\n",
      "Model_name =  temp/e23 Violations =  0.0\n",
      "Average_violations =  -18923.381726822376 297.20109255837303\n",
      "MSE =  1.0333765972772022 0.015051071527687535\n",
      "Model_name =  temp/e24 Violations =  0.0\n",
      "Average_violations =  -18939.814298087458 296.75445088971935\n",
      "MSE =  1.0363794449835206 0.01458222029368474\n",
      "Model_name =  temp/e25 Violations =  0.0\n",
      "Average_violations =  -19105.03664430968 316.0225136540261\n",
      "MSE =  1.0379296117752248 0.016428138790366895\n",
      "Model_name =  temp/e26 Violations =  0.0\n",
      "Average_violations =  -19020.686919356103 304.3968776685794\n",
      "MSE =  1.0345534932056215 0.011187485167756354\n",
      "Model_name =  temp/e27 Violations =  0.0\n",
      "Average_violations =  -18932.126388042445 308.9910743955768\n",
      "MSE =  1.0374544305738862 0.013713185894883198\n",
      "Model_name =  temp/e28 Violations =  0.0\n",
      "Average_violations =  -19023.508030755514 315.6541136223259\n",
      "MSE =  1.0431796858922873 0.0127354010706444\n",
      "Model_name =  temp/e29 Violations =  0.0\n",
      "Average_violations =  -18832.079886495216 303.20870956554995\n",
      "MSE =  1.0377034817795736 0.014769961580190267\n",
      "Model_name =  temp/e30 Violations =  0.0\n",
      "Average_violations =  -18851.29659443368 307.21819959525817\n",
      "MSE =  1.0340162933456731 0.018287884752375427\n",
      "Model_name =  temp/e31 Violations =  0.0\n",
      "Average_violations =  -19092.83811236513 307.32383098552083\n",
      "MSE =  1.0361544427579132 0.008624819578934005\n",
      "Model_name =  temp/e32 Violations =  0.0\n",
      "Average_violations =  -19022.544400082654 310.3125119619571\n",
      "MSE =  1.0419539523674382 0.01198743708316606\n",
      "Model_name =  temp/e33 Violations =  0.0\n",
      "Average_violations =  -18876.82448472907 304.6459421216919\n",
      "MSE =  1.0374874803892182 0.010137557570561347\n",
      "Model_name =  temp/e34 Violations =  0.0\n",
      "Average_violations =  -19085.58586590989 305.7284958332247\n",
      "MSE =  1.0377303658898067 0.00900675522289026\n",
      "Model_name =  temp/e35 Violations =  0.0\n",
      "Average_violations =  -18921.86154427808 304.01986539252755\n",
      "MSE =  1.040331552213004 0.01041332247972041\n",
      "Model_name =  temp/e36 Violations =  0.0\n",
      "Average_violations =  -18881.52733588212 320.4867985297607\n",
      "MSE =  1.0335019080277579 0.013882094482246012\n",
      "Model_name =  temp/e37 Violations =  0.0\n",
      "Average_violations =  -18885.172167200086 305.18413340286094\n",
      "MSE =  1.034547906474169 0.010424609771452988\n",
      "Model_name =  temp/e38 Violations =  0.0\n",
      "Average_violations =  -18994.589022965614 309.11347820099525\n",
      "MSE =  1.0372197497462838 0.011239595215937367\n",
      "Model_name =  temp/e39 Violations =  0.0\n",
      "Average_violations =  -18857.837813427654 306.7400487770463\n",
      "MSE =  1.0313031080054873 0.01186078935731651\n",
      "Model_name =  temp/e40 Violations =  0.0\n",
      "Average_violations =  -19101.355155777193 307.7837777218091\n",
      "MSE =  1.0368845172468726 0.011906648426589986\n",
      "Model_name =  temp/e41 Violations =  0.0\n",
      "Average_violations =  -18916.26536949731 308.7326465364617\n",
      "MSE =  1.043572973531276 0.01202580076908012\n",
      "Model_name =  temp/e42 Violations =  0.0\n",
      "Average_violations =  -18883.904392318236 314.971300676979\n",
      "MSE =  1.0421251069310824 0.013865930127307304\n",
      "Model_name =  temp/e43 Violations =  0.0\n",
      "Average_violations =  -18962.41637777161 308.6839515437266\n",
      "MSE =  1.034663644664768 0.011457232934616122\n",
      "Model_name =  temp/e44 Violations =  0.0\n",
      "Average_violations =  -19095.762829801366 311.4789221547357\n",
      "MSE =  1.0348154357661719 0.021034646995089147\n",
      "Model_name =  temp/e45 Violations =  0.0\n",
      "Average_violations =  -18814.551459970833 294.5953081095452\n",
      "MSE =  1.0426071057582622 0.009436961535978537\n",
      "Model_name =  temp/e46 Violations =  0.0\n",
      "Average_violations =  -18884.02429530309 307.2608073929627\n",
      "MSE =  1.0344894890529592 0.011567921454491021\n",
      "Model_name =  temp/e47 Violations =  0.0\n",
      "Average_violations =  -19028.369049452267 307.55203783693264\n",
      "MSE =  1.0427746251617602 0.011461013326418051\n",
      "Model_name =  temp/e48 Violations =  0.0\n",
      "Average_violations =  -18953.52717151444 302.9435707680002\n",
      "MSE =  1.0338460590986709 0.011696885246505528\n",
      "Model_name =  temp/e49 Violations =  0.0\n",
      "Average_violations =  -18834.23508810068 308.62335262902747\n",
      "MSE =  1.0434045486089225 0.014265097418828629\n",
      "Model_name =  temp/e50 Violations =  0.0\n",
      "Average_violations =  -18971.361631232925 306.712523021716\n",
      "MSE =  1.030918637184525 0.014578990342310427\n",
      "Model_name =  temp/e51 Violations =  0.0\n",
      "Average_violations =  -18918.453746951887 299.6312998424628\n",
      "MSE =  1.0363732946080437 0.014235482834945027\n",
      "Model_name =  temp/e52 Violations =  0.0\n",
      "Average_violations =  -18960.473682687836 307.10658536666625\n",
      "MSE =  1.0340575016471987 0.011300528309930327\n",
      "Model_name =  temp/e53 Violations =  0.0\n",
      "Average_violations =  -18860.513115553073 301.01673651256374\n",
      "MSE =  1.0351297814612832 0.010929544295267947\n",
      "Model_name =  temp/e54 Violations =  0.0\n",
      "Average_violations =  -18862.379768414645 309.5537214139099\n",
      "MSE =  1.0346198779171274 0.012338055737600604\n",
      "Model_name =  temp/e55 Violations =  0.0\n",
      "Average_violations =  -18894.782884106047 307.9230807418548\n",
      "MSE =  1.0349418459669544 0.01664570978788785\n",
      "Model_name =  temp/e56 Violations =  0.0\n",
      "Average_violations =  -18979.65231174189 313.0367743135216\n",
      "MSE =  1.0343539454303106 0.009899632909974865\n",
      "Model_name =  temp/e57 Violations =  0.0\n",
      "Average_violations =  -19046.85648217424 304.14426496374847\n",
      "MSE =  1.043741431162557 0.014128076793816328\n",
      "Model_name =  temp/e58 Violations =  0.0\n",
      "Average_violations =  -18908.61295186458 301.5515728233773\n",
      "MSE =  1.0340188468489042 0.011722233179777707\n",
      "Model_name =  temp/e59 Violations =  0.0\n",
      "Average_violations =  -19033.258840546216 305.1259283289241\n",
      "MSE =  1.035450263537281 0.011979435833145754\n",
      "Model_name =  temp/e60 Violations =  0.0\n",
      "Average_violations =  -18984.646056081245 299.8398245476661\n",
      "MSE =  1.0386692504442756 0.013005592526883755\n",
      "Model_name =  temp/e61 Violations =  0.0\n",
      "Average_violations =  -18769.465981500627 310.55146135659436\n",
      "MSE =  1.0383725999704994 0.010580238323463925\n",
      "Model_name =  temp/e62 Violations =  0.0\n",
      "Average_violations =  -18953.22256784392 303.4294093285846\n",
      "MSE =  1.0346696643796276 0.01569047373408475\n",
      "Model_name =  temp/e63 Violations =  0.0\n",
      "Average_violations =  -18909.72947241682 303.39138486900976\n",
      "MSE =  1.0419751543709705 0.013763296844416536\n",
      "Model_name =  temp/e64 Violations =  0.0\n",
      "Average_violations =  -19111.036373168048 307.04965468100465\n",
      "MSE =  1.0395039327643327 0.012607356353333805\n",
      "Model_name =  temp/e65 Violations =  0.0\n",
      "Average_violations =  -18875.169210178294 309.7104463655467\n",
      "MSE =  1.0312476638803916 0.013180237697903227\n",
      "Model_name =  temp/e66 Violations =  0.0\n",
      "Average_violations =  -18996.154862540174 306.77232778369194\n",
      "MSE =  1.0376761981133489 0.02015308499997266\n",
      "Model_name =  temp/e67 Violations =  0.0\n",
      "Average_violations =  -18877.75141358619 305.2103043234414\n",
      "MSE =  1.0370549610941653 0.010867664867663189\n",
      "Model_name =  temp/e68 Violations =  0.0\n",
      "Average_violations =  -18947.429656349417 314.9154452712399\n",
      "MSE =  1.034872938828339 0.009899517877158202\n",
      "Model_name =  temp/e69 Violations =  0.0\n",
      "Average_violations =  -19044.63038710517 306.56158146958796\n",
      "MSE =  1.0345979137433576 0.009658926577972909\n",
      "Model_name =  temp/e70 Violations =  0.0\n",
      "Average_violations =  -18690.919760964265 312.8061369112551\n",
      "MSE =  1.0535123344572526 0.01376845779779984\n",
      "Model_name =  temp/e71 Violations =  0.0\n",
      "Average_violations =  -19035.530536488957 308.23991894470686\n",
      "MSE =  1.034582252694912 0.014286490567277061\n",
      "Model_name =  temp/e72 Violations =  0.0\n",
      "Average_violations =  -18910.541771639422 305.49855302243185\n",
      "MSE =  1.0343733398380184 0.01132754745396855\n",
      "Model_name =  temp/e73 Violations =  0.0\n",
      "Average_violations =  -18992.96655694273 311.6695718869644\n",
      "MSE =  1.0333902148426983 0.014344141635735368\n",
      "Model_name =  temp/e74 Violations =  0.0\n",
      "Average_violations =  -19154.642472307693 307.3620264431167\n",
      "MSE =  1.0388793519020838 0.012582766434622979\n",
      "Model_name =  temp/e75 Violations =  0.0\n",
      "Average_violations =  -18873.155546527873 310.79099615749794\n",
      "MSE =  1.0378713082949536 0.014757517303901034\n",
      "Model_name =  temp/e76 Violations =  0.0\n",
      "Average_violations =  -18861.342400357564 312.7167090865616\n",
      "MSE =  1.0384577354114282 0.014785998854596118\n",
      "Model_name =  temp/e77 Violations =  0.0\n",
      "Average_violations =  -19093.358991590696 313.3714457614051\n",
      "MSE =  1.0403615658975052 0.0109797458132592\n",
      "Model_name =  temp/e78 Violations =  0.0\n",
      "Average_violations =  -19083.405900277412 302.8089779931225\n",
      "MSE =  1.0339633995050868 0.013498545930721835\n",
      "Model_name =  temp/e79 Violations =  0.0\n",
      "Average_violations =  -18990.676844756334 309.02942757077267\n",
      "MSE =  1.0311418069213296 0.009736640498322634\n",
      "Model_name =  temp/e80 Violations =  0.0\n",
      "Average_violations =  -18844.67129145281 299.3646659842371\n",
      "MSE =  1.0356973271059988 0.011711310560372029\n",
      "Model_name =  temp/e81 Violations =  0.0\n",
      "Average_violations =  -18994.645176784234 301.54647057056525\n",
      "MSE =  1.035573894159812 0.007778883766125731\n",
      "Model_name =  temp/e82 Violations =  0.0\n",
      "Average_violations =  -18892.439840286454 306.79699436288615\n",
      "MSE =  1.032681231095054 0.013281177233026282\n",
      "Model_name =  temp/e83 Violations =  0.0\n",
      "Average_violations =  -18993.52253511221 311.67725572390555\n",
      "MSE =  1.0334171828712857 0.01089678101061753\n",
      "Model_name =  temp/e84 Violations =  0.0\n",
      "Average_violations =  -19001.982607573747 304.79999820700414\n",
      "MSE =  1.0345987292625363 0.012960018563990995\n",
      "Model_name =  temp/e85 Violations =  0.0\n",
      "Average_violations =  -18999.01994457387 307.98706402635185\n",
      "MSE =  1.0446138648207668 0.011744956150196387\n",
      "Model_name =  temp/e86 Violations =  0.0\n",
      "Average_violations =  -18912.75869995593 303.1221374842519\n",
      "MSE =  1.036117554199046 0.011698869076936865\n",
      "Model_name =  temp/e87 Violations =  0.0\n",
      "Average_violations =  -18971.22819314124 298.5756453561413\n",
      "MSE =  1.029545248029518 0.011521418217042327\n",
      "Model_name =  temp/e88 Violations =  0.0\n",
      "Average_violations =  -19059.58953392003 308.02407728624377\n",
      "MSE =  1.0385729205652683 0.007751576829385849\n",
      "Model_name =  temp/e89 Violations =  0.0\n",
      "Average_violations =  -18862.573512890267 303.27752200328376\n",
      "MSE =  1.0346506969848277 0.01522639112647683\n",
      "Model_name =  temp/e90 Violations =  0.0\n",
      "Average_violations =  -18944.568957696938 306.2071642329673\n",
      "MSE =  1.0373302895423575 0.010652466600592474\n",
      "Model_name =  temp/e91 Violations =  0.0\n",
      "Average_violations =  -18983.91788757019 303.45438375877904\n",
      "MSE =  1.0348847610530627 0.013414937270721418\n",
      "Model_name =  temp/e92 Violations =  0.0\n",
      "Average_violations =  -19108.755339635496 312.20047218483256\n",
      "MSE =  1.0344699996567792 0.011229955630711835\n",
      "Model_name =  temp/e93 Violations =  0.0\n",
      "Average_violations =  -18975.730623783205 311.6687852319125\n",
      "MSE =  1.0351438504373227 0.013916230514526244\n",
      "Model_name =  temp/e94 Violations =  0.0\n",
      "Average_violations =  -18977.306828932367 305.9324805966963\n",
      "MSE =  1.0365144744714587 0.016056959501959782\n",
      "Model_name =  temp/e95 Violations =  0.0\n",
      "Average_violations =  -19031.996920031204 310.55588955381785\n",
      "MSE =  1.0397202635068905 0.01142016192039956\n",
      "Model_name =  temp/e96 Violations =  0.0\n",
      "Average_violations =  -19033.762146091827 306.7114834717924\n",
      "MSE =  1.0408723385762788 0.014148197613836153\n",
      "Model_name =  temp/e97 Violations =  0.0\n",
      "Average_violations =  -18807.598945771362 305.15160980200824\n",
      "MSE =  1.0421346153594544 0.015366123351832902\n",
      "Model_name =  temp/e98 Violations =  0.0\n",
      "Average_violations =  -18850.792503273668 307.55282100515905\n",
      "MSE =  1.0397365984700773 0.009703054322452533\n",
      "Model_name =  temp/e99 Violations =  0.0\n",
      "Average_violations =  -18915.15798657604 301.00907727416467\n",
      "MSE =  1.034891354068563 0.006258166637234751\n",
      "Model_name =  temp/e100 Violations =  0.0\n",
      "Average_violations =  -19006.318669477165 304.6263823729934\n",
      "MSE =  1.0374256035855989 0.011689431306986266\n",
      "Model_name =  temp/e101 Violations =  0.0\n",
      "Average_violations =  -19002.80040645972 307.1125090099804\n",
      "MSE =  1.0366865378996004 0.015137978768888574\n",
      "Model_name =  temp/e102 Violations =  0.0\n",
      "Average_violations =  -19019.889995215315 303.80981423724177\n",
      "MSE =  1.0440945267529722 0.018390399211702654\n",
      "Model_name =  temp/e103 Violations =  0.0\n",
      "Average_violations =  -18783.131956744062 300.2383063993874\n",
      "MSE =  1.0465945542847035 0.006798946164129735\n",
      "Model_name =  temp/e104 Violations =  0.0\n",
      "Average_violations =  -18996.186521085685 314.7953217509066\n",
      "MSE =  1.0337791887057552 0.015300774611583398\n",
      "Model_name =  temp/e105 Violations =  0.0\n",
      "Average_violations =  -19059.263792895115 302.52615071931234\n",
      "MSE =  1.0370666864937932 0.013511546299069555\n",
      "Model_name =  temp/e106 Violations =  0.0\n",
      "Average_violations =  -18827.57940449453 304.15290124530117\n",
      "MSE =  1.0399404525365523 0.011054105983560688\n",
      "Model_name =  temp/e107 Violations =  0.0\n",
      "Average_violations =  -19016.849191890564 304.14899874162705\n",
      "MSE =  1.0355765376343298 0.011020986081145327\n",
      "Model_name =  temp/e108 Violations =  0.0\n",
      "Average_violations =  -18912.30886505088 316.9694020561526\n",
      "MSE =  1.0419096583463947 0.007697998868181324\n",
      "Model_name =  temp/e109 Violations =  0.0\n",
      "Average_violations =  -18944.853494129136 309.345337899551\n",
      "MSE =  1.0360992562091518 0.009591367712774686\n",
      "Model_name =  temp/e110 Violations =  0.0\n",
      "Average_violations =  -19070.240436308715 309.0273050926287\n",
      "MSE =  1.0437294238964219 0.009386597381191006\n",
      "Model_name =  temp/e111 Violations =  0.0\n",
      "Average_violations =  -18987.32647916143 305.94024045806054\n",
      "MSE =  1.037372171816358 0.015041056037789123\n",
      "Model_name =  temp/e112 Violations =  0.0\n",
      "Average_violations =  -18943.66058969545 301.99897025735163\n",
      "MSE =  1.0353600178160363 0.016149787967983012\n",
      "Model_name =  temp/e113 Violations =  0.0\n",
      "Average_violations =  -18973.401559774942 301.7959021583727\n",
      "MSE =  1.0333619634218894 0.01666082098275745\n",
      "Model_name =  temp/e114 Violations =  0.0\n",
      "Average_violations =  -18924.28255131923 309.37134807119315\n",
      "MSE =  1.0339530774347794 0.012624493307716084\n",
      "Model_name =  temp/e115 Violations =  0.0\n",
      "Average_violations =  -18882.88648741064 308.0020704240488\n",
      "MSE =  1.0323353512035374 0.012320650159031294\n",
      "Model_name =  temp/e116 Violations =  0.0\n",
      "Average_violations =  -18939.347547615624 301.37760935383886\n",
      "MSE =  1.04579289060518 0.015877335618688457\n",
      "Model_name =  temp/e117 Violations =  0.0\n",
      "Average_violations =  -19062.713075937667 314.44758891760887\n",
      "MSE =  1.0324206175158985 0.011060849772968265\n",
      "Model_name =  temp/e118 Violations =  0.0\n",
      "Average_violations =  -19102.047314892236 303.3054264933564\n",
      "MSE =  1.034695984821131 0.012418204446652795\n",
      "Model_name =  temp/e119 Violations =  0.0\n",
      "Average_violations =  -18804.971310680892 308.51566692442947\n",
      "MSE =  1.0395146780503377 0.015539416119690645\n",
      "Model_name =  temp/e120 Violations =  0.0\n",
      "Average_violations =  -19059.370008055812 311.2426546656704\n",
      "MSE =  1.0419637264725545 0.017719751249961207\n",
      "Model_name =  temp/e121 Violations =  0.0\n",
      "Average_violations =  -18958.272321903503 313.2722062904944\n",
      "MSE =  1.0351514008034015 0.015121359447861402\n",
      "Model_name =  temp/e122 Violations =  0.0\n",
      "Average_violations =  -18930.99999223889 300.17844904540397\n",
      "MSE =  1.0412333934015447 0.013819175806333116\n",
      "Model_name =  temp/e123 Violations =  0.0\n",
      "Average_violations =  -18984.440230450848 312.6901214959087\n",
      "MSE =  1.0360846863017443 0.009504899645713303\n",
      "Model_name =  temp/e124 Violations =  0.0\n",
      "Average_violations =  -18917.594428611006 303.9237415090867\n",
      "MSE =  1.0351876355872898 0.010890254612012089\n",
      "Model_name =  temp/e125 Violations =  0.0\n",
      "Average_violations =  -19034.951855817537 302.626436587789\n",
      "MSE =  1.0351427854185817 0.017997205445402432\n",
      "Model_name =  temp/e126 Violations =  0.0\n",
      "Average_violations =  -19007.91250897557 302.8582456477773\n",
      "MSE =  1.037206837055035 0.012657750735945036\n",
      "Model_name =  temp/e127 Violations =  0.0\n",
      "Average_violations =  -18939.42775607909 312.5446698085493\n",
      "MSE =  1.0345759294518528 0.011925893869152826\n",
      "Model_name =  temp/e128 Violations =  0.0\n",
      "Average_violations =  -19092.974282503863 309.50791571975117\n",
      "MSE =  1.0422093210587176 0.012230685654078562\n",
      "Model_name =  temp/e129 Violations =  0.0\n",
      "Average_violations =  -18972.290334904046 303.1940271458595\n",
      "MSE =  1.0345063722020478 0.014442731574904886\n",
      "Model_name =  temp/e130 Violations =  0.0\n",
      "Average_violations =  -18986.28414542893 315.43214925317545\n",
      "MSE =  1.0362390422811234 0.01587342824793261\n",
      "Model_name =  temp/e131 Violations =  0.0\n",
      "Average_violations =  -18954.452070550487 296.72015646107496\n",
      "MSE =  1.0374282498320069 0.010760682770026547\n",
      "Model_name =  temp/e132 Violations =  0.0\n",
      "Average_violations =  -18982.237089927024 298.9341499344092\n",
      "MSE =  1.0356444261017341 0.015478160532226911\n",
      "Model_name =  temp/e133 Violations =  0.0\n",
      "Average_violations =  -18901.339144003403 303.7748320358956\n",
      "MSE =  1.0301073287677733 0.011828394447762918\n",
      "Model_name =  temp/e134 Violations =  0.0\n",
      "Average_violations =  -18912.97660815739 300.26073985818755\n",
      "MSE =  1.0319881839243128 0.014300432504224346\n",
      "Model_name =  temp/e135 Violations =  0.0\n",
      "Average_violations =  -18915.191904449057 311.03752013698494\n",
      "MSE =  1.0339710919549803 0.013811450065628282\n",
      "Model_name =  temp/e136 Violations =  0.0\n",
      "Average_violations =  -18818.707654530903 310.5730507214664\n",
      "MSE =  1.0356425670684488 0.012726845910170224\n",
      "Model_name =  temp/e137 Violations =  0.0\n",
      "Average_violations =  -18879.15010742736 304.87558903205604\n",
      "MSE =  1.036610442328661 0.015809142361446902\n",
      "Model_name =  temp/e138 Violations =  0.0\n",
      "Average_violations =  -18864.168629011223 307.429360592235\n",
      "MSE =  1.0340755209568708 0.015589551129719893\n",
      "Model_name =  temp/e139 Violations =  0.0\n",
      "Average_violations =  -19015.22726800467 309.70431386546943\n",
      "MSE =  1.0414668911522367 0.012057133138906447\n",
      "Model_name =  temp/e140 Violations =  0.0\n",
      "Average_violations =  -18893.322951761515 299.61538262192477\n",
      "MSE =  1.0349334806640151 0.009242418237601099\n",
      "Model_name =  temp/e141 Violations =  0.0\n",
      "Average_violations =  -19049.401473175734 314.4149643034353\n",
      "MSE =  1.0368091695491952 0.017763263348799782\n",
      "Model_name =  temp/e142 Violations =  0.0\n",
      "Average_violations =  -18931.195029336006 299.55546875269715\n",
      "MSE =  1.0386721755513917 0.014832760014925324\n",
      "Model_name =  temp/e143 Violations =  0.0\n",
      "Average_violations =  -18971.079094239078 315.40748263503815\n",
      "MSE =  1.0362678784491233 0.011371515140417064\n",
      "Model_name =  temp/e144 Violations =  0.0\n",
      "Average_violations =  -19047.466003450394 302.69279746891334\n",
      "MSE =  1.035608561548903 0.01214289062592807\n",
      "Model_name =  temp/e145 Violations =  0.0\n",
      "Average_violations =  -18979.554666115757 299.30387490910266\n",
      "MSE =  1.0353969966960093 0.01594562310337443\n",
      "Model_name =  temp/e146 Violations =  0.0\n",
      "Average_violations =  -19036.65552810593 303.4900994818179\n",
      "MSE =  1.0375453432734543 0.015296290672970265\n",
      "Model_name =  temp/e147 Violations =  0.0\n",
      "Average_violations =  -18955.24978422172 308.8057154575832\n",
      "MSE =  1.037401627957482 0.010346686641331915\n",
      "Model_name =  temp/e148 Violations =  0.0\n",
      "Average_violations =  -18971.960601063063 298.3973345530709\n",
      "MSE =  1.045910057094011 0.012984612960396838\n",
      "Model_name =  temp/e149 Violations =  0.0\n",
      "Average_violations =  -18963.07763019781 303.0477935629911\n",
      "MSE =  1.040112437252008 0.01187015268623982\n",
      "Model_name =  temp/e150 Violations =  0.0\n",
      "Average_violations =  -18946.906303122167 317.24275217419483\n",
      "MSE =  1.033199101905715 0.009097916650735437\n",
      "Model_name =  temp/e151 Violations =  0.0\n",
      "Average_violations =  -18979.08258653176 301.3352767455855\n",
      "MSE =  1.035689366117581 0.01324709783436747\n",
      "Model_name =  temp/e152 Violations =  0.0\n",
      "Average_violations =  -18907.71756439388 303.3712239337303\n",
      "MSE =  1.0363917112672507 0.015310249961039891\n",
      "Model_name =  temp/e153 Violations =  0.0\n",
      "Average_violations =  -19033.58970945309 307.847192493351\n",
      "MSE =  1.0318712243402588 0.014549641435366942\n",
      "Model_name =  temp/e154 Violations =  0.0\n",
      "Average_violations =  -18974.47575015584 317.220199636488\n",
      "MSE =  1.034955943260267 0.012976267673190149\n",
      "Model_name =  temp/e155 Violations =  0.0\n",
      "Average_violations =  -18887.423586118202 296.4238478567047\n",
      "MSE =  1.037551111147593 0.01242501656464872\n",
      "Model_name =  temp/e156 Violations =  0.0\n",
      "Average_violations =  -18981.864307452906 293.633479869521\n",
      "MSE =  1.0366695076524342 0.011615079778876528\n",
      "Model_name =  temp/e157 Violations =  0.0\n",
      "Average_violations =  -18954.890193848878 300.04204059095304\n",
      "MSE =  1.0354851456071554 0.012813686052186874\n",
      "Model_name =  temp/e158 Violations =  0.0\n",
      "Average_violations =  -18946.276368713443 304.3612415897399\n",
      "MSE =  1.0352895539682696 0.01395812973419009\n",
      "Model_name =  temp/e159 Violations =  0.0\n",
      "Average_violations =  -19017.163833633917 310.3293232341172\n",
      "MSE =  1.0382778351125215 0.016244930441191286\n",
      "Model_name =  temp/e160 Violations =  0.0\n",
      "Average_violations =  -18903.351278815906 297.6025291276117\n",
      "MSE =  1.0379390381492455 0.010850243148395822\n",
      "Model_name =  temp/e161 Violations =  0.0\n",
      "Average_violations =  -18977.494075227063 305.4406255266824\n",
      "MSE =  1.039639429341683 0.01508979266011977\n",
      "Model_name =  temp/e162 Violations =  0.0\n",
      "Average_violations =  -18883.847887328906 298.55913597329743\n",
      "MSE =  1.0384736520573843 0.012375133878693047\n",
      "Model_name =  temp/e163 Violations =  0.0\n",
      "Average_violations =  -18896.78114601501 311.35482783301785\n",
      "MSE =  1.0359747667842503 0.013906259964260077\n",
      "Model_name =  temp/e164 Violations =  0.0\n",
      "Average_violations =  -18962.935878805747 305.32561820759076\n",
      "MSE =  1.057721318432323 0.011129947271376952\n",
      "Model_name =  temp/e165 Violations =  0.0\n",
      "Average_violations =  -19012.31148895091 301.0867947904261\n",
      "MSE =  1.0325373700814178 0.014296045867171887\n",
      "Model_name =  temp/e166 Violations =  0.0\n",
      "Average_violations =  -18917.677823494694 310.2122437283351\n",
      "MSE =  1.0428338791730982 0.010355985954212391\n",
      "Model_name =  temp/e167 Violations =  0.0\n",
      "Average_violations =  -18903.37110231513 294.3682926383186\n",
      "MSE =  1.0439636002328416 0.005153890906286929\n",
      "Model_name =  temp/e168 Violations =  0.0\n",
      "Average_violations =  -19023.098466957083 301.92058128857917\n",
      "MSE =  1.0384834853453628 0.010769862699502216\n",
      "Model_name =  temp/e169 Violations =  0.0\n",
      "Average_violations =  -18940.72363813627 302.16671420778573\n",
      "MSE =  1.0318063135006503 0.013212809789783236\n",
      "Model_name =  temp/e170 Violations =  0.0\n",
      "Average_violations =  -18868.797458690904 298.86883301733644\n",
      "MSE =  1.041722101409733 0.012238699380511761\n",
      "Model_name =  temp/e171 Violations =  0.0\n",
      "Average_violations =  -18896.52886945792 303.9428637837846\n",
      "MSE =  1.0367694783087964 0.009270993121430673\n",
      "Model_name =  temp/e172 Violations =  0.0\n",
      "Average_violations =  -18849.09473081772 301.8183901394518\n",
      "MSE =  1.0384171285105412 0.0131875916107151\n",
      "Model_name =  temp/e173 Violations =  0.0\n",
      "Average_violations =  -19012.4560478718 302.137832680421\n",
      "MSE =  1.0418012006683854 0.019802572212208516\n",
      "Model_name =  temp/e174 Violations =  0.0\n",
      "Average_violations =  -18972.92781415415 303.29426272392436\n",
      "MSE =  1.0334989608140996 0.01567120371869779\n",
      "Model_name =  temp/e175 Violations =  0.0\n",
      "Average_violations =  -18919.63749719515 302.3451016530188\n",
      "MSE =  1.0443336025671681 0.013255644638849085\n",
      "Model_name =  temp/e176 Violations =  0.0\n",
      "Average_violations =  -18991.181784929242 310.3801165695546\n",
      "MSE =  1.041860238705698 0.016365145638467155\n",
      "Model_name =  temp/e177 Violations =  0.0\n",
      "Average_violations =  -18960.780248634277 309.6582914392889\n",
      "MSE =  1.0322153068011237 0.01245376665824144\n",
      "Model_name =  temp/e178 Violations =  0.0\n",
      "Average_violations =  -19046.168574448435 304.7194669476011\n",
      "MSE =  1.0349105563909717 0.014716716189479213\n",
      "Model_name =  temp/e179 Violations =  0.0\n",
      "Average_violations =  -19035.11274546177 305.0267583385965\n",
      "MSE =  1.0350747569785947 0.01234148502819143\n",
      "Model_name =  temp/e180 Violations =  0.0\n",
      "Average_violations =  -19080.462435586913 303.0162713570895\n",
      "MSE =  1.0388617196071457 0.014640142007412388\n",
      "Model_name =  temp/e181 Violations =  0.0\n",
      "Average_violations =  -19036.169861385748 310.18827228150064\n",
      "MSE =  1.037032134960461 0.01275908805685268\n",
      "Model_name =  temp/e182 Violations =  0.0\n",
      "Average_violations =  -18933.192425248875 303.89485091167\n",
      "MSE =  1.0325101490049777 0.014054897029776303\n",
      "Model_name =  temp/e183 Violations =  0.0\n",
      "Average_violations =  -18977.061963452084 309.4196406159448\n",
      "MSE =  1.0439487418969673 0.013261053893465847\n",
      "Model_name =  temp/e184 Violations =  0.0\n",
      "Average_violations =  -19100.371955613424 295.04504819317657\n",
      "MSE =  1.0388951472318808 0.010276950262732367\n",
      "Model_name =  temp/e185 Violations =  0.0\n",
      "Average_violations =  -18993.712969249387 310.88653396073295\n",
      "MSE =  1.0330674362584857 0.010037089359367044\n",
      "Model_name =  temp/e186 Violations =  0.0\n",
      "Average_violations =  -18838.697583999317 306.90876827657064\n",
      "MSE =  1.0370226425906228 0.012226216023778503\n",
      "Model_name =  temp/e187 Violations =  0.0\n",
      "Average_violations =  -18942.952634139674 299.04147324755684\n",
      "MSE =  1.0315547003181416 0.008782542022703116\n",
      "Model_name =  temp/e188 Violations =  0.0\n",
      "Average_violations =  -19056.832601708513 305.68785665828943\n",
      "MSE =  1.0378583418284544 0.013408445359248651\n",
      "Model_name =  temp/e189 Violations =  0.0\n",
      "Average_violations =  -18927.927431133056 309.4622590426629\n",
      "MSE =  1.031023206276202 0.010403483182036113\n",
      "Model_name =  temp/e190 Violations =  0.0\n",
      "Average_violations =  -19043.823171909342 304.6426389240545\n",
      "MSE =  1.037490369979623 0.009888485542454912\n",
      "Model_name =  temp/e191 Violations =  0.0\n",
      "Average_violations =  -18974.80992487329 300.63427352907314\n",
      "MSE =  1.0438528296110046 0.013982758616564134\n",
      "Model_name =  temp/e192 Violations =  0.0\n",
      "Average_violations =  -18874.62311379647 296.65513706826806\n",
      "MSE =  1.03579526690199 0.015279867893555487\n",
      "Model_name =  temp/e193 Violations =  0.0\n",
      "Average_violations =  -18890.76439772546 311.50285664761225\n",
      "MSE =  1.0375568032986406 0.011921995599040482\n",
      "Model_name =  temp/e194 Violations =  0.0\n",
      "Average_violations =  -18954.201831893744 318.082962459314\n",
      "MSE =  1.0394677166256514 0.015269878427136934\n",
      "Model_name =  temp/e195 Violations =  0.0\n",
      "Average_violations =  -18955.18230257404 307.03121403811616\n",
      "MSE =  1.0358311021535047 0.011350034241327824\n",
      "Model_name =  temp/e196 Violations =  0.0\n",
      "Average_violations =  -18937.932944573597 308.4451416011346\n",
      "MSE =  1.0399576401680806 0.007983703700315922\n",
      "Model_name =  temp/e197 Violations =  0.0\n",
      "Average_violations =  -18829.545295758406 299.5738325403769\n",
      "MSE =  1.038394201959925 0.011895757675032606\n",
      "Model_name =  temp/e198 Violations =  0.0\n",
      "Average_violations =  -18928.520135725303 302.10590964321545\n",
      "MSE =  1.0400617649287454 0.016282278935943768\n",
      "Model_name =  temp/e199 Violations =  0.0\n",
      "Average_violations =  -18962.314190536155 310.6196821397782\n",
      "MSE =  1.035242123511374 0.014839850978751578\n",
      "Model_name =  temp/e200 Violations =  0.0\n",
      "Average_violations =  -18924.38009073951 305.03733689342164\n",
      "MSE =  1.0441087897841468 0.006844918960081651\n",
      "Model_name =  temp/e201 Violations =  0.0\n",
      "Average_violations =  -18825.433967386172 302.989147663815\n",
      "MSE =  1.0365220621256823 0.012154445957885856\n",
      "Model_name =  temp/e202 Violations =  0.0\n",
      "Average_violations =  -18914.073110213554 315.69413257385605\n",
      "MSE =  1.0342412480870977 0.013692819016290173\n",
      "Model_name =  temp/e203 Violations =  0.0\n",
      "Average_violations =  -18959.461062930448 307.25250851018467\n",
      "MSE =  1.0374700756416457 0.009628328927895967\n",
      "Model_name =  temp/e204 Violations =  0.0\n",
      "Average_violations =  -18995.687241956042 309.677073680999\n",
      "MSE =  1.0359239332913721 0.01303881261644993\n",
      "Model_name =  temp/e205 Violations =  0.0\n",
      "Average_violations =  -18965.127414395236 313.6053001046347\n",
      "MSE =  1.0378256728500008 0.007801193862044996\n",
      "Model_name =  temp/e206 Violations =  0.0\n",
      "Average_violations =  -18986.747132697106 307.39987354932913\n",
      "MSE =  1.0437750605856828 0.01074705847763282\n",
      "Model_name =  temp/e207 Violations =  0.0\n",
      "Average_violations =  -18969.595634405654 297.96928125952354\n",
      "MSE =  1.0361775790977201 0.015870595637976086\n",
      "Model_name =  temp/e208 Violations =  0.0\n",
      "Average_violations =  -18847.089140048054 317.571293918489\n",
      "MSE =  1.0394349579731958 0.010459892140386173\n",
      "Model_name =  temp/e209 Violations =  0.0\n",
      "Average_violations =  -18957.89219173452 302.32467129083886\n",
      "MSE =  1.042473227625606 0.014339464355902753\n",
      "Model_name =  temp/e210 Violations =  0.0\n",
      "Average_violations =  -18986.061953759217 298.2137838981556\n",
      "MSE =  1.0343314927961884 0.013538987072796783\n",
      "Model_name =  temp/e211 Violations =  0.0\n",
      "Average_violations =  -18960.200031993572 303.9189968024818\n",
      "MSE =  1.0376908214837681 0.012171480594179203\n",
      "Model_name =  temp/e212 Violations =  0.0\n",
      "Average_violations =  -19013.606750503073 299.78211816752014\n",
      "MSE =  1.0364219098837562 0.010008821660520773\n",
      "Model_name =  temp/e213 Violations =  0.0\n",
      "Average_violations =  -18827.32415914615 312.64787214927145\n",
      "MSE =  1.038323027212932 0.013132183719957977\n",
      "Model_name =  temp/e214 Violations =  0.0\n",
      "Average_violations =  -19006.821957953292 301.1447897900309\n",
      "MSE =  1.0389258617867132 0.012500446417770312\n",
      "Model_name =  temp/e215 Violations =  0.0\n",
      "Average_violations =  -18902.11026287524 305.69869373053314\n",
      "MSE =  1.0342408003475883 0.01370438226721531\n",
      "Model_name =  temp/e216 Violations =  0.0\n",
      "Average_violations =  -18920.53089429793 304.7846738020197\n",
      "MSE =  1.03640143196876 0.013455962649809767\n",
      "Model_name =  temp/e217 Violations =  0.0\n",
      "Average_violations =  -18893.763950337794 307.7420048300237\n",
      "MSE =  1.0391844815677524 0.009664737074806666\n",
      "Model_name =  temp/e218 Violations =  0.0\n",
      "Average_violations =  -18798.88072631998 307.4401867341157\n",
      "MSE =  1.04096957760167 0.017332792686552148\n",
      "Model_name =  temp/e219 Violations =  0.0\n",
      "Average_violations =  -19042.67782563924 313.33104457584074\n",
      "MSE =  1.03361154058961 0.016988020330619698\n",
      "Model_name =  temp/e220 Violations =  0.0\n",
      "Average_violations =  -18954.862825658132 302.1984277834233\n",
      "MSE =  1.0385996934980104 0.011290810704314792\n",
      "Model_name =  temp/e221 Violations =  0.0\n",
      "Average_violations =  -18928.449651159084 299.5362632678669\n",
      "MSE =  1.0382883767491486 0.013393132954590568\n",
      "Model_name =  temp/e222 Violations =  0.0\n",
      "Average_violations =  -18936.38540228982 312.4362334593094\n",
      "MSE =  1.0359560648268114 0.014471412456755192\n",
      "Model_name =  temp/e223 Violations =  0.0\n",
      "Average_violations =  -18940.11948220827 305.10214672672794\n",
      "MSE =  1.0362135017780738 0.01255126514481493\n",
      "Model_name =  temp/e224 Violations =  0.0\n",
      "Average_violations =  -18968.176290387026 304.73309231363424\n",
      "MSE =  1.039483911452732 0.010799428813264128\n",
      "Model_name =  temp/e225 Violations =  0.0\n",
      "Average_violations =  -19162.694415452144 307.4577370576971\n",
      "MSE =  1.0422160165420902 0.019541012188577588\n",
      "Model_name =  temp/e226 Violations =  0.0\n",
      "Average_violations =  -18743.82732720002 303.084947529916\n",
      "MSE =  1.038672696945643 0.012711725188721526\n",
      "Model_name =  temp/e227 Violations =  0.0\n",
      "Average_violations =  -18767.39724861547 306.10284000940294\n",
      "MSE =  1.047640702324546 0.011024317875855047\n",
      "Model_name =  temp/e228 Violations =  0.0\n",
      "Average_violations =  -19009.67858572517 307.16918660487664\n",
      "MSE =  1.033739776918003 0.008487381600475626\n",
      "Model_name =  temp/e229 Violations =  0.0\n",
      "Average_violations =  -18889.57834668057 308.03123483102263\n",
      "MSE =  1.0384632232719937 0.013920539590349215\n",
      "Model_name =  temp/e230 Violations =  0.0\n",
      "Average_violations =  -19057.167541711497 308.52211891957836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE =  1.034745227977425 0.012575186160751043\n",
      "Model_name =  temp/e231 Violations =  0.0\n",
      "Average_violations =  -18901.782814922844 306.2216005559755\n",
      "MSE =  1.0317302391979954 0.013199391550013366\n",
      "Model_name =  temp/e232 Violations =  0.0\n",
      "Average_violations =  -18839.87292162302 307.377954095304\n",
      "MSE =  1.0379192019202819 0.009805558719798784\n",
      "Model_name =  temp/e233 Violations =  0.0\n",
      "Average_violations =  -18956.132863101506 302.5289824807482\n",
      "MSE =  1.0367959259399326 0.00715389114511969\n",
      "Model_name =  temp/e234 Violations =  0.0\n",
      "Average_violations =  -18885.04390243286 304.2157549373286\n",
      "MSE =  1.0393962796601663 0.01249713585697767\n",
      "Model_name =  temp/e235 Violations =  0.0\n",
      "Average_violations =  -19061.124150909967 303.53009193530767\n",
      "MSE =  1.0384404079853984 0.012993786669619119\n",
      "Model_name =  temp/e236 Violations =  0.0\n",
      "Average_violations =  -18859.24663115709 307.0345508451123\n",
      "MSE =  1.039616172896812 0.01071382581228088\n",
      "Model_name =  temp/e237 Violations =  0.0\n",
      "Average_violations =  -18989.104113910977 295.926261547018\n",
      "MSE =  1.042308565834726 0.013354956481119682\n",
      "Model_name =  temp/e238 Violations =  0.0\n",
      "Average_violations =  -18964.707376789112 299.29043620975796\n",
      "MSE =  1.0409416250721808 0.013840110131442243\n",
      "Model_name =  temp/e239 Violations =  0.0\n",
      "Average_violations =  -18982.37832920459 303.2376651597887\n",
      "MSE =  1.0375710213399638 0.016155602281266924\n",
      "Model_name =  temp/e240 Violations =  0.0\n",
      "Average_violations =  -19052.955844385808 309.49517648179125\n",
      "MSE =  1.0346793198776667 0.010706073796491649\n",
      "Model_name =  temp/e241 Violations =  0.0\n",
      "Average_violations =  -18966.026172841775 307.64970112607097\n",
      "MSE =  1.0369311014184752 0.012700281159900026\n",
      "Model_name =  temp/e242 Violations =  0.0\n",
      "Average_violations =  -19028.467860104654 311.9205112776102\n",
      "MSE =  1.0385419294867966 0.018362175067772074\n",
      "Model_name =  temp/e243 Violations =  0.0\n",
      "Average_violations =  -19017.90776353919 304.36199343598446\n",
      "MSE =  1.0361015422806577 0.010712797090444304\n",
      "Model_name =  temp/e244 Violations =  0.0\n",
      "Average_violations =  -18918.374406527833 302.81664393406777\n",
      "MSE =  1.0437132178278798 0.012022459184647652\n",
      "Model_name =  temp/e245 Violations =  0.0\n",
      "Average_violations =  -19070.43902812169 310.08337170518683\n",
      "MSE =  1.0378984538465383 0.012836605944950391\n",
      "Model_name =  temp/e246 Violations =  0.0\n",
      "Average_violations =  -18921.28513534715 301.67748705372287\n",
      "MSE =  1.0379160738338369 0.010798776705226904\n",
      "Model_name =  temp/e247 Violations =  0.0\n",
      "Average_violations =  -18974.729756311182 309.8602987142525\n",
      "MSE =  1.0372646115394646 0.01046889767918548\n",
      "Model_name =  temp/e248 Violations =  0.0\n",
      "Average_violations =  -19030.032326515386 306.9265915124161\n",
      "MSE =  1.0339140182855202 0.01478703432877825\n",
      "Model_name =  temp/e249 Violations =  0.0\n",
      "Average_violations =  -18995.082450761274 313.4334399047254\n",
      "MSE =  1.0351337244018322 0.01020761432842617\n",
      "Model_name =  temp/e250 Violations =  0.0\n",
      "Average_violations =  -18914.293580204318 305.14044566050217\n",
      "MSE =  1.0328525569033737 0.011776474829783256\n",
      "Model_name =  temp/e251 Violations =  0.0\n",
      "Average_violations =  -19038.829907947693 298.9115700924921\n",
      "MSE =  1.0331664121505963 0.015323727455570521\n",
      "Model_name =  temp/e252 Violations =  0.0\n",
      "Average_violations =  -18827.23076306984 300.2980016104924\n",
      "MSE =  1.041405818020387 0.01381835639432767\n",
      "Model_name =  temp/e253 Violations =  0.0\n",
      "Average_violations =  -18961.152429951904 300.1977160214042\n",
      "MSE =  1.0322902695437575 0.00746282121835915\n",
      "Model_name =  temp/e254 Violations =  0.0\n",
      "Average_violations =  -18873.024111387047 300.15816802200675\n",
      "MSE =  1.0358917354034562 0.007547372887314621\n",
      "Model_name =  temp/e255 Violations =  0.0\n",
      "Average_violations =  -18825.35450204778 312.0646894331811\n",
      "MSE =  1.0320595597695434 0.013847524399433715\n",
      "Model_name =  temp/e256 Violations =  0.0\n",
      "Average_violations =  -18996.971432229027 302.6676230381794\n",
      "MSE =  1.0350901333219602 0.012903131112944412\n",
      "Model_name =  temp/e257 Violations =  0.0\n",
      "Average_violations =  -18966.78817315948 307.34318705158876\n",
      "MSE =  1.0428922955455608 0.010971336650162603\n",
      "Model_name =  temp/e258 Violations =  0.0\n",
      "Average_violations =  -18977.864463239035 307.67872550944935\n",
      "MSE =  1.0390637039018258 0.015818170813808378\n",
      "Model_name =  temp/e259 Violations =  0.0\n",
      "Average_violations =  -18883.369636020838 302.6538754050205\n",
      "MSE =  1.036635325659843 0.01033256860440969\n",
      "Model_name =  temp/e260 Violations =  0.0\n",
      "Average_violations =  -18887.791176310908 303.02138895087637\n",
      "MSE =  1.0393964020521347 0.016078637078800924\n",
      "Model_name =  temp/e261 Violations =  0.0\n",
      "Average_violations =  -18882.63858403489 306.8630442486643\n",
      "MSE =  1.0349009166070684 0.015983372864890853\n",
      "Model_name =  temp/e262 Violations =  0.0\n",
      "Average_violations =  -18897.111547874014 309.13881559026714\n",
      "MSE =  1.037609241223932 0.013199343999677027\n",
      "Model_name =  temp/e263 Violations =  0.0\n",
      "Average_violations =  -19035.19294337542 304.0786408869561\n",
      "MSE =  1.0341965241453026 0.0143201611119455\n",
      "Model_name =  temp/e264 Violations =  0.0\n",
      "Average_violations =  -18874.58061950934 293.9735424133531\n",
      "MSE =  1.042073822248972 0.013873617559170165\n",
      "Model_name =  temp/e265 Violations =  0.0\n",
      "Average_violations =  -18916.645899521216 304.5495414649846\n",
      "MSE =  1.0370172791816308 0.009571463331038789\n",
      "Model_name =  temp/e266 Violations =  0.0\n",
      "Average_violations =  -18923.375636297853 304.39840445515085\n",
      "MSE =  1.0383252865341206 0.010691810024182364\n",
      "Model_name =  temp/e267 Violations =  0.0\n",
      "Average_violations =  -18990.120418445476 304.47490208833085\n",
      "MSE =  1.0362650284976336 0.011624355368203181\n",
      "Model_name =  temp/e268 Violations =  0.0\n",
      "Average_violations =  -19072.047847469454 306.21916092279065\n",
      "MSE =  1.0382050373441412 0.014464084118503123\n",
      "Model_name =  temp/e269 Violations =  0.0\n",
      "Average_violations =  -18836.195458512015 310.1471651299974\n",
      "MSE =  1.0410041470139972 0.01636406215240157\n",
      "Model_name =  temp/e270 Violations =  0.0\n",
      "Average_violations =  -19071.42919729439 298.6185185350138\n",
      "MSE =  1.0335508790654404 0.015334474923076817\n",
      "Model_name =  temp/e271 Violations =  0.0\n",
      "Average_violations =  -19008.30572592531 303.48151685468537\n",
      "MSE =  1.0419536082621565 0.009573410319365034\n",
      "Model_name =  temp/e272 Violations =  0.0\n",
      "Average_violations =  -18985.20889587749 308.6003977114694\n",
      "MSE =  1.0339076838307872 0.014655712911289136\n",
      "Model_name =  temp/e273 Violations =  0.0\n",
      "Average_violations =  -19107.082005753015 307.1207253415353\n",
      "MSE =  1.0362080653283805 0.013112065808415684\n",
      "Model_name =  temp/e274 Violations =  0.0\n",
      "Average_violations =  -18977.937328597385 304.5953296462386\n",
      "MSE =  1.038348569701449 0.011793923757534703\n",
      "Model_name =  temp/e275 Violations =  0.0\n",
      "Average_violations =  -18990.72465253282 309.0170460130579\n",
      "MSE =  1.0398732190859274 0.0106348283284624\n",
      "Model_name =  temp/e276 Violations =  0.0\n",
      "Average_violations =  -18890.20568138203 295.1431951337452\n",
      "MSE =  1.040102389721532 0.010952808388936244\n",
      "Model_name =  temp/e277 Violations =  0.0\n",
      "Average_violations =  -18803.19896040745 297.9371531002797\n",
      "MSE =  1.0379040402237056 0.013613122045441543\n",
      "Model_name =  temp/e278 Violations =  0.0\n",
      "Average_violations =  -18889.902241468466 303.03030538311214\n",
      "MSE =  1.0337418570011276 0.015460325479895294\n",
      "Model_name =  temp/e279 Violations =  0.0\n",
      "Average_violations =  -19009.8831016914 310.6103320579544\n",
      "MSE =  1.038953815992024 0.011937261790417663\n",
      "Model_name =  temp/e280 Violations =  0.0\n",
      "Average_violations =  -18971.43702350085 318.4716712904409\n",
      "MSE =  1.038773656245288 0.01538488154073629\n",
      "Model_name =  temp/e281 Violations =  0.0\n",
      "Average_violations =  -18992.731242436064 304.88030330803355\n",
      "MSE =  1.0390104643144666 0.017142503760603642\n",
      "Model_name =  temp/e282 Violations =  0.0\n",
      "Average_violations =  -19038.1088609837 310.1493266082258\n",
      "MSE =  1.0360264121617564 0.013508711439044044\n",
      "Model_name =  temp/e283 Violations =  0.0\n",
      "Average_violations =  -18898.356487585253 306.8489997663007\n",
      "MSE =  1.0372688439462272 0.013630997329561778\n",
      "Model_name =  temp/e284 Violations =  0.0\n",
      "Average_violations =  -19019.37690874908 308.1810634368204\n",
      "MSE =  1.0363520008388862 0.01457388517069629\n",
      "Model_name =  temp/e285 Violations =  0.0\n",
      "Average_violations =  -18985.460051018446 296.34074063800256\n",
      "MSE =  1.0343810134179607 0.011770491614878508\n",
      "Model_name =  temp/e286 Violations =  0.0\n",
      "Average_violations =  -18956.752668572444 307.5420457050421\n",
      "MSE =  1.0415653463429342 0.01224911560981228\n",
      "Model_name =  temp/e287 Violations =  0.0\n",
      "Average_violations =  -19002.351074277634 314.25456065102804\n",
      "MSE =  1.0358575725449037 0.011278136509856705\n",
      "Model_name =  temp/e288 Violations =  0.0\n",
      "Average_violations =  -18898.849932437002 308.9272645371204\n",
      "MSE =  1.0396968310865597 0.008390322434875468\n",
      "Model_name =  temp/e289 Violations =  0.0\n",
      "Average_violations =  -18974.394948068562 305.8331009808531\n",
      "MSE =  1.038044870711968 0.009596941374801335\n",
      "Model_name =  temp/e290 Violations =  0.0\n",
      "Average_violations =  -18789.7298768391 309.1445698523531\n",
      "MSE =  1.039159025726238 0.0090670863738358\n",
      "Model_name =  temp/e291 Violations =  0.0\n",
      "Average_violations =  -18909.652770855064 305.3866759503147\n",
      "MSE =  1.0348019941979612 0.01347302829318756\n",
      "Model_name =  temp/e292 Violations =  0.0\n",
      "Average_violations =  -18954.318035923057 308.20946556316716\n",
      "MSE =  1.0351584539825374 0.013559958231938821\n",
      "Model_name =  temp/e293 Violations =  0.0\n",
      "Average_violations =  -19035.819367320462 304.8340686295173\n",
      "MSE =  1.0331668541359398 0.0156986895819029\n",
      "Model_name =  temp/e294 Violations =  0.0\n",
      "Average_violations =  -19012.317955842685 302.5629394373808\n",
      "MSE =  1.046722884768955 0.019302059341867402\n",
      "Model_name =  temp/e295 Violations =  0.0\n",
      "Average_violations =  -18897.733225031632 305.59917054529313\n",
      "MSE =  1.034853449927684 0.016032553842437795\n",
      "Model_name =  temp/e296 Violations =  0.0\n",
      "Average_violations =  -18993.640882471984 301.91138231905626\n",
      "MSE =  1.0357749092768775 0.009003191423649717\n",
      "Model_name =  temp/e297 Violations =  0.0\n",
      "Average_violations =  -19063.545053338756 308.6359945828368\n",
      "MSE =  1.0331907850773763 0.011939455567527789\n",
      "Model_name =  temp/e298 Violations =  0.0\n",
      "Average_violations =  -19002.20359599353 309.75948624790664\n",
      "MSE =  1.0361717182460448 0.01397235860637084\n",
      "Model_name =  temp/e299 Violations =  0.0\n",
      "Average_violations =  -19071.02634518906 302.39307303066994\n",
      "MSE =  1.0404422254618413 0.008129153696465621\n",
      "Model_name =  temp/e300 Violations =  0.0\n",
      "Average_violations =  -18972.66384154758 299.1039590945287\n",
      "MSE =  1.0427934250042363 0.017393601632012864\n",
      "Model_name =  temp/e301 Violations =  0.0\n",
      "Average_violations =  -18892.45543382577 297.05389992975773\n",
      "MSE =  1.038363499940005 0.014061485112149615\n",
      "Model_name =  temp/e302 Violations =  0.0\n",
      "Average_violations =  -19070.30453958772 303.41476994636724\n",
      "MSE =  1.0432096409024436 0.013650980519583047\n",
      "Model_name =  temp/e303 Violations =  0.0\n",
      "Average_violations =  -18977.53518472928 302.19144230419164\n",
      "MSE =  1.0377280087060574 0.016149414458600835\n",
      "Model_name =  temp/e304 Violations =  0.0\n",
      "Average_violations =  -19062.15387216578 301.56256799438324\n",
      "MSE =  1.0352325049243885 0.011089934239862584\n",
      "Model_name =  temp/e305 Violations =  0.0\n",
      "Average_violations =  -18898.4350365575 305.3879824981668\n",
      "MSE =  1.0402420799490515 0.016247920315067683\n",
      "Model_name =  temp/e306 Violations =  0.0\n",
      "Average_violations =  -18837.352053166844 307.831048177702\n",
      "MSE =  1.036258225620458 0.014283911509080784\n",
      "Model_name =  temp/e307 Violations =  0.0\n",
      "Average_violations =  -19085.815556345093 306.09916761601073\n",
      "MSE =  1.0365448210699655 0.01156878453759375\n",
      "Model_name =  temp/e308 Violations =  0.0\n",
      "Average_violations =  -18844.357892047814 309.48041467016725\n",
      "MSE =  1.045229112671624 0.01292264427632182\n",
      "Model_name =  temp/e309 Violations =  0.0\n",
      "Average_violations =  -18998.13876232769 307.10282289371594\n",
      "MSE =  1.0371648778902327 0.013722251480805957\n",
      "Model_name =  temp/e310 Violations =  0.0\n",
      "Average_violations =  -18990.31645947364 311.21694883633324\n",
      "MSE =  1.0316470158468483 0.01305095970951752\n",
      "Model_name =  temp/e311 Violations =  0.0\n",
      "Average_violations =  -19071.043575289666 309.58532642878856\n",
      "MSE =  1.0435949882544933 0.012871104614049382\n",
      "Model_name =  temp/e312 Violations =  0.0\n",
      "Average_violations =  -19024.448373791693 304.5897157057714\n",
      "MSE =  1.0388537889133058 0.01224833631108682\n",
      "Model_name =  temp/e313 Violations =  0.0\n",
      "Average_violations =  -19015.71497087812 304.2120658417844\n",
      "MSE =  1.0406992596794549 0.010110759102415424\n",
      "Model_name =  temp/e314 Violations =  0.0\n",
      "Average_violations =  -18925.140141712403 304.83752297657713\n",
      "MSE =  1.0367314339103073 0.011543405291061962\n",
      "Model_name =  temp/e315 Violations =  0.0\n",
      "Average_violations =  -18957.75870971443 304.1412909372133\n",
      "MSE =  1.0345183726273004 0.009406641399434944\n",
      "Model_name =  temp/e316 Violations =  0.0\n",
      "Average_violations =  -18791.228500911173 299.4212633670838\n",
      "MSE =  1.0427411937608047 0.011492958208892366\n",
      "Model_name =  temp/e317 Violations =  0.0\n",
      "Average_violations =  -18920.68265501938 300.1628485027655\n",
      "MSE =  1.0358064436701013 0.01318887976143924\n",
      "Model_name =  temp/e318 Violations =  0.0\n",
      "Average_violations =  -19035.632309354973 308.21222203883724\n",
      "MSE =  1.0340956317081023 0.010514665450871465\n",
      "Model_name =  temp/e319 Violations =  0.0\n",
      "Average_violations =  -18984.347405132157 302.15932803096837\n",
      "MSE =  1.0406743177082571 0.014423403653036437\n",
      "Model_name =  temp/e320 Violations =  0.0\n",
      "Average_violations =  -18758.851799376673 307.8666562878531\n",
      "MSE =  1.0422024555375908 0.012106927756853108\n",
      "Model_name =  temp/e321 Violations =  0.0\n",
      "Average_violations =  -18877.43676844886 298.06945117494894\n",
      "MSE =  1.040038453076365 0.013322079548300772\n",
      "Model_name =  temp/e322 Violations =  0.0\n",
      "Average_violations =  -19070.068618221198 306.77660772838226\n",
      "MSE =  1.0372410851833975 0.016468005987052545\n",
      "Model_name =  temp/e323 Violations =  0.0\n",
      "Average_violations =  -19110.898208094997 312.7859916048576\n",
      "MSE =  1.0346503934755678 0.012101134124947732\n",
      "Model_name =  temp/e324 Violations =  0.0\n",
      "Average_violations =  -19046.15352786593 303.9753569619182\n",
      "MSE =  1.036056372060599 0.01365187840827597\n",
      "Model_name =  temp/e325 Violations =  0.0\n",
      "Average_violations =  -18897.041360627274 312.6491756642369\n",
      "MSE =  1.0387379195806974 0.012579136589971895\n",
      "Model_name =  temp/e326 Violations =  0.0\n",
      "Average_violations =  -19074.56825065404 314.78466714730916\n",
      "MSE =  1.0384461464542984 0.013552838714529296\n",
      "Model_name =  temp/e327 Violations =  0.0\n",
      "Average_violations =  -18970.29860932781 304.78421984143284\n",
      "MSE =  1.0344288953988388 0.013695536115929706\n",
      "Model_name =  temp/e328 Violations =  0.0\n",
      "Average_violations =  -19007.111990439204 305.21995893947883\n",
      "MSE =  1.0344239626211542 0.012019166601578129\n",
      "Model_name =  temp/e329 Violations =  0.0\n",
      "Average_violations =  -19053.348800048534 304.5130726755291\n",
      "MSE =  1.0393519861306548 0.014976363368508685\n",
      "Model_name =  temp/e330 Violations =  0.0\n",
      "Average_violations =  -18976.386063212223 307.2442711876805\n",
      "MSE =  1.0366945667885767 0.011096910968650474\n",
      "Model_name =  temp/e331 Violations =  0.0\n",
      "Average_violations =  -18958.970652707947 306.6343432921321\n",
      "MSE =  1.036306615558709 0.011269866369717595\n",
      "Model_name =  temp/e332 Violations =  0.0\n",
      "Average_violations =  -18932.377012887002 304.93848189200975\n",
      "MSE =  1.0397637870810286 0.014255133953301441\n",
      "Model_name =  temp/e333 Violations =  0.0\n",
      "Average_violations =  -18863.13084622065 314.647205109267\n",
      "MSE =  1.0300606882812775 0.014237560818973628\n",
      "Model_name =  temp/e334 Violations =  0.0\n",
      "Average_violations =  -18922.3915910904 308.11864345011577\n",
      "MSE =  1.0370805277321278 0.01488122474500954\n",
      "Model_name =  temp/e335 Violations =  0.0\n",
      "Average_violations =  -18917.597608605836 306.5927835763159\n",
      "MSE =  1.033941958616629 0.012698605815349078\n",
      "Model_name =  temp/e336 Violations =  0.0\n",
      "Average_violations =  -18876.106145181733 304.07932891011296\n",
      "MSE =  1.0391362086774174 0.015822325381448732\n",
      "Model_name =  temp/e337 Violations =  0.0\n",
      "Average_violations =  -18856.996420189906 305.55330361257325\n",
      "MSE =  1.0345195737601154 0.015832859948333063\n",
      "Model_name =  temp/e338 Violations =  0.0\n",
      "Average_violations =  -19054.48991586069 297.16188976795087\n",
      "MSE =  1.0380442741292462 0.01298992643856215\n",
      "Model_name =  temp/e339 Violations =  0.0\n",
      "Average_violations =  -18999.645916514983 309.709151009049\n",
      "MSE =  1.038236781023702 0.01593566858027505\n",
      "Model_name =  temp/e340 Violations =  0.0\n",
      "Average_violations =  -18942.705861573515 314.0051762418745\n",
      "MSE =  1.0336487922823656 0.014611693262932903\n",
      "Model_name =  temp/e341 Violations =  0.0\n",
      "Average_violations =  -18896.56718739468 308.3512963694808\n",
      "MSE =  1.0340264974770919 0.014827016162801588\n",
      "Model_name =  temp/e342 Violations =  0.0\n",
      "Average_violations =  -19108.04521369 304.55085404926695\n",
      "MSE =  1.0379064954489454 0.009246686430048529\n",
      "Model_name =  temp/e343 Violations =  0.0\n",
      "Average_violations =  -18960.28208649568 296.441997847057\n",
      "MSE =  1.0407586810489982 0.01313257887738171\n",
      "Model_name =  temp/e344 Violations =  0.0\n",
      "Average_violations =  -19087.403040671008 303.8705227867176\n",
      "MSE =  1.0390810013569227 0.009183350321310357\n",
      "Model_name =  temp/e345 Violations =  0.0\n",
      "Average_violations =  -18991.733929592992 301.80730174772475\n",
      "MSE =  1.0366234957696212 0.00861310711240865\n",
      "Model_name =  temp/e346 Violations =  0.0\n",
      "Average_violations =  -18871.412842770285 313.1487398678283\n",
      "MSE =  1.0422226349161434 0.015242312229536807\n",
      "Model_name =  temp/e347 Violations =  0.0\n",
      "Average_violations =  -19022.77368858964 307.25829683591735\n",
      "MSE =  1.0384726506013244 0.008602535999399297\n",
      "Model_name =  temp/e348 Violations =  0.0\n",
      "Average_violations =  -18967.531696240476 311.36447818253816\n",
      "MSE =  1.0394228993034376 0.011604428871700824\n",
      "Model_name =  temp/e349 Violations =  0.0\n",
      "Average_violations =  -19047.919619715336 318.5800883941172\n",
      "MSE =  1.0515140824243954 0.015634972738530782\n",
      "Model_name =  temp/e350 Violations =  0.0\n",
      "Average_violations =  -18997.8361933843 301.5120016951198\n",
      "MSE =  1.0359296986635282 0.010673164272598094\n",
      "Model_name =  temp/e351 Violations =  0.0\n",
      "Average_violations =  -18937.690681382144 311.94809308926966\n",
      "MSE =  1.0322766718555683 0.014618564407835636\n",
      "Model_name =  temp/e352 Violations =  0.0\n",
      "Average_violations =  -19057.806417901695 306.6920646323704\n",
      "MSE =  1.0362503273872057 0.014416329621332364\n",
      "Model_name =  temp/e353 Violations =  0.0\n",
      "Average_violations =  -19095.29467812446 304.09962611487634\n",
      "MSE =  1.0434735890842346 0.01457430645848145\n",
      "Model_name =  temp/e354 Violations =  0.0\n",
      "Average_violations =  -19000.89843703845 306.10873299657686\n",
      "MSE =  1.0400113562765223 0.008136638164624597\n",
      "Model_name =  temp/e355 Violations =  0.0\n",
      "Average_violations =  -18959.333800455574 305.7134125475513\n",
      "MSE =  1.0333484339152006 0.009388408116196817\n",
      "Model_name =  temp/e356 Violations =  0.0\n",
      "Average_violations =  -19008.078058321666 303.9458713519595\n",
      "MSE =  1.0385482910570996 0.013610260353078286\n",
      "Model_name =  temp/e357 Violations =  0.0\n",
      "Average_violations =  -19033.658144661145 307.909360031497\n",
      "MSE =  1.0334262938766559 0.015625595767506918\n",
      "Model_name =  temp/e358 Violations =  0.0\n",
      "Average_violations =  -19001.840372509578 308.21056082544266\n",
      "MSE =  1.034796565849586 0.015914744623282517\n",
      "Model_name =  temp/e359 Violations =  0.0\n",
      "Average_violations =  -19012.122786005053 302.03183582506887\n",
      "MSE =  1.0421405064052733 0.013049077679299685\n",
      "Model_name =  temp/e360 Violations =  0.0\n",
      "Average_violations =  -18913.947452715598 307.79239398856333\n",
      "MSE =  1.035047428010695 0.010364391865985124\n",
      "Model_name =  temp/e361 Violations =  0.0\n",
      "Average_violations =  -19126.715681255773 307.8100932038422\n",
      "MSE =  1.0458473527927539 0.010801184495244157\n",
      "Model_name =  temp/e362 Violations =  0.0\n",
      "Average_violations =  -18873.823295621172 308.8638664027532\n",
      "MSE =  1.037376278924722 0.015345973465228879\n",
      "Model_name =  temp/e363 Violations =  0.0\n",
      "Average_violations =  -18932.00618865609 304.9591243403409\n",
      "MSE =  1.0408313190656042 0.010018032342236336\n",
      "Model_name =  temp/e364 Violations =  0.0\n",
      "Average_violations =  -18886.180218127403 303.5028460264669\n",
      "MSE =  1.0370504354187051 0.011994265758603706\n",
      "Model_name =  temp/e365 Violations =  0.0\n",
      "Average_violations =  -18893.152933075435 305.765507963202\n",
      "MSE =  1.042674908327117 0.011443782774026946\n",
      "Model_name =  temp/e366 Violations =  0.0\n",
      "Average_violations =  -18932.00604793939 303.81964014335966\n",
      "MSE =  1.0369213878723746 0.014546835966722616\n",
      "Model_name =  temp/e367 Violations =  0.0\n",
      "Average_violations =  -19070.467001776844 306.88502514887034\n",
      "MSE =  1.0351782389247863 0.010088052747430768\n",
      "Model_name =  temp/e368 Violations =  0.0\n",
      "Average_violations =  -19024.416120077007 304.99860771246887\n",
      "MSE =  1.0403848627855625 0.010039472894993762\n",
      "Model_name =  temp/e369 Violations =  0.0\n",
      "Average_violations =  -18945.722866959295 301.0387737919785\n",
      "MSE =  1.044917898845408 0.013770313751758514\n",
      "Model_name =  temp/e370 Violations =  0.0\n",
      "Average_violations =  -19021.99791776861 308.7842450258437\n",
      "MSE =  1.034952499292627 0.015585672559216654\n",
      "Model_name =  temp/e371 Violations =  0.0\n",
      "Average_violations =  -18962.060122782765 311.02175377435975\n",
      "MSE =  1.034350500602402 0.012742382477189148\n",
      "Model_name =  temp/e372 Violations =  0.0\n",
      "Average_violations =  -18950.764046136264 293.5210399995267\n",
      "MSE =  1.0483380973782288 0.011151148653655135\n",
      "Model_name =  temp/e373 Violations =  0.0\n",
      "Average_violations =  -18996.7451105844 301.4024191055601\n",
      "MSE =  1.0397150883206019 0.009384909500411386\n",
      "Model_name =  temp/e374 Violations =  0.0\n",
      "Average_violations =  -18975.373436337813 299.93014540170907\n",
      "MSE =  1.0352471935867809 0.012362013135300321\n",
      "Model_name =  temp/e375 Violations =  0.0\n",
      "Average_violations =  -18775.891696205195 298.64740672014307\n",
      "MSE =  1.0448296908518526 0.007561498592279869\n",
      "Model_name =  temp/e376 Violations =  0.0\n",
      "Average_violations =  -18884.19024525159 310.6703260854581\n",
      "MSE =  1.0398181525908161 0.01302370435787951\n",
      "Model_name =  temp/e377 Violations =  0.0\n",
      "Average_violations =  -19011.799417469352 300.53761851668236\n",
      "MSE =  1.033322769911923 0.011703159654928479\n",
      "Model_name =  temp/e378 Violations =  0.0\n",
      "Average_violations =  -18824.08479728611 315.7557383756456\n",
      "MSE =  1.0398491334635487 0.010016356264220514\n",
      "Model_name =  temp/e379 Violations =  0.0\n",
      "Average_violations =  -18873.94071583596 307.4141679962231\n",
      "MSE =  1.0375222800415185 0.015394066754321227\n",
      "Model_name =  temp/e380 Violations =  0.0\n",
      "Average_violations =  -18956.02462602657 308.0169191214375\n",
      "MSE =  1.0396943401793837 0.012461179738672359\n",
      "Model_name =  temp/e381 Violations =  0.0\n",
      "Average_violations =  -19001.536930914997 308.3203579625032\n",
      "MSE =  1.044066707538056 0.010940584729114841\n",
      "Model_name =  temp/e382 Violations =  0.0\n",
      "Average_violations =  -18886.040119586283 312.3532640655467\n",
      "MSE =  1.04010413336654 0.010384158560607704\n",
      "Model_name =  temp/e383 Violations =  0.0\n",
      "Average_violations =  -18929.90500428518 306.6328031946023\n",
      "MSE =  1.0364965326787736 0.011752947769042914\n",
      "Model_name =  temp/e384 Violations =  0.0\n",
      "Average_violations =  -18942.303801836748 311.2275186020808\n",
      "MSE =  1.0350339198667164 0.017127065262833797\n",
      "Model_name =  temp/e385 Violations =  0.0\n",
      "Average_violations =  -18864.78130435798 310.97950213537695\n",
      "MSE =  1.0362818061112653 0.014597869634212045\n",
      "Model_name =  temp/e386 Violations =  0.0\n",
      "Average_violations =  -18953.35526610753 306.8593284601463\n",
      "MSE =  1.0407699132743253 0.017358043684897104\n",
      "Model_name =  temp/e387 Violations =  0.0\n",
      "Average_violations =  -18942.53094259953 296.63814009540306\n",
      "MSE =  1.0287298935186353 0.014323951446579095\n",
      "Model_name =  temp/e388 Violations =  0.0\n",
      "Average_violations =  -18960.174509399207 302.5854009391946\n",
      "MSE =  1.0328827374310152 0.007157189029833654\n",
      "Model_name =  temp/e389 Violations =  0.0\n",
      "Average_violations =  -18911.58016202781 309.5551491433184\n",
      "MSE =  1.0371331959429988 0.01571736419971606\n",
      "Model_name =  temp/e390 Violations =  0.0\n",
      "Average_violations =  -18958.455997828438 310.9721688104469\n",
      "MSE =  1.0345987271275583 0.010350676163714783\n",
      "Model_name =  temp/e391 Violations =  0.0\n",
      "Average_violations =  -18871.554312435543 309.48422849826943\n",
      "MSE =  1.0416488790574843 0.016055789097841243\n",
      "Model_name =  temp/e392 Violations =  0.0\n",
      "Average_violations =  -18975.633192208752 312.5273070265923\n",
      "MSE =  1.0352824879770424 0.0102395649639237\n",
      "Model_name =  temp/e393 Violations =  0.0\n",
      "Average_violations =  -19137.141216571697 302.38228737441057\n",
      "MSE =  1.0385531667477357 0.011583020217150432\n",
      "Model_name =  temp/e394 Violations =  0.0\n",
      "Average_violations =  -18913.5110849863 292.55128001941415\n",
      "MSE =  1.0406127297001073 0.01118084683180595\n",
      "Model_name =  temp/e395 Violations =  0.0\n",
      "Average_violations =  -18984.848685979778 309.20930266223144\n",
      "MSE =  1.035565168141341 0.016384227781839386\n",
      "Model_name =  temp/e396 Violations =  0.0\n",
      "Average_violations =  -18943.737201620588 310.8229629198806\n",
      "MSE =  1.035518334293883 0.014489826672374105\n",
      "Model_name =  temp/e397 Violations =  0.0\n",
      "Average_violations =  -18897.886513083737 311.40570226585936\n",
      "MSE =  1.03283120295058 0.012318322006922327\n",
      "Model_name =  temp/e398 Violations =  0.0\n",
      "Average_violations =  -18908.36484660855 310.3640909618948\n",
      "MSE =  1.0424023416201191 0.013546995784897757\n",
      "Model_name =  temp/e399 Violations =  0.0\n",
      "Average_violations =  -18925.23149770729 315.5097746436723\n",
      "MSE =  1.0287484517224283 0.013145831215153303\n",
      "Model_name =  temp/e400 Violations =  0.0\n",
      "Average_violations =  -18985.56909668282 314.6268516321487\n",
      "MSE =  1.0335827121047578 0.010930348385240417\n",
      "Model_name =  temp/e401 Violations =  0.0\n",
      "Average_violations =  -19059.069767910376 311.6287581136776\n",
      "MSE =  1.0368822548772179 0.011270060239495455\n",
      "Model_name =  temp/e402 Violations =  0.0\n",
      "Average_violations =  -18828.639573162807 305.1818031316498\n",
      "MSE =  1.0377689130738887 0.015253266963459326\n",
      "Model_name =  temp/e403 Violations =  0.0\n",
      "Average_violations =  -18977.682364800177 308.1447011017696\n",
      "MSE =  1.0351733999967005 0.012038541830423265\n",
      "Model_name =  temp/e404 Violations =  0.0\n",
      "Average_violations =  -19016.756021535424 297.13101870430364\n",
      "MSE =  1.0356809325140894 0.01278722216809029\n",
      "Model_name =  temp/e405 Violations =  0.0\n",
      "Average_violations =  -18898.858448968207 304.898368316008\n",
      "MSE =  1.036253752943551 0.014214305892661937\n",
      "Model_name =  temp/e406 Violations =  0.0\n",
      "Average_violations =  -19007.071678034215 313.23696331739444\n",
      "MSE =  1.0367372940470743 0.010006609331779189\n",
      "Model_name =  temp/e407 Violations =  0.0\n",
      "Average_violations =  -18853.657329161084 295.23614255002843\n",
      "MSE =  1.0362106593968345 0.011716047555530946\n",
      "Model_name =  temp/e408 Violations =  0.0\n",
      "Average_violations =  -18917.468802021987 311.0527519020962\n",
      "MSE =  1.0380088545602668 0.011208061667616806\n",
      "Model_name =  temp/e409 Violations =  0.0\n",
      "Average_violations =  -19025.77646689053 306.7432625999217\n",
      "MSE =  1.038673528768648 0.012899847164576364\n",
      "Model_name =  temp/e410 Violations =  0.0\n",
      "Average_violations =  -18826.996455486344 298.12528403628994\n",
      "MSE =  1.0427016011534502 0.01043167271619021\n",
      "Model_name =  temp/e411 Violations =  0.0\n",
      "Average_violations =  -18934.589168601997 308.6296154758665\n",
      "MSE =  1.0411139479567404 0.01419913351851897\n",
      "Model_name =  temp/e412 Violations =  0.0\n",
      "Average_violations =  -19012.840897621812 305.73058089365355\n",
      "MSE =  1.0400868306593725 0.01413200042781921\n",
      "Model_name =  temp/e413 Violations =  0.0\n",
      "Average_violations =  -19044.579237280326 299.1973376221815\n",
      "MSE =  1.0391989239810722 0.015816047647627333\n",
      "Model_name =  temp/e414 Violations =  0.0\n",
      "Average_violations =  -18903.308504960776 306.09522999243956\n",
      "MSE =  1.0374759943931084 0.009023697391670719\n",
      "Model_name =  temp/e415 Violations =  0.0\n",
      "Average_violations =  -19033.13000996425 301.04130827912286\n",
      "MSE =  1.0363097306759659 0.00773760586535604\n",
      "Model_name =  temp/e416 Violations =  0.0\n",
      "Average_violations =  -18922.655666114595 303.642454969243\n",
      "MSE =  1.0334152654010285 0.012384166145132137\n",
      "Model_name =  temp/e417 Violations =  0.0\n",
      "Average_violations =  -18846.051530129866 308.7679969727431\n",
      "MSE =  1.0383313924420168 0.012835255190078988\n",
      "Model_name =  temp/e418 Violations =  0.0\n",
      "Average_violations =  -18934.04438380024 312.2898002574054\n",
      "MSE =  1.0351086805222882 0.012514192463794066\n",
      "Model_name =  temp/e419 Violations =  0.0\n",
      "Average_violations =  -18947.285896334477 309.10599537363703\n",
      "MSE =  1.0350228692076595 0.014049265943483652\n",
      "Model_name =  temp/e420 Violations =  0.0\n",
      "Average_violations =  -19071.833544439633 310.37217076550974\n",
      "MSE =  1.037031245059892 0.015134139929512062\n",
      "Model_name =  temp/e421 Violations =  0.0\n",
      "Average_violations =  -19068.58893259947 309.87441626300165\n",
      "MSE =  1.0378969545087613 0.005163352960484075\n",
      "Model_name =  temp/e422 Violations =  0.0\n",
      "Average_violations =  -18966.2586062737 307.91077446800506\n",
      "MSE =  1.0355889119796653 0.014142353396945811\n",
      "Model_name =  temp/e423 Violations =  0.0\n",
      "Average_violations =  -18950.430129171866 307.9612067392759\n",
      "MSE =  1.0362211537640242 0.015185145774588567\n",
      "Model_name =  temp/e424 Violations =  0.0\n",
      "Average_violations =  -18922.921402120228 299.13935417347943\n",
      "MSE =  1.0383884679922986 0.009923006159697266\n",
      "Model_name =  temp/e425 Violations =  0.0\n",
      "Average_violations =  -18953.568546706963 300.97707056490765\n",
      "MSE =  1.0355787932276337 0.016722326232033933\n",
      "Model_name =  temp/e426 Violations =  0.0\n",
      "Average_violations =  -19065.06095510439 304.5943511962639\n",
      "MSE =  1.0411304429291968 0.01330096837281995\n",
      "Model_name =  temp/e427 Violations =  0.0\n",
      "Average_violations =  -18869.544805618752 299.1193528028416\n",
      "MSE =  1.034392936431662 0.008757654999812738\n",
      "Model_name =  temp/e428 Violations =  0.0\n",
      "Average_violations =  -18950.481613962424 302.5076235971738\n",
      "MSE =  1.0354401190338391 0.009894829234154467\n",
      "Model_name =  temp/e429 Violations =  0.0\n",
      "Average_violations =  -19021.826682507013 300.1030212679744\n",
      "MSE =  1.0364377455021676 0.011294218154146819\n",
      "Model_name =  temp/e430 Violations =  0.0\n",
      "Average_violations =  -18919.8863245074 313.164287754042\n",
      "MSE =  1.0355155176678639 0.011639270863043795\n",
      "Model_name =  temp/e431 Violations =  0.0\n",
      "Average_violations =  -18922.40679542543 310.2228153953274\n",
      "MSE =  1.0403545366262077 0.018754687988814155\n",
      "Model_name =  temp/e432 Violations =  0.0\n",
      "Average_violations =  -19021.18475816318 310.92177202467417\n",
      "MSE =  1.0450618444062887 0.016932735334324964\n",
      "Model_name =  temp/e433 Violations =  0.0\n",
      "Average_violations =  -19004.915905650243 302.05566805914515\n",
      "MSE =  1.0377110630314301 0.011140065975832576\n",
      "Model_name =  temp/e434 Violations =  0.0\n",
      "Average_violations =  -18936.848146192668 310.36078792590774\n",
      "MSE =  1.0326935672613011 0.014573661582963008\n",
      "Model_name =  temp/e435 Violations =  0.0\n",
      "Average_violations =  -18956.565659200067 301.19731396827166\n",
      "MSE =  1.0384386390177431 0.011130514693366598\n",
      "Model_name =  temp/e436 Violations =  0.0\n",
      "Average_violations =  -18903.473637709998 306.2925345430902\n",
      "MSE =  1.0342807960244713 0.01049695914387116\n",
      "Model_name =  temp/e437 Violations =  0.0\n",
      "Average_violations =  -19010.93279193337 311.3516326513952\n",
      "MSE =  1.0375314533012245 0.012601298957033906\n",
      "Model_name =  temp/e438 Violations =  0.0\n",
      "Average_violations =  -18984.4246403783 309.6195632768326\n",
      "MSE =  1.0334914034208185 0.013489874672167085\n",
      "Model_name =  temp/e439 Violations =  0.0\n",
      "Average_violations =  -18924.156956688093 297.724093490406\n",
      "MSE =  1.0341115131554135 0.01229723533428206\n",
      "Model_name =  temp/e440 Violations =  0.0\n",
      "Average_violations =  -18917.02957866218 307.87149173102904\n",
      "MSE =  1.0330739880271402 0.015115113539617636\n",
      "Model_name =  temp/e441 Violations =  0.0\n",
      "Average_violations =  -18929.99929597944 311.4206387070163\n",
      "MSE =  1.0366908830597852 0.016647321829550893\n",
      "Model_name =  temp/e442 Violations =  0.0\n",
      "Average_violations =  -18982.995054480627 309.8284739647421\n",
      "MSE =  1.0348009536503198 0.011712323022046525\n",
      "Model_name =  temp/e443 Violations =  0.0\n",
      "Average_violations =  -18991.21442834829 304.89517449057195\n",
      "MSE =  1.0410214131515398 0.01163502205753769\n",
      "Model_name =  temp/e444 Violations =  0.0\n",
      "Average_violations =  -18944.13735485762 313.90564767474626\n",
      "MSE =  1.0373434856624173 0.010972425848426415\n",
      "Model_name =  temp/e445 Violations =  0.0\n",
      "Average_violations =  -18868.71235279384 308.9251295406299\n",
      "MSE =  1.0403627211635853 0.0038075324293743024\n",
      "Model_name =  temp/e446 Violations =  0.0\n",
      "Average_violations =  -19089.82934408409 301.61444887089357\n",
      "MSE =  1.042146174476286 0.016008465289148465\n",
      "Model_name =  temp/e447 Violations =  0.0\n",
      "Average_violations =  -18828.574033280496 313.7833451325465\n",
      "MSE =  1.0408285517586409 0.01527763065949601\n",
      "Model_name =  temp/e448 Violations =  0.0\n",
      "Average_violations =  -18918.551062194572 311.2736309627187\n",
      "MSE =  1.0362569602423546 0.013828640374617993\n",
      "Model_name =  temp/e449 Violations =  0.0\n",
      "Average_violations =  -19084.59378442149 308.65861643218966\n",
      "MSE =  1.0365503439137214 0.012252646275313576\n",
      "Model_name =  temp/e450 Violations =  0.0\n",
      "Average_violations =  -18967.056862540885 301.18551823356074\n",
      "MSE =  1.0362274455461535 0.01782487586112884\n",
      "Model_name =  temp/e451 Violations =  0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average_violations =  -18817.52628770388 303.3478971374268\n",
      "MSE =  1.0383253168505404 0.012542622788195294\n",
      "Model_name =  temp/e452 Violations =  0.0\n",
      "Average_violations =  -19093.26499002116 297.26731334804305\n",
      "MSE =  1.0405017498055034 0.008406894010842787\n",
      "Model_name =  temp/e453 Violations =  0.0\n",
      "Average_violations =  -19094.22130244054 304.3390416477725\n",
      "MSE =  1.0377829191925863 0.011032837607241467\n",
      "Model_name =  temp/e454 Violations =  0.0\n",
      "Average_violations =  -18780.177856761173 311.84181297055795\n",
      "MSE =  1.041553932489846 0.012955759382562933\n",
      "Model_name =  temp/e455 Violations =  0.0\n",
      "Average_violations =  -18918.89104460016 297.5581809222527\n",
      "MSE =  1.0371913281342056 0.016163404795380955\n",
      "Model_name =  temp/e456 Violations =  0.0\n",
      "Average_violations =  -18922.894770791532 309.31373933935157\n",
      "MSE =  1.0372827036423538 0.016041665512460702\n",
      "Model_name =  temp/e457 Violations =  0.0\n",
      "Average_violations =  -18965.342262017944 309.17471785211234\n",
      "MSE =  1.0335874140070869 0.013454555302153287\n",
      "Model_name =  temp/e458 Violations =  0.0\n",
      "Average_violations =  -18992.578476950795 306.28780459377396\n",
      "MSE =  1.02985936884886 0.011880401053706278\n",
      "Model_name =  temp/e459 Violations =  0.0\n",
      "Average_violations =  -18973.492919614444 306.7230523517721\n",
      "MSE =  1.0387461209779683 0.01632093756625706\n",
      "Model_name =  temp/e460 Violations =  0.0\n",
      "Average_violations =  -18958.860630432362 300.7684524629368\n",
      "MSE =  1.035461985144511 0.014194978552909474\n",
      "Model_name =  temp/e461 Violations =  0.0\n",
      "Average_violations =  -18960.584631406004 307.04924790823685\n",
      "MSE =  1.0363900040591887 0.011870063125278137\n",
      "Model_name =  temp/e462 Violations =  0.0\n",
      "Average_violations =  -18832.174673162906 310.17868026284856\n",
      "MSE =  1.0360831694140127 0.01139345910976563\n",
      "Model_name =  temp/e463 Violations =  0.0\n",
      "Average_violations =  -18980.691002345025 304.0596594634383\n",
      "MSE =  1.037291463049999 0.012419851048801011\n",
      "Model_name =  temp/e464 Violations =  0.0\n",
      "Average_violations =  -18766.971777849176 316.825144509832\n",
      "MSE =  1.0393056536174334 0.016817155168466935\n",
      "Model_name =  temp/e465 Violations =  0.0\n",
      "Average_violations =  -18880.914237896395 302.4335649939661\n",
      "MSE =  1.0340237490067914 0.01598407280439043\n",
      "Model_name =  temp/e466 Violations =  0.0\n",
      "Average_violations =  -18918.37048098502 309.7679834678839\n",
      "MSE =  1.0403648124262623 0.011290683066333565\n",
      "Model_name =  temp/e467 Violations =  0.0\n",
      "Average_violations =  -19049.118982156506 301.6403023462063\n",
      "MSE =  1.0408313338674002 0.012330352422297417\n",
      "Model_name =  temp/e468 Violations =  0.0\n",
      "Average_violations =  -18922.7296958912 305.2259347819543\n",
      "MSE =  1.0326773623239196 0.009926746739503767\n",
      "Model_name =  temp/e469 Violations =  0.0\n",
      "Average_violations =  -19060.799181235667 307.35654213225905\n",
      "MSE =  1.033521268824129 0.01626523677235914\n",
      "Model_name =  temp/e470 Violations =  0.0\n",
      "Average_violations =  -19039.832594707183 310.79845879911386\n",
      "MSE =  1.041699337896885 0.01723942642352738\n",
      "Model_name =  temp/e471 Violations =  0.0\n",
      "Average_violations =  -18946.95675547581 307.6619551577458\n",
      "MSE =  1.0466917234214332 0.013261163840635389\n",
      "Model_name =  temp/e472 Violations =  0.0\n",
      "Average_violations =  -19007.149870253168 311.0613922095524\n",
      "MSE =  1.0354626017115351 0.012178303630781689\n",
      "Model_name =  temp/e473 Violations =  0.0\n",
      "Average_violations =  -18944.052107104486 312.2306855577229\n",
      "MSE =  1.039558388667847 0.014332518354762685\n",
      "Model_name =  temp/e474 Violations =  0.0\n",
      "Average_violations =  -18866.986900747703 309.9524188618984\n",
      "MSE =  1.035087346462146 0.010060435980809707\n",
      "Model_name =  temp/e475 Violations =  0.0\n",
      "Average_violations =  -18859.98982599793 303.6825501916747\n",
      "MSE =  1.0396337489087473 0.01409191074000269\n",
      "Model_name =  temp/e476 Violations =  0.0\n",
      "Average_violations =  -19098.482507903496 310.8775647477194\n",
      "MSE =  1.0360991539057138 0.015262593959089934\n",
      "Model_name =  temp/e477 Violations =  0.0\n",
      "Average_violations =  -18991.158885459874 296.9198289150414\n",
      "MSE =  1.0350916103952372 0.013182808173830798\n",
      "Model_name =  temp/e478 Violations =  0.0\n",
      "Average_violations =  -18923.680129219185 305.16505105978894\n",
      "MSE =  1.0454281445430114 0.004834980678559988\n",
      "Model_name =  temp/e479 Violations =  0.0\n",
      "Average_violations =  -18929.538212132087 309.3898329979377\n",
      "MSE =  1.0369650439715496 0.0085704491941\n",
      "Model_name =  temp/e480 Violations =  0.0\n",
      "Average_violations =  -18895.23396873881 303.8668327180387\n",
      "MSE =  1.033115937212191 0.012059761512235995\n",
      "Model_name =  temp/e481 Violations =  0.0\n",
      "Average_violations =  -19001.78163811108 289.73465107678993\n",
      "MSE =  1.039469096902585 0.012137053588498667\n",
      "Model_name =  temp/e482 Violations =  0.0\n",
      "Average_violations =  -18838.680254348386 299.26048722912003\n",
      "MSE =  1.04128379849578 0.010895229074525158\n",
      "Model_name =  temp/e483 Violations =  0.0\n",
      "Average_violations =  -19027.52872354413 304.59403508344604\n",
      "MSE =  1.0337128900641257 0.014570331583302575\n",
      "Model_name =  temp/e484 Violations =  0.0\n",
      "Average_violations =  -18896.972645167214 310.8195321952879\n",
      "MSE =  1.0438517504070348 0.016426522668380183\n",
      "Model_name =  temp/e485 Violations =  0.0\n",
      "Average_violations =  -19059.5729413452 300.68389238271965\n",
      "MSE =  1.0382655604018818 0.01774604607520252\n",
      "Model_name =  temp/e486 Violations =  0.0\n",
      "Average_violations =  -18916.0862770749 314.57389831962155\n",
      "MSE =  1.041131010500487 0.014379621231667099\n",
      "Model_name =  temp/e487 Violations =  0.0\n",
      "Average_violations =  -18937.119959272695 310.8984890343886\n",
      "MSE =  1.0333444776761804 0.01104706144570784\n",
      "Model_name =  temp/e488 Violations =  0.0\n",
      "Average_violations =  -18937.965502550953 313.8596724662586\n",
      "MSE =  1.0372518925214722 0.00715149934539959\n",
      "Model_name =  temp/e489 Violations =  0.0\n",
      "Average_violations =  -18917.47070811244 307.02235292044355\n",
      "MSE =  1.0399206422391765 0.011993185066713168\n",
      "Model_name =  temp/e490 Violations =  0.0\n",
      "Average_violations =  -18941.841813310064 300.53673463233577\n",
      "MSE =  1.0406425025261299 0.008754828133701664\n",
      "Model_name =  temp/e491 Violations =  0.0\n",
      "Average_violations =  -18885.071029041126 306.21067916243953\n",
      "MSE =  1.0415505590240026 0.01191821348841489\n",
      "Model_name =  temp/e492 Violations =  0.0\n",
      "Average_violations =  -19003.41071483249 296.9359530857668\n",
      "MSE =  1.035917643973437 0.013935673313036063\n",
      "Model_name =  temp/e493 Violations =  0.0\n",
      "Average_violations =  -18906.829812616874 311.60960518216336\n",
      "MSE =  1.0354156024894636 0.013925162558057057\n",
      "Model_name =  temp/e494 Violations =  0.0\n",
      "Average_violations =  -18886.465312480545 301.5954345264062\n",
      "MSE =  1.0365712356020473 0.015206237417777144\n",
      "Model_name =  temp/e495 Violations =  0.0\n",
      "Average_violations =  -18848.37941194263 314.814200074625\n",
      "MSE =  1.0398348919398388 0.014023306636699276\n",
      "Model_name =  temp/e496 Violations =  0.0\n",
      "Average_violations =  -18867.790159026685 314.97283592766195\n",
      "MSE =  1.0368584758314645 0.01625142755808582\n",
      "Model_name =  temp/e497 Violations =  0.0\n",
      "Average_violations =  -18982.788994051938 306.49740191677955\n",
      "MSE =  1.0354407494023095 0.013687011782933543\n",
      "Model_name =  temp/e498 Violations =  0.0\n",
      "Average_violations =  -19070.90209492866 306.8575768321218\n",
      "MSE =  1.0392714499557925 0.01235020010421964\n",
      "Model_name =  temp/e499 Violations =  0.0\n",
      "Average_violations =  -18852.26210144793 309.54855507160846\n",
      "MSE =  1.0416801117500092 0.017468845987808505\n",
      "[1.04024698 1.03996163 1.04179429 1.02994125 1.03551111 1.03466646\n",
      " 1.03231625 1.03615838 1.03972578 1.04049605 1.04319172 1.03427267\n",
      " 1.03857188 1.04216013 1.03257082 1.0365411  1.03998257 1.03694438\n",
      " 1.03749569 1.03180021 1.03681229 1.03593632 1.04110539 1.0333766\n",
      " 1.03637944 1.03792961 1.03455349 1.03745443 1.04317969 1.03770348\n",
      " 1.03401629 1.03615444 1.04195395 1.03748748 1.03773037 1.04033155\n",
      " 1.03350191 1.03454791 1.03721975 1.03130311 1.03688452 1.04357297\n",
      " 1.04212511 1.03466364 1.03481544 1.04260711 1.03448949 1.04277463\n",
      " 1.03384606 1.04340455 1.03091864 1.03637329 1.0340575  1.03512978\n",
      " 1.03461988 1.03494185 1.03435395 1.04374143 1.03401885 1.03545026\n",
      " 1.03866925 1.0383726  1.03466966 1.04197515 1.03950393 1.03124766\n",
      " 1.0376762  1.03705496 1.03487294 1.03459791 1.05351233 1.03458225\n",
      " 1.03437334 1.03339021 1.03887935 1.03787131 1.03845774 1.04036157\n",
      " 1.0339634  1.03114181 1.03569733 1.03557389 1.03268123 1.03341718\n",
      " 1.03459873 1.04461386 1.03611755 1.02954525 1.03857292 1.0346507\n",
      " 1.03733029 1.03488476 1.03447    1.03514385 1.03651447 1.03972026\n",
      " 1.04087234 1.04213462 1.0397366  1.03489135 1.0374256  1.03668654\n",
      " 1.04409453 1.04659455 1.03377919 1.03706669 1.03994045 1.03557654\n",
      " 1.04190966 1.03609926 1.04372942 1.03737217 1.03536002 1.03336196\n",
      " 1.03395308 1.03233535 1.04579289 1.03242062 1.03469598 1.03951468\n",
      " 1.04196373 1.0351514  1.04123339 1.03608469 1.03518764 1.03514279\n",
      " 1.03720684 1.03457593 1.04220932 1.03450637 1.03623904 1.03742825\n",
      " 1.03564443 1.03010733 1.03198818 1.03397109 1.03564257 1.03661044\n",
      " 1.03407552 1.04146689 1.03493348 1.03680917 1.03867218 1.03626788\n",
      " 1.03560856 1.035397   1.03754534 1.03740163 1.04591006 1.04011244\n",
      " 1.0331991  1.03568937 1.03639171 1.03187122 1.03495594 1.03755111\n",
      " 1.03666951 1.03548515 1.03528955 1.03827784 1.03793904 1.03963943\n",
      " 1.03847365 1.03597477 1.05772132 1.03253737 1.04283388 1.0439636\n",
      " 1.03848349 1.03180631 1.0417221  1.03676948 1.03841713 1.0418012\n",
      " 1.03349896 1.0443336  1.04186024 1.03221531 1.03491056 1.03507476\n",
      " 1.03886172 1.03703213 1.03251015 1.04394874 1.03889515 1.03306744\n",
      " 1.03702264 1.0315547  1.03785834 1.03102321 1.03749037 1.04385283\n",
      " 1.03579527 1.0375568  1.03946772 1.0358311  1.03995764 1.0383942\n",
      " 1.04006176 1.03524212 1.04410879 1.03652206 1.03424125 1.03747008\n",
      " 1.03592393 1.03782567 1.04377506 1.03617758 1.03943496 1.04247323\n",
      " 1.03433149 1.03769082 1.03642191 1.03832303 1.03892586 1.0342408\n",
      " 1.03640143 1.03918448 1.04096958 1.03361154 1.03859969 1.03828838\n",
      " 1.03595606 1.0362135  1.03948391 1.04221602 1.0386727  1.0476407\n",
      " 1.03373978 1.03846322 1.03474523 1.03173024 1.0379192  1.03679593\n",
      " 1.03939628 1.03844041 1.03961617 1.04230857 1.04094163 1.03757102\n",
      " 1.03467932 1.0369311  1.03854193 1.03610154 1.04371322 1.03789845\n",
      " 1.03791607 1.03726461 1.03391402 1.03513372 1.03285256 1.03316641\n",
      " 1.04140582 1.03229027 1.03589174 1.03205956 1.03509013 1.0428923\n",
      " 1.0390637  1.03663533 1.0393964  1.03490092 1.03760924 1.03419652\n",
      " 1.04207382 1.03701728 1.03832529 1.03626503 1.03820504 1.04100415\n",
      " 1.03355088 1.04195361 1.03390768 1.03620807 1.03834857 1.03987322\n",
      " 1.04010239 1.03790404 1.03374186 1.03895382 1.03877366 1.03901046\n",
      " 1.03602641 1.03726884 1.036352   1.03438101 1.04156535 1.03585757\n",
      " 1.03969683 1.03804487 1.03915903 1.03480199 1.03515845 1.03316685\n",
      " 1.04672288 1.03485345 1.03577491 1.03319079 1.03617172 1.04044223\n",
      " 1.04279343 1.0383635  1.04320964 1.03772801 1.0352325  1.04024208\n",
      " 1.03625823 1.03654482 1.04522911 1.03716488 1.03164702 1.04359499\n",
      " 1.03885379 1.04069926 1.03673143 1.03451837 1.04274119 1.03580644\n",
      " 1.03409563 1.04067432 1.04220246 1.04003845 1.03724109 1.03465039\n",
      " 1.03605637 1.03873792 1.03844615 1.0344289  1.03442396 1.03935199\n",
      " 1.03669457 1.03630662 1.03976379 1.03006069 1.03708053 1.03394196\n",
      " 1.03913621 1.03451957 1.03804427 1.03823678 1.03364879 1.0340265\n",
      " 1.0379065  1.04075868 1.039081   1.0366235  1.04222263 1.03847265\n",
      " 1.0394229  1.05151408 1.0359297  1.03227667 1.03625033 1.04347359\n",
      " 1.04001136 1.03334843 1.03854829 1.03342629 1.03479657 1.04214051\n",
      " 1.03504743 1.04584735 1.03737628 1.04083132 1.03705044 1.04267491\n",
      " 1.03692139 1.03517824 1.04038486 1.0449179  1.0349525  1.0343505\n",
      " 1.0483381  1.03971509 1.03524719 1.04482969 1.03981815 1.03332277\n",
      " 1.03984913 1.03752228 1.03969434 1.04406671 1.04010413 1.03649653\n",
      " 1.03503392 1.03628181 1.04076991 1.02872989 1.03288274 1.0371332\n",
      " 1.03459873 1.04164888 1.03528249 1.03855317 1.04061273 1.03556517\n",
      " 1.03551833 1.0328312  1.04240234 1.02874845 1.03358271 1.03688225\n",
      " 1.03776891 1.0351734  1.03568093 1.03625375 1.03673729 1.03621066\n",
      " 1.03800885 1.03867353 1.0427016  1.04111395 1.04008683 1.03919892\n",
      " 1.03747599 1.03630973 1.03341527 1.03833139 1.03510868 1.03502287\n",
      " 1.03703125 1.03789695 1.03558891 1.03622115 1.03838847 1.03557879\n",
      " 1.04113044 1.03439294 1.03544012 1.03643775 1.03551552 1.04035454\n",
      " 1.04506184 1.03771106 1.03269357 1.03843864 1.0342808  1.03753145\n",
      " 1.0334914  1.03411151 1.03307399 1.03669088 1.03480095 1.04102141\n",
      " 1.03734349 1.04036272 1.04214617 1.04082855 1.03625696 1.03655034\n",
      " 1.03622745 1.03832532 1.04050175 1.03778292 1.04155393 1.03719133\n",
      " 1.0372827  1.03358741 1.02985937 1.03874612 1.03546199 1.03639\n",
      " 1.03608317 1.03729146 1.03930565 1.03402375 1.04036481 1.04083133\n",
      " 1.03267736 1.03352127 1.04169934 1.04669172 1.0354626  1.03955839\n",
      " 1.03508735 1.03963375 1.03609915 1.03509161 1.04542814 1.03696504\n",
      " 1.03311594 1.0394691  1.0412838  1.03371289 1.04385175 1.03826556\n",
      " 1.04113101 1.03334448 1.03725189 1.03992064 1.0406425  1.04155056\n",
      " 1.03591764 1.0354156  1.03657124 1.03983489 1.03685848 1.03544075\n",
      " 1.03927145 1.04168011] [0.01130314 0.00955578 0.00786524 0.01243656 0.01368981 0.01404992\n",
      " 0.01329548 0.00886874 0.00934868 0.01328569 0.01254676 0.0125319\n",
      " 0.01214424 0.0130536  0.01614638 0.01204766 0.01357213 0.01471627\n",
      " 0.01030151 0.01217607 0.01171602 0.012288   0.00982404 0.01505107\n",
      " 0.01458222 0.01642814 0.01118749 0.01371319 0.0127354  0.01476996\n",
      " 0.01828788 0.00862482 0.01198744 0.01013756 0.00900676 0.01041332\n",
      " 0.01388209 0.01042461 0.0112396  0.01186079 0.01190665 0.0120258\n",
      " 0.01386593 0.01145723 0.02103465 0.00943696 0.01156792 0.01146101\n",
      " 0.01169689 0.0142651  0.01457899 0.01423548 0.01130053 0.01092954\n",
      " 0.01233806 0.01664571 0.00989963 0.01412808 0.01172223 0.01197944\n",
      " 0.01300559 0.01058024 0.01569047 0.0137633  0.01260736 0.01318024\n",
      " 0.02015308 0.01086766 0.00989952 0.00965893 0.01376846 0.01428649\n",
      " 0.01132755 0.01434414 0.01258277 0.01475752 0.014786   0.01097975\n",
      " 0.01349855 0.00973664 0.01171131 0.00777888 0.01328118 0.01089678\n",
      " 0.01296002 0.01174496 0.01169887 0.01152142 0.00775158 0.01522639\n",
      " 0.01065247 0.01341494 0.01122996 0.01391623 0.01605696 0.01142016\n",
      " 0.0141482  0.01536612 0.00970305 0.00625817 0.01168943 0.01513798\n",
      " 0.0183904  0.00679895 0.01530077 0.01351155 0.01105411 0.01102099\n",
      " 0.007698   0.00959137 0.0093866  0.01504106 0.01614979 0.01666082\n",
      " 0.01262449 0.01232065 0.01587734 0.01106085 0.0124182  0.01553942\n",
      " 0.01771975 0.01512136 0.01381918 0.0095049  0.01089025 0.01799721\n",
      " 0.01265775 0.01192589 0.01223069 0.01444273 0.01587343 0.01076068\n",
      " 0.01547816 0.01182839 0.01430043 0.01381145 0.01272685 0.01580914\n",
      " 0.01558955 0.01205713 0.00924242 0.01776326 0.01483276 0.01137152\n",
      " 0.01214289 0.01594562 0.01529629 0.01034669 0.01298461 0.01187015\n",
      " 0.00909792 0.0132471  0.01531025 0.01454964 0.01297627 0.01242502\n",
      " 0.01161508 0.01281369 0.01395813 0.01624493 0.01085024 0.01508979\n",
      " 0.01237513 0.01390626 0.01112995 0.01429605 0.01035599 0.00515389\n",
      " 0.01076986 0.01321281 0.0122387  0.00927099 0.01318759 0.01980257\n",
      " 0.0156712  0.01325564 0.01636515 0.01245377 0.01471672 0.01234149\n",
      " 0.01464014 0.01275909 0.0140549  0.01326105 0.01027695 0.01003709\n",
      " 0.01222622 0.00878254 0.01340845 0.01040348 0.00988849 0.01398276\n",
      " 0.01527987 0.011922   0.01526988 0.01135003 0.0079837  0.01189576\n",
      " 0.01628228 0.01483985 0.00684492 0.01215445 0.01369282 0.00962833\n",
      " 0.01303881 0.00780119 0.01074706 0.0158706  0.01045989 0.01433946\n",
      " 0.01353899 0.01217148 0.01000882 0.01313218 0.01250045 0.01370438\n",
      " 0.01345596 0.00966474 0.01733279 0.01698802 0.01129081 0.01339313\n",
      " 0.01447141 0.01255127 0.01079943 0.01954101 0.01271173 0.01102432\n",
      " 0.00848738 0.01392054 0.01257519 0.01319939 0.00980556 0.00715389\n",
      " 0.01249714 0.01299379 0.01071383 0.01335496 0.01384011 0.0161556\n",
      " 0.01070607 0.01270028 0.01836218 0.0107128  0.01202246 0.01283661\n",
      " 0.01079878 0.0104689  0.01478703 0.01020761 0.01177647 0.01532373\n",
      " 0.01381836 0.00746282 0.00754737 0.01384752 0.01290313 0.01097134\n",
      " 0.01581817 0.01033257 0.01607864 0.01598337 0.01319934 0.01432016\n",
      " 0.01387362 0.00957146 0.01069181 0.01162436 0.01446408 0.01636406\n",
      " 0.01533447 0.00957341 0.01465571 0.01311207 0.01179392 0.01063483\n",
      " 0.01095281 0.01361312 0.01546033 0.01193726 0.01538488 0.0171425\n",
      " 0.01350871 0.013631   0.01457389 0.01177049 0.01224912 0.01127814\n",
      " 0.00839032 0.00959694 0.00906709 0.01347303 0.01355996 0.01569869\n",
      " 0.01930206 0.01603255 0.00900319 0.01193946 0.01397236 0.00812915\n",
      " 0.0173936  0.01406149 0.01365098 0.01614941 0.01108993 0.01624792\n",
      " 0.01428391 0.01156878 0.01292264 0.01372225 0.01305096 0.0128711\n",
      " 0.01224834 0.01011076 0.01154341 0.00940664 0.01149296 0.01318888\n",
      " 0.01051467 0.0144234  0.01210693 0.01332208 0.01646801 0.01210113\n",
      " 0.01365188 0.01257914 0.01355284 0.01369554 0.01201917 0.01497636\n",
      " 0.01109691 0.01126987 0.01425513 0.01423756 0.01488122 0.01269861\n",
      " 0.01582233 0.01583286 0.01298993 0.01593567 0.01461169 0.01482702\n",
      " 0.00924669 0.01313258 0.00918335 0.00861311 0.01524231 0.00860254\n",
      " 0.01160443 0.01563497 0.01067316 0.01461856 0.01441633 0.01457431\n",
      " 0.00813664 0.00938841 0.01361026 0.0156256  0.01591474 0.01304908\n",
      " 0.01036439 0.01080118 0.01534597 0.01001803 0.01199427 0.01144378\n",
      " 0.01454684 0.01008805 0.01003947 0.01377031 0.01558567 0.01274238\n",
      " 0.01115115 0.00938491 0.01236201 0.0075615  0.0130237  0.01170316\n",
      " 0.01001636 0.01539407 0.01246118 0.01094058 0.01038416 0.01175295\n",
      " 0.01712707 0.01459787 0.01735804 0.01432395 0.00715719 0.01571736\n",
      " 0.01035068 0.01605579 0.01023956 0.01158302 0.01118085 0.01638423\n",
      " 0.01448983 0.01231832 0.013547   0.01314583 0.01093035 0.01127006\n",
      " 0.01525327 0.01203854 0.01278722 0.01421431 0.01000661 0.01171605\n",
      " 0.01120806 0.01289985 0.01043167 0.01419913 0.014132   0.01581605\n",
      " 0.0090237  0.00773761 0.01238417 0.01283526 0.01251419 0.01404927\n",
      " 0.01513414 0.00516335 0.01414235 0.01518515 0.00992301 0.01672233\n",
      " 0.01330097 0.00875765 0.00989483 0.01129422 0.01163927 0.01875469\n",
      " 0.01693274 0.01114007 0.01457366 0.01113051 0.01049696 0.0126013\n",
      " 0.01348987 0.01229724 0.01511511 0.01664732 0.01171232 0.01163502\n",
      " 0.01097243 0.00380753 0.01600847 0.01527763 0.01382864 0.01225265\n",
      " 0.01782488 0.01254262 0.00840689 0.01103284 0.01295576 0.0161634\n",
      " 0.01604167 0.01345456 0.0118804  0.01632094 0.01419498 0.01187006\n",
      " 0.01139346 0.01241985 0.01681716 0.01598407 0.01129068 0.01233035\n",
      " 0.00992675 0.01626524 0.01723943 0.01326116 0.0121783  0.01433252\n",
      " 0.01006044 0.01409191 0.01526259 0.01318281 0.00483498 0.00857045\n",
      " 0.01205976 0.01213705 0.01089523 0.01457033 0.01642652 0.01774605\n",
      " 0.01437962 0.01104706 0.0071515  0.01199319 0.00875483 0.01191821\n",
      " 0.01393567 0.01392516 0.01520624 0.01402331 0.01625143 0.01368701\n",
      " 0.0123502  0.01746885] [-18948.44493458 -18905.37558987 -18839.23396813 -18930.30589705\n",
      " -19103.64239246 -18893.95290058 -18985.52461377 -19061.71679029\n",
      " -19091.8674148  -18971.14522708 -19074.12726493 -18970.63567119\n",
      " -18870.08211447 -18886.77566043 -18976.86056944 -18912.00223026\n",
      " -19013.53844349 -19057.86121884 -18894.50185927 -18968.28633372\n",
      " -18782.96613759 -18957.07276116 -18931.78309981 -18923.38172682\n",
      " -18939.81429809 -19105.03664431 -19020.68691936 -18932.12638804\n",
      " -19023.50803076 -18832.0798865  -18851.29659443 -19092.83811237\n",
      " -19022.54440008 -18876.82448473 -19085.58586591 -18921.86154428\n",
      " -18881.52733588 -18885.1721672  -18994.58902297 -18857.83781343\n",
      " -19101.35515578 -18916.2653695  -18883.90439232 -18962.41637777\n",
      " -19095.7628298  -18814.55145997 -18884.0242953  -19028.36904945\n",
      " -18953.52717151 -18834.2350881  -18971.36163123 -18918.45374695\n",
      " -18960.47368269 -18860.51311555 -18862.37976841 -18894.78288411\n",
      " -18979.65231174 -19046.85648217 -18908.61295186 -19033.25884055\n",
      " -18984.64605608 -18769.4659815  -18953.22256784 -18909.72947242\n",
      " -19111.03637317 -18875.16921018 -18996.15486254 -18877.75141359\n",
      " -18947.42965635 -19044.63038711 -18690.91976096 -19035.53053649\n",
      " -18910.54177164 -18992.96655694 -19154.64247231 -18873.15554653\n",
      " -18861.34240036 -19093.35899159 -19083.40590028 -18990.67684476\n",
      " -18844.67129145 -18994.64517678 -18892.43984029 -18993.52253511\n",
      " -19001.98260757 -18999.01994457 -18912.75869996 -18971.22819314\n",
      " -19059.58953392 -18862.57351289 -18944.5689577  -18983.91788757\n",
      " -19108.75533964 -18975.73062378 -18977.30682893 -19031.99692003\n",
      " -19033.76214609 -18807.59894577 -18850.79250327 -18915.15798658\n",
      " -19006.31866948 -19002.80040646 -19019.88999522 -18783.13195674\n",
      " -18996.18652109 -19059.2637929  -18827.57940449 -19016.84919189\n",
      " -18912.30886505 -18944.85349413 -19070.24043631 -18987.32647916\n",
      " -18943.6605897  -18973.40155977 -18924.28255132 -18882.88648741\n",
      " -18939.34754762 -19062.71307594 -19102.04731489 -18804.97131068\n",
      " -19059.37000806 -18958.2723219  -18930.99999224 -18984.44023045\n",
      " -18917.59442861 -19034.95185582 -19007.91250898 -18939.42775608\n",
      " -19092.9742825  -18972.2903349  -18986.28414543 -18954.45207055\n",
      " -18982.23708993 -18901.339144   -18912.97660816 -18915.19190445\n",
      " -18818.70765453 -18879.15010743 -18864.16862901 -19015.227268\n",
      " -18893.32295176 -19049.40147318 -18931.19502934 -18971.07909424\n",
      " -19047.46600345 -18979.55466612 -19036.65552811 -18955.24978422\n",
      " -18971.96060106 -18963.0776302  -18946.90630312 -18979.08258653\n",
      " -18907.71756439 -19033.58970945 -18974.47575016 -18887.42358612\n",
      " -18981.86430745 -18954.89019385 -18946.27636871 -19017.16383363\n",
      " -18903.35127882 -18977.49407523 -18883.84788733 -18896.78114602\n",
      " -18962.93587881 -19012.31148895 -18917.67782349 -18903.37110232\n",
      " -19023.09846696 -18940.72363814 -18868.79745869 -18896.52886946\n",
      " -18849.09473082 -19012.45604787 -18972.92781415 -18919.6374972\n",
      " -18991.18178493 -18960.78024863 -19046.16857445 -19035.11274546\n",
      " -19080.46243559 -19036.16986139 -18933.19242525 -18977.06196345\n",
      " -19100.37195561 -18993.71296925 -18838.697584   -18942.95263414\n",
      " -19056.83260171 -18927.92743113 -19043.82317191 -18974.80992487\n",
      " -18874.6231138  -18890.76439773 -18954.20183189 -18955.18230257\n",
      " -18937.93294457 -18829.54529576 -18928.52013573 -18962.31419054\n",
      " -18924.38009074 -18825.43396739 -18914.07311021 -18959.46106293\n",
      " -18995.68724196 -18965.1274144  -18986.7471327  -18969.59563441\n",
      " -18847.08914005 -18957.89219173 -18986.06195376 -18960.20003199\n",
      " -19013.6067505  -18827.32415915 -19006.82195795 -18902.11026288\n",
      " -18920.5308943  -18893.76395034 -18798.88072632 -19042.67782564\n",
      " -18954.86282566 -18928.44965116 -18936.38540229 -18940.11948221\n",
      " -18968.17629039 -19162.69441545 -18743.8273272  -18767.39724862\n",
      " -19009.67858573 -18889.57834668 -19057.16754171 -18901.78281492\n",
      " -18839.87292162 -18956.1328631  -18885.04390243 -19061.12415091\n",
      " -18859.24663116 -18989.10411391 -18964.70737679 -18982.3783292\n",
      " -19052.95584439 -18966.02617284 -19028.4678601  -19017.90776354\n",
      " -18918.37440653 -19070.43902812 -18921.28513535 -18974.72975631\n",
      " -19030.03232652 -18995.08245076 -18914.2935802  -19038.82990795\n",
      " -18827.23076307 -18961.15242995 -18873.02411139 -18825.35450205\n",
      " -18996.97143223 -18966.78817316 -18977.86446324 -18883.36963602\n",
      " -18887.79117631 -18882.63858403 -18897.11154787 -19035.19294338\n",
      " -18874.58061951 -18916.64589952 -18923.3756363  -18990.12041845\n",
      " -19072.04784747 -18836.19545851 -19071.42919729 -19008.30572593\n",
      " -18985.20889588 -19107.08200575 -18977.9373286  -18990.72465253\n",
      " -18890.20568138 -18803.19896041 -18889.90224147 -19009.88310169\n",
      " -18971.4370235  -18992.73124244 -19038.10886098 -18898.35648759\n",
      " -19019.37690875 -18985.46005102 -18956.75266857 -19002.35107428\n",
      " -18898.84993244 -18974.39494807 -18789.72987684 -18909.65277086\n",
      " -18954.31803592 -19035.81936732 -19012.31795584 -18897.73322503\n",
      " -18993.64088247 -19063.54505334 -19002.20359599 -19071.02634519\n",
      " -18972.66384155 -18892.45543383 -19070.30453959 -18977.53518473\n",
      " -19062.15387217 -18898.43503656 -18837.35205317 -19085.81555635\n",
      " -18844.35789205 -18998.13876233 -18990.31645947 -19071.04357529\n",
      " -19024.44837379 -19015.71497088 -18925.14014171 -18957.75870971\n",
      " -18791.22850091 -18920.68265502 -19035.63230935 -18984.34740513\n",
      " -18758.85179938 -18877.43676845 -19070.06861822 -19110.89820809\n",
      " -19046.15352787 -18897.04136063 -19074.56825065 -18970.29860933\n",
      " -19007.11199044 -19053.34880005 -18976.38606321 -18958.97065271\n",
      " -18932.37701289 -18863.13084622 -18922.39159109 -18917.59760861\n",
      " -18876.10614518 -18856.99642019 -19054.48991586 -18999.64591651\n",
      " -18942.70586157 -18896.56718739 -19108.04521369 -18960.2820865\n",
      " -19087.40304067 -18991.73392959 -18871.41284277 -19022.77368859\n",
      " -18967.53169624 -19047.91961972 -18997.83619338 -18937.69068138\n",
      " -19057.8064179  -19095.29467812 -19000.89843704 -18959.33380046\n",
      " -19008.07805832 -19033.65814466 -19001.84037251 -19012.12278601\n",
      " -18913.94745272 -19126.71568126 -18873.82329562 -18932.00618866\n",
      " -18886.18021813 -18893.15293308 -18932.00604794 -19070.46700178\n",
      " -19024.41612008 -18945.72286696 -19021.99791777 -18962.06012278\n",
      " -18950.76404614 -18996.74511058 -18975.37343634 -18775.89169621\n",
      " -18884.19024525 -19011.79941747 -18824.08479729 -18873.94071584\n",
      " -18956.02462603 -19001.53693091 -18886.04011959 -18929.90500429\n",
      " -18942.30380184 -18864.78130436 -18953.35526611 -18942.5309426\n",
      " -18960.1745094  -18911.58016203 -18958.45599783 -18871.55431244\n",
      " -18975.63319221 -19137.14121657 -18913.51108499 -18984.84868598\n",
      " -18943.73720162 -18897.88651308 -18908.36484661 -18925.23149771\n",
      " -18985.56909668 -19059.06976791 -18828.63957316 -18977.6823648\n",
      " -19016.75602154 -18898.85844897 -19007.07167803 -18853.65732916\n",
      " -18917.46880202 -19025.77646689 -18826.99645549 -18934.5891686\n",
      " -19012.84089762 -19044.57923728 -18903.30850496 -19033.13000996\n",
      " -18922.65566611 -18846.05153013 -18934.0443838  -18947.28589633\n",
      " -19071.83354444 -19068.5889326  -18966.25860627 -18950.43012917\n",
      " -18922.92140212 -18953.56854671 -19065.0609551  -18869.54480562\n",
      " -18950.48161396 -19021.82668251 -18919.88632451 -18922.40679543\n",
      " -19021.18475816 -19004.91590565 -18936.84814619 -18956.5656592\n",
      " -18903.47363771 -19010.93279193 -18984.42464038 -18924.15695669\n",
      " -18917.02957866 -18929.99929598 -18982.99505448 -18991.21442835\n",
      " -18944.13735486 -18868.71235279 -19089.82934408 -18828.57403328\n",
      " -18918.55106219 -19084.59378442 -18967.05686254 -18817.5262877\n",
      " -19093.26499002 -19094.22130244 -18780.17785676 -18918.8910446\n",
      " -18922.89477079 -18965.34226202 -18992.57847695 -18973.49291961\n",
      " -18958.86063043 -18960.58463141 -18832.17467316 -18980.69100235\n",
      " -18766.97177785 -18880.9142379  -18918.37048099 -19049.11898216\n",
      " -18922.72969589 -19060.79918124 -19039.83259471 -18946.95675548\n",
      " -19007.14987025 -18944.0521071  -18866.98690075 -18859.989826\n",
      " -19098.4825079  -18991.15888546 -18923.68012922 -18929.53821213\n",
      " -18895.23396874 -19001.78163811 -18838.68025435 -19027.52872354\n",
      " -18896.97264517 -19059.57294135 -18916.08627707 -18937.11995927\n",
      " -18937.96550255 -18917.47070811 -18941.84181331 -18885.07102904\n",
      " -19003.41071483 -18906.82981262 -18886.46531248 -18848.37941194\n",
      " -18867.79015903 -18982.78899405 -19070.90209493 -18852.26210145] [299.12518384 298.89761941 304.57337261 298.10313106 305.67777402\n",
      " 312.12079325 302.57563653 306.03416659 297.86100013 305.83208783\n",
      " 304.99267292 309.34720311 305.17862115 306.42502672 306.32115772\n",
      " 303.89945131 306.78589479 307.28407571 310.79445592 310.8254986\n",
      " 301.78022367 309.84764702 311.17905991 297.20109256 296.75445089\n",
      " 316.02251365 304.39687767 308.9910744  315.65411362 303.20870957\n",
      " 307.2181996  307.32383099 310.31251196 304.64594212 305.72849583\n",
      " 304.01986539 320.48679853 305.1841334  309.1134782  306.74004878\n",
      " 307.78377772 308.73264654 314.97130068 308.68395154 311.47892215\n",
      " 294.59530811 307.26080739 307.55203784 302.94357077 308.62335263\n",
      " 306.71252302 299.63129984 307.10658537 301.01673651 309.55372141\n",
      " 307.92308074 313.03677431 304.14426496 301.55157282 305.12592833\n",
      " 299.83982455 310.55146136 303.42940933 303.39138487 307.04965468\n",
      " 309.71044637 306.77232778 305.21030432 314.91544527 306.56158147\n",
      " 312.80613691 308.23991894 305.49855302 311.66957189 307.36202644\n",
      " 310.79099616 312.71670909 313.37144576 302.80897799 309.02942757\n",
      " 299.36466598 301.54647057 306.79699436 311.67725572 304.79999821\n",
      " 307.98706403 303.12213748 298.57564536 308.02407729 303.277522\n",
      " 306.20716423 303.45438376 312.20047218 311.66878523 305.9324806\n",
      " 310.55588955 306.71148347 305.1516098  307.55282101 301.00907727\n",
      " 304.62638237 307.11250901 303.80981424 300.2383064  314.79532175\n",
      " 302.52615072 304.15290125 304.14899874 316.96940206 309.3453379\n",
      " 309.02730509 305.94024046 301.99897026 301.79590216 309.37134807\n",
      " 308.00207042 301.37760935 314.44758892 303.30542649 308.51566692\n",
      " 311.24265467 313.27220629 300.17844905 312.6901215  303.92374151\n",
      " 302.62643659 302.85824565 312.54466981 309.50791572 303.19402715\n",
      " 315.43214925 296.72015646 298.93414993 303.77483204 300.26073986\n",
      " 311.03752014 310.57305072 304.87558903 307.42936059 309.70431387\n",
      " 299.61538262 314.4149643  299.55546875 315.40748264 302.69279747\n",
      " 299.30387491 303.49009948 308.80571546 298.39733455 303.04779356\n",
      " 317.24275217 301.33527675 303.37122393 307.84719249 317.22019964\n",
      " 296.42384786 293.63347987 300.04204059 304.36124159 310.32932323\n",
      " 297.60252913 305.44062553 298.55913597 311.35482783 305.32561821\n",
      " 301.08679479 310.21224373 294.36829264 301.92058129 302.16671421\n",
      " 298.86883302 303.94286378 301.81839014 302.13783268 303.29426272\n",
      " 302.34510165 310.38011657 309.65829144 304.71946695 305.02675834\n",
      " 303.01627136 310.18827228 303.89485091 309.41964062 295.04504819\n",
      " 310.88653396 306.90876828 299.04147325 305.68785666 309.46225904\n",
      " 304.64263892 300.63427353 296.65513707 311.50285665 318.08296246\n",
      " 307.03121404 308.4451416  299.57383254 302.10590964 310.61968214\n",
      " 305.03733689 302.98914766 315.69413257 307.25250851 309.67707368\n",
      " 313.6053001  307.39987355 297.96928126 317.57129392 302.32467129\n",
      " 298.2137839  303.9189968  299.78211817 312.64787215 301.14478979\n",
      " 305.69869373 304.7846738  307.74200483 307.44018673 313.33104458\n",
      " 302.19842778 299.53626327 312.43623346 305.10214673 304.73309231\n",
      " 307.45773706 303.08494753 306.10284001 307.1691866  308.03123483\n",
      " 308.52211892 306.22160056 307.3779541  302.52898248 304.21575494\n",
      " 303.53009194 307.03455085 295.92626155 299.29043621 303.23766516\n",
      " 309.49517648 307.64970113 311.92051128 304.36199344 302.81664393\n",
      " 310.08337171 301.67748705 309.86029871 306.92659151 313.4334399\n",
      " 305.14044566 298.91157009 300.29800161 300.19771602 300.15816802\n",
      " 312.06468943 302.66762304 307.34318705 307.67872551 302.65387541\n",
      " 303.02138895 306.86304425 309.13881559 304.07864089 293.97354241\n",
      " 304.54954146 304.39840446 304.47490209 306.21916092 310.14716513\n",
      " 298.61851854 303.48151685 308.60039771 307.12072534 304.59532965\n",
      " 309.01704601 295.14319513 297.9371531  303.03030538 310.61033206\n",
      " 318.47167129 304.88030331 310.14932661 306.84899977 308.18106344\n",
      " 296.34074064 307.54204571 314.25456065 308.92726454 305.83310098\n",
      " 309.14456985 305.38667595 308.20946556 304.83406863 302.56293944\n",
      " 305.59917055 301.91138232 308.63599458 309.75948625 302.39307303\n",
      " 299.10395909 297.05389993 303.41476995 302.1914423  301.56256799\n",
      " 305.3879825  307.83104818 306.09916762 309.48041467 307.10282289\n",
      " 311.21694884 309.58532643 304.58971571 304.21206584 304.83752298\n",
      " 304.14129094 299.42126337 300.1628485  308.21222204 302.15932803\n",
      " 307.86665629 298.06945117 306.77660773 312.7859916  303.97535696\n",
      " 312.64917566 314.78466715 304.78421984 305.21995894 304.51307268\n",
      " 307.24427119 306.63434329 304.93848189 314.64720511 308.11864345\n",
      " 306.59278358 304.07932891 305.55330361 297.16188977 309.70915101\n",
      " 314.00517624 308.35129637 304.55085405 296.44199785 303.87052279\n",
      " 301.80730175 313.14873987 307.25829684 311.36447818 318.58008839\n",
      " 301.5120017  311.94809309 306.69206463 304.09962611 306.108733\n",
      " 305.71341255 303.94587135 307.90936003 308.21056083 302.03183583\n",
      " 307.79239399 307.8100932  308.8638664  304.95912434 303.50284603\n",
      " 305.76550796 303.81964014 306.88502515 304.99860771 301.03877379\n",
      " 308.78424503 311.02175377 293.52104    301.40241911 299.9301454\n",
      " 298.64740672 310.67032609 300.53761852 315.75573838 307.414168\n",
      " 308.01691912 308.32035796 312.35326407 306.63280319 311.2275186\n",
      " 310.97950214 306.85932846 296.6381401  302.58540094 309.55514914\n",
      " 310.97216881 309.4842285  312.52730703 302.38228737 292.55128002\n",
      " 309.20930266 310.82296292 311.40570227 310.36409096 315.50977464\n",
      " 314.62685163 311.62875811 305.18180313 308.1447011  297.1310187\n",
      " 304.89836832 313.23696332 295.23614255 311.0527519  306.7432626\n",
      " 298.12528404 308.62961548 305.73058089 299.19733762 306.09522999\n",
      " 301.04130828 303.64245497 308.76799697 312.28980026 309.10599537\n",
      " 310.37217077 309.87441626 307.91077447 307.96120674 299.13935417\n",
      " 300.97707056 304.5943512  299.1193528  302.5076236  300.10302127\n",
      " 313.16428775 310.2228154  310.92177202 302.05566806 310.36078793\n",
      " 301.19731397 306.29253454 311.35163265 309.61956328 297.72409349\n",
      " 307.87149173 311.42063871 309.82847396 304.89517449 313.90564767\n",
      " 308.92512954 301.61444887 313.78334513 311.27363096 308.65861643\n",
      " 301.18551823 303.34789714 297.26731335 304.33904165 311.84181297\n",
      " 297.55818092 309.31373934 309.17471785 306.28780459 306.72305235\n",
      " 300.76845246 307.04924791 310.17868026 304.05965946 316.82514451\n",
      " 302.43356499 309.76798347 301.64030235 305.22593478 307.35654213\n",
      " 310.7984588  307.66195516 311.06139221 312.23068556 309.95241886\n",
      " 303.68255019 310.87756475 296.91982892 305.16505106 309.389833\n",
      " 303.86683272 289.73465108 299.26048723 304.59403508 310.8195322\n",
      " 300.68389238 314.57389832 310.89848903 313.85967247 307.02235292\n",
      " 300.53673463 306.21067916 296.93595309 311.60960518 301.59543453\n",
      " 314.81420007 314.97283593 306.49740192 306.85757683 309.54855507]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1332x756 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp/e0\n",
      "Area under surface (rectangular approx) =  58.65155947295414\n",
      "Violations =  0.0\n",
      "Average_violations =  -18948.44493457977\n",
      "MSE =  1.040246978564548\n",
      "temp/e1\n",
      "Area under surface (rectangular approx) =  61.9165212491274\n",
      "Violations =  0.0\n",
      "Average_violations =  -18905.375589873594\n",
      "MSE =  1.03996163087596\n",
      "temp/e2\n",
      "Area under surface (rectangular approx) =  66.1116090538109\n",
      "Violations =  0.0\n",
      "Average_violations =  -18839.23396812999\n",
      "MSE =  1.041794294316458\n",
      "temp/e3\n",
      "Area under surface (rectangular approx) =  58.75794111752265\n",
      "Violations =  0.0\n",
      "Average_violations =  -18930.30589704993\n",
      "MSE =  1.0299412499772722\n",
      "temp/e4\n",
      "Area under surface (rectangular approx) =  57.55640403771297\n",
      "Violations =  0.0\n",
      "Average_violations =  -19103.64239245923\n",
      "MSE =  1.0355111129865768\n",
      "temp/e5\n",
      "Area under surface (rectangular approx) =  57.595040368524906\n",
      "Violations =  0.0\n",
      "Average_violations =  -18893.952900578144\n",
      "MSE =  1.0346664572300797\n",
      "temp/e6\n",
      "Area under surface (rectangular approx) =  58.45653295212577\n",
      "Violations =  0.0\n",
      "Average_violations =  -18985.52461376953\n",
      "MSE =  1.0323162452064483\n",
      "temp/e7\n",
      "Area under surface (rectangular approx) =  55.76283797106019\n",
      "Violations =  0.0\n",
      "Average_violations =  -19061.71679028699\n",
      "MSE =  1.0361583809651602\n",
      "temp/e8\n",
      "Area under surface (rectangular approx) =  57.3826826445196\n",
      "Violations =  0.0\n",
      "Average_violations =  -19091.86741480302\n",
      "MSE =  1.039725780932961\n",
      "temp/e9\n",
      "Area under surface (rectangular approx) =  61.18035566510309\n",
      "Violations =  0.0\n",
      "Average_violations =  -18971.145227078512\n",
      "MSE =  1.040496047770921\n",
      "temp/e10\n",
      "Area under surface (rectangular approx) =  56.78687768784366\n",
      "Violations =  0.0\n",
      "Average_violations =  -19074.127264932555\n",
      "MSE =  1.04319172345738\n",
      "temp/e11\n",
      "Area under surface (rectangular approx) =  58.283640045964255\n",
      "Violations =  0.0\n",
      "Average_violations =  -18970.635671192595\n",
      "MSE =  1.0342726660386057\n",
      "temp/e12\n",
      "Area under surface (rectangular approx) =  61.51197336221854\n",
      "Violations =  0.0\n",
      "Average_violations =  -18870.08211446923\n",
      "MSE =  1.0385718805350597\n",
      "temp/e13\n",
      "Area under surface (rectangular approx) =  56.312638208527574\n",
      "Violations =  0.0\n",
      "Average_violations =  -18886.775660427695\n",
      "MSE =  1.042160125079373\n",
      "temp/e14\n",
      "Area under surface (rectangular approx) =  55.0033471575333\n",
      "Violations =  0.0\n",
      "Average_violations =  -18976.860569439894\n",
      "MSE =  1.0325708205085087\n",
      "temp/e15\n",
      "Area under surface (rectangular approx) =  58.03758396267371\n",
      "Violations =  0.0\n",
      "Average_violations =  -18912.00223025764\n",
      "MSE =  1.0365410958863912\n",
      "temp/e16\n",
      "Area under surface (rectangular approx) =  58.558098306204826\n",
      "Violations =  0.0\n",
      "Average_violations =  -19013.53844348671\n",
      "MSE =  1.0399825688937818\n",
      "temp/e17\n",
      "Area under surface (rectangular approx) =  54.87887361432316\n",
      "Violations =  0.0\n",
      "Average_violations =  -19057.861218841117\n",
      "MSE =  1.0369443798328448\n",
      "temp/e18\n",
      "Area under surface (rectangular approx) =  59.57372579995367\n",
      "Violations =  0.0\n",
      "Average_violations =  -18894.501859266893\n",
      "MSE =  1.0374956875934764\n",
      "temp/e19\n",
      "Area under surface (rectangular approx) =  57.628286036057204\n",
      "Violations =  0.0\n",
      "Average_violations =  -18968.286333723034\n",
      "MSE =  1.0318002143270195\n",
      "temp/e20\n",
      "Area under surface (rectangular approx) =  61.9239647275781\n",
      "Violations =  0.0\n",
      "Average_violations =  -18782.966137593314\n",
      "MSE =  1.0368122877414125\n",
      "temp/e21\n",
      "Area under surface (rectangular approx) =  60.84185311976572\n",
      "Violations =  0.0\n",
      "Average_violations =  -18957.072761155665\n",
      "MSE =  1.035936324435989\n",
      "temp/e22\n",
      "Area under surface (rectangular approx) =  59.83515861554122\n",
      "Violations =  0.0\n",
      "Average_violations =  -18931.783099809072\n",
      "MSE =  1.0411053878331564\n",
      "temp/e23\n",
      "Area under surface (rectangular approx) =  56.97983353508144\n",
      "Violations =  0.0\n",
      "Average_violations =  -18923.381726822376\n",
      "MSE =  1.0333765972772022\n",
      "temp/e24\n",
      "Area under surface (rectangular approx) =  58.69347830736977\n",
      "Violations =  0.0\n",
      "Average_violations =  -18939.814298087458\n",
      "MSE =  1.0363794449835206\n",
      "temp/e25\n",
      "Area under surface (rectangular approx) =  56.04534277622356\n",
      "Violations =  0.0\n",
      "Average_violations =  -19105.03664430968\n",
      "MSE =  1.0379296117752248\n",
      "temp/e26\n",
      "Area under surface (rectangular approx) =  57.93580374611571\n",
      "Violations =  0.0\n",
      "Average_violations =  -19020.686919356103\n",
      "MSE =  1.0345534932056215\n",
      "temp/e27\n",
      "Area under surface (rectangular approx) =  60.95227816317229\n",
      "Violations =  0.0\n",
      "Average_violations =  -18932.126388042445\n",
      "MSE =  1.0374544305738862\n",
      "temp/e28\n",
      "Area under surface (rectangular approx) =  57.1288068792946\n",
      "Violations =  0.0\n",
      "Average_violations =  -19023.508030755514\n",
      "MSE =  1.0431796858922873\n",
      "temp/e29\n",
      "Area under surface (rectangular approx) =  63.50711339838466\n",
      "Violations =  0.0\n",
      "Average_violations =  -18832.079886495216\n",
      "MSE =  1.0377034817795736\n",
      "temp/e30\n",
      "Area under surface (rectangular approx) =  59.03779950071911\n",
      "Violations =  0.0\n",
      "Average_violations =  -18851.29659443368\n",
      "MSE =  1.0340162933456731\n",
      "temp/e31\n",
      "Area under surface (rectangular approx) =  55.162454224073954\n",
      "Violations =  0.0\n",
      "Average_violations =  -19092.83811236513\n",
      "MSE =  1.0361544427579132\n",
      "temp/e32\n",
      "Area under surface (rectangular approx) =  60.13588106667423\n",
      "Violations =  0.0\n",
      "Average_violations =  -19022.544400082654\n",
      "MSE =  1.0419539523674382\n",
      "temp/e33\n",
      "Area under surface (rectangular approx) =  60.385073104220155\n",
      "Violations =  0.0\n",
      "Average_violations =  -18876.82448472907\n",
      "MSE =  1.0374874803892182\n",
      "temp/e34\n",
      "Area under surface (rectangular approx) =  56.75035352118299\n",
      "Violations =  0.0\n",
      "Average_violations =  -19085.58586590989\n",
      "MSE =  1.0377303658898067\n",
      "temp/e35\n",
      "Area under surface (rectangular approx) =  59.34449429666231\n",
      "Violations =  0.0\n",
      "Average_violations =  -18921.86154427808\n",
      "MSE =  1.040331552213004\n",
      "temp/e36\n",
      "Area under surface (rectangular approx) =  58.10709948116696\n",
      "Violations =  0.0\n",
      "Average_violations =  -18881.52733588212\n",
      "MSE =  1.0335019080277579\n",
      "temp/e37\n",
      "Area under surface (rectangular approx) =  61.186773637865016\n",
      "Violations =  0.0\n",
      "Average_violations =  -18885.172167200086\n",
      "MSE =  1.034547906474169\n",
      "temp/e38\n",
      "Area under surface (rectangular approx) =  55.780429981399074\n",
      "Violations =  0.0\n",
      "Average_violations =  -18994.589022965614\n",
      "MSE =  1.0372197497462838\n",
      "temp/e39\n",
      "Area under surface (rectangular approx) =  61.16264785802511\n",
      "Violations =  0.0\n",
      "Average_violations =  -18857.837813427654\n",
      "MSE =  1.0313031080054873\n",
      "temp/e40\n",
      "Area under surface (rectangular approx) =  55.48673695666096\n",
      "Violations =  0.0\n",
      "Average_violations =  -19101.355155777193\n",
      "MSE =  1.0368845172468726\n",
      "temp/e41\n",
      "Area under surface (rectangular approx) =  63.58863817375408\n",
      "Violations =  0.0\n",
      "Average_violations =  -18916.26536949731\n",
      "MSE =  1.043572973531276\n",
      "temp/e42\n",
      "Area under surface (rectangular approx) =  63.0383246648822\n",
      "Violations =  0.0\n",
      "Average_violations =  -18883.904392318236\n",
      "MSE =  1.0421251069310824\n",
      "temp/e43\n",
      "Area under surface (rectangular approx) =  59.71515898102908\n",
      "Violations =  0.0\n",
      "Average_violations =  -18962.41637777161\n",
      "MSE =  1.034663644664768\n",
      "temp/e44\n",
      "Area under surface (rectangular approx) =  55.98820529551736\n",
      "Violations =  0.0\n",
      "Average_violations =  -19095.762829801366\n",
      "MSE =  1.0348154357661719\n",
      "temp/e45\n",
      "Area under surface (rectangular approx) =  64.65306690262116\n",
      "Violations =  0.0\n",
      "Average_violations =  -18814.551459970833\n",
      "MSE =  1.0426071057582622\n",
      "temp/e46\n",
      "Area under surface (rectangular approx) =  60.05887576024399\n",
      "Violations =  0.0\n",
      "Average_violations =  -18884.02429530309\n",
      "MSE =  1.0344894890529592\n",
      "temp/e47\n",
      "Area under surface (rectangular approx) =  59.34725156901432\n",
      "Violations =  0.0\n",
      "Average_violations =  -19028.369049452267\n",
      "MSE =  1.0427746251617602\n",
      "temp/e48\n",
      "Area under surface (rectangular approx) =  57.160195294738486\n",
      "Violations =  0.0\n",
      "Average_violations =  -18953.52717151444\n",
      "MSE =  1.0338460590986709\n",
      "temp/e49\n",
      "Area under surface (rectangular approx) =  58.63255165285882\n",
      "Violations =  0.0\n",
      "Average_violations =  -18834.23508810068\n",
      "MSE =  1.0434045486089225\n",
      "temp/e50\n",
      "Area under surface (rectangular approx) =  58.3742741013279\n",
      "Violations =  0.0\n",
      "Average_violations =  -18971.361631232925\n",
      "MSE =  1.030918637184525\n",
      "temp/e51\n",
      "Area under surface (rectangular approx) =  56.719158842909124\n",
      "Violations =  0.0\n",
      "Average_violations =  -18918.453746951887\n",
      "MSE =  1.0363732946080437\n",
      "temp/e52\n",
      "Area under surface (rectangular approx) =  56.841210242279715\n",
      "Violations =  0.0\n",
      "Average_violations =  -18960.473682687836\n",
      "MSE =  1.0340575016471987\n",
      "temp/e53\n",
      "Area under surface (rectangular approx) =  59.52417339020685\n",
      "Violations =  0.0\n",
      "Average_violations =  -18860.513115553073\n",
      "MSE =  1.0351297814612832\n",
      "temp/e54\n",
      "Area under surface (rectangular approx) =  57.111383074746215\n",
      "Violations =  0.0\n",
      "Average_violations =  -18862.379768414645\n",
      "MSE =  1.0346198779171274\n",
      "temp/e55\n",
      "Area under surface (rectangular approx) =  61.41431138634374\n",
      "Violations =  0.0\n",
      "Average_violations =  -18894.782884106047\n",
      "MSE =  1.0349418459669544\n",
      "temp/e56\n",
      "Area under surface (rectangular approx) =  60.52982156209786\n",
      "Violations =  0.0\n",
      "Average_violations =  -18979.65231174189\n",
      "MSE =  1.0343539454303106\n",
      "temp/e57\n",
      "Area under surface (rectangular approx) =  55.521149989826355\n",
      "Violations =  0.0\n",
      "Average_violations =  -19046.85648217424\n",
      "MSE =  1.043741431162557\n",
      "temp/e58\n",
      "Area under surface (rectangular approx) =  60.16564736921318\n",
      "Violations =  0.0\n",
      "Average_violations =  -18908.61295186458\n",
      "MSE =  1.0340188468489042\n",
      "temp/e59\n",
      "Area under surface (rectangular approx) =  59.709876473050485\n",
      "Violations =  0.0\n",
      "Average_violations =  -19033.258840546216\n",
      "MSE =  1.035450263537281\n",
      "temp/e60\n",
      "Area under surface (rectangular approx) =  60.383193877852456\n",
      "Violations =  0.0\n",
      "Average_violations =  -18984.646056081245\n",
      "MSE =  1.0386692504442756\n",
      "temp/e61\n",
      "Area under surface (rectangular approx) =  63.77180849458889\n",
      "Violations =  0.0\n",
      "Average_violations =  -18769.465981500627\n",
      "MSE =  1.0383725999704994\n",
      "temp/e62\n",
      "Area under surface (rectangular approx) =  56.77911781144594\n",
      "Violations =  0.0\n",
      "Average_violations =  -18953.22256784392\n",
      "MSE =  1.0346696643796276\n",
      "temp/e63\n",
      "Area under surface (rectangular approx) =  61.88992990936099\n",
      "Violations =  0.0\n",
      "Average_violations =  -18909.72947241682\n",
      "MSE =  1.0419751543709705\n",
      "temp/e64\n",
      "Area under surface (rectangular approx) =  55.73861674398693\n",
      "Violations =  0.0\n",
      "Average_violations =  -19111.036373168048\n",
      "MSE =  1.0395039327643327\n",
      "temp/e65\n",
      "Area under surface (rectangular approx) =  57.04823697897267\n",
      "Violations =  0.0\n",
      "Average_violations =  -18875.169210178294\n",
      "MSE =  1.0312476638803916\n",
      "temp/e66\n",
      "Area under surface (rectangular approx) =  56.29211848439316\n",
      "Violations =  0.0\n",
      "Average_violations =  -18996.154862540174\n",
      "MSE =  1.0376761981133489\n",
      "temp/e67\n",
      "Area under surface (rectangular approx) =  57.95842812868064\n",
      "Violations =  0.0\n",
      "Average_violations =  -18877.75141358619\n",
      "MSE =  1.0370549610941653\n",
      "temp/e68\n",
      "Area under surface (rectangular approx) =  60.174411305630166\n",
      "Violations =  0.0\n",
      "Average_violations =  -18947.429656349417\n",
      "MSE =  1.034872938828339\n",
      "temp/e69\n",
      "Area under surface (rectangular approx) =  57.353733512787684\n",
      "Violations =  0.0\n",
      "Average_violations =  -19044.63038710517\n",
      "MSE =  1.0345979137433576\n",
      "temp/e70\n",
      "Area under surface (rectangular approx) =  69.28067430440252\n",
      "Violations =  0.0\n",
      "Average_violations =  -18690.919760964265\n",
      "MSE =  1.0535123344572526\n",
      "temp/e71\n",
      "Area under surface (rectangular approx) =  55.84109640775426\n",
      "Violations =  0.0\n",
      "Average_violations =  -19035.530536488957\n",
      "MSE =  1.034582252694912\n",
      "temp/e72\n",
      "Area under surface (rectangular approx) =  57.8430890740894\n",
      "Violations =  0.0\n",
      "Average_violations =  -18910.541771639422\n",
      "MSE =  1.0343733398380184\n",
      "temp/e73\n",
      "Area under surface (rectangular approx) =  55.741401148043344\n",
      "Violations =  0.0\n",
      "Average_violations =  -18992.96655694273\n",
      "MSE =  1.0333902148426983\n",
      "temp/e74\n",
      "Area under surface (rectangular approx) =  55.12262099667077\n",
      "Violations =  0.0\n",
      "Average_violations =  -19154.642472307693\n",
      "MSE =  1.0388793519020838\n",
      "temp/e75\n",
      "Area under surface (rectangular approx) =  58.63556701621714\n",
      "Violations =  0.0\n",
      "Average_violations =  -18873.155546527873\n",
      "MSE =  1.0378713082949536\n",
      "temp/e76\n",
      "Area under surface (rectangular approx) =  61.4585115105225\n",
      "Violations =  0.0\n",
      "Average_violations =  -18861.342400357564\n",
      "MSE =  1.0384577354114282\n",
      "temp/e77\n",
      "Area under surface (rectangular approx) =  53.05458426416648\n",
      "Violations =  0.0\n",
      "Average_violations =  -19093.358991590696\n",
      "MSE =  1.0403615658975052\n",
      "temp/e78\n",
      "Area under surface (rectangular approx) =  56.72814824321593\n",
      "Violations =  0.0\n",
      "Average_violations =  -19083.405900277412\n",
      "MSE =  1.0339633995050868\n",
      "temp/e79\n",
      "Area under surface (rectangular approx) =  57.5909588550635\n",
      "Violations =  0.0\n",
      "Average_violations =  -18990.676844756334\n",
      "MSE =  1.0311418069213296\n",
      "temp/e80\n",
      "Area under surface (rectangular approx) =  60.49734059357472\n",
      "Violations =  0.0\n",
      "Average_violations =  -18844.67129145281\n",
      "MSE =  1.0356973271059988\n",
      "temp/e81\n",
      "Area under surface (rectangular approx) =  58.391315079911415\n",
      "Violations =  0.0\n",
      "Average_violations =  -18994.645176784234\n",
      "MSE =  1.035573894159812\n",
      "temp/e82\n",
      "Area under surface (rectangular approx) =  58.429302088974666\n",
      "Violations =  0.0\n",
      "Average_violations =  -18892.439840286454\n",
      "MSE =  1.032681231095054\n",
      "temp/e83\n",
      "Area under surface (rectangular approx) =  55.793783369618495\n",
      "Violations =  0.0\n",
      "Average_violations =  -18993.52253511221\n",
      "MSE =  1.0334171828712857\n",
      "temp/e84\n",
      "Area under surface (rectangular approx) =  58.04080303956966\n",
      "Violations =  0.0\n",
      "Average_violations =  -19001.982607573747\n",
      "MSE =  1.0345987292625363\n",
      "temp/e85\n",
      "Area under surface (rectangular approx) =  61.72269340693776\n",
      "Violations =  0.0\n",
      "Average_violations =  -18999.01994457387\n",
      "MSE =  1.0446138648207668\n",
      "temp/e86\n",
      "Area under surface (rectangular approx) =  57.96316512807345\n",
      "Violations =  0.0\n",
      "Average_violations =  -18912.75869995593\n",
      "MSE =  1.036117554199046\n",
      "temp/e87\n",
      "Area under surface (rectangular approx) =  60.187142962225245\n",
      "Violations =  0.0\n",
      "Average_violations =  -18971.22819314124\n",
      "MSE =  1.029545248029518\n",
      "temp/e88\n",
      "Area under surface (rectangular approx) =  56.417239482480156\n",
      "Violations =  0.0\n",
      "Average_violations =  -19059.58953392003\n",
      "MSE =  1.0385729205652683\n",
      "temp/e89\n",
      "Area under surface (rectangular approx) =  59.1119734894352\n",
      "Violations =  0.0\n",
      "Average_violations =  -18862.573512890267\n",
      "MSE =  1.0346506969848277\n",
      "temp/e90\n",
      "Area under surface (rectangular approx) =  61.50202616642463\n",
      "Violations =  0.0\n",
      "Average_violations =  -18944.568957696938\n",
      "MSE =  1.0373302895423575\n",
      "temp/e91\n",
      "Area under surface (rectangular approx) =  57.95516159551754\n",
      "Violations =  0.0\n",
      "Average_violations =  -18983.91788757019\n",
      "MSE =  1.0348847610530627\n",
      "temp/e92\n",
      "Area under surface (rectangular approx) =  56.23917339631353\n",
      "Violations =  0.0\n",
      "Average_violations =  -19108.755339635496\n",
      "MSE =  1.0344699996567792\n",
      "temp/e93\n",
      "Area under surface (rectangular approx) =  58.761609331428865\n",
      "Violations =  0.0\n",
      "Average_violations =  -18975.730623783205\n",
      "MSE =  1.0351438504373227\n",
      "temp/e94\n",
      "Area under surface (rectangular approx) =  58.471344657604135\n",
      "Violations =  0.0\n",
      "Average_violations =  -18977.306828932367\n",
      "MSE =  1.0365144744714587\n",
      "temp/e95\n",
      "Area under surface (rectangular approx) =  55.99058529691214\n",
      "Violations =  0.0\n",
      "Average_violations =  -19031.996920031204\n",
      "MSE =  1.0397202635068905\n",
      "temp/e96\n",
      "Area under surface (rectangular approx) =  61.05479605075947\n",
      "Violations =  0.0\n",
      "Average_violations =  -19033.762146091827\n",
      "MSE =  1.0408723385762788\n",
      "temp/e97\n",
      "Area under surface (rectangular approx) =  61.722224444180526\n",
      "Violations =  0.0\n",
      "Average_violations =  -18807.598945771362\n",
      "MSE =  1.0421346153594544\n",
      "temp/e98\n",
      "Area under surface (rectangular approx) =  63.298666042692766\n",
      "Violations =  0.0\n",
      "Average_violations =  -18850.792503273668\n",
      "MSE =  1.0397365984700773\n",
      "temp/e99\n",
      "Area under surface (rectangular approx) =  57.318059941999294\n",
      "Violations =  0.0\n",
      "Average_violations =  -18915.15798657604\n",
      "MSE =  1.034891354068563\n",
      "temp/e100\n",
      "Area under surface (rectangular approx) =  56.76911050269858\n",
      "Violations =  0.0\n",
      "Average_violations =  -19006.318669477165\n",
      "MSE =  1.0374256035855989\n",
      "temp/e101\n",
      "Area under surface (rectangular approx) =  56.74557036607444\n",
      "Violations =  0.0\n",
      "Average_violations =  -19002.80040645972\n",
      "MSE =  1.0366865378996004\n",
      "temp/e102\n",
      "Area under surface (rectangular approx) =  60.77630414156903\n",
      "Violations =  0.0\n",
      "Average_violations =  -19019.889995215315\n",
      "MSE =  1.0440945267529722\n",
      "temp/e103\n",
      "Area under surface (rectangular approx) =  69.31627637615037\n",
      "Violations =  0.0\n",
      "Average_violations =  -18783.131956744062\n",
      "MSE =  1.0465945542847035\n",
      "temp/e104\n",
      "Area under surface (rectangular approx) =  56.883763697544516\n",
      "Violations =  0.0\n",
      "Average_violations =  -18996.186521085685\n",
      "MSE =  1.0337791887057552\n",
      "temp/e105\n",
      "Area under surface (rectangular approx) =  56.21171236259647\n",
      "Violations =  0.0\n",
      "Average_violations =  -19059.263792895115\n",
      "MSE =  1.0370666864937932\n",
      "temp/e106\n",
      "Area under surface (rectangular approx) =  57.52906569570443\n",
      "Violations =  0.0\n",
      "Average_violations =  -18827.57940449453\n",
      "MSE =  1.0399404525365523\n",
      "temp/e107\n",
      "Area under surface (rectangular approx) =  57.17517015915733\n",
      "Violations =  0.0\n",
      "Average_violations =  -19016.849191890564\n",
      "MSE =  1.0355765376343298\n",
      "temp/e108\n",
      "Area under surface (rectangular approx) =  60.19832532746839\n",
      "Violations =  0.0\n",
      "Average_violations =  -18912.30886505088\n",
      "MSE =  1.0419096583463947\n",
      "temp/e109\n",
      "Area under surface (rectangular approx) =  59.79534565492783\n",
      "Violations =  0.0\n",
      "Average_violations =  -18944.853494129136\n",
      "MSE =  1.0360992562091518\n",
      "temp/e110\n",
      "Area under surface (rectangular approx) =  60.507146018105054\n",
      "Violations =  0.0\n",
      "Average_violations =  -19070.240436308715\n",
      "MSE =  1.0437294238964219\n",
      "temp/e111\n",
      "Area under surface (rectangular approx) =  59.25190064822307\n",
      "Violations =  0.0\n",
      "Average_violations =  -18987.32647916143\n",
      "MSE =  1.037372171816358\n",
      "temp/e112\n",
      "Area under surface (rectangular approx) =  60.00904354742129\n",
      "Violations =  0.0\n",
      "Average_violations =  -18943.66058969545\n",
      "MSE =  1.0353600178160363\n",
      "temp/e113\n",
      "Area under surface (rectangular approx) =  58.74187144470876\n",
      "Violations =  0.0\n",
      "Average_violations =  -18973.401559774942\n",
      "MSE =  1.0333619634218894\n",
      "temp/e114\n",
      "Area under surface (rectangular approx) =  59.396964472570964\n",
      "Violations =  0.0\n",
      "Average_violations =  -18924.28255131923\n",
      "MSE =  1.0339530774347794\n",
      "temp/e115\n",
      "Area under surface (rectangular approx) =  61.748463619904214\n",
      "Violations =  0.0\n",
      "Average_violations =  -18882.88648741064\n",
      "MSE =  1.0323353512035374\n",
      "temp/e116\n",
      "Area under surface (rectangular approx) =  60.21241856337153\n",
      "Violations =  0.0\n",
      "Average_violations =  -18939.347547615624\n",
      "MSE =  1.04579289060518\n",
      "temp/e117\n",
      "Area under surface (rectangular approx) =  56.72389117346319\n",
      "Violations =  0.0\n",
      "Average_violations =  -19062.713075937667\n",
      "MSE =  1.0324206175158985\n",
      "temp/e118\n",
      "Area under surface (rectangular approx) =  56.389806883076766\n",
      "Violations =  0.0\n",
      "Average_violations =  -19102.047314892236\n",
      "MSE =  1.034695984821131\n",
      "temp/e119\n",
      "Area under surface (rectangular approx) =  62.59759983212912\n",
      "Violations =  0.0\n",
      "Average_violations =  -18804.971310680892\n",
      "MSE =  1.0395146780503377\n",
      "temp/e120\n",
      "Area under surface (rectangular approx) =  55.64573167608204\n",
      "Violations =  0.0\n",
      "Average_violations =  -19059.370008055812\n",
      "MSE =  1.0419637264725545\n",
      "temp/e121\n",
      "Area under surface (rectangular approx) =  59.66515121638845\n",
      "Violations =  0.0\n",
      "Average_violations =  -18958.272321903503\n",
      "MSE =  1.0351514008034015\n",
      "temp/e122\n",
      "Area under surface (rectangular approx) =  63.771781441846194\n",
      "Violations =  0.0\n",
      "Average_violations =  -18930.99999223889\n",
      "MSE =  1.0412333934015447\n",
      "temp/e123\n",
      "Area under surface (rectangular approx) =  57.399364972797464\n",
      "Violations =  0.0\n",
      "Average_violations =  -18984.440230450848\n",
      "MSE =  1.0360846863017443\n",
      "temp/e124\n",
      "Area under surface (rectangular approx) =  55.49359647863125\n",
      "Violations =  0.0\n",
      "Average_violations =  -18917.594428611006\n",
      "MSE =  1.0351876355872898\n",
      "temp/e125\n",
      "Area under surface (rectangular approx) =  57.39953404788962\n",
      "Violations =  0.0\n",
      "Average_violations =  -19034.951855817537\n",
      "MSE =  1.0351427854185817\n",
      "temp/e126\n",
      "Area under surface (rectangular approx) =  60.37746115777192\n",
      "Violations =  0.0\n",
      "Average_violations =  -19007.91250897557\n",
      "MSE =  1.037206837055035\n",
      "temp/e127\n",
      "Area under surface (rectangular approx) =  58.83626388480532\n",
      "Violations =  0.0\n",
      "Average_violations =  -18939.42775607909\n",
      "MSE =  1.0345759294518528\n",
      "temp/e128\n",
      "Area under surface (rectangular approx) =  55.89954679027355\n",
      "Violations =  0.0\n",
      "Average_violations =  -19092.974282503863\n",
      "MSE =  1.0422093210587176\n",
      "temp/e129\n",
      "Area under surface (rectangular approx) =  56.209047865444866\n",
      "Violations =  0.0\n",
      "Average_violations =  -18972.290334904046\n",
      "MSE =  1.0345063722020478\n",
      "temp/e130\n",
      "Area under surface (rectangular approx) =  57.680889935006014\n",
      "Violations =  0.0\n",
      "Average_violations =  -18986.28414542893\n",
      "MSE =  1.0362390422811234\n",
      "temp/e131\n",
      "Area under surface (rectangular approx) =  60.98997330764021\n",
      "Violations =  0.0\n",
      "Average_violations =  -18954.452070550487\n",
      "MSE =  1.0374282498320069\n",
      "temp/e132\n",
      "Area under surface (rectangular approx) =  55.26800699673737\n",
      "Violations =  0.0\n",
      "Average_violations =  -18982.237089927024\n",
      "MSE =  1.0356444261017341\n",
      "temp/e133\n",
      "Area under surface (rectangular approx) =  56.214296230676936\n",
      "Violations =  0.0\n",
      "Average_violations =  -18901.339144003403\n",
      "MSE =  1.0301073287677733\n",
      "temp/e134\n",
      "Area under surface (rectangular approx) =  59.06638537767988\n",
      "Violations =  0.0\n",
      "Average_violations =  -18912.97660815739\n",
      "MSE =  1.0319881839243128\n",
      "temp/e135\n",
      "Area under surface (rectangular approx) =  58.80321298128092\n",
      "Violations =  0.0\n",
      "Average_violations =  -18915.191904449057\n",
      "MSE =  1.0339710919549803\n",
      "temp/e136\n",
      "Area under surface (rectangular approx) =  60.712849482522614\n",
      "Violations =  0.0\n",
      "Average_violations =  -18818.707654530903\n",
      "MSE =  1.0356425670684488\n",
      "temp/e137\n",
      "Area under surface (rectangular approx) =  57.89325490576779\n",
      "Violations =  0.0\n",
      "Average_violations =  -18879.15010742736\n",
      "MSE =  1.036610442328661\n",
      "temp/e138\n",
      "Area under surface (rectangular approx) =  59.963242055175144\n",
      "Violations =  0.0\n",
      "Average_violations =  -18864.168629011223\n",
      "MSE =  1.0340755209568708\n",
      "temp/e139\n",
      "Area under surface (rectangular approx) =  59.57584844725524\n",
      "Violations =  0.0\n",
      "Average_violations =  -19015.22726800467\n",
      "MSE =  1.0414668911522367\n",
      "temp/e140\n",
      "Area under surface (rectangular approx) =  59.29057570177008\n",
      "Violations =  0.0\n",
      "Average_violations =  -18893.322951761515\n",
      "MSE =  1.0349334806640151\n",
      "temp/e141\n",
      "Area under surface (rectangular approx) =  55.65378362727975\n",
      "Violations =  0.0\n",
      "Average_violations =  -19049.401473175734\n",
      "MSE =  1.0368091695491952\n",
      "temp/e142\n",
      "Area under surface (rectangular approx) =  61.96308398751485\n",
      "Violations =  0.0\n",
      "Average_violations =  -18931.195029336006\n",
      "MSE =  1.0386721755513917\n",
      "temp/e143\n",
      "Area under surface (rectangular approx) =  57.59865251661189\n",
      "Violations =  0.0\n",
      "Average_violations =  -18971.079094239078\n",
      "MSE =  1.0362678784491233\n",
      "temp/e144\n",
      "Area under surface (rectangular approx) =  54.560956304461534\n",
      "Violations =  0.0\n",
      "Average_violations =  -19047.466003450394\n",
      "MSE =  1.035608561548903\n",
      "temp/e145\n",
      "Area under surface (rectangular approx) =  56.06491311322997\n",
      "Violations =  0.0\n",
      "Average_violations =  -18979.554666115757\n",
      "MSE =  1.0353969966960093\n",
      "temp/e146\n",
      "Area under surface (rectangular approx) =  57.75489116677733\n",
      "Violations =  0.0\n",
      "Average_violations =  -19036.65552810593\n",
      "MSE =  1.0375453432734543\n",
      "temp/e147\n",
      "Area under surface (rectangular approx) =  58.289156420537935\n",
      "Violations =  0.0\n",
      "Average_violations =  -18955.24978422172\n",
      "MSE =  1.037401627957482\n",
      "temp/e148\n",
      "Area under surface (rectangular approx) =  57.34810282850214\n",
      "Violations =  0.0\n",
      "Average_violations =  -18971.960601063063\n",
      "MSE =  1.045910057094011\n",
      "temp/e149\n",
      "Area under surface (rectangular approx) =  61.37889105612044\n",
      "Violations =  0.0\n",
      "Average_violations =  -18963.07763019781\n",
      "MSE =  1.040112437252008\n",
      "temp/e150\n",
      "Area under surface (rectangular approx) =  57.763705534136264\n",
      "Violations =  0.0\n",
      "Average_violations =  -18946.906303122167\n",
      "MSE =  1.033199101905715\n",
      "temp/e151\n",
      "Area under surface (rectangular approx) =  55.61864608846762\n",
      "Violations =  0.0\n",
      "Average_violations =  -18979.08258653176\n",
      "MSE =  1.035689366117581\n",
      "temp/e152\n",
      "Area under surface (rectangular approx) =  56.473142199154395\n",
      "Violations =  0.0\n",
      "Average_violations =  -18907.71756439388\n",
      "MSE =  1.0363917112672507\n",
      "temp/e153\n",
      "Area under surface (rectangular approx) =  56.51490020364242\n",
      "Violations =  0.0\n",
      "Average_violations =  -19033.58970945309\n",
      "MSE =  1.0318712243402588\n",
      "temp/e154\n",
      "Area under surface (rectangular approx) =  60.05904657152966\n",
      "Violations =  0.0\n",
      "Average_violations =  -18974.47575015584\n",
      "MSE =  1.034955943260267\n",
      "temp/e155\n",
      "Area under surface (rectangular approx) =  58.58614544781166\n",
      "Violations =  0.0\n",
      "Average_violations =  -18887.423586118202\n",
      "MSE =  1.037551111147593\n",
      "temp/e156\n",
      "Area under surface (rectangular approx) =  58.31777840891943\n",
      "Violations =  0.0\n",
      "Average_violations =  -18981.864307452906\n",
      "MSE =  1.0366695076524342\n",
      "temp/e157\n",
      "Area under surface (rectangular approx) =  59.155423201727444\n",
      "Violations =  0.0\n",
      "Average_violations =  -18954.890193848878\n",
      "MSE =  1.0354851456071554\n",
      "temp/e158\n",
      "Area under surface (rectangular approx) =  57.793912552350044\n",
      "Violations =  0.0\n",
      "Average_violations =  -18946.276368713443\n",
      "MSE =  1.0352895539682696\n",
      "temp/e159\n",
      "Area under surface (rectangular approx) =  58.78697202239889\n",
      "Violations =  0.0\n",
      "Average_violations =  -19017.163833633917\n",
      "MSE =  1.0382778351125215\n",
      "temp/e160\n",
      "Area under surface (rectangular approx) =  63.10077501502333\n",
      "Violations =  0.0\n",
      "Average_violations =  -18903.351278815906\n",
      "MSE =  1.0379390381492455\n",
      "temp/e161\n",
      "Area under surface (rectangular approx) =  53.18864523671732\n",
      "Violations =  0.0\n",
      "Average_violations =  -18977.494075227063\n",
      "MSE =  1.039639429341683\n",
      "temp/e162\n",
      "Area under surface (rectangular approx) =  59.45322678598525\n",
      "Violations =  0.0\n",
      "Average_violations =  -18883.847887328906\n",
      "MSE =  1.0384736520573843\n",
      "temp/e163\n",
      "Area under surface (rectangular approx) =  59.358435997195606\n",
      "Violations =  0.0\n",
      "Average_violations =  -18896.78114601501\n",
      "MSE =  1.0359747667842503\n",
      "temp/e164\n",
      "Area under surface (rectangular approx) =  68.22038322877061\n",
      "Violations =  0.0\n",
      "Average_violations =  -18962.935878805747\n",
      "MSE =  1.057721318432323\n",
      "temp/e165\n",
      "Area under surface (rectangular approx) =  56.91085758398349\n",
      "Violations =  0.0\n",
      "Average_violations =  -19012.31148895091\n",
      "MSE =  1.0325373700814178\n",
      "temp/e166\n",
      "Area under surface (rectangular approx) =  62.58778764601012\n",
      "Violations =  0.0\n",
      "Average_violations =  -18917.677823494694\n",
      "MSE =  1.0428338791730982\n",
      "temp/e167\n",
      "Area under surface (rectangular approx) =  66.0633923645304\n",
      "Violations =  0.0\n",
      "Average_violations =  -18903.37110231513\n",
      "MSE =  1.0439636002328416\n",
      "temp/e168\n",
      "Area under surface (rectangular approx) =  55.19999512374139\n",
      "Violations =  0.0\n",
      "Average_violations =  -19023.098466957083\n",
      "MSE =  1.0384834853453628\n",
      "temp/e169\n",
      "Area under surface (rectangular approx) =  59.016226341425345\n",
      "Violations =  0.0\n",
      "Average_violations =  -18940.72363813627\n",
      "MSE =  1.0318063135006503\n",
      "temp/e170\n",
      "Area under surface (rectangular approx) =  60.75011108149745\n",
      "Violations =  0.0\n",
      "Average_violations =  -18868.797458690904\n",
      "MSE =  1.041722101409733\n",
      "temp/e171\n",
      "Area under surface (rectangular approx) =  60.633486597072526\n",
      "Violations =  0.0\n",
      "Average_violations =  -18896.52886945792\n",
      "MSE =  1.0367694783087964\n",
      "temp/e172\n",
      "Area under surface (rectangular approx) =  58.35629831995159\n",
      "Violations =  0.0\n",
      "Average_violations =  -18849.09473081772\n",
      "MSE =  1.0384171285105412\n",
      "temp/e173\n",
      "Area under surface (rectangular approx) =  57.416919626219695\n",
      "Violations =  0.0\n",
      "Average_violations =  -19012.4560478718\n",
      "MSE =  1.0418012006683854\n",
      "temp/e174\n",
      "Area under surface (rectangular approx) =  56.35944733373928\n",
      "Violations =  0.0\n",
      "Average_violations =  -18972.92781415415\n",
      "MSE =  1.0334989608140996\n",
      "temp/e175\n",
      "Area under surface (rectangular approx) =  63.273966322343746\n",
      "Violations =  0.0\n",
      "Average_violations =  -18919.63749719515\n",
      "MSE =  1.0443336025671681\n",
      "temp/e176\n",
      "Area under surface (rectangular approx) =  61.21556992449443\n",
      "Violations =  0.0\n",
      "Average_violations =  -18991.181784929242\n",
      "MSE =  1.041860238705698\n",
      "temp/e177\n",
      "Area under surface (rectangular approx) =  57.30079736770432\n",
      "Violations =  0.0\n",
      "Average_violations =  -18960.780248634277\n",
      "MSE =  1.0322153068011237\n",
      "temp/e178\n",
      "Area under surface (rectangular approx) =  57.196166022915286\n",
      "Violations =  0.0\n",
      "Average_violations =  -19046.168574448435\n",
      "MSE =  1.0349105563909717\n",
      "temp/e179\n",
      "Area under surface (rectangular approx) =  55.253243389154335\n",
      "Violations =  0.0\n",
      "Average_violations =  -19035.11274546177\n",
      "MSE =  1.0350747569785947\n",
      "temp/e180\n",
      "Area under surface (rectangular approx) =  60.39863868610176\n",
      "Violations =  0.0\n",
      "Average_violations =  -19080.462435586913\n",
      "MSE =  1.0388617196071457\n",
      "temp/e181\n",
      "Area under surface (rectangular approx) =  56.799922793669765\n",
      "Violations =  0.0\n",
      "Average_violations =  -19036.169861385748\n",
      "MSE =  1.037032134960461\n",
      "temp/e182\n",
      "Area under surface (rectangular approx) =  58.842058221028616\n",
      "Violations =  0.0\n",
      "Average_violations =  -18933.192425248875\n",
      "MSE =  1.0325101490049777\n",
      "temp/e183\n",
      "Area under surface (rectangular approx) =  60.186857836413324\n",
      "Violations =  0.0\n",
      "Average_violations =  -18977.061963452084\n",
      "MSE =  1.0439487418969673\n",
      "temp/e184\n",
      "Area under surface (rectangular approx) =  59.46780651499118\n",
      "Violations =  0.0\n",
      "Average_violations =  -19100.371955613424\n",
      "MSE =  1.0388951472318808\n",
      "temp/e185\n",
      "Area under surface (rectangular approx) =  58.655674925355264\n",
      "Violations =  0.0\n",
      "Average_violations =  -18993.712969249387\n",
      "MSE =  1.0330674362584857\n",
      "temp/e186\n",
      "Area under surface (rectangular approx) =  59.53361980774659\n",
      "Violations =  0.0\n",
      "Average_violations =  -18838.697583999317\n",
      "MSE =  1.0370226425906228\n",
      "temp/e187\n",
      "Area under surface (rectangular approx) =  57.43209410121402\n",
      "Violations =  0.0\n",
      "Average_violations =  -18942.952634139674\n",
      "MSE =  1.0315547003181416\n",
      "temp/e188\n",
      "Area under surface (rectangular approx) =  55.10085418600364\n",
      "Violations =  0.0\n",
      "Average_violations =  -19056.832601708513\n",
      "MSE =  1.0378583418284544\n",
      "temp/e189\n",
      "Area under surface (rectangular approx) =  57.894007313415564\n",
      "Violations =  0.0\n",
      "Average_violations =  -18927.927431133056\n",
      "MSE =  1.031023206276202\n",
      "temp/e190\n",
      "Area under surface (rectangular approx) =  59.81282130156379\n",
      "Violations =  0.0\n",
      "Average_violations =  -19043.823171909342\n",
      "MSE =  1.037490369979623\n",
      "temp/e191\n",
      "Area under surface (rectangular approx) =  60.73346279487913\n",
      "Violations =  0.0\n",
      "Average_violations =  -18974.80992487329\n",
      "MSE =  1.0438528296110046\n",
      "temp/e192\n",
      "Area under surface (rectangular approx) =  60.53371076949\n",
      "Violations =  0.0\n",
      "Average_violations =  -18874.62311379647\n",
      "MSE =  1.03579526690199\n",
      "temp/e193\n",
      "Area under surface (rectangular approx) =  55.85055628422166\n",
      "Violations =  0.0\n",
      "Average_violations =  -18890.76439772546\n",
      "MSE =  1.0375568032986406\n",
      "temp/e194\n",
      "Area under surface (rectangular approx) =  62.97648340726471\n",
      "Violations =  0.0\n",
      "Average_violations =  -18954.201831893744\n",
      "MSE =  1.0394677166256514\n",
      "temp/e195\n",
      "Area under surface (rectangular approx) =  59.24605392214964\n",
      "Violations =  0.0\n",
      "Average_violations =  -18955.18230257404\n",
      "MSE =  1.0358311021535047\n",
      "temp/e196\n",
      "Area under surface (rectangular approx) =  58.161223272569664\n",
      "Violations =  0.0\n",
      "Average_violations =  -18937.932944573597\n",
      "MSE =  1.0399576401680806\n",
      "temp/e197\n",
      "Area under surface (rectangular approx) =  61.371641380944965\n",
      "Violations =  0.0\n",
      "Average_violations =  -18829.545295758406\n",
      "MSE =  1.038394201959925\n",
      "temp/e198\n",
      "Area under surface (rectangular approx) =  60.30891669717724\n",
      "Violations =  0.0\n",
      "Average_violations =  -18928.520135725303\n",
      "MSE =  1.0400617649287454\n",
      "temp/e199\n",
      "Area under surface (rectangular approx) =  56.05725029046029\n",
      "Violations =  0.0\n",
      "Average_violations =  -18962.314190536155\n",
      "MSE =  1.035242123511374\n",
      "temp/e200\n",
      "Area under surface (rectangular approx) =  62.535583257390535\n",
      "Violations =  0.0\n",
      "Average_violations =  -18924.38009073951\n",
      "MSE =  1.0441087897841468\n",
      "temp/e201\n",
      "Area under surface (rectangular approx) =  62.44660663108185\n",
      "Violations =  0.0\n",
      "Average_violations =  -18825.433967386172\n",
      "MSE =  1.0365220621256823\n",
      "temp/e202\n",
      "Area under surface (rectangular approx) =  56.59159041830854\n",
      "Violations =  0.0\n",
      "Average_violations =  -18914.073110213554\n",
      "MSE =  1.0342412480870977\n",
      "temp/e203\n",
      "Area under surface (rectangular approx) =  54.92361065238362\n",
      "Violations =  0.0\n",
      "Average_violations =  -18959.461062930448\n",
      "MSE =  1.0374700756416457\n",
      "temp/e204\n",
      "Area under surface (rectangular approx) =  55.999393967458566\n",
      "Violations =  0.0\n",
      "Average_violations =  -18995.687241956042\n",
      "MSE =  1.0359239332913721\n",
      "temp/e205\n",
      "Area under surface (rectangular approx) =  59.417272695725146\n",
      "Violations =  0.0\n",
      "Average_violations =  -18965.127414395236\n",
      "MSE =  1.0378256728500008\n",
      "temp/e206\n",
      "Area under surface (rectangular approx) =  54.00615993446064\n",
      "Violations =  0.0\n",
      "Average_violations =  -18986.747132697106\n",
      "MSE =  1.0437750605856828\n",
      "temp/e207\n",
      "Area under surface (rectangular approx) =  56.65113141751077\n",
      "Violations =  0.0\n",
      "Average_violations =  -18969.595634405654\n",
      "MSE =  1.0361775790977201\n",
      "temp/e208\n",
      "Area under surface (rectangular approx) =  59.76165200499942\n",
      "Violations =  0.0\n",
      "Average_violations =  -18847.089140048054\n",
      "MSE =  1.0394349579731958\n",
      "temp/e209\n",
      "Area under surface (rectangular approx) =  56.0353312086568\n",
      "Violations =  0.0\n",
      "Average_violations =  -18957.89219173452\n",
      "MSE =  1.042473227625606\n",
      "temp/e210\n",
      "Area under surface (rectangular approx) =  56.3917654188074\n",
      "Violations =  0.0\n",
      "Average_violations =  -18986.061953759217\n",
      "MSE =  1.0343314927961884\n",
      "temp/e211\n",
      "Area under surface (rectangular approx) =  61.0592865938801\n",
      "Violations =  0.0\n",
      "Average_violations =  -18960.200031993572\n",
      "MSE =  1.0376908214837681\n",
      "temp/e212\n",
      "Area under surface (rectangular approx) =  56.52372510320727\n",
      "Violations =  0.0\n",
      "Average_violations =  -19013.606750503073\n",
      "MSE =  1.0364219098837562\n",
      "temp/e213\n",
      "Area under surface (rectangular approx) =  60.000404236099186\n",
      "Violations =  0.0\n",
      "Average_violations =  -18827.32415914615\n",
      "MSE =  1.038323027212932\n",
      "temp/e214\n",
      "Area under surface (rectangular approx) =  54.29100012204049\n",
      "Violations =  0.0\n",
      "Average_violations =  -19006.821957953292\n",
      "MSE =  1.0389258617867132\n",
      "temp/e215\n",
      "Area under surface (rectangular approx) =  57.662213088407\n",
      "Violations =  0.0\n",
      "Average_violations =  -18902.11026287524\n",
      "MSE =  1.0342408003475883\n",
      "temp/e216\n",
      "Area under surface (rectangular approx) =  58.814643043420084\n",
      "Violations =  0.0\n",
      "Average_violations =  -18920.53089429793\n",
      "MSE =  1.03640143196876\n",
      "temp/e217\n",
      "Area under surface (rectangular approx) =  61.719043464099784\n",
      "Violations =  0.0\n",
      "Average_violations =  -18893.763950337794\n",
      "MSE =  1.0391844815677524\n",
      "temp/e218\n",
      "Area under surface (rectangular approx) =  64.62802965821858\n",
      "Violations =  0.0\n",
      "Average_violations =  -18798.88072631998\n",
      "MSE =  1.04096957760167\n",
      "temp/e219\n",
      "Area under surface (rectangular approx) =  55.13886078467114\n",
      "Violations =  0.0\n",
      "Average_violations =  -19042.67782563924\n",
      "MSE =  1.03361154058961\n",
      "temp/e220\n",
      "Area under surface (rectangular approx) =  59.20829450251523\n",
      "Violations =  0.0\n",
      "Average_violations =  -18954.862825658132\n",
      "MSE =  1.0385996934980104\n",
      "temp/e221\n",
      "Area under surface (rectangular approx) =  58.505879971937105\n",
      "Violations =  0.0\n",
      "Average_violations =  -18928.449651159084\n",
      "MSE =  1.0382883767491486\n",
      "temp/e222\n",
      "Area under surface (rectangular approx) =  57.82699464983244\n",
      "Violations =  0.0\n",
      "Average_violations =  -18936.38540228982\n",
      "MSE =  1.0359560648268114\n",
      "temp/e223\n",
      "Area under surface (rectangular approx) =  57.594436733616675\n",
      "Violations =  0.0\n",
      "Average_violations =  -18940.11948220827\n",
      "MSE =  1.0362135017780738\n",
      "temp/e224\n",
      "Area under surface (rectangular approx) =  55.21377552834441\n",
      "Violations =  0.0\n",
      "Average_violations =  -18968.176290387026\n",
      "MSE =  1.039483911452732\n",
      "temp/e225\n",
      "Area under surface (rectangular approx) =  54.81507835054839\n",
      "Violations =  0.0\n",
      "Average_violations =  -19162.694415452144\n",
      "MSE =  1.0422160165420902\n",
      "temp/e226\n",
      "Area under surface (rectangular approx) =  63.168842507591634\n",
      "Violations =  0.0\n",
      "Average_violations =  -18743.82732720002\n",
      "MSE =  1.038672696945643\n",
      "temp/e227\n",
      "Area under surface (rectangular approx) =  60.978392486569504\n",
      "Violations =  0.0\n",
      "Average_violations =  -18767.39724861547\n",
      "MSE =  1.047640702324546\n",
      "temp/e228\n",
      "Area under surface (rectangular approx) =  58.7511635204678\n",
      "Violations =  0.0\n",
      "Average_violations =  -19009.67858572517\n",
      "MSE =  1.033739776918003\n",
      "temp/e229\n",
      "Area under surface (rectangular approx) =  56.3683916148022\n",
      "Violations =  0.0\n",
      "Average_violations =  -18889.57834668057\n",
      "MSE =  1.0384632232719937\n",
      "temp/e230\n",
      "Area under surface (rectangular approx) =  58.08930321768572\n",
      "Violations =  0.0\n",
      "Average_violations =  -19057.167541711497\n",
      "MSE =  1.034745227977425\n",
      "temp/e231\n",
      "Area under surface (rectangular approx) =  59.96483993279523\n",
      "Violations =  0.0\n",
      "Average_violations =  -18901.782814922844\n",
      "MSE =  1.0317302391979954\n",
      "temp/e232\n",
      "Area under surface (rectangular approx) =  60.49895563806695\n",
      "Violations =  0.0\n",
      "Average_violations =  -18839.87292162302\n",
      "MSE =  1.0379192019202819\n",
      "temp/e233\n",
      "Area under surface (rectangular approx) =  59.20936083373196\n",
      "Violations =  0.0\n",
      "Average_violations =  -18956.132863101506\n",
      "MSE =  1.0367959259399326\n",
      "temp/e234\n",
      "Area under surface (rectangular approx) =  60.23276076957264\n",
      "Violations =  0.0\n",
      "Average_violations =  -18885.04390243286\n",
      "MSE =  1.0393962796601663\n",
      "temp/e235\n",
      "Area under surface (rectangular approx) =  54.15647333501347\n",
      "Violations =  0.0\n",
      "Average_violations =  -19061.124150909967\n",
      "MSE =  1.0384404079853984\n",
      "temp/e236\n",
      "Area under surface (rectangular approx) =  58.13118023433017\n",
      "Violations =  0.0\n",
      "Average_violations =  -18859.24663115709\n",
      "MSE =  1.039616172896812\n",
      "temp/e237\n",
      "Area under surface (rectangular approx) =  55.62551545425705\n",
      "Violations =  0.0\n",
      "Average_violations =  -18989.104113910977\n",
      "MSE =  1.042308565834726\n",
      "temp/e238\n",
      "Area under surface (rectangular approx) =  59.25512513965209\n",
      "Violations =  0.0\n",
      "Average_violations =  -18964.707376789112\n",
      "MSE =  1.0409416250721808\n",
      "temp/e239\n",
      "Area under surface (rectangular approx) =  60.08728395101822\n",
      "Violations =  0.0\n",
      "Average_violations =  -18982.37832920459\n",
      "MSE =  1.0375710213399638\n",
      "temp/e240\n",
      "Area under surface (rectangular approx) =  55.29644962003315\n",
      "Violations =  0.0\n",
      "Average_violations =  -19052.955844385808\n",
      "MSE =  1.0346793198776667\n",
      "temp/e241\n",
      "Area under surface (rectangular approx) =  57.94396411798463\n",
      "Violations =  0.0\n",
      "Average_violations =  -18966.026172841775\n",
      "MSE =  1.0369311014184752\n",
      "temp/e242\n",
      "Area under surface (rectangular approx) =  55.43793763885841\n",
      "Violations =  0.0\n",
      "Average_violations =  -19028.467860104654\n",
      "MSE =  1.0385419294867966\n",
      "temp/e243\n",
      "Area under surface (rectangular approx) =  60.741401057746074\n",
      "Violations =  0.0\n",
      "Average_violations =  -19017.90776353919\n",
      "MSE =  1.0361015422806577\n",
      "temp/e244\n",
      "Area under surface (rectangular approx) =  63.76527039757168\n",
      "Violations =  0.0\n",
      "Average_violations =  -18918.374406527833\n",
      "MSE =  1.0437132178278798\n",
      "temp/e245\n",
      "Area under surface (rectangular approx) =  55.30621514625999\n",
      "Violations =  0.0\n",
      "Average_violations =  -19070.43902812169\n",
      "MSE =  1.0378984538465383\n",
      "temp/e246\n",
      "Area under surface (rectangular approx) =  59.30995960871496\n",
      "Violations =  0.0\n",
      "Average_violations =  -18921.28513534715\n",
      "MSE =  1.0379160738338369\n",
      "temp/e247\n",
      "Area under surface (rectangular approx) =  59.174451681240676\n",
      "Violations =  0.0\n",
      "Average_violations =  -18974.729756311182\n",
      "MSE =  1.0372646115394646\n",
      "temp/e248\n",
      "Area under surface (rectangular approx) =  57.75751024975368\n",
      "Violations =  0.0\n",
      "Average_violations =  -19030.032326515386\n",
      "MSE =  1.0339140182855202\n",
      "temp/e249\n",
      "Area under surface (rectangular approx) =  54.774970659124406\n",
      "Violations =  0.0\n",
      "Average_violations =  -18995.082450761274\n",
      "MSE =  1.0351337244018322\n",
      "temp/e250\n",
      "Area under surface (rectangular approx) =  59.609778694454846\n",
      "Violations =  0.0\n",
      "Average_violations =  -18914.293580204318\n",
      "MSE =  1.0328525569033737\n",
      "temp/e251\n",
      "Area under surface (rectangular approx) =  55.92524050639236\n",
      "Violations =  0.0\n",
      "Average_violations =  -19038.829907947693\n",
      "MSE =  1.0331664121505963\n",
      "temp/e252\n",
      "Area under surface (rectangular approx) =  60.57171157253137\n",
      "Violations =  0.0\n",
      "Average_violations =  -18827.23076306984\n",
      "MSE =  1.041405818020387\n",
      "temp/e253\n",
      "Area under surface (rectangular approx) =  57.9274958245706\n",
      "Violations =  0.0\n",
      "Average_violations =  -18961.152429951904\n",
      "MSE =  1.0322902695437575\n",
      "temp/e254\n",
      "Area under surface (rectangular approx) =  61.10377307137239\n",
      "Violations =  0.0\n",
      "Average_violations =  -18873.024111387047\n",
      "MSE =  1.0358917354034562\n",
      "temp/e255\n",
      "Area under surface (rectangular approx) =  59.47503238965602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Violations =  0.0\n",
      "Average_violations =  -18825.35450204778\n",
      "MSE =  1.0320595597695434\n",
      "temp/e256\n",
      "Area under surface (rectangular approx) =  57.31770638398645\n",
      "Violations =  0.0\n",
      "Average_violations =  -18996.971432229027\n",
      "MSE =  1.0350901333219602\n",
      "temp/e257\n",
      "Area under surface (rectangular approx) =  63.32264889523736\n",
      "Violations =  0.0\n",
      "Average_violations =  -18966.78817315948\n",
      "MSE =  1.0428922955455608\n",
      "temp/e258\n",
      "Area under surface (rectangular approx) =  60.53844297222125\n",
      "Violations =  0.0\n",
      "Average_violations =  -18977.864463239035\n",
      "MSE =  1.0390637039018258\n",
      "temp/e259\n",
      "Area under surface (rectangular approx) =  60.27255453865091\n",
      "Violations =  0.0\n",
      "Average_violations =  -18883.369636020838\n",
      "MSE =  1.036635325659843\n",
      "temp/e260\n",
      "Area under surface (rectangular approx) =  59.552971223399595\n",
      "Violations =  0.0\n",
      "Average_violations =  -18887.791176310908\n",
      "MSE =  1.0393964020521347\n",
      "temp/e261\n",
      "Area under surface (rectangular approx) =  62.93061177924591\n",
      "Violations =  0.0\n",
      "Average_violations =  -18882.63858403489\n",
      "MSE =  1.0349009166070684\n",
      "temp/e262\n",
      "Area under surface (rectangular approx) =  58.2771171330735\n",
      "Violations =  0.0\n",
      "Average_violations =  -18897.111547874014\n",
      "MSE =  1.037609241223932\n",
      "temp/e263\n",
      "Area under surface (rectangular approx) =  60.335289827465914\n",
      "Violations =  0.0\n",
      "Average_violations =  -19035.19294337542\n",
      "MSE =  1.0341965241453026\n",
      "temp/e264\n",
      "Area under surface (rectangular approx) =  59.9825324003452\n",
      "Violations =  0.0\n",
      "Average_violations =  -18874.58061950934\n",
      "MSE =  1.042073822248972\n",
      "temp/e265\n",
      "Area under surface (rectangular approx) =  57.722317455873586\n",
      "Violations =  0.0\n",
      "Average_violations =  -18916.645899521216\n",
      "MSE =  1.0370172791816308\n",
      "temp/e266\n",
      "Area under surface (rectangular approx) =  62.321220703135616\n",
      "Violations =  0.0\n",
      "Average_violations =  -18923.375636297853\n",
      "MSE =  1.0383252865341206\n",
      "temp/e267\n",
      "Area under surface (rectangular approx) =  59.3914698043177\n",
      "Violations =  0.0\n",
      "Average_violations =  -18990.120418445476\n",
      "MSE =  1.0362650284976336\n",
      "temp/e268\n",
      "Area under surface (rectangular approx) =  56.71105733570374\n",
      "Violations =  0.0\n",
      "Average_violations =  -19072.047847469454\n",
      "MSE =  1.0382050373441412\n",
      "temp/e269\n",
      "Area under surface (rectangular approx) =  63.375649000723655\n",
      "Violations =  0.0\n",
      "Average_violations =  -18836.195458512015\n",
      "MSE =  1.0410041470139972\n",
      "temp/e270\n",
      "Area under surface (rectangular approx) =  56.07292897020802\n",
      "Violations =  0.0\n",
      "Average_violations =  -19071.42919729439\n",
      "MSE =  1.0335508790654404\n",
      "temp/e271\n",
      "Area under surface (rectangular approx) =  60.806547876188716\n",
      "Violations =  0.0\n",
      "Average_violations =  -19008.30572592531\n",
      "MSE =  1.0419536082621565\n",
      "temp/e272\n",
      "Area under surface (rectangular approx) =  56.461692626355614\n",
      "Violations =  0.0\n",
      "Average_violations =  -18985.20889587749\n",
      "MSE =  1.0339076838307872\n",
      "temp/e273\n",
      "Area under surface (rectangular approx) =  56.72019426960291\n",
      "Violations =  0.0\n",
      "Average_violations =  -19107.082005753015\n",
      "MSE =  1.0362080653283805\n",
      "temp/e274\n",
      "Area under surface (rectangular approx) =  58.76583638538335\n",
      "Violations =  0.0\n",
      "Average_violations =  -18977.937328597385\n",
      "MSE =  1.038348569701449\n",
      "temp/e275\n",
      "Area under surface (rectangular approx) =  59.90695863904493\n",
      "Violations =  0.0\n",
      "Average_violations =  -18990.72465253282\n",
      "MSE =  1.0398732190859274\n",
      "temp/e276\n",
      "Area under surface (rectangular approx) =  62.20115729259342\n",
      "Violations =  0.0\n",
      "Average_violations =  -18890.20568138203\n",
      "MSE =  1.040102389721532\n",
      "temp/e277\n",
      "Area under surface (rectangular approx) =  62.459773007375034\n",
      "Violations =  0.0\n",
      "Average_violations =  -18803.19896040745\n",
      "MSE =  1.0379040402237056\n",
      "temp/e278\n",
      "Area under surface (rectangular approx) =  57.61837815994586\n",
      "Violations =  0.0\n",
      "Average_violations =  -18889.902241468466\n",
      "MSE =  1.0337418570011276\n",
      "temp/e279\n",
      "Area under surface (rectangular approx) =  60.549362228309235\n",
      "Violations =  0.0\n",
      "Average_violations =  -19009.8831016914\n",
      "MSE =  1.038953815992024\n",
      "temp/e280\n",
      "Area under surface (rectangular approx) =  58.73110425949325\n",
      "Violations =  0.0\n",
      "Average_violations =  -18971.43702350085\n",
      "MSE =  1.038773656245288\n",
      "temp/e281\n",
      "Area under surface (rectangular approx) =  58.23743877675052\n",
      "Violations =  0.0\n",
      "Average_violations =  -18992.731242436064\n",
      "MSE =  1.0390104643144666\n",
      "temp/e282\n",
      "Area under surface (rectangular approx) =  54.860948143042854\n",
      "Violations =  0.0\n",
      "Average_violations =  -19038.1088609837\n",
      "MSE =  1.0360264121617564\n",
      "temp/e283\n",
      "Area under surface (rectangular approx) =  60.086958480220936\n",
      "Violations =  0.0\n",
      "Average_violations =  -18898.356487585253\n",
      "MSE =  1.0372688439462272\n",
      "temp/e284\n",
      "Area under surface (rectangular approx) =  59.050559543724376\n",
      "Violations =  0.0\n",
      "Average_violations =  -19019.37690874908\n",
      "MSE =  1.0363520008388862\n",
      "temp/e285\n",
      "Area under surface (rectangular approx) =  57.38649514373174\n",
      "Violations =  0.0\n",
      "Average_violations =  -18985.460051018446\n",
      "MSE =  1.0343810134179607\n",
      "temp/e286\n",
      "Area under surface (rectangular approx) =  55.1678876791666\n",
      "Violations =  0.0\n",
      "Average_violations =  -18956.752668572444\n",
      "MSE =  1.0415653463429342\n",
      "temp/e287\n",
      "Area under surface (rectangular approx) =  59.56481016108286\n",
      "Violations =  0.0\n",
      "Average_violations =  -19002.351074277634\n",
      "MSE =  1.0358575725449037\n",
      "temp/e288\n",
      "Area under surface (rectangular approx) =  63.44511795365034\n",
      "Violations =  0.0\n",
      "Average_violations =  -18898.849932437002\n",
      "MSE =  1.0396968310865597\n",
      "temp/e289\n",
      "Area under surface (rectangular approx) =  59.32057075558901\n",
      "Violations =  0.0\n",
      "Average_violations =  -18974.394948068562\n",
      "MSE =  1.038044870711968\n",
      "temp/e290\n",
      "Area under surface (rectangular approx) =  60.1765103719377\n",
      "Violations =  0.0\n",
      "Average_violations =  -18789.7298768391\n",
      "MSE =  1.039159025726238\n",
      "temp/e291\n",
      "Area under surface (rectangular approx) =  61.38038788177277\n",
      "Violations =  0.0\n",
      "Average_violations =  -18909.652770855064\n",
      "MSE =  1.0348019941979612\n",
      "temp/e292\n",
      "Area under surface (rectangular approx) =  58.751716383459105\n",
      "Violations =  0.0\n",
      "Average_violations =  -18954.318035923057\n",
      "MSE =  1.0351584539825374\n",
      "temp/e293\n",
      "Area under surface (rectangular approx) =  57.02333423596075\n",
      "Violations =  0.0\n",
      "Average_violations =  -19035.819367320462\n",
      "MSE =  1.0331668541359398\n",
      "temp/e294\n",
      "Area under surface (rectangular approx) =  58.06219698645383\n",
      "Violations =  0.0\n",
      "Average_violations =  -19012.317955842685\n",
      "MSE =  1.046722884768955\n",
      "temp/e295\n",
      "Area under surface (rectangular approx) =  57.50766696396382\n",
      "Violations =  0.0\n",
      "Average_violations =  -18897.733225031632\n",
      "MSE =  1.034853449927684\n",
      "temp/e296\n",
      "Area under surface (rectangular approx) =  56.15421644282028\n",
      "Violations =  0.0\n",
      "Average_violations =  -18993.640882471984\n",
      "MSE =  1.0357749092768775\n",
      "temp/e297\n",
      "Area under surface (rectangular approx) =  56.40395338357884\n",
      "Violations =  0.0\n",
      "Average_violations =  -19063.545053338756\n",
      "MSE =  1.0331907850773763\n",
      "temp/e298\n",
      "Area under surface (rectangular approx) =  56.3252132907532\n",
      "Violations =  0.0\n",
      "Average_violations =  -19002.20359599353\n",
      "MSE =  1.0361717182460448\n",
      "temp/e299\n",
      "Area under surface (rectangular approx) =  55.70991664526474\n",
      "Violations =  0.0\n",
      "Average_violations =  -19071.02634518906\n",
      "MSE =  1.0404422254618413\n",
      "temp/e300\n",
      "Area under surface (rectangular approx) =  57.68048346543003\n",
      "Violations =  0.0\n",
      "Average_violations =  -18972.66384154758\n",
      "MSE =  1.0427934250042363\n",
      "temp/e301\n",
      "Area under surface (rectangular approx) =  59.70464370416566\n",
      "Violations =  0.0\n",
      "Average_violations =  -18892.45543382577\n",
      "MSE =  1.038363499940005\n",
      "temp/e302\n",
      "Area under surface (rectangular approx) =  56.519954278779\n",
      "Violations =  0.0\n",
      "Average_violations =  -19070.30453958772\n",
      "MSE =  1.0432096409024436\n",
      "temp/e303\n",
      "Area under surface (rectangular approx) =  57.57072348107779\n",
      "Violations =  0.0\n",
      "Average_violations =  -18977.53518472928\n",
      "MSE =  1.0377280087060574\n",
      "temp/e304\n",
      "Area under surface (rectangular approx) =  55.79188616266704\n",
      "Violations =  0.0\n",
      "Average_violations =  -19062.15387216578\n",
      "MSE =  1.0352325049243885\n",
      "temp/e305\n",
      "Area under surface (rectangular approx) =  61.5558912499789\n",
      "Violations =  0.0\n",
      "Average_violations =  -18898.4350365575\n",
      "MSE =  1.0402420799490515\n",
      "temp/e306\n",
      "Area under surface (rectangular approx) =  61.469470482745905\n",
      "Violations =  0.0\n",
      "Average_violations =  -18837.352053166844\n",
      "MSE =  1.036258225620458\n",
      "temp/e307\n",
      "Area under surface (rectangular approx) =  53.20278952590605\n",
      "Violations =  0.0\n",
      "Average_violations =  -19085.815556345093\n",
      "MSE =  1.0365448210699655\n",
      "temp/e308\n",
      "Area under surface (rectangular approx) =  62.833462236546865\n",
      "Violations =  0.0\n",
      "Average_violations =  -18844.357892047814\n",
      "MSE =  1.045229112671624\n",
      "temp/e309\n",
      "Area under surface (rectangular approx) =  56.065673806895525\n",
      "Violations =  0.0\n",
      "Average_violations =  -18998.13876232769\n",
      "MSE =  1.0371648778902327\n",
      "temp/e310\n",
      "Area under surface (rectangular approx) =  56.45652480903915\n",
      "Violations =  0.0\n",
      "Average_violations =  -18990.31645947364\n",
      "MSE =  1.0316470158468483\n",
      "temp/e311\n",
      "Area under surface (rectangular approx) =  58.075044602542604\n",
      "Violations =  0.0\n",
      "Average_violations =  -19071.043575289666\n",
      "MSE =  1.0435949882544933\n",
      "temp/e312\n",
      "Area under surface (rectangular approx) =  55.791719475405955\n",
      "Violations =  0.0\n",
      "Average_violations =  -19024.448373791693\n",
      "MSE =  1.0388537889133058\n",
      "temp/e313\n",
      "Area under surface (rectangular approx) =  58.5183300438908\n",
      "Violations =  0.0\n",
      "Average_violations =  -19015.71497087812\n",
      "MSE =  1.0406992596794549\n",
      "temp/e314\n",
      "Area under surface (rectangular approx) =  58.381173178596775\n",
      "Violations =  0.0\n",
      "Average_violations =  -18925.140141712403\n",
      "MSE =  1.0367314339103073\n",
      "temp/e315\n",
      "Area under surface (rectangular approx) =  61.10077409922556\n",
      "Violations =  0.0\n",
      "Average_violations =  -18957.75870971443\n",
      "MSE =  1.0345183726273004\n",
      "temp/e316\n",
      "Area under surface (rectangular approx) =  59.62517498404948\n",
      "Violations =  0.0\n",
      "Average_violations =  -18791.228500911173\n",
      "MSE =  1.0427411937608047\n",
      "temp/e317\n",
      "Area under surface (rectangular approx) =  59.09042567154267\n",
      "Violations =  0.0\n",
      "Average_violations =  -18920.68265501938\n",
      "MSE =  1.0358064436701013\n",
      "temp/e318\n",
      "Area under surface (rectangular approx) =  58.57013151507746\n",
      "Violations =  0.0\n",
      "Average_violations =  -19035.632309354973\n",
      "MSE =  1.0340956317081023\n",
      "temp/e319\n",
      "Area under surface (rectangular approx) =  59.61135004359656\n",
      "Violations =  0.0\n",
      "Average_violations =  -18984.347405132157\n",
      "MSE =  1.0406743177082571\n",
      "temp/e320\n",
      "Area under surface (rectangular approx) =  64.61325125006351\n",
      "Violations =  0.0\n",
      "Average_violations =  -18758.851799376673\n",
      "MSE =  1.0422024555375908\n",
      "temp/e321\n",
      "Area under surface (rectangular approx) =  62.3779959596483\n",
      "Violations =  0.0\n",
      "Average_violations =  -18877.43676844886\n",
      "MSE =  1.040038453076365\n",
      "temp/e322\n",
      "Area under surface (rectangular approx) =  56.79477401469224\n",
      "Violations =  0.0\n",
      "Average_violations =  -19070.068618221198\n",
      "MSE =  1.0372410851833975\n",
      "temp/e323\n",
      "Area under surface (rectangular approx) =  57.98856060570752\n",
      "Violations =  0.0\n",
      "Average_violations =  -19110.898208094997\n",
      "MSE =  1.0346503934755678\n",
      "temp/e324\n",
      "Area under surface (rectangular approx) =  58.06713969205501\n",
      "Violations =  0.0\n",
      "Average_violations =  -19046.15352786593\n",
      "MSE =  1.036056372060599\n",
      "temp/e325\n",
      "Area under surface (rectangular approx) =  57.900997418456434\n",
      "Violations =  0.0\n",
      "Average_violations =  -18897.041360627274\n",
      "MSE =  1.0387379195806974\n",
      "temp/e326\n",
      "Area under surface (rectangular approx) =  56.24185822366641\n",
      "Violations =  0.0\n",
      "Average_violations =  -19074.56825065404\n",
      "MSE =  1.0384461464542984\n",
      "temp/e327\n",
      "Area under surface (rectangular approx) =  59.93670904561074\n",
      "Violations =  0.0\n",
      "Average_violations =  -18970.29860932781\n",
      "MSE =  1.0344288953988388\n",
      "temp/e328\n",
      "Area under surface (rectangular approx) =  56.74853473321641\n",
      "Violations =  0.0\n",
      "Average_violations =  -19007.111990439204\n",
      "MSE =  1.0344239626211542\n",
      "temp/e329\n",
      "Area under surface (rectangular approx) =  58.94856632982224\n",
      "Violations =  0.0\n",
      "Average_violations =  -19053.348800048534\n",
      "MSE =  1.0393519861306548\n",
      "temp/e330\n",
      "Area under surface (rectangular approx) =  62.89879829723036\n",
      "Violations =  0.0\n",
      "Average_violations =  -18976.386063212223\n",
      "MSE =  1.0366945667885767\n",
      "temp/e331\n",
      "Area under surface (rectangular approx) =  61.362366790842906\n",
      "Violations =  0.0\n",
      "Average_violations =  -18958.970652707947\n",
      "MSE =  1.036306615558709\n",
      "temp/e332\n",
      "Area under surface (rectangular approx) =  58.52947998914138\n",
      "Violations =  0.0\n",
      "Average_violations =  -18932.377012887002\n",
      "MSE =  1.0397637870810286\n",
      "temp/e333\n",
      "Area under surface (rectangular approx) =  60.58077850983461\n",
      "Violations =  0.0\n",
      "Average_violations =  -18863.13084622065\n",
      "MSE =  1.0300606882812775\n",
      "temp/e334\n",
      "Area under surface (rectangular approx) =  55.891481569924906\n",
      "Violations =  0.0\n",
      "Average_violations =  -18922.3915910904\n",
      "MSE =  1.0370805277321278\n",
      "temp/e335\n",
      "Area under surface (rectangular approx) =  59.31025886932899\n",
      "Violations =  0.0\n",
      "Average_violations =  -18917.597608605836\n",
      "MSE =  1.033941958616629\n",
      "temp/e336\n",
      "Area under surface (rectangular approx) =  57.857256784308625\n",
      "Violations =  0.0\n",
      "Average_violations =  -18876.106145181733\n",
      "MSE =  1.0391362086774174\n",
      "temp/e337\n",
      "Area under surface (rectangular approx) =  57.888116820516885\n",
      "Violations =  0.0\n",
      "Average_violations =  -18856.996420189906\n",
      "MSE =  1.0345195737601154\n",
      "temp/e338\n",
      "Area under surface (rectangular approx) =  54.88434226884107\n",
      "Violations =  0.0\n",
      "Average_violations =  -19054.48991586069\n",
      "MSE =  1.0380442741292462\n",
      "temp/e339\n",
      "Area under surface (rectangular approx) =  56.75117382966308\n",
      "Violations =  0.0\n",
      "Average_violations =  -18999.645916514983\n",
      "MSE =  1.038236781023702\n",
      "temp/e340\n",
      "Area under surface (rectangular approx) =  58.718116121656415\n",
      "Violations =  0.0\n",
      "Average_violations =  -18942.705861573515\n",
      "MSE =  1.0336487922823656\n",
      "temp/e341\n",
      "Area under surface (rectangular approx) =  61.03231861679604\n",
      "Violations =  0.0\n",
      "Average_violations =  -18896.56718739468\n",
      "MSE =  1.0340264974770919\n",
      "temp/e342\n",
      "Area under surface (rectangular approx) =  58.430394141475205\n",
      "Violations =  0.0\n",
      "Average_violations =  -19108.04521369\n",
      "MSE =  1.0379064954489454\n",
      "temp/e343\n",
      "Area under surface (rectangular approx) =  58.354542330208005\n",
      "Violations =  0.0\n",
      "Average_violations =  -18960.28208649568\n",
      "MSE =  1.0407586810489982\n",
      "temp/e344\n",
      "Area under surface (rectangular approx) =  57.16698868844524\n",
      "Violations =  0.0\n",
      "Average_violations =  -19087.403040671008\n",
      "MSE =  1.0390810013569227\n",
      "temp/e345\n",
      "Area under surface (rectangular approx) =  60.62533666921609\n",
      "Violations =  0.0\n",
      "Average_violations =  -18991.733929592992\n",
      "MSE =  1.0366234957696212\n",
      "temp/e346\n",
      "Area under surface (rectangular approx) =  62.423824777554636\n",
      "Violations =  0.0\n",
      "Average_violations =  -18871.412842770285\n",
      "MSE =  1.0422226349161434\n",
      "temp/e347\n",
      "Area under surface (rectangular approx) =  60.08114859662808\n",
      "Violations =  0.0\n",
      "Average_violations =  -19022.77368858964\n",
      "MSE =  1.0384726506013244\n",
      "temp/e348\n",
      "Area under surface (rectangular approx) =  61.527229543570165\n",
      "Violations =  0.0\n",
      "Average_violations =  -18967.531696240476\n",
      "MSE =  1.0394228993034376\n",
      "temp/e349\n",
      "Area under surface (rectangular approx) =  55.64198320054106\n",
      "Violations =  0.0\n",
      "Average_violations =  -19047.919619715336\n",
      "MSE =  1.0515140824243954\n",
      "temp/e350\n",
      "Area under surface (rectangular approx) =  59.562601453505\n",
      "Violations =  0.0\n",
      "Average_violations =  -18997.8361933843\n",
      "MSE =  1.0359296986635282\n",
      "temp/e351\n",
      "Area under surface (rectangular approx) =  54.269193750829594\n",
      "Violations =  0.0\n",
      "Average_violations =  -18937.690681382144\n",
      "MSE =  1.0322766718555683\n",
      "temp/e352\n",
      "Area under surface (rectangular approx) =  57.56183817914325\n",
      "Violations =  0.0\n",
      "Average_violations =  -19057.806417901695\n",
      "MSE =  1.0362503273872057\n",
      "temp/e353\n",
      "Area under surface (rectangular approx) =  57.75097745137044\n",
      "Violations =  0.0\n",
      "Average_violations =  -19095.29467812446\n",
      "MSE =  1.0434735890842346\n",
      "temp/e354\n",
      "Area under surface (rectangular approx) =  58.86891623815036\n",
      "Violations =  0.0\n",
      "Average_violations =  -19000.89843703845\n",
      "MSE =  1.0400113562765223\n",
      "temp/e355\n",
      "Area under surface (rectangular approx) =  55.888591435157714\n",
      "Violations =  0.0\n",
      "Average_violations =  -18959.333800455574\n",
      "MSE =  1.0333484339152006\n",
      "temp/e356\n",
      "Area under surface (rectangular approx) =  60.05373122136619\n",
      "Violations =  0.0\n",
      "Average_violations =  -19008.078058321666\n",
      "MSE =  1.0385482910570996\n",
      "temp/e357\n",
      "Area under surface (rectangular approx) =  56.064117039615226\n",
      "Violations =  0.0\n",
      "Average_violations =  -19033.658144661145\n",
      "MSE =  1.0334262938766559\n",
      "temp/e358\n",
      "Area under surface (rectangular approx) =  58.24012548034686\n",
      "Violations =  0.0\n",
      "Average_violations =  -19001.840372509578\n",
      "MSE =  1.034796565849586\n",
      "temp/e359\n",
      "Area under surface (rectangular approx) =  57.83154199795545\n",
      "Violations =  0.0\n",
      "Average_violations =  -19012.122786005053\n",
      "MSE =  1.0421405064052733\n",
      "temp/e360\n",
      "Area under surface (rectangular approx) =  58.107721730763124\n",
      "Violations =  0.0\n",
      "Average_violations =  -18913.947452715598\n",
      "MSE =  1.035047428010695\n",
      "temp/e361\n",
      "Area under surface (rectangular approx) =  55.624388550498104\n",
      "Violations =  0.0\n",
      "Average_violations =  -19126.715681255773\n",
      "MSE =  1.0458473527927539\n",
      "temp/e362\n",
      "Area under surface (rectangular approx) =  65.23149098270565\n",
      "Violations =  0.0\n",
      "Average_violations =  -18873.823295621172\n",
      "MSE =  1.037376278924722\n",
      "temp/e363\n",
      "Area under surface (rectangular approx) =  59.28892140071494\n",
      "Violations =  0.0\n",
      "Average_violations =  -18932.00618865609\n",
      "MSE =  1.0408313190656042\n",
      "temp/e364\n",
      "Area under surface (rectangular approx) =  62.0881354067319\n",
      "Violations =  0.0\n",
      "Average_violations =  -18886.180218127403\n",
      "MSE =  1.0370504354187051\n",
      "temp/e365\n",
      "Area under surface (rectangular approx) =  60.906155824798596\n",
      "Violations =  0.0\n",
      "Average_violations =  -18893.152933075435\n",
      "MSE =  1.042674908327117\n",
      "temp/e366\n",
      "Area under surface (rectangular approx) =  58.73877609477473\n",
      "Violations =  0.0\n",
      "Average_violations =  -18932.00604793939\n",
      "MSE =  1.0369213878723746\n",
      "temp/e367\n",
      "Area under surface (rectangular approx) =  56.29502259415666\n",
      "Violations =  0.0\n",
      "Average_violations =  -19070.467001776844\n",
      "MSE =  1.0351782389247863\n",
      "temp/e368\n",
      "Area under surface (rectangular approx) =  59.46110914337224\n",
      "Violations =  0.0\n",
      "Average_violations =  -19024.416120077007\n",
      "MSE =  1.0403848627855625\n",
      "temp/e369\n",
      "Area under surface (rectangular approx) =  60.11967030231445\n",
      "Violations =  0.0\n",
      "Average_violations =  -18945.722866959295\n",
      "MSE =  1.044917898845408\n",
      "temp/e370\n",
      "Area under surface (rectangular approx) =  58.49420027104975\n",
      "Violations =  0.0\n",
      "Average_violations =  -19021.99791776861\n",
      "MSE =  1.034952499292627\n",
      "temp/e371\n",
      "Area under surface (rectangular approx) =  59.315205489873414\n",
      "Violations =  0.0\n",
      "Average_violations =  -18962.060122782765\n",
      "MSE =  1.034350500602402\n",
      "temp/e372\n",
      "Area under surface (rectangular approx) =  65.73458535222846\n",
      "Violations =  0.0\n",
      "Average_violations =  -18950.764046136264\n",
      "MSE =  1.0483380973782288\n",
      "temp/e373\n",
      "Area under surface (rectangular approx) =  58.31033443373091\n",
      "Violations =  0.0\n",
      "Average_violations =  -18996.7451105844\n",
      "MSE =  1.0397150883206019\n",
      "temp/e374\n",
      "Area under surface (rectangular approx) =  58.07617970039067\n",
      "Violations =  0.0\n",
      "Average_violations =  -18975.373436337813\n",
      "MSE =  1.0352471935867809\n",
      "temp/e375\n",
      "Area under surface (rectangular approx) =  65.86926076347441\n",
      "Violations =  0.0\n",
      "Average_violations =  -18775.891696205195\n",
      "MSE =  1.0448296908518526\n",
      "temp/e376\n",
      "Area under surface (rectangular approx) =  62.198536244344574\n",
      "Violations =  0.0\n",
      "Average_violations =  -18884.19024525159\n",
      "MSE =  1.0398181525908161\n",
      "temp/e377\n",
      "Area under surface (rectangular approx) =  57.971603400788894\n",
      "Violations =  0.0\n",
      "Average_violations =  -19011.799417469352\n",
      "MSE =  1.033322769911923\n",
      "temp/e378\n",
      "Area under surface (rectangular approx) =  60.69094834680133\n",
      "Violations =  0.0\n",
      "Average_violations =  -18824.08479728611\n",
      "MSE =  1.0398491334635487\n",
      "temp/e379\n",
      "Area under surface (rectangular approx) =  58.768201198628226\n",
      "Violations =  0.0\n",
      "Average_violations =  -18873.94071583596\n",
      "MSE =  1.0375222800415185\n",
      "temp/e380\n",
      "Area under surface (rectangular approx) =  59.197339293318336\n",
      "Violations =  0.0\n",
      "Average_violations =  -18956.02462602657\n",
      "MSE =  1.0396943401793837\n",
      "temp/e381\n",
      "Area under surface (rectangular approx) =  57.56678363324443\n",
      "Violations =  0.0\n",
      "Average_violations =  -19001.536930914997\n",
      "MSE =  1.044066707538056\n",
      "temp/e382\n",
      "Area under surface (rectangular approx) =  64.16873061707237\n",
      "Violations =  0.0\n",
      "Average_violations =  -18886.040119586283\n",
      "MSE =  1.04010413336654\n",
      "temp/e383\n",
      "Area under surface (rectangular approx) =  58.42882685235045\n",
      "Violations =  0.0\n",
      "Average_violations =  -18929.90500428518\n",
      "MSE =  1.0364965326787736\n",
      "temp/e384\n",
      "Area under surface (rectangular approx) =  57.71045850890114\n",
      "Violations =  0.0\n",
      "Average_violations =  -18942.303801836748\n",
      "MSE =  1.0350339198667164\n",
      "temp/e385\n",
      "Area under surface (rectangular approx) =  57.77191871170312\n",
      "Violations =  0.0\n",
      "Average_violations =  -18864.78130435798\n",
      "MSE =  1.0362818061112653\n",
      "temp/e386\n",
      "Area under surface (rectangular approx) =  56.048859540768554\n",
      "Violations =  0.0\n",
      "Average_violations =  -18953.35526610753\n",
      "MSE =  1.0407699132743253\n",
      "temp/e387\n",
      "Area under surface (rectangular approx) =  58.20131726129813\n",
      "Violations =  0.0\n",
      "Average_violations =  -18942.53094259953\n",
      "MSE =  1.0287298935186353\n",
      "temp/e388\n",
      "Area under surface (rectangular approx) =  57.8997069243999\n",
      "Violations =  0.0\n",
      "Average_violations =  -18960.174509399207\n",
      "MSE =  1.0328827374310152\n",
      "temp/e389\n",
      "Area under surface (rectangular approx) =  59.303187581282415\n",
      "Violations =  0.0\n",
      "Average_violations =  -18911.58016202781\n",
      "MSE =  1.0371331959429988\n",
      "temp/e390\n",
      "Area under surface (rectangular approx) =  58.92069774400781\n",
      "Violations =  0.0\n",
      "Average_violations =  -18958.455997828438\n",
      "MSE =  1.0345987271275583\n",
      "temp/e391\n",
      "Area under surface (rectangular approx) =  58.16581382626438\n",
      "Violations =  0.0\n",
      "Average_violations =  -18871.554312435543\n",
      "MSE =  1.0416488790574843\n",
      "temp/e392\n",
      "Area under surface (rectangular approx) =  59.195223185326526\n",
      "Violations =  0.0\n",
      "Average_violations =  -18975.633192208752\n",
      "MSE =  1.0352824879770424\n",
      "temp/e393\n",
      "Area under surface (rectangular approx) =  54.54232513477637\n",
      "Violations =  0.0\n",
      "Average_violations =  -19137.141216571697\n",
      "MSE =  1.0385531667477357\n",
      "temp/e394\n",
      "Area under surface (rectangular approx) =  61.927049023623766\n",
      "Violations =  0.0\n",
      "Average_violations =  -18913.5110849863\n",
      "MSE =  1.0406127297001073\n",
      "temp/e395\n",
      "Area under surface (rectangular approx) =  59.83895559552232\n",
      "Violations =  0.0\n",
      "Average_violations =  -18984.848685979778\n",
      "MSE =  1.035565168141341\n",
      "temp/e396\n",
      "Area under surface (rectangular approx) =  56.91044674303019\n",
      "Violations =  0.0\n",
      "Average_violations =  -18943.737201620588\n",
      "MSE =  1.035518334293883\n",
      "temp/e397\n",
      "Area under surface (rectangular approx) =  57.10615417898309\n",
      "Violations =  0.0\n",
      "Average_violations =  -18897.886513083737\n",
      "MSE =  1.03283120295058\n",
      "temp/e398\n",
      "Area under surface (rectangular approx) =  57.53576432589563\n",
      "Violations =  0.0\n",
      "Average_violations =  -18908.36484660855\n",
      "MSE =  1.0424023416201191\n",
      "temp/e399\n",
      "Area under surface (rectangular approx) =  58.733467962449254\n",
      "Violations =  0.0\n",
      "Average_violations =  -18925.23149770729\n",
      "MSE =  1.0287484517224283\n",
      "temp/e400\n",
      "Area under surface (rectangular approx) =  57.7684407987202\n",
      "Violations =  0.0\n",
      "Average_violations =  -18985.56909668282\n",
      "MSE =  1.0335827121047578\n",
      "temp/e401\n",
      "Area under surface (rectangular approx) =  55.03538421950585\n",
      "Violations =  0.0\n",
      "Average_violations =  -19059.069767910376\n",
      "MSE =  1.0368822548772179\n",
      "temp/e402\n",
      "Area under surface (rectangular approx) =  62.73196126479742\n",
      "Violations =  0.0\n",
      "Average_violations =  -18828.639573162807\n",
      "MSE =  1.0377689130738887\n",
      "temp/e403\n",
      "Area under surface (rectangular approx) =  56.20382209559083\n",
      "Violations =  0.0\n",
      "Average_violations =  -18977.682364800177\n",
      "MSE =  1.0351733999967005\n",
      "temp/e404\n",
      "Area under surface (rectangular approx) =  60.95455575853332\n",
      "Violations =  0.0\n",
      "Average_violations =  -19016.756021535424\n",
      "MSE =  1.0356809325140894\n",
      "temp/e405\n",
      "Area under surface (rectangular approx) =  61.67819943210435\n",
      "Violations =  0.0\n",
      "Average_violations =  -18898.858448968207\n",
      "MSE =  1.036253752943551\n",
      "temp/e406\n",
      "Area under surface (rectangular approx) =  57.440524029230716\n",
      "Violations =  0.0\n",
      "Average_violations =  -19007.071678034215\n",
      "MSE =  1.0367372940470743\n",
      "temp/e407\n",
      "Area under surface (rectangular approx) =  62.744439950301924\n",
      "Violations =  0.0\n",
      "Average_violations =  -18853.657329161084\n",
      "MSE =  1.0362106593968345\n",
      "temp/e408\n",
      "Area under surface (rectangular approx) =  60.28331928509604\n",
      "Violations =  0.0\n",
      "Average_violations =  -18917.468802021987\n",
      "MSE =  1.0380088545602668\n",
      "temp/e409\n",
      "Area under surface (rectangular approx) =  54.84416641606879\n",
      "Violations =  0.0\n",
      "Average_violations =  -19025.77646689053\n",
      "MSE =  1.038673528768648\n",
      "temp/e410\n",
      "Area under surface (rectangular approx) =  62.93913445760485\n",
      "Violations =  0.0\n",
      "Average_violations =  -18826.996455486344\n",
      "MSE =  1.0427016011534502\n",
      "temp/e411\n",
      "Area under surface (rectangular approx) =  56.97372687320939\n",
      "Violations =  0.0\n",
      "Average_violations =  -18934.589168601997\n",
      "MSE =  1.0411139479567404\n",
      "temp/e412\n",
      "Area under surface (rectangular approx) =  54.36808848359348\n",
      "Violations =  0.0\n",
      "Average_violations =  -19012.840897621812\n",
      "MSE =  1.0400868306593725\n",
      "temp/e413\n",
      "Area under surface (rectangular approx) =  56.41474771272729\n",
      "Violations =  0.0\n",
      "Average_violations =  -19044.579237280326\n",
      "MSE =  1.0391989239810722\n",
      "temp/e414\n",
      "Area under surface (rectangular approx) =  60.26950981656475\n",
      "Violations =  0.0\n",
      "Average_violations =  -18903.308504960776\n",
      "MSE =  1.0374759943931084\n",
      "temp/e415\n",
      "Area under surface (rectangular approx) =  57.65956660894652\n",
      "Violations =  0.0\n",
      "Average_violations =  -19033.13000996425\n",
      "MSE =  1.0363097306759659\n",
      "temp/e416\n",
      "Area under surface (rectangular approx) =  60.33087702671033\n",
      "Violations =  0.0\n",
      "Average_violations =  -18922.655666114595\n",
      "MSE =  1.0334152654010285\n",
      "temp/e417\n",
      "Area under surface (rectangular approx) =  60.16305277107271\n",
      "Violations =  0.0\n",
      "Average_violations =  -18846.051530129866\n",
      "MSE =  1.0383313924420168\n",
      "temp/e418\n",
      "Area under surface (rectangular approx) =  58.29708091661281\n",
      "Violations =  0.0\n",
      "Average_violations =  -18934.04438380024\n",
      "MSE =  1.0351086805222882\n",
      "temp/e419\n",
      "Area under surface (rectangular approx) =  58.34431071280138\n",
      "Violations =  0.0\n",
      "Average_violations =  -18947.285896334477\n",
      "MSE =  1.0350228692076595\n",
      "temp/e420\n",
      "Area under surface (rectangular approx) =  56.47527360204856\n",
      "Violations =  0.0\n",
      "Average_violations =  -19071.833544439633\n",
      "MSE =  1.037031245059892\n",
      "temp/e421\n",
      "Area under surface (rectangular approx) =  54.795273001275966\n",
      "Violations =  0.0\n",
      "Average_violations =  -19068.58893259947\n",
      "MSE =  1.0378969545087613\n",
      "temp/e422\n",
      "Area under surface (rectangular approx) =  59.64546995809061\n",
      "Violations =  0.0\n",
      "Average_violations =  -18966.2586062737\n",
      "MSE =  1.0355889119796653\n",
      "temp/e423\n",
      "Area under surface (rectangular approx) =  57.51879100634544\n",
      "Violations =  0.0\n",
      "Average_violations =  -18950.430129171866\n",
      "MSE =  1.0362211537640242\n",
      "temp/e424\n",
      "Area under surface (rectangular approx) =  63.07464182446427\n",
      "Violations =  0.0\n",
      "Average_violations =  -18922.921402120228\n",
      "MSE =  1.0383884679922986\n",
      "temp/e425\n",
      "Area under surface (rectangular approx) =  55.00638307721516\n",
      "Violations =  0.0\n",
      "Average_violations =  -18953.568546706963\n",
      "MSE =  1.0355787932276337\n",
      "temp/e426\n",
      "Area under surface (rectangular approx) =  55.7258255833883\n",
      "Violations =  0.0\n",
      "Average_violations =  -19065.06095510439\n",
      "MSE =  1.0411304429291968\n",
      "temp/e427\n",
      "Area under surface (rectangular approx) =  60.711338250690595\n",
      "Violations =  0.0\n",
      "Average_violations =  -18869.544805618752\n",
      "MSE =  1.034392936431662\n",
      "temp/e428\n",
      "Area under surface (rectangular approx) =  55.46125156040969\n",
      "Violations =  0.0\n",
      "Average_violations =  -18950.481613962424\n",
      "MSE =  1.0354401190338391\n",
      "temp/e429\n",
      "Area under surface (rectangular approx) =  58.81531816616517\n",
      "Violations =  0.0\n",
      "Average_violations =  -19021.826682507013\n",
      "MSE =  1.0364377455021676\n",
      "temp/e430\n",
      "Area under surface (rectangular approx) =  59.75195297931886\n",
      "Violations =  0.0\n",
      "Average_violations =  -18919.8863245074\n",
      "MSE =  1.0355155176678639\n",
      "temp/e431\n",
      "Area under surface (rectangular approx) =  62.43161501561554\n",
      "Violations =  0.0\n",
      "Average_violations =  -18922.40679542543\n",
      "MSE =  1.0403545366262077\n",
      "temp/e432\n",
      "Area under surface (rectangular approx) =  60.48814966885017\n",
      "Violations =  0.0\n",
      "Average_violations =  -19021.18475816318\n",
      "MSE =  1.0450618444062887\n",
      "temp/e433\n",
      "Area under surface (rectangular approx) =  59.55635209449916\n",
      "Violations =  0.0\n",
      "Average_violations =  -19004.915905650243\n",
      "MSE =  1.0377110630314301\n",
      "temp/e434\n",
      "Area under surface (rectangular approx) =  58.50225744151797\n",
      "Violations =  0.0\n",
      "Average_violations =  -18936.848146192668\n",
      "MSE =  1.0326935672613011\n",
      "temp/e435\n",
      "Area under surface (rectangular approx) =  61.92712742869829\n",
      "Violations =  0.0\n",
      "Average_violations =  -18956.565659200067\n",
      "MSE =  1.0384386390177431\n",
      "temp/e436\n",
      "Area under surface (rectangular approx) =  59.20431013689901\n",
      "Violations =  0.0\n",
      "Average_violations =  -18903.473637709998\n",
      "MSE =  1.0342807960244713\n",
      "temp/e437\n",
      "Area under surface (rectangular approx) =  58.42085242666531\n",
      "Violations =  0.0\n",
      "Average_violations =  -19010.93279193337\n",
      "MSE =  1.0375314533012245\n",
      "temp/e438\n",
      "Area under surface (rectangular approx) =  57.14205436501148\n",
      "Violations =  0.0\n",
      "Average_violations =  -18984.4246403783\n",
      "MSE =  1.0334914034208185\n",
      "temp/e439\n",
      "Area under surface (rectangular approx) =  57.68278926245037\n",
      "Violations =  0.0\n",
      "Average_violations =  -18924.156956688093\n",
      "MSE =  1.0341115131554135\n",
      "temp/e440\n",
      "Area under surface (rectangular approx) =  60.14287785355374\n",
      "Violations =  0.0\n",
      "Average_violations =  -18917.02957866218\n",
      "MSE =  1.0330739880271402\n",
      "temp/e441\n",
      "Area under surface (rectangular approx) =  56.96952286779259\n",
      "Violations =  0.0\n",
      "Average_violations =  -18929.99929597944\n",
      "MSE =  1.0366908830597852\n",
      "temp/e442\n",
      "Area under surface (rectangular approx) =  57.57588624770125\n",
      "Violations =  0.0\n",
      "Average_violations =  -18982.995054480627\n",
      "MSE =  1.0348009536503198\n",
      "temp/e443\n",
      "Area under surface (rectangular approx) =  60.84671924155996\n",
      "Violations =  0.0\n",
      "Average_violations =  -18991.21442834829\n",
      "MSE =  1.0410214131515398\n",
      "temp/e444\n",
      "Area under surface (rectangular approx) =  58.65201589281123\n",
      "Violations =  0.0\n",
      "Average_violations =  -18944.13735485762\n",
      "MSE =  1.0373434856624173\n",
      "temp/e445\n",
      "Area under surface (rectangular approx) =  61.207912411060136\n",
      "Violations =  0.0\n",
      "Average_violations =  -18868.71235279384\n",
      "MSE =  1.0403627211635853\n",
      "temp/e446\n",
      "Area under surface (rectangular approx) =  57.630249110026064\n",
      "Violations =  0.0\n",
      "Average_violations =  -19089.82934408409\n",
      "MSE =  1.042146174476286\n",
      "temp/e447\n",
      "Area under surface (rectangular approx) =  63.36132670229918\n",
      "Violations =  0.0\n",
      "Average_violations =  -18828.574033280496\n",
      "MSE =  1.0408285517586409\n",
      "temp/e448\n",
      "Area under surface (rectangular approx) =  59.79107621424086\n",
      "Violations =  0.0\n",
      "Average_violations =  -18918.551062194572\n",
      "MSE =  1.0362569602423546\n",
      "temp/e449\n",
      "Area under surface (rectangular approx) =  56.53067463443121\n",
      "Violations =  0.0\n",
      "Average_violations =  -19084.59378442149\n",
      "MSE =  1.0365503439137214\n",
      "temp/e450\n",
      "Area under surface (rectangular approx) =  58.14946146157576\n",
      "Violations =  0.0\n",
      "Average_violations =  -18967.056862540885\n",
      "MSE =  1.0362274455461535\n",
      "temp/e451\n",
      "Area under surface (rectangular approx) =  60.751040263896314\n",
      "Violations =  0.0\n",
      "Average_violations =  -18817.52628770388\n",
      "MSE =  1.0383253168505404\n",
      "temp/e452\n",
      "Area under surface (rectangular approx) =  58.63784659764099\n",
      "Violations =  0.0\n",
      "Average_violations =  -19093.26499002116\n",
      "MSE =  1.0405017498055034\n",
      "temp/e453\n",
      "Area under surface (rectangular approx) =  56.654619247346446\n",
      "Violations =  0.0\n",
      "Average_violations =  -19094.22130244054\n",
      "MSE =  1.0377829191925863\n",
      "temp/e454\n",
      "Area under surface (rectangular approx) =  61.52389449846039\n",
      "Violations =  0.0\n",
      "Average_violations =  -18780.177856761173\n",
      "MSE =  1.041553932489846\n",
      "temp/e455\n",
      "Area under surface (rectangular approx) =  56.53894873995662\n",
      "Violations =  0.0\n",
      "Average_violations =  -18918.89104460016\n",
      "MSE =  1.0371913281342056\n",
      "temp/e456\n",
      "Area under surface (rectangular approx) =  58.348855591718035\n",
      "Violations =  0.0\n",
      "Average_violations =  -18922.894770791532\n",
      "MSE =  1.0372827036423538\n",
      "temp/e457\n",
      "Area under surface (rectangular approx) =  56.36467481420898\n",
      "Violations =  0.0\n",
      "Average_violations =  -18965.342262017944\n",
      "MSE =  1.0335874140070869\n",
      "temp/e458\n",
      "Area under surface (rectangular approx) =  56.83047119760941\n",
      "Violations =  0.0\n",
      "Average_violations =  -18992.578476950795\n",
      "MSE =  1.02985936884886\n",
      "temp/e459\n",
      "Area under surface (rectangular approx) =  58.74773807443265\n",
      "Violations =  0.0\n",
      "Average_violations =  -18973.492919614444\n",
      "MSE =  1.0387461209779683\n",
      "temp/e460\n",
      "Area under surface (rectangular approx) =  58.867965648695794\n",
      "Violations =  0.0\n",
      "Average_violations =  -18958.860630432362\n",
      "MSE =  1.035461985144511\n",
      "temp/e461\n",
      "Area under surface (rectangular approx) =  57.4027802188896\n",
      "Violations =  0.0\n",
      "Average_violations =  -18960.584631406004\n",
      "MSE =  1.0363900040591887\n",
      "temp/e462\n",
      "Area under surface (rectangular approx) =  60.56376244579259\n",
      "Violations =  0.0\n",
      "Average_violations =  -18832.174673162906\n",
      "MSE =  1.0360831694140127\n",
      "temp/e463\n",
      "Area under surface (rectangular approx) =  57.07041711273335\n",
      "Violations =  0.0\n",
      "Average_violations =  -18980.691002345025\n",
      "MSE =  1.037291463049999\n",
      "temp/e464\n",
      "Area under surface (rectangular approx) =  58.99616124652056\n",
      "Violations =  0.0\n",
      "Average_violations =  -18766.971777849176\n",
      "MSE =  1.0393056536174334\n",
      "temp/e465\n",
      "Area under surface (rectangular approx) =  58.16893493432613\n",
      "Violations =  0.0\n",
      "Average_violations =  -18880.914237896395\n",
      "MSE =  1.0340237490067914\n",
      "temp/e466\n",
      "Area under surface (rectangular approx) =  59.78089466334252\n",
      "Violations =  0.0\n",
      "Average_violations =  -18918.37048098502\n",
      "MSE =  1.0403648124262623\n",
      "temp/e467\n",
      "Area under surface (rectangular approx) =  59.16091883643135\n",
      "Violations =  0.0\n",
      "Average_violations =  -19049.118982156506\n",
      "MSE =  1.0408313338674002\n",
      "temp/e468\n",
      "Area under surface (rectangular approx) =  57.2082359297184\n",
      "Violations =  0.0\n",
      "Average_violations =  -18922.7296958912\n",
      "MSE =  1.0326773623239196\n",
      "temp/e469\n",
      "Area under surface (rectangular approx) =  58.15792193358645\n",
      "Violations =  0.0\n",
      "Average_violations =  -19060.799181235667\n",
      "MSE =  1.033521268824129\n",
      "temp/e470\n",
      "Area under surface (rectangular approx) =  55.26853596548959\n",
      "Violations =  0.0\n",
      "Average_violations =  -19039.832594707183\n",
      "MSE =  1.041699337896885\n",
      "temp/e471\n",
      "Area under surface (rectangular approx) =  60.78166074713059\n",
      "Violations =  0.0\n",
      "Average_violations =  -18946.95675547581\n",
      "MSE =  1.0466917234214332\n",
      "temp/e472\n",
      "Area under surface (rectangular approx) =  55.67418076253295\n",
      "Violations =  0.0\n",
      "Average_violations =  -19007.149870253168\n",
      "MSE =  1.0354626017115351\n",
      "temp/e473\n",
      "Area under surface (rectangular approx) =  57.575302065666264\n",
      "Violations =  0.0\n",
      "Average_violations =  -18944.052107104486\n",
      "MSE =  1.039558388667847\n",
      "temp/e474\n",
      "Area under surface (rectangular approx) =  59.20069739329588\n",
      "Violations =  0.0\n",
      "Average_violations =  -18866.986900747703\n",
      "MSE =  1.035087346462146\n",
      "temp/e475\n",
      "Area under surface (rectangular approx) =  59.82003140170549\n",
      "Violations =  0.0\n",
      "Average_violations =  -18859.98982599793\n",
      "MSE =  1.0396337489087473\n",
      "temp/e476\n",
      "Area under surface (rectangular approx) =  54.88984627429405\n",
      "Violations =  0.0\n",
      "Average_violations =  -19098.482507903496\n",
      "MSE =  1.0360991539057138\n",
      "temp/e477\n",
      "Area under surface (rectangular approx) =  56.51176023426632\n",
      "Violations =  0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average_violations =  -18991.158885459874\n",
      "MSE =  1.0350916103952372\n",
      "temp/e478\n",
      "Area under surface (rectangular approx) =  62.81741391422402\n",
      "Violations =  0.0\n",
      "Average_violations =  -18923.680129219185\n",
      "MSE =  1.0454281445430114\n",
      "temp/e479\n",
      "Area under surface (rectangular approx) =  59.50562547862484\n",
      "Violations =  0.0\n",
      "Average_violations =  -18929.538212132087\n",
      "MSE =  1.0369650439715496\n",
      "temp/e480\n",
      "Area under surface (rectangular approx) =  57.65874729333392\n",
      "Violations =  0.0\n",
      "Average_violations =  -18895.23396873881\n",
      "MSE =  1.033115937212191\n",
      "temp/e481\n",
      "Area under surface (rectangular approx) =  59.57925682048956\n",
      "Violations =  0.0\n",
      "Average_violations =  -19001.78163811108\n",
      "MSE =  1.039469096902585\n",
      "temp/e482\n",
      "Area under surface (rectangular approx) =  63.21570120235111\n",
      "Violations =  0.0\n",
      "Average_violations =  -18838.680254348386\n",
      "MSE =  1.04128379849578\n",
      "temp/e483\n",
      "Area under surface (rectangular approx) =  57.66576340972467\n",
      "Violations =  0.0\n",
      "Average_violations =  -19027.52872354413\n",
      "MSE =  1.0337128900641257\n",
      "temp/e484\n",
      "Area under surface (rectangular approx) =  58.98786021242725\n",
      "Violations =  0.0\n",
      "Average_violations =  -18896.972645167214\n",
      "MSE =  1.0438517504070348\n",
      "temp/e485\n",
      "Area under surface (rectangular approx) =  56.44393627928944\n",
      "Violations =  0.0\n",
      "Average_violations =  -19059.5729413452\n",
      "MSE =  1.0382655604018818\n",
      "temp/e486\n",
      "Area under surface (rectangular approx) =  59.64011439707044\n",
      "Violations =  0.0\n",
      "Average_violations =  -18916.0862770749\n",
      "MSE =  1.041131010500487\n",
      "temp/e487\n",
      "Area under surface (rectangular approx) =  58.29902467684627\n",
      "Violations =  0.0\n",
      "Average_violations =  -18937.119959272695\n",
      "MSE =  1.0333444776761804\n",
      "temp/e488\n",
      "Area under surface (rectangular approx) =  61.097328954173\n",
      "Violations =  0.0\n",
      "Average_violations =  -18937.965502550953\n",
      "MSE =  1.0372518925214722\n",
      "temp/e489\n",
      "Area under surface (rectangular approx) =  56.362239175059926\n",
      "Violations =  0.0\n",
      "Average_violations =  -18917.47070811244\n",
      "MSE =  1.0399206422391765\n",
      "temp/e490\n",
      "Area under surface (rectangular approx) =  54.572610289551285\n",
      "Violations =  0.0\n",
      "Average_violations =  -18941.841813310064\n",
      "MSE =  1.0406425025261299\n",
      "temp/e491\n",
      "Area under surface (rectangular approx) =  59.88515218817997\n",
      "Violations =  0.0\n",
      "Average_violations =  -18885.071029041126\n",
      "MSE =  1.0415505590240026\n",
      "temp/e492\n",
      "Area under surface (rectangular approx) =  60.24954889981299\n",
      "Violations =  0.0\n",
      "Average_violations =  -19003.41071483249\n",
      "MSE =  1.035917643973437\n",
      "temp/e493\n",
      "Area under surface (rectangular approx) =  56.85034449534571\n",
      "Violations =  0.0\n",
      "Average_violations =  -18906.829812616874\n",
      "MSE =  1.0354156024894636\n",
      "temp/e494\n",
      "Area under surface (rectangular approx) =  58.27461390668551\n",
      "Violations =  0.0\n",
      "Average_violations =  -18886.465312480545\n",
      "MSE =  1.0365712356020473\n",
      "temp/e495\n",
      "Area under surface (rectangular approx) =  58.407363813898684\n",
      "Violations =  0.0\n",
      "Average_violations =  -18848.37941194263\n",
      "MSE =  1.0398348919398388\n",
      "temp/e496\n",
      "Area under surface (rectangular approx) =  60.51350665513386\n",
      "Violations =  0.0\n",
      "Average_violations =  -18867.790159026685\n",
      "MSE =  1.0368584758314645\n",
      "temp/e497\n",
      "Area under surface (rectangular approx) =  59.12020271677487\n",
      "Violations =  0.0\n",
      "Average_violations =  -18982.788994051938\n",
      "MSE =  1.0354407494023095\n",
      "temp/e498\n",
      "Area under surface (rectangular approx) =  56.901015341351965\n",
      "Violations =  0.0\n",
      "Average_violations =  -19070.90209492866\n",
      "MSE =  1.0392714499557925\n",
      "temp/e499\n",
      "Area under surface (rectangular approx) =  62.58908385139071\n",
      "Violations =  0.0\n",
      "Average_violations =  -18852.26210144793\n",
      "MSE =  1.0416801117500092\n"
     ]
    }
   ],
   "source": [
    "metric = []\n",
    "metric_err = []\n",
    "viol = []\n",
    "viol_err = []\n",
    "\n",
    "#normalize the violations for prettier graphing.\n",
    "#also violations are always positive, so just divide by max.\n",
    "\n",
    "#TMK\n",
    "#violation_mean = violation_mean / np.max(violation_mean)\n",
    "\n",
    "for i in range(len(violations)):\n",
    "    print(\"Model_name = \", model_names[i], \"Violations = \", violations[i])\n",
    "    print(\"Average_violations = \", np.mean(violation_mean[i]), np.std(violation_mean[i]))\n",
    "    print(\"MSE = \", np.mean(mean[i]), np.std(mean[i]))\n",
    "    #print(\"mean = \", mean[i])\n",
    "    metric.append(np.mean(mean[i]))\n",
    "    metric_err.append(np.std(mean[i]))\n",
    "    viol.append(np.mean(violation_mean[i]))\n",
    "    #viol.append(violations[i]/times)\n",
    "    viol_err.append(np.std(violation_mean[i]))\n",
    "print(np.array(metric), \n",
    "         np.array(metric_err), \n",
    "         np.array(viol), \n",
    "         np.array(viol_err))    \n",
    "\n",
    "bar_plot(model_names, \n",
    "         np.array(metric), \n",
    "         np.array(metric_err), \n",
    "         np.array(viol), \n",
    "         np.array(viol_err))\n",
    "\n",
    "\n",
    "def heat_plot(x,y,z, xlab = 'Mean', ylab = 'Variance', clim_low = 0, clim_high = 1):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    cax = ax.scatter(x, y, c=z, s=450, edgecolor='')\n",
    "    cax.set_clim(clim_low, clim_high)\n",
    "    ax.set_xlabel(xlab)\n",
    "    ax.set_ylabel(ylab)\n",
    "    plt.colorbar(cax)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "MSE = []\n",
    "VIO = []\n",
    "VIO2 = []\n",
    "AUS = []\n",
    "for i, m in enumerate(models):\n",
    "    print(model_names[i])\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "    \n",
    "    \n",
    "    rectangular_approx = 0\n",
    "    for k, v in metrics_dicts[i].items():\n",
    "        x.append(float(k.split('_')[0]))\n",
    "        y.append(float(k.split('_')[-1]))\n",
    "        z.append(np.mean(v))\n",
    "        rectangular_approx += np.mean(v)\n",
    "    print(\"Area under surface (rectangular approx) = \", rectangular_approx)\n",
    "    print(\"Violations = \", violations[i])\n",
    "    print(\"Average_violations = \", np.mean(violation_mean[i]))\n",
    "    print(\"MSE = \", np.mean(mean[i]))   \n",
    "    MSE.append(np.mean(mean[i]))\n",
    "    VIO.append(np.mean(violation_mean[i]))\n",
    "    VIO2.append(np.mean(violation_mean2[i]))\n",
    "    #VIO.append(violations[i]/times)\n",
    "    AUS.append(rectangular_approx)\n",
    "    \n",
    "    #heat_plot(x,y,z, clim_low = 0, clim_high = 10)\n",
    "    \n",
    "#heat_plot(MSE,VIO,AUS, xlab = 'MSE', ylab='Violations', clim_low = np.min(AUS), clim_high = np.max(AUS))\n",
    "    \n",
    "#VIO = np.abs(VIO)\n",
    "#VIO2 = np.abs(VIO2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5909303967231058\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6482452661966371\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJztnXuclHW9+N+f2WVRvMAKoiACkkIJArKIkFe8laZiaHk9x0uGmXnSfp0uVmh0Mq1Oah3LiDIrREQwzbKjlqh0XIRFRUARRS4rd1huctmdmc/vj+d5Zp959pnb7szO7M7n/XrxYuY7z+U7z+5+Pt/v5yqqimEYhlG+RIo9AcMwDKO4mCIwDMMoc0wRGIZhlDmmCAzDMMocUwSGYRhljikCwzCMMscUgWEYRplTMEUgIkNE5A3fv50icpuIHCYiz4vICvf/6kLNwTAMw8iMtEdCmYhUAB8CJwO3ANtU9R4R+RZQrarfLPgkDMMwjFDaSxGcB9ypqqeIyHLgTFVdLyJ9gLmqOiTd+b169dKBAwcWfJ6GYRidibq6ui2qenim4yrbYzLAFcAM9/URqroewFUGvTOdPHDgQBYuXFjI+RmGYXQ6RGR1NscV3FksIlXAxcCsHM+bJCILRWTh5s2bCzM5wzAMo12ihs4HFqnqRvf9RtckhPv/prCTVHWqqo5W1dGHH55xZ2MYhmG0kvZQBFfSbBYCeBq41n19LfBUO8zBMAzDSEFBFYGIdAPOBeb4hu8BzhWRFe5n9xRyDoZhGEZ6CuosVtU9QM/A2Fbg7ELe1zAMw8geyyw2DMMoc0wRGIbRaupWN/Dgi+9Rt7qh2FMx2kB75REYhtHJqFvdwNXTammMxqmqjDD9xrHUDLCKMfmkbnUDtSu3MnZQz4I+W1MEhmG0itqVW2mMxokrNEXj1K7caoogj7SnojXTkGEYrWLsoJ5UVUaoEOhSGWHsoJ6ZTzKyJkzRFgrbERiG0SpqBlQz/cax7WK6KEc8RdsUjRdc0bZL0bm2Mnr0aLVaQ4ZhlBtt9RGISJ2qjs50nO0IDMMwSpSaAdXtstMyH4FhGEaZY4rAMAyjzCkrRVBRUcHIkSMZNmwYn/vc59izZ0+xp1RwrrzySoYPH859993HO++8w8iRIznxxBN5//33k4774IMPOPnkkznuuOO4/PLLaWxsDL3e4sWLGTduHEOHDuWEE05g3759SZ9ffPHFDBs2rGDfxzCM/FNWiuDAAw/kjTfeYMmSJVRVVfHQQw+1+ZqxWCwPMysMGzZs4P/+7/9YvHgxt99+O3/+85+ZMGECr7/+Oh/72MeSjv3mN7/J7bffzooVK6iurua3v/1ti+tFo1GuueYaHnroIZYuXcrcuXPp0qVL4vM5c+Zw8MEHF/x7GYaRX8pKEfg57bTTeO+99wD405/+xJgxYxg5ciQ33XRTQrjffPPNjB49mqFDh3LnnXcmzh04cCBTpkzh1FNPZdasWfz85z/n+OOPZ/jw4VxxxRUAbNu2jUsuuYThw4czduxYFi9eDMBdd93FDTfcwJlnnsmgQYP4+c9/Hjq/v//974waNYoRI0Zw9tlnp73mRx99xA033MBJJ53EiSeeyFNPOZW9zzvvPDZt2sTIkSP5/ve/z/3338+0adMYP3580r1UlX/+859cdtllAFx77bX8+c9/bjGn5557juHDhzNixAgAevbsSUVFBQC7d+/mZz/7Gd/97ndz/VEYhlFsVLXk/9XU1Gg+OOigg1RVtampSS+++GL95S9/qcuWLdMLL7xQGxsbVVX15ptv1kceeURVVbdu3aqqqtFoVM844wx98803VVV1wIABeu+99yau26dPH923b5+qqjY0NKiq6le+8hW96667VFX1H//4h44YMUJVVe+8804dN26c7tu3Tzdv3qyHHXZY4t4emzZt0n79+unKlSuT5pHqmt/+9rf1j3/8Y+L+xx13nO7evVs/+OADHTp0aOK6d955p/7kJz9JvD///PP1ww8/1M2bN+vHPvaxxPiaNWuSzvO477779JprrtHzzjtPTzzxxKRncNttt+mcOXNa3NMwjOIBLNQsZGxZhY/u3buXkSNHAs6O4Atf+AJTp06lrq6Ok046KXFM795OG+XHH3+cqVOnEo1GWb9+PcuWLWP48OEAXH755YnrDh8+nKuvvppLLrmESy65BIB58+Yxe/ZsAM466yy2bt3Kjh07APjMZz5D165d6dq1K71792bjxo3069cvcb3a2lpOP/10jjnmGAAOO+ywtNd87rnnePrpp/npT38KwL59+1izZg0HHnhg2ufxt7/9DYCwVqAi0mIsGo0yb948FixYQLdu3Tj77LOpqamhZ8+evPfee9x3332sWrUq7T0Nwyg9ykoReD4CP6rKtddey49+9KOk8Q8++ICf/vSnLFiwgOrqaq677rokx+hBBx2UeP3Xv/6Vl19+maeffpof/OAHLF26FA1J1POEa9euXRNjFRUVRKPRFnMKE8SprqmqzJ49myFDhiR9lq1Q7tWrF9u3bycajVJZWUl9fT19+/ZtcVy/fv0444wz6NWrFwAXXHABixYt4uCDD6auro6BAwcSjUbZtGkTZ555JnPnzs3q/kZ50F4F1IzcKXSHsh4i8oSIvCMib4vIOBEZKSK1IvKG25x+TCHnkImzzz6bJ554gk2bnNbJ27ZtY/Xq1ezcuZODDjqI7t27s3HjRp599tnQ8+PxOGvXrmX8+PH8+Mc/Zvv27ezevZvTTz+d6dOnAzB37lx69erFoYcemtWcxo0bx0svvcQHH3yQmBOQ8pqf+tSn+MUvfpFQFK+//npOz0BEGD9+PE888QQAjzzyCBMmTGhx3Kc+9SkWL17Mnj17iEajvPTSSxx//PHcfPPNrFu3jlWrVjFv3jwGDx5sSsBIwiug9t/PLefqabVWtrrEKPSO4AHg76p6mYhUAd2Ax4Hvq+qzInIB8GPgzALPIyXHH388//Vf/8V5551HPB6nS5cuPPjgg4wdO5YTTzyRoUOHMmjQIE455ZTQ82OxGNdccw07duxAVbn99tvp0aMHd911F9dffz3Dhw+nW7duPPLII1nP6fDDD2fq1KlMnDiReDxO7969ef7551Ne83vf+x633XYbw4cPR1UZOHAgzzzzTMb7XHDBBUybNo2+ffty7733csUVV/Dd736XE088kS984QsAPP300yxcuJApU6ZQXV3N1772NU466SREhAsuuIDPfOYzWX8vo3yxSqWlTcFqDYnIocCbwCD13URE/hf4narOFJErgYtU9ap017JaQ4bRsfF2BF4BNetd0D5kW2uokIpgJDAVWAaMAOqArwL9gf8FBMc09UlVXR1y/iRgEkD//v1rVq9ucYhhGB0I8xG0P6WgCEYDtcApqjpfRB4AdgLdgZdUdbaIfB6YpKrnpLuW7QgMwzByJ1tFUEhncT1Qr6rz3fdPAKOAa4E57tgsoKjOYsMwjHKnYIpAVTcAa0XEi2k8G8dMtA44wx07C1hRqDkYhmEYmSl01NCtwHQ3YmglcD3wFPCAiFQC+3D9AIZhGEZxKKgiUNU3gKB9ah5QU8j7GoZhGNlTtkXnDMMwDAdTBIZhGGWOKQLDMIxSY/9u+N2n4Z7+sGdbwW9XVkXnDMMwSppdG2DqmbBrffNYSAHKfGOKwDAMo9hsXAa/Gpc8Nu4rcO4PIFJ4w40pAsMwjGKx8iX4w8XJY+f/BE5u36h6UwSGYRjtzRsz4M9fSh67YgZ8/IKiTMcUgWEYRnugCi/dC3OTm2DxxX/CUcVNrTJFYBiGUUhiUXj6K/DmjOaxqoPh5n9B9cCiTcuPKQLDMIxCsH83TL8M1rzaPHbECXDdX+DA0irDbYrAMAwjn+xcD78ZnxwC+vEL4bLfQWXX1OcVEVMEhmEY+SBVCOh5/9UuuQBtwRSBYRhGW1g5F/4wIXmsCCGgbcEUgWEYRmsosRDQtmCKwDAMI1tShoC+CEeNKs6c8kBBFYGI9ACmAcMABW5Q1VdF5FbgK0AU+KuqfqOQ8zCMYmDN2jsRHSAEtC0UekfwAPB3Vb3M7VLWTUTGAxOA4aq6X0R6F3gOhpF3Mgn5utUNXD2tlsZonKrKCNNvHGvKoCOyfzf86VJYW9s8VqIhoG2hYIpARA4FTgeuA1DVRqBRRG4G7lHV/e74pkLNwTAKQTZCvnblVhqjceIKTdE4tSu3miLoSOxc71QB3b2heazEQ0DbQiF3BIOAzcDDIjICqAO+CgwGThORH+L0LP66qi4o4DwMI69kI+THDupJVWWEpmicLpURxg7qWaTZto6yNWt14BDQtlBIRVAJjAJuVdX5IvIA8C13vBoYC5wEPC4ig1RV/SeLyCTcxvb9+/cv4DQNIzeyEfI1A6qZfuPYDilMy9KsFRYCesFPYcwXizKd9qaQiqAeqFfV+e77J3AUQT0wxxX8r4lIHOiFs3tIoKpTgakAo0ePTlIShlFMshXyNQOqkz7rKKvssjJrhYWAXvkYDDm/OPMpEgVTBKq6QUTWisgQVV0OnA0sA94HzgLmishgoArYUqh5GEYhCAr5THSkVXZHN2tlRBXm3gMv3ZM83sFDQNtCoaOGbgWmuxFDK4HrgY+A34nIEqARuDZoFjKMzkZHWmV3ZLNWWmJReOoWWPxY81jVIW4I6IDizasEKKgiUNU3gNEhH11TyPsaRqnR0VbZue54Spr9u+BPl3X6ENC2YJnFhtEOdNpVdimzcz1MPQN2b2we+8RFcOlvO2UIaFswRWAY7USnWmWXMmEhoJ+81WkE34lDQNuCKQKj5Oko0TZGkXn/RfjjJcljZRQC2hZMERglTUeKtjGKxOvT4akvJ4+VYQhoWzBFYJQ0HSnaxmhHLAQ0r5giMEqajhZtYxSYsBDQrofCl+aVfQhoWzBFYJQ0Fm1jAOEhoEeeANdaCGg+MEVglDwWbVPGWAhou2CKwDCM0mPjUvjVJ5PHLAS0YJgiMIwikyo8Np9hsx0mBNdCQIuCKQLDKCKpwmPzGTbbIUJwLQS0qJgiMIwikio8Np9hsyUbgqvqNIF/6d7kcQsBbXdMERhGEUkVHpvPsNmSC8GNRZ3V/+KZzWMWAlpUpCNUgB49erQuXLiw2NMwjIJQNj6C/bvcRvDzm8csBLSgiEidqoZVgE4+zhSBYRgFxUJAi0a2iqCgpiER6QFMA4YBCtygqq+6n30d+AlwuKpahzKjIJTESrhcsRDQDkOhfQQPAH9X1cvcLmXdAETkaOBcYE2B72+UMR0iWqYzYiGgHY6CKQIRORQ4HbgOQFUbcVpTAtwHfAN4qlD3N4ySjZbprISGgM6EIZ8uznyMrCnkjmAQsBl4WERGAHXAV3Ga2H+oqm+KbQ+NAlJy0TKdEQsB7RQUUhFUAqOAW1V1vog8ANyFs0s4L9PJIjIJmATQv3//Ak7T6KxYwboCYiGgnYqCRQ2JyJFAraoOdN+fhqMITgD2uIf1A9YBY1R1Q6prWdSQEUYhHcGdLnQzX4SGgA53Q0B7FG9eRihFjxpS1Q0islZEhqjqchyT0CJVPds3yVXAaIsaMnKlkI7gsivvkA0718HUMy0EtJNS6KihW4HpbsTQSuD6At/PKBMK6Qj2X7uxs5Z3yJbQEND/gHOnWAhoJ6KgikBV3wBSbks8s5Fh5EprHcHZmGmqu1URdy2mcXXet/c8i46FgJYVVmvI6JB4juDZi+rJdl2arZmmYU8jgpMBGXHfZ3PtMAXT4RzWFgJalpgiMDocntCt7lbFnEX1NEbjzF5Un9H+nslM479u1y7Zr+L9CqYyInxu9NFMHNUvce2S77CmCi/eDS//OHl80lzoe2IxZmS0M6YIjA6FX+hGRIirZm1/T2emCe4WJl84lIY9jYlVfDqTUpJPIaZMn7+GWQvXMmPSuNJWALEo/PlmeOvx5rGuh8LN/4IeFrJdTpgiMDoUfqGLKpGIIGhWK/d0ZprgbqFhTyO3jD8WyGxS8hTMvqZ4YqwxpsxeVN+maKOCmZP274I/ToT615rHLAS0rDFFYLQ7QSGXi9ALruqDK/dMpDLTpNstZDIpeQpmyl+W8mb9jsR4a2NqChZyunMd/PoM+GhT89gnLnZDQFvvEDc6PqYIjILjF/RACxPMlGeWZi30CuV8TXXdutUNfLh9L5UVEWKx1D6DmgHVTL5oKFf+pjahTCaO6tequeQ95NRCQI0MmCIoM9o7Gze4ur10VL8kIffskvU5C71COV+D1w06ga8Y0z/JCRx2/owvtl5J+Z3VeQk5DQsB/cx/w0k3tu56RqfFFEEZUYxs3ODqViFJyJ0/rA8LVm0ryTh7/9xjcaVvjwNDn1dQAbbmmWZyVme6ZxIWAmrkiCmCMqK9snH91w7a3i8d1Y9LR/VLEmJDjjwkpVAr1A4mm+tmkwyWL+Wazlmd1T3797AQUKPVmCIoIwqZ5Zrq2qls70Fna6pVbyF2MNleNxt/RL6Uay4/G/89NdrIwX+9GTY923xA1+5w8zwLATWyxhRBGVHILNd0126tuaRQO5hgLaH7X3iX284ZnFIZZMpNqIwITTGlIiJUd6viwRffy/n55vKzGTuoJ9WV+/mN3M2oyArwgoAsBNRoJaYIyoxCZrnm+9qF2sF41/WUwbwVW1iwaluLnUHWZilxClIocNfTS4jGtVU7mKye38511Dx+BnUVvhDQ4yfAxGkWAmq0GlMERskStkrOVjinO8677v0vvMu8FVtQWu44MpWN8K6/bvteojHHCR6NOZXqwq7XZsJCQE/5KpzzfQsBNdqMKQKjXcnV+etfJWdr2091XPDet50zOGXEUrBsxKPz1yTqGQFJSsLLMaioiIAqsXh2mc5Z8f4/4Y+fTR77zM/gpC+0/dqG4WKKwGg32ur8zdZnEHYcEHrvVHZ5z3y0v8lZ7ftX+UBSWOnlY47mqB4HJgR/Xnwwi/4IT38leeyqx2Hwp1p/TcNIgSkCo91oq/M3W59BUIjv2tuU8t6p7PL+MtdP1NW3yCoOhsQGneOtQhVe/CG8/JPk8UkvQd+RrbumYWRBQRWBiPQApgHDcBZVNwATgYuARuB94HpV3V7IeRilQabqn5lW0tlG1tQMqOa6cQN56OWVqMJDL6/kS6cPytnx7CmJYN4DkN/oq1gT/PnLyVVAD+juNIK3EFCjHShY83oAEXkEeEVVp7ntKrsBY4B/qmpURO4FUNVvpruONa/vPGRThiIf+QL/9tv5vLKiuRX2acf14rZzBpdWg5h9O+FPE6F+QfOYhYAaeaTozetF5FDgdOA6AFVtxNkFPOc7rBa4rFBzMApPPjJ/22IyqlvdkOhS5o/qOX9YnyRFcP6wPm0q/5BXBbJzHfz6dPhoc/OYhYAaRaSQpqFBwGbgYREZAdQBX1XVj3zH3ADMDDtZRCYBkwD697ftcSmS60o+1fFt6T985dRXaXTDNmfV1TPji841rzrZ+Z15dsl6zh/WJ/HeOy9bwZ7X3cqGJfDQKcljFgJqlACFVASVwCjgVlWdLyIPAN8CvgcgIt8BosD0sJNVdSowFRzTUAHnWda0ZbWb60o+ncO2NTb32pVbaYo1/2oE53DVyf2TFID3fXMR7HnJbrYQUKPEKaQiqAfqVXW++/4JHEWAiFwLXAicrYV0UhhpaetqN9NKPqhk0h2fi9nGMwdt2bWfighE3cZg2ewmchXsbcpubkMIaEE7lBlGgJSKQES+CMxV1RUiIsDvgEuBVcB1qroo3YVVdYOIrBWRIaq6HDgbWCYinwa+CZyhqnvy9UWM3GnrajfdSj6VkmlrtE3QHNSlQjj3+N70PqRr2l4BHrkK9pznnIcQ0EKWCzeMMNLtCL4K/N59fSUwHDgGOBF4ADgti+vfCkx3I4ZWAtcDC4CuwPOOfqFWVb/UmskbbaMttvmw+vv+8Vzj9rMlaA6KxpSRR/fglvHHUre6gTuefKuF49hPa5RRVnOONbmN4Gc1j7UyBDSTgrbdgpFv0imCqKo2ua8vBP6gqluBF0Tkx2nOS6CqbwDB0KXwIutGu9MaoZhUg6ciwmU1Tn8BaNmCslAF47pUSGJHEHErfgZ3Co8vXMvnRh/dItnL+955E6AfbYGffCx5rM8I+PenWx0CminfwnYLRr5JpwjiItIHaMAx6/zQ99mBBZ2VUTIEV5/BEs4z5q9hzqL6Fi0oG/Y0JikZoFXlmYPUDKhmxqRxPPTS+/zznU2oKlOeWcrEUf2SHccxTcwtG2GZ8yp783J4cEzy2Ccugkt/1+YQ0HQKupDNhYzyJZ0imAwsBCqAp1V1KYCInIFj5jE6OJlWl2Gfp6rBE2xB6Tcb5XsVWzOgmpFH9+Afb29MCESBpJ0CZF8FNKf5hUUA9Rnh+ADyGAKaatdSyOZCRvmSUhGo6jMiMgA4RFUbfB8tBC4v+MyMghNcXc5ZVJ9y9e8J1FvGH8v0G8cyZ1E9sxauJRpXRIRhfbuHlmII3md/U/pGMNkSFIgTR/Vj4qh+iWiiucs3ZV0FdPai+oRiS6k4FkyDv/6/5LFR/w4X/6LV36E15MPhbhhB0kUNTfS9BmeBtQV4Q1V3FX5qRq7kat7wC9OKikhCsAdX/2HtJ2sGVHNI10qmvrKSaNwxz0y/cWxon93qblXE3YW64jSCmb9ya4sa/7mQSiCGOa4z7QaeqKvH20dURCRZcfztP+G1qcknffoeGHtzznPOF/luAGQY6UxDF4WMHQYMF5EvqOo/CzQnoxW0xvziF6Yfbt/LY6+tSazaZ7t2/0tH9WPzrv2As3L2zqtb3cC0eR8kBHxjU2oTTMOeRpweXg5Kyxr/rVUG6QrPZXPN2pVbicacRAQBPjf6aKcR/LRzof615IOtDLTRSUlnGro+bNw1Fz0OnFyoSZUzrQ0NbK0T0W/Hf2LhWhpjTsvFxxeu5Ym6eqKxeELYAzyxcC0zJo2jduVW4r5cwEiafr1jB/Wka5cIjU1x4pBQCgXp5JUj/l1Pt8oYP3jrLHizMemYGTWPMXi4RefkEwuBLS1yzixW1dUi0qUQkyl32uJUzdWJGJYL8LnRR/Po/DUoEIspMbcPr5+mmCbO8/r+RkS48dRjmPLM0qTw0YY9jYnrezuP6m5VLFm3I7TGfzGoGVDNzGuOY8SMGmcg3vzZm1cs4PLp79P4f3GqXqu1UM08YSGwpUfOikBEPg7sL8Bcyp5Mztt05OJETPWH6Dlbm6JxKiICIi12BF0qpIVwDyaQNTbF+d5TS4i7zlqvEJx/Tn7HMuQntDRn3BDQEcHx72ygbt0+7n/h3SQncqqfh61uc8NCYEuPdM7iv0CLBeFhQB/gmkJOqlzJ5LzNRhlkaxfPpvibd2x1tyqWrtuBQlKCVvB+3twVp4UjOLkGcxbVp0zqysfqsG51A3MW1beYX0pCQ0BHwqS5IJI0JwUiEPrzABIdzKIxW91mi4XAlh7pdgQ/DbxXYBuOMrgGeLVQkypXUjlvW1Oj3xPgfvOMRy7F37K9pzf32Yvqeey1NUmf+VcT6RLUWrM6rFvdwJW/cYQ2NPswQq8RGgJ6LVz886Qh/5wiAqcc24ujD+uW9POYvaieOb6wU7DVbbZYCGzpkc5Z/JL3WkRGAlcBnwc+AGYXfmrliX+lPMcz01RE+HD7XupWN2SVIeutZuPqOGa7dkleqfqFdrYpUJmUi3ddr7m7R4WQKEHhzW1/k2N6mjJhWMbV4aPz14T2FPCYvag+oQSg2YeRNLccQ0CDc7rtnMEAiZ9Hl8oIAokdAzjPOdXq1kxHLbEQ2NIinWloMHAFTsG5rTgNZERVx7fT3MoaT1h7iVuPvZZduQT/ahbSR+bMcYVophBOvwBXnFVyKjNI0Ik8ZcKwxDG1K7cmrhGNK5OfWsLMm8alXB3e87e3eehlJ4nd6zYWbDDzRF190v09Hwaq8NvzWoSATop9k5tuvDntM0y1Yg2azfz+lFQ5EeYYNToC6UxD7wCvABep6nsAInJ7u8yqA5KPVV9YJE/tyq1E45qV6aRudQMfbt9LZUUk4eSN4CRJrQvsKLIxyXjzWbd9b9LqN9056bb9Ywf1pCIiRF0tFVdNZCuH3XvqK8mVTGYuWJOkCII5AMP7defOC45l1CPHQbwp6dzzG+/h7Xh/KgRGZGG+CVuxBseyMW+YY9ToCKRTBJfi7AheFJG/A49B1paEsiJfDs9MbRzDBHrY+ZUR4cox/Rnat3siVHPGa8nJW9k0lfFfr7IiQjTq5AFEJH0TmFTb/poB1dx46jFMfWWla38Xdu1tCo0YcvIUks9fum5H0nf3f4felbt5astV8IfATb++grqtXfhgWi0Vml/nZDbmDXOMGh2BdD6CJ4EnReQg4BLgduAIEfkV8KSqPpfq3M5GptV+PlZ9mSJ5vOiUoEAPOz8WV/r2OJCrTu7Pgy++l9gd7G9qjuAJW7mn6icQiyuXjzmao3ocmNZHkM1z/P2rq/Dy0GJx5aGXV4b6Maq7tazgqUrSs60ZUM3sy3oy9MlzW97sOxugi1Mkt+ZgEma2fLTDy6V8Re3KrS1yKgyj1MiYR+A2m5+O02DmMOBzOC0nMyoCEekBTAOG4ZirbwCW4/gbBuJ0O/t8oKhdSZHNaj8fq75MkTyeGSSVskl1fnW3KqdWlDrJYbMWrk3Ysv1mouUbdrVICPNfL1VYZi4mMU+5+EtNeP8Hv1OwLEULZ+x7/4A/TWSo7/qL48dwSdN/UVVZwfR1+6gZkFwtfdbCtTTFNG1kUabvk+3uz3wDRkcip4QyVd0G/Nr9lw0PAH9X1cvcLmXdgDuAf6jqPSLyLRyl8s1c5tGeZLPaz0c4XKZrpBL0fsEVdGbe8eRbTgavz8YSiyuz3cSoXXub3HpBSkSEWFwTQjnYTyCVsLty6qs0xZQuFZI6bDPwHbxSE36Cxd68shRe1JTXAKdm02x4ODkEdEZsPN9u+mLifdjPafai+kSJ6saY8wyCSWHZ5ARku/sz34DRkShY83oRORQ4HbgOQFUbgUYRmQCc6R72CDCXElYE2a728xEOl+4afkVR3a0qdBXvVQz1BFqTb/UNzqq6IiKhNYRUlYqIoKot+gmkIky4AimVh/87vLF2Oy8s24jiK/aLy1gBAAAgAElEQVQWyF9IUkRLfggP/yZ5Ap++l7o+l/P9abVESO+/CDq3/O+DUVGQukXkuu17qYxIxhLX5hswOhIFUwTAIGAz8LCIjADqcPogH6Gq6wFUdb2I9A47WUQmAZMA+vfPredrPiml5Bfv3p7JISLNETiNTeFJTtBsVrmsph8CzHATo/x4cf3Z2LK9XciWXcmVRrbs2p/RHOLPk3hlxeakfgItvm//HtQ8/zl4aUHyB1fNoq7rSc7PBJIUZKr5TxzVj1l19aH3C5qsoKUyCbbovHxMeBtM//csld8bw8hEIRVBJTAKuFVV54vIAzhmoKxQ1anAVIDRo0fnw8fXaoqV/BJmr/abHNRX/TOOI4hbJDlVJMe4eyYQf8KZpwSuOrl/4p4QnlXsF4iRiBARJ5y0qjLC4Yd0zdocklZQRvfD3Ue1CAHl5v+DI4by6Pw1TH7qVWJxTTiZw/ogBO8344vhzvHqblVURnx9kAUmXzg0tTM+FueoHge2KgTVMEqRQiqCeqBeVee775/AUQQbRaSPuxvoA2wq4BxKjlwiTjKFk4rPri/Axp37qKyIEIulTnIKmpj8K+hU90wVTRR3BWdlRLjroqEMOfKQRJJVNuaQFoIyrBE8ThXQeesjjN3XE1Y3MPmpJUk7oVxLboc935FH9+C1VU7MQlydUFU/ZuoxOjMFUwSqukFE1orIEFVdDpwNLHP/XQvc4/7/VKHmUGrkEkmSTWG46m5VCR9BXGFx/Q66VAhXjOnPxFH9HB/CX5ZyxKEHcNMZH0sKu8zWwQkkzdmLJvKbn1SVhj2NoX4M735p2fQO/DLY3kLcKqB7k+5/6ah+LfogBE042Sja4HfdH012Xwe3oKVs6rESFkZbKeSOAOBWnLDTKpyG99fjJLs+LiJfANbghKOWBblEkoStQP1/8N7qfPKFQ3l2yXrmrdiSqPrZt8eBLN+wizuefMu92g7+8c5GHr/pk1lF9fiT1/xmJH800Wy39EVTTEEkEfcf9GOkUnh1qxtYsfhVrqi7Mmn8o17DOeiWlxON4GtXJt9fIbSEhWfymrVwLdFYcvnrbJ7v5Sf15+31SxIRUJeG+SxK0NRjYapGPiioIlDVN4DRIR+dXcj7lirV3aqIuDH9mcwLwRUokJTp6/UK8FbpC1ZtS1Ia97/wbtL1YnFahEyG3XPyhUOZuWANy9bvZMZra6isiLSIkvGu8fiCNe61lbv+spQhRx5CzYDqJId1mMJb8X9/pua5a6nx3fvx+Fl8q+lGqjZGmL5me2j2sJfPMKxv96RCdGFRP6nKX6d6vjUDqhly5CEdbmVtYapGPij0jsBwqVvdwJRnljox+xFp4YwMw78CffDF95r/4GNOo8dgzL+/muj5w/okCrV5COnNCN4c/QI1GotzwlHdGXZU9yR/Q+3KrcR81hRPCC3fsIvHXluTOD/JdLPoD/D0rRznu+cNTd+gadA5/Ou9LaHCLCiwl2/YxeSnlhBXZcGqbQnhHYz6gZbmnXTPN+x9R8B8F0Y+MEXQTvhXboJjU8+FoNkGkYRTeN32vSzfsKtFNdEvnT6IX7+yEnWjeob27Z60qwg6k8MEqud7WL5xV1LIpVNADjzTepfKCNXdqpj81BJivgvEYnF2/fV7sOmPSd/ns7EfsTg6gC6VESYP69NiR+PHE9CPzl/Dd//8ViL0tdFVGv6Kp14kVCrzTmejlH0XRsfBFEE70daVW5ipyF9/SCAhgL1V9bcu+ATnDj0yNOKnMaZMn7+GWXX1CVv62EE9qayItEhES2XiiUQiEHeU0Q2fHMizS9YnonkqifKzLr/i4opXE3FhDXowFzXezZcvGc933QgjAYYceUhWWczOTqB5LCIt22a2pRZSR6Uj7mSM0sIUQTuRrzIUwdj2oNCG5HIN/nOeX7qhRSJZYzTOlL8sZfJFbtUebdmwPnhN795eCWiNa6JUxcHs4U9VP2Jk5P3EsW/GB3FN4x3sohsA33tqCT+YMKzFDiZdLoBTjdQXLSQk9TowYWgYrccUQTuSi7DKpiNYdbeqFkI7rFyDd71p8z4Ivdeb9Tu4elotE0f1S6zow64JzU3mg/kMvXULf6m6g56yK3He4u5nccWW69kTq0i6XjyuPLtkfU5OzrCGN2EdywzDyB1TBCVIth3BGvY0JjJ7PRQY2rd7i2sGV9RBmqJxBHzCHRSn9lDQv+DNZfqNY0NDQNcP/zJ9Pns3w0U49Q8LeW7ZxqTPu1RGOD+DXyBIKdrC8xW/b3kARrExRVCCBJ226UpPV1ZEknr2AiwJZMX6i6U1xZozkSMRcQS+Gxo6cVQ/Jo7ql+SH8F7PDoSEfrjwL1z81q1JIaAvHvcdDj31i0lz7HVI16S5HNv7YO69dHirwjVLyfyTr/h9ywMwSgFTBCVIsFyzVw8oLIP2zMGH87xbxdPjibr6RMSMv7RyRUQY3q874wb15JADuyTCMb2YfL+93bsHwPINu3iizmnq8vmKF/lxl9/AW833W3Huwxx3ykTCmllfOqofT7iJZ10qJKEEvPvkaiorlVVzvuL3LQ/AKAVMERSAoNDyv4fUZZr9506+cChL1u1IZMp6mbbeMUmF39yEL49YzEmmmh2oRBqPaSIUdPqNYwESJSq8mPywWjyg/L+Kx7nlgORqIBc33c1bsYF0+Zswo1/L9pngFnubNK5NQrwUV835it+3PACjFDBFkGeCQmvyhUMTwlZEEDRRrTMo0ILnXjqqX6KoXDQa5/4X3uW2cwaHFn7zU1ERQSFlkpW/jlCq1Wjtyq3Eo408UPlLLqqoTZzfoAdzwf4fsZ5mgRXW6MVPppV/rq1A57iNdYq5O8iXzyLsOqW2+zE6P6YI8kCq6pxN0XhSdAw+Z22YGSB4bqKujmsi+td7W1iwalto4TcPgUQ3r3S9CbyVZ9hq9PUVa7j4tWu4pevbifPejA/ijoN/wNJtwRYvzddt7bMLJrkN7ds9KVIqKZmuIuLskuJa9N1BvnwW6aqilsLux+j8mCJoI2E7AL9wHdrnUP713pYW54mvUJtHWF2dS0f14/4X3k0qwdCwpzFRE2jpuh3E4yS6c3k7CW+l6fcRiAhnf7x3UiXSpNVojz1Ef3QiJ+5vbiH9TGwcX2u6GSq6cOaRvXm7YWOLXISqFI1lgs8pbJUbluQGtGho783zw+17ecxtrNMZbermMzCKgSmCNlK7cmti1b2/ydkBTL5wKA17GhNloj3BKUBFhaAKcVWmPLM0yS6fytxw2zmDE6GWFRUR3li7nZfe3Uw05nTL+vxJTiE2757+8s81A6oZ1re7U/ohrry8YjM3neHU/PeE8/geGzn+YSf+x/uFeDB6MT+LXc7lYwZwGY4D+oW3NyLu91CcMg7jh/RuERkUJN0q1yvEFwxtDWYze//qVjcwJ4eeB/45ZGtuKaZpxnwGRjEwRdBG/EldCryywjHfeALdC+2MAKcc14v+h3VLtIoMW/GFmRs8BTFnUT0zF67leV9cvtcta8iRhzBnUT0PvPBuC7PJknU7Er6G/W5LS4BfTfs10yp+lHSv1Z+8m0+9MoimWPOuxMsA9lMZEb5wyjH8/tVViUqfYY1sagZUp+xzMMctGx1XJeLallSd5xghvPdwa2zzuZhbim2aKcV8CaPzY4qglXjC7sPte1skdXnCLri6u+2cwQBpu3ilWo165Z2jPuewZ++v7laVsvk6kAj9BEfIyqI/UPPmVKb5En6va/wGtRWjmD5kLNOHkKgDtHzDLma65ab9qCpL1+/M2Mhm+o1jWzyHsPl6VEaEG089JhHemioSKRcBmYu5pRRMM6WUL2GUBwVVBCKyCtgFxICoqo4WkZHAQ8ABQBT4sqq+Vsh5ZEtr2kh69fqjcScayL+STbW6S7Xiy7QaDQrN4f26M/mioS0S0PwO4eaaQMp/Vs7klsqnk67xmf13s1QHAlChzQrM2wX4i9nhu35FRDiwS0WLXgVhgvSW8ccmfedUZaPB6W2wa3+Ub13wiZTPP1dSmVvCft6ZGgKZgDY6I+2xIxivqn5v6Y+B76vqsyJygfv+zHaYR1pyMQn4o3FisThXjOlP3x4HhtYFCkaEeAIlWGCtbnUD97/wbtrVaDA5a/JFTk+D5Rt2OQ1vUCorIomooZoB1RBv4hddfsFnIq8mrrNND+aS2D186aIzGLFuByvq6onF4qHCPEhFBM7++BHMfXczL7y9kcqKCJePOTpxP/9c/EI3uMr1RwGdOfhw/rl8E1E363nWwrUtei23Bv/zDgvRDPt5B5U3ZO62ZhgdnWKYhhQ41H3dHVhXyJu1todtKpNA3eoGZi1cm1jNVlREWgitutUNieJs2YQFhtUWSuUo/Nzoo1FICF6vmUwsrlS4TeSvOrk/7NsJU8dTs26Rs00BPug6hAk7/pOddKNCnFpFd3/2hKSOX96cUoWnnvXxIxhxdA9eeNuJHorG4qzdtifxPfxzSdV8J2yndMeTbzFjvtPQJhrXRM5Ea4Vu2PP2K990P2+/0kpqCGRRPEYnpdCKQIHnRESBX6vqVOA24H9F5Kc4IuqThbp5Lqv8bKM1alduTVTo9GL2szHvpGvf6M8AjgAnHNWd3ocewBzXqRu2gvVKSPjNLKpKY8NauPcM2LutedJDJ8LEqWyr303jtFoqAmaPsOxiL/TUC9X06H1I1xaNYOa5DvKJo/olzSXYfCeolIOlLLpURojGkq+ZygGdiUyKPduft0XxGOVAoRXBKaq6TkR6A8+LyDvAZcDtqjpbRD4P/BY4J3iiiEwCJgH079+6csO5OP6yjdYIi/XPdE+gxS7Cb6f2O3MjEVi6fidv1juF47zGMam+izefY2MreabqDqj1TebUr8HZkxPlKfzRR979UmXtVner4qgeBzLptEH8Zt4HxNxIpIm+HIX7X3iXeSu2JJSbv3qp1zmtbnVDWlNMMKHshKO6s7h+R4sM6FzMM/4ie37/RWt+3hbFY5QDhW5ev879f5OIPAmMAa4FvuoeMguYluLcqcBUgNGjR2dqPxtKrqu5bKI1UgkGf/+A4D3T7SK85jIeQ/s6gtDDE4b+xvdJQraxjncqrgB/yf8L74fR16f8DrNcP8PjC9Zw1sePSAhML2vXq1DqJaj9YMKwUN+HP7/BX730oZfe55/vbGLGa2sSTWdSKTL/eCyuHHHoAVREdhLP4IDOJvwz6L9ozc87l+MMo6NSMEUgIgcBEVXd5b4+D5iC4xM4A5gLnAWsKNQcCrWaCwqGutUNXDn11YQT966LWwpOf1OVYb5+AcHmMsf0Ooi31++k0Q3V8cItvcb3Tp8AiC18hJo3f5M8satnU1dV43xf19wS/O6zF9Unrh2Nw3PLNlJZIQxzzVH/eLu5kmlcYV9TnJkL1iQc08HnEOaEffGdTYkieMG+wkGlHOzFPPfdzU5eQcDH4LXQ9O+mwkhSLG6OhQlxw0hPIXcERwBPimOWqAQeVdW/i8hu4AERqQT24Zp/CkU2q7lc7c/B4/3CtTGmLFm3g7s/e0LSsdeNG8g018Tizyhu2NOYyNQFeGbxeqZMGMaSdTsQSPQHcIRbeAgoN70CfYa3MLMgQjTWbE4BeM01tfiJulVJKyt2JmUOe7xZv4Mrf1Ob6G2cytYPjiD2V0JN1VfYn/3sja/bvjeRbCc4PgYvkzged3dNaZrrQO67QAsNNYwCKgJVXQmMCBmfB0n9TIpKrpmkYccHC65JyLERkUR2b9DGX+HmIYBTesKL5kmQIgR0Yvwe/vvGC6np02xmSphQYgo0388rS72vKTlD2EPxznFyBOLx5N7Ffnu9f/czY9K4Fk7Yrl2cQnmRiLToKwzh9n5PwfiT7cISz2JxzYuvJ/jzyTU01BSI0Zko+8ziXDNJw46fOKofs+rqk2zlwWPVV0YhGF8/ZcKwRC0gEeGNtdupW91ApHEnfZ++gppdyxIhoG/EB3Ft0x0MP7Y//+1mKvv7CDebUMRRPm5+gFeW2k+XCmHk0T2oW92QFBkUi2sL5ebNObj78Zef9vdSSNVnOVPYZrrEs2Dl1FRka9NvbRZxsctQGEa+KXtFkKspIdXxl9X0S5hy/FmqXitJBSQifH50S+flkCMPYfzHe/OPtzcSiytLli3jmPcncJjsThyz7LBz+Pzm69gbjSSVqwhWPvVMJxHgrouaBTKQKNZW4Sv37C+K51HhtrCMxRyfxDmfOCJRsdQLafUI2/2kE46Znne6xLOkRLk84A+DDasGm4pSKENhGPmk7BVBrg7lTJmn/nLMNQOquaymXyJRSuPawnnpTyb7hKzibwfckXS//4lO4KfRz1O5IcKUQPROMNlp5oI1iYifWNwxMfmTqILf0zvfwysdceOpx/C7f31ADKf2j79sdTa7n3TCMZfnXejQzZoB1Uy+cGhiNxasBpsKyy0wOhtlrwgg9/DAVJmnjU3xFhmxXvXOVEKjduVWTo69ziMH3Js0/p3ojTwWPzvheI2HCHa/KUgiwrL1O5tzFQI9jsO+Z7Dhi7fi9sJdPYUSNN94eQ2ZavR4hDmXs33ehQ7dbNjTSFxb+m4yzclyC4zOhCmCNpIwLwS6iKWqXZMkNOp+zy0vfZVbfBaJaxu/yTwdwQ8uOYEfAJOfWkI8rlR1SbHydE1BGle8tb3glKIICii/QPaa1l83bmBopc9czDfeWKr8inTO5WLT2tW95RYYnQlTBFmSrjy0l2Xr7yKWqnYNqvCPKTDvZ0nXv2D/3SzzqoC6dYBuGX8sQ448JOXK079yV4VIxOmJ7DfZ+BPdvFISEXFyCMDpn3D3Z08I/U7pVrxhzyNMOKZzLreWfEbs2OreMEwRZEW68gieAAlm2bZYWcaaYM4kWDonMbS3Sw9+d/wj/GT+R4mxYGSMPwPZ/x5armaD0TrB8NW4OqWyg87hqS+/38I2nm7Fm0vUTKrQ2tZSiIgdW90b5Y4pAh/Z9NVNV/8mdGW5bwf8YQKsez1xvS3dT+DT225n2/4DqFq0ly+dPoil63cytM+hSWYaL67e6zkcpoSyDdXEzdYVnEggfyTpqq17uHpabdZCtXZlc3vOxqb0dvVUzuXWYhE7hpF/ykYR+IU8tCy9EKxR4w9VDLMjp2rAkhBKO+rhoVNhb3Mj+L/GP8ntTV8ivrlLUuvI97d8lMgDCM4nVdexYNiof8fgZeNu3rU/qfCaX2k8v3QDjy1cy/Y9TUnXDsscDuIvixGHRN5DqiihMOdya7GIHcPIP2WhCIJCHtUWfX39gr0xGmfG/DVJfXjDVvuhAmn9Yvj1aUn3fzA6gfvilyfMMhGc5LKY25/3+WUbeWHZRrp2aTmfVF3H/JFKk59aQlw1oRTu+svSRFhoZYVw+Zj+ieb23jx//+oq9rtZxv7+B9mYXhr2NCa153xh2UZeWbE55Y4in6YXs+kbRv4pC0UQXL0DoaUe/M1Ygp8HhVkLgdRYB3ddmnTf70Rv5NHoWYk+A35n7unHHc7zy5oLvKWajz8BzJ+s5n0mPtt/UzTOs0vWJ1UzjbmOWs9R7PUy8JRMBDjl2F6cP6wPtSudHsyZTC+ZnlWhMZu+YeSXslAEwXh5VEPr1E8c1Y8tu/Yzd/mmlHXs/dQMqKbX8hkMePjbSePXNX6TufERRKS5bk8k0JQd4OUVmxNC19/r2Lt2MHHNqxd06ah+SQXcpjyzNLEzOX9YH+Z/sC2xI+hSIQgkCXcleTdz/rA+CUXh9WD2uoz5ewr4v7fX12DWwrVJz8ofpZTKd2EYRmkhmqGaYykwevRoXbhwYZuu4TleBafmf7D0wqyFaxPmonQOWCBlCOjfTpnFEYNP4upptQkh61Ud9Uw3wRaVQaEJzg5m194mlq7fyfnD+gDwPTf7FaAqEI8fvE51tyqWrtuRaGkJJM3Jq0Tqr+nz388tJ65O6OoVY/qjEOqkDnuuwSxrf8tNq8VjGMVDROpUdXSm48piR+AxZ1F9ku0bCHXIBjN4E8SaYM4XYemTiaGteggX7r+b9fSk8sUoMwcnl3KoXbk1yXSTMr+AZl+Gv0LoKyu2tCgL3eTG4wft5Ols+2F29VQJZF7pa69tZKaSEcEsa38/g3yZjDwHuKfc2kOxWIVRo1zo1IrA/4ecKgQ0q+qWISGgHFXD62c+zOceXkqU5jIQLaKHSJ+l65/r/S+8m3Dg+gnu2SoiLVfrmcIq0+Uj5OQMT0Mwy9rvhG4LdasbuPI3tQlz1xML1xY8Q9kqjBrlREEVgYisAnYBMSDqbVFE5FbgK0AU+KuqfiPf9w7+IU++cGioYEtb3TIkBPQvsXF8m1t45LxTqRlQzZQJkrYMRLrSC2GF6zIZ6o49/CAGHX4wzy3bCDjhp/e/8C7nD+uTVnBnEmwZneFZCEH/Ofn0EdSuTG7n2RRL35MgH1i+glFOtMeOYLyqbvHeiMh4YAIwXFX3u43t807wD7lhT2NoW8WJo1qWjw4LAV3Y/wt8fsVZxFWICInicled3D9tGQhIbQIKRvE4nbmcXvNxdVbUpx7bi5dXJB4f53ziCKbNW5l4r8C8FVt49f2tSc7o5Rt2JRTEVSf3b5Vga010TiEiesYO6kkXt1w0OA7wQucPWL6CUU4UwzR0M3CPqu4Hp7F9IW4S9ofsF1KeuSEp43XFCzA9OQSUi34ONdciqxuo+qA2IUznrUguLgfhZpcwgkI5GMXjd1bXrtzKKyu2JEI9l67fSSxgPVIgGlemzfuAmTeNY/mGXdzx5FuA42PwP4/Gptxq77cn6eo5zfji2ISPYFjf7lk/69Zi+QpGOVFoRaDAcyKiwK9VdSowGDhNRH6I07P466q6IN83zvSH7DmOASbqC9Q8fEXyBa6eDcedAyR33np2yXrmuYI5XbmJdIIjqKQuHdVc/jlsrv7Wj0P7HMqr729JKhHh4fkovDl5PLtkPVed3D9Rez+u2dfez5XWOlizNV21p+3e8hWMcqHQiuAUVV3nmn+eF5F33HtWA2OBk4DHRWSQBuJYRWQSbmP7/v37t+rm6f6QVZVvVD7Gl1M0gvcI8zUEi8sFV/hhET3BeWWK4vEf6xfgv391FWd9/IiEjwAcExJKwkexa29TYicAMLTPoUBz7f1C2b3bIqT9z3B/k9NjOexcs90bRv4pqCJQ1XXu/5tE5ElgDFAPzHEF/2siEgd6AZsD504FpoKTR5C3SUUb4clJ3L30ycS336aHUH/5cww//vgWh6fzNVR3q0r8788C9iJ6IuI0b7/qZEeReatlf45ANkIsKMAPP6QrB3RJXXU0uCN44e2NvLpyK+MKbPdui5AOtvWctXBtst/Gd5zZ7g0jvxRMEYjIQUBEVXe5r88DpgC7gbOAuSIyGKgCtqS+Up4ICQH9qNcIHh3yAKMGD0wpsFL5GqBl4beGPY2s276XGa+tcesKKZOfWsKQIw9JHB/MEViz9aPQxjB+E0twDhNH9UvE+vsd354CGDuoJwd0iSSE8nubnTLXb9bv4EunD8p4v9ausNsipGsGJLf1DHZG8x9XTNu95RYYnZFC7giOAJ4UEe8+j6rq30WkCvidiCwBGoFrg2ahvBISAsqwy+CzD3FQRRe+mOH0VIInbKdwy/hjqVvdwMwFa4lrcm4BkNQf2GPqK04EULDEdDYlrv2Ob3/fgSkThiWa5fhNROA4m//4hZOTxpKK8oXUNcqW1ghpv2DN1NbTf59iCGHLLTA6KwVTBKq6EhgRMt4IXFOo+ybxzt/gsSub35/2dTjru058Zg4Eo42C5qBgjaApE4aF5hb4C7V5qLYs2paxxHUA//HeLmTmTeO47ZzBLRzLXsmKVOc3xpRH569htlt51fu8ECvgbBVeqWD+CaOz0qkzizngUJAKuPA+qLm2zZcLcxyHJU2lyi3whJznIxja51B+/+qqFsokVxPL2EE9Ex3IwDGreHkOM2/6JA+99D6bdu7j8pP6c9XJ/VuYN1JVE/WK3GW7As51xZyrwis25p8wOiudWxEMPBXu3Ja3y6UyB4URZr4IGzt36JGhJp9cVsb+XYjX8OZf7zXnOfzm35trTqUS1tNvHJvohhaLOYJOIacVcK4r5o4mWIvtnzCMQtG5FUGeCQqu6m5VPPjiexmFQioHYzrHY6528CFHHsLnTzqapR/u4K0Pd6QUxqmEtffPn8+wfMMuIiKgmUtyhz2fTMd3RMFquQVGZ8QUQZYEewR7fQAymUHSNb7Pl+Mx6OytrIgQjYVnEGcS1v7ErSnPLCXu9jqefOHQrOZ36ah+OVUITSdYLULHMNoHUwQpSFVnvyLiROU07GnMygySagWezoySqwD0XysWV876RG9efMdprhPMIM52Fe6/pqA07GnM+LyC9ZPagkXoGEb7YYoghKAQmjiqX8KRGo07UTlTJgzLygwSZk76zpNvsWnXfiorIgl7vHd+awRg8B69D+lKXDW0hWS2SiZXM0++I2osQscw2g9TBCEEhZDgtJyMuh3C4qqh1UzD8K/Aq7tVJTeWjzjdwPwx+62tEhpsazk7JB4/FyXjXdMr9JaJfDt+O5oj2TA6MqYIQghW6hzatztTJnRP1PupCqlmmg7vuAdffC+prn407oRq+q8RbFof1jM43T08vCggf8ZEa5SMFz46x80ryEbh5cOm3xEdyYbRUTFFEEKw0NuUZ5Yy/caxzLxpXJsEU3W3KtwgnAQzF6xJ6ocQDOWc8doaZtXVt2yakwVehVUvOazQ5p58R9RYhI5htA+RYk+gVElVqbO1CU/NUTjJ47E4PDp/DVdPq6VutVMGo2ZANUf1ODDRM7gxGmdG4JhMpBLi028cy9fOG5KT76EiTy0nDcMoTWxHkIJ826g9wRxGmFM3VbZvtk7TVPPPZZVdTPOMhY4aRvthiiAFYQ7YbJLHgoTVJqqoiHDG4MMRYO67m1tEDvnvP2dRPbMWriUWzy6pK9X8WytMi2GesdBRw2hfTNRLfyUAAArGSURBVBGkwZ9c1RrBlE1tomyyi4Mlp3Odf0fDQkcNo30xRZAFrRVM2dQmykZYF2tVXizTjIWOGkb7YoogC1ormPIp0AolmMOuW2zTjIWOGkb7YoogC1ormPIl0AolmFNdtxRMMx3VrGUYHZGCKgIRWQXsAmJAVFVH+z77OvAT4HBVLXyryjbSWsHUFoHmrdbXbd9bEMGcSuCbacYwyov22BGMDwp6ETkaOBdY0w7375CEVRQNiy5qC+lCTEvVNGNhpYaRf4plGroP+AbwVJHuX/IEK4pePuZojupxYF4FYDqBX4qmmWL7Lgyjs1JoRaDAcyKiwK9VdaqIXAx8qKpvSprewSIyCZgE0L9//wJPs/QIrtZzLS+RLaUo8FNRCr4Lw+iMFFoRnKKq60SkN/C8iLwDfAc4L9OJqjoVmAowevTobApgdipK2TxTLMx3YRiFQVTbR8aKyF04TuNbgT3ucD9gHTBGVTekOnf06NG6cOHCgs/RKH3MR2AY2SMidf4gnVQUbEcgIgcBEVXd5b4+D5iiqr19x6wCRneEqCGjNOhIpizD6CgU0jR0BPCk6weoBB5V1b8X8H6GYRhGKyiYIlDVlcCIDMcMLNT9DcMwjOywfgSGYRhljikCwzCMMscUQRuoW93Agy++l3XXsM6KPQfD6NhY0blWYlmuDvYcDKPjYzuCVhKW5VooSnnF3Z7PwTCMwmA7glbSXlmupb7itmxfw+j4mCJoJe1VAqLU6+tYKQzD6PiYImgD7ZHl2hFW3JbtaxgdG1MEJY6tuA3DKDSmCDoAtuI2DKOQWNSQYRhGmWOKwDAMo8wxRWAYhlHmmCIwDMMoc0wRGIZhlDmmCAzDMMqcdutZ3BZEZBewvNjzKHF6AdbyMzP2nDJjzyg7OsJzGqCqh2c6qKPkESzPpgFzOSMiC+0ZZcaeU2bsGWVHZ3pOZhoyDMMoc0wRGIZhlDkdRRFMLfYEOgD2jLLDnlNm7BllR6d5Th3CWWwYhmEUjo6yIzAMwzAKRFEVgYh8WkSWi8h7IvKtkM+7ishM9/P5IjLQHe8pIi+KyG4R+Z/2nnd704bndK6I1InIW+7/Z7X33NuLNjyjMSLyhvvvTRH5bHvPvT1p7XPyfd7f/bv7envNub1pw+/SQBHZ6/t9eqi9595qVLUo/4AK4H1gEFAFvAkcHzjmy8BD7usrgJnu64OAU4EvAf9TrO/QAZ7TiUBf9/Uw4MNif58SfEbdgEr3dR9gk/e+s/1ry3PyfT4bmAV8vdjfp9SeETAQWFLs79Caf8XcEYwB3lPVlaraCDwGTAgcMwF4xH39BHC2iIiqfqSq84B97TfdotGW5/S6qq5zx5cCB4hI13aZdfvSlme0R1Wj7vgBQGd2mrX6OQGIyCXASpzfpc5Km55RR6WYiuAoYK3vfb07FnqM+8e6Ayi9Xo2FJV/P6VLgdVXdX6B5FpM2PSMROVlElgJvAV/yKYbORqufk4gcBHwT+H47zLOYtPXv7RgReV1EXhKR0wo92XxRzMziMA0aXI1lc0xnp83PSUSGAvcC5+VxXqVEm56Rqs4HhorIJ4BHRORZVe2Mu822PKfvA/ep6u4OvvjNRFue0Xqgv6puFZEa4M8iMlRVd+Z7kvmmmDuCeuBo3/t+wLpUx4hIJdAd2NYusysd2vScRKQf8CTw76r6fsFnWxzy8rukqm8DH+H4UzojbXlOJwM/FpFVwG3AHSLylUJPuAi0+hmp6n5V3QqgqnU4vobBBZ9xHiimIlgAHCcix4hIFY7T5enAMU8D17qvLwP+qa5Xpoxo9XMSkR7AX4Fvq+q/2m3G7U9bntEx7h8zIjIAGAKsap9ptzutfk6qepqqDlTVgcD9wN2q2hkj9tryu3S4iFQAiMgg4Dgcn0rpU0xPNXAB8C6O5vyOOzYFuNh9fQBOhMJ7wGvAIN+5q3BWKrtxNPTx7T3/Un9OwHdxVrhv+P71Lvb3KbFn9G84zs83gEXAJcX+LqX4nALXuItOGjXUxt+lS93fpTfd36WLiv1dsv1nmcWGYRhljmUWG4ZhlDmmCAzDMMocUwSGYRhljikCwzCMMscUgWEYRpljisBoN0Qk5lZlXCIis0SkW7HnlAoRmSsiof1oReQJN04cEVnlVnd9w/1/gu+43b7Xg0Xkb27FyrdF5HEROaKNc/y9iOwRkUN8Yw+IiIpIL/f9d0RkqYgsdud4su/7LfdVynzCHf+KiFzflnkZHQ9TBEZ7sldVR6rqMKARp3psAnEo6d9Jt1xHhar6E4XGq+pInOSin4eccwBOYt+vVPVYVf0E8Cvg8DT3uU5E7spiSu/hFkVzn9144EP3/TjgQmCUqg4HziG5js7V7s9jpKpe5o79DviPLO5rdCJK+o/O6NS8Ahzr1nB/W0R+iZOEc7SIXOmurpeIyL3eCeLUwf9vEVkkIv8QkcPd8ZEiUuuuep8UkWp3/D9EZJk7/pg7dpCI/E5EFrjFwTwheqCIPOYeOxM4MMW8rwaeSvHZoUBDyPhVwKuq+hdvQFVfVNUluTywFMwALndfnwn8C/CK5vUBtqhbaFBVt2hzNdpQVHUPsEpExuRhbkYHwRSB0e64JR3Ox6n2CU5Zhz+o6olAE06BvLOAkcBJ4pQ/BqcPxSJVHQW8BNzpjv8B+Ka76n3LN/4t4ER33Nt9fAenJMBJOKvnn7iVNW8G9rjH/hCoSTH9U4C6wNiLIrLEndN3Q84ZFnJOvlgBHO4qvytxyiZ7PIejWN8VkV+KyBmBc6f7TEM/8Y0vBDpM5Uyj7ZgiMNqTA0XkDRxBswb4rTu+WlVr3dcnAXNVdbM6JX6nA6e7n8WBme7rPwGnikh3oIeqvuSOP+I7fjGOsLuG5lXyecC33HnMxSkX0N89508AqrrYPTeMPsDmwNh419x1AvA/InJwNg8jiDid995w5zYF+JJPUJ+Q5tQ5ODVxTsbZaeF+j904Cm2SO+eZInKd7zy/aeg/feObgL6t+Q5Gx6SYZaiN8mOva0tPIE5J44/8QzlcL1N9lM/gCPiLge+59n0BLlXV5SHzyKbeyl4c5dFyMqrvi8hG4HicGjQeS4Hgajzs/K04uyBcgT1QVe/KYk6P4ZjVHlHVuPjKRKtqDEfhzRWRt3CKpf0+w/UOwPmeRplgOwKj1JgPnCEivdxKjlfimFzA+X31nJpXAfNUdQfQIM1NQP4NeMl1nB6tqi8C3wB6AAcD/wvcKpLounWie97LOPZ/RGQYMDzF/N4Gjg37QER6A8cAqwMfPQp8UkQ+4zv20xlW+VmjqmtwTF6/DMxniIgc5xsaGTK3MAYD+fBfGB0E2xEYJYWqrheRbwMv4qze/6aqnnP2I5wGMnU4XaE8J+m1wENuOOpK4Hqc3rN/ck1HgtNUZbuI/ACnjPJiVxmswoms+RXwsIgsxqlE6l/R+/krjlP2Bd/YiyISA7oA31LVjYHvtFdELgTuF5H7cfwgi4Gv5vyAUqCqvw4ZPhj4hTjlyKM4EUaTfJ9PFxFv5b9FVc9xX59C5+9EZviw6qNGh0FEdqtqq+zveZzDgThK6hTX7NKpcHdIX1PVfyv2XIz2w0xDhpEDqroXJyop2Me2s9AL+F6xJ2G0L7YjMAzDKHNsR2AYhlHmmCIwDMMoc0wRGIZhlDmmCAzDMMocUwSGYRhljikCwzCMMuf/A/RwnOZ3ibt9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419.34192707582355 0.01902175963023734\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1073946268650054 3.6865133027666817e-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.1797080285641 282.2273046999977\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-0.22375631869923254"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.polynomial.polynomial import polyfit  \n",
    "from scipy.stats import pearsonr\n",
    "from pylab import text\n",
    "\n",
    "\n",
    "print(pearsonr(VIO,AUS)[0])\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(VIO,AUS, 1)\n",
    "ax.plot(VIO,AUS, '.')\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(VIO,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "plt.plot(VIO, b + m * np.array(VIO), '-')\n",
    "ax.set_xlabel(\"BIC\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "fig.savefig('Ex4VIOVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "METRIC = -(VIO/np.max(VIO)) + np.array(MSE)\n",
    "print(pearsonr(METRIC,AUS)[0])\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(METRIC,AUS, 1)\n",
    "ax.plot(METRIC,AUS, '.')\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(METRIC,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "plt.plot(METRIC, b + m * np.array(METRIC), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"Proposed (BIC + MSE)\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "fig.savefig('Ex4ProposedVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(MSE,AUS, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(MSE,AUS)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(MSE,AUS, '.')\n",
    "plt.plot(MSE, b + m * np.array(MSE), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(VIO2,AUS, 1)\n",
    "print(b,m)\n",
    "ax.plot(VIO2,AUS, '.')\n",
    "plt.plot(VIO2, b + m * np.array(VIO2), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"Violations2\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(VIO,MSE, 1)\n",
    "print(b,m)\n",
    "ax.plot(VIO,MSE, '.')\n",
    "plt.plot(VIO, b + m * np.array(VIO), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"Violations\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(METRIC,AUS, 1)\n",
    "print(b,m)\n",
    "ax.plot(METRIC,AUS, '.')\n",
    "plt.plot(METRIC, b + m * np.array(METRIC), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"MSExVIO\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "plt.show()\n",
    "\n",
    "MSE = np.array(MSE)\n",
    "METRIC = VIO/np.max(VIO)+ MSE\n",
    "x = []\n",
    "y1 = []\n",
    "y2 = []\n",
    "y3 = []\n",
    "for split in range(10, len(AUS), 5):\n",
    "    #print(\"******\", split, \"*******\")\n",
    "    sorted_aus = [AUS for _,AUS in sorted(zip(VIO,AUS))]\n",
    "    sorted_mse = [MSE for _,MSE in sorted(zip(VIO,MSE))]\n",
    "\n",
    "    low = []\n",
    "    high = []\n",
    "    low = sorted_aus[:split]\n",
    "    high = sorted_aus[split:]\n",
    "\n",
    "    x.append(split)\n",
    "    \n",
    "    \n",
    "    #print(\"Low Violations = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High Violations = \", np.mean(high), \"for\", len(high))\n",
    "    y1.append(np.mean(low))\n",
    "    sorted_aus_by_mse = [AUS for _,AUS in sorted(zip(MSE,AUS))]\n",
    "    low = sorted_aus_by_mse[:split]\n",
    "    high = sorted_aus_by_mse[split:]\n",
    "    #print(\"Low AUS by MSE = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High AUS by MSE = \", np.mean(high), \"for\", len(high))\n",
    "    y2.append(np.mean(low))\n",
    "    sorted_aus = [AUS for _,AUS in sorted(zip(METRIC,AUS))]\n",
    "    sorted_mse = [MSE for _,MSE in sorted(zip(METRIC,MSE))]\n",
    "\n",
    "    low = []\n",
    "    high = []\n",
    "    low = sorted_aus[:split]\n",
    "    high = sorted_aus[split:]\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"Low Metric = \", np.mean(low), \"for\", len(low))\n",
    "    #print(\"High Metric = \", np.mean(high), \"for\", len(high))\n",
    "    y3.append(np.mean(low))\n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x,y1, '-', label = 'Violations')\n",
    "ax.plot(x,y2, '-', label = 'MSE')\n",
    "ax.plot(x,y3, '-', label = 'METRIC')\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "plt.show()  \n",
    "pearsonr(METRIC,AUS)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
