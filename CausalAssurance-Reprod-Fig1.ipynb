{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes \n",
    "### Required installing Oracle JAVA 8 to get javabridge installed\n",
    "### Then, I was able to install py-causal from https://bd2kccd.github.io/docs/py-causal/\n",
    "### GFCI is slower than RFCI, but more accurate (SPIRTES), GFCI and RFCI account for unobserved variables, FGES assumes no unobserved variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure Learning Performance Guarantees If the assumptions in the previous section hold, then in the large sample limit, the CBN structure output by GFCId will contain an edge of one of four kinds between Xand Y   if and only if Xand Yare not independent conditional on any subset of the other measured variables of less than or equal to a specified size. In addition, there is (1) an arc X->Y   if and only if Xdirectly or indirectly causes Y, and Y   does not directly or indirectly cause X; (2) an edge X <-->Y   if and only if X   is not a direct or indirect cause of Yand Y   is not a direct or indirect cause of X(which can only occur if there are latent confounders of Xand some other variable or Yand some other variable; (3) an edge Xo->Y   only if Yis not a direct or indirect cause of X, but Xmay or may not be an indirect cause of Y; (4) an edge X oâ€“o Y   indicates that Xand Y   are dependent no matter what subset of observed variables is conditioned on, but contains no orientation information (X   may be a direct or indirect cause of Y, and Ymay be an indirect cause of X, or there may be a latent common cause of Xand Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying some various ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import configparser\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, \\\n",
    "                        Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "import keras.optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "import glob, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "# select your GPU Here\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #Comment this line out if you want all GPUS (2 hehe)\n",
    "\n",
    "# python full-display web browser\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "def get_model(dense, dropouts, inputs):\n",
    "    # dense is an ordered list of the number of dense neurons like [1024, 2048, 1024]\n",
    "    # dropouts is an ordered list of the dropout masks like [0.2, 0.3, 0.4]\n",
    "    inputs = keras.Input(shape = (inputs,))\n",
    "\n",
    "    x = keras.layers.Dense(dense[0], activation = 'relu')(inputs)\n",
    "    x = keras.layers.Dropout(dropouts[0])(x, training=True)\n",
    "    for den, drop in zip(dense[1:], dropouts[1:]):\n",
    "        x = keras.layers.Dense(den, activation = 'relu')(x)\n",
    "        x = keras.layers.Dropout(drop)(x, training=True)\n",
    "    outputs = keras.layers.Dense(1, activation = 'linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256], [256, 256]] ['temp/b0', 'temp/b1', 'temp/b2', 'temp/b3', 'temp/b4', 'temp/b5', 'temp/b6', 'temp/b7', 'temp/b8', 'temp/b9', 'temp/b10', 'temp/b11', 'temp/b12', 'temp/b13', 'temp/b14', 'temp/b15', 'temp/b16', 'temp/b17', 'temp/b18', 'temp/b19', 'temp/b20', 'temp/b21', 'temp/b22', 'temp/b23', 'temp/b24', 'temp/b25', 'temp/b26', 'temp/b27', 'temp/b28', 'temp/b29', 'temp/b30', 'temp/b31', 'temp/b32', 'temp/b33', 'temp/b34', 'temp/b35', 'temp/b36', 'temp/b37', 'temp/b38', 'temp/b39', 'temp/b40', 'temp/b41', 'temp/b42', 'temp/b43', 'temp/b44', 'temp/b45', 'temp/b46', 'temp/b47', 'temp/b48', 'temp/b49', 'temp/b50', 'temp/b51', 'temp/b52', 'temp/b53', 'temp/b54', 'temp/b55', 'temp/b56', 'temp/b57', 'temp/b58', 'temp/b59', 'temp/b60', 'temp/b61', 'temp/b62', 'temp/b63', 'temp/b64', 'temp/b65', 'temp/b66', 'temp/b67', 'temp/b68', 'temp/b69', 'temp/b70', 'temp/b71', 'temp/b72', 'temp/b73', 'temp/b74', 'temp/b75', 'temp/b76', 'temp/b77', 'temp/b78', 'temp/b79', 'temp/b80', 'temp/b81', 'temp/b82', 'temp/b83', 'temp/b84', 'temp/b85', 'temp/b86', 'temp/b87', 'temp/b88', 'temp/b89', 'temp/b90', 'temp/b91', 'temp/b92', 'temp/b93', 'temp/b94', 'temp/b95', 'temp/b96', 'temp/b97', 'temp/b98', 'temp/b99', 'temp/b100', 'temp/b101', 'temp/b102', 'temp/b103', 'temp/b104', 'temp/b105', 'temp/b106', 'temp/b107', 'temp/b108', 'temp/b109', 'temp/b110', 'temp/b111', 'temp/b112', 'temp/b113', 'temp/b114', 'temp/b115', 'temp/b116', 'temp/b117', 'temp/b118', 'temp/b119', 'temp/b120', 'temp/b121', 'temp/b122', 'temp/b123', 'temp/b124', 'temp/b125', 'temp/b126', 'temp/b127', 'temp/b128', 'temp/b129', 'temp/b130', 'temp/b131', 'temp/b132', 'temp/b133', 'temp/b134', 'temp/b135', 'temp/b136', 'temp/b137', 'temp/b138', 'temp/b139', 'temp/b140', 'temp/b141', 'temp/b142', 'temp/b143', 'temp/b144', 'temp/b145', 'temp/b146', 'temp/b147', 'temp/b148', 'temp/b149', 'temp/b150', 'temp/b151', 'temp/b152', 'temp/b153', 'temp/b154', 'temp/b155', 'temp/b156', 'temp/b157', 'temp/b158', 'temp/b159', 'temp/b160', 'temp/b161', 'temp/b162', 'temp/b163', 'temp/b164', 'temp/b165', 'temp/b166', 'temp/b167', 'temp/b168', 'temp/b169', 'temp/b170', 'temp/b171', 'temp/b172', 'temp/b173', 'temp/b174', 'temp/b175', 'temp/b176', 'temp/b177', 'temp/b178', 'temp/b179', 'temp/b180', 'temp/b181', 'temp/b182', 'temp/b183', 'temp/b184', 'temp/b185', 'temp/b186', 'temp/b187', 'temp/b188', 'temp/b189', 'temp/b190', 'temp/b191', 'temp/b192', 'temp/b193', 'temp/b194', 'temp/b195', 'temp/b196', 'temp/b197', 'temp/b198', 'temp/b199', 'temp/b200', 'temp/b201', 'temp/b202', 'temp/b203', 'temp/b204', 'temp/b205', 'temp/b206', 'temp/b207', 'temp/b208', 'temp/b209', 'temp/b210', 'temp/b211', 'temp/b212', 'temp/b213', 'temp/b214', 'temp/b215', 'temp/b216', 'temp/b217', 'temp/b218', 'temp/b219', 'temp/b220', 'temp/b221', 'temp/b222', 'temp/b223', 'temp/b224', 'temp/b225', 'temp/b226', 'temp/b227', 'temp/b228', 'temp/b229', 'temp/b230', 'temp/b231', 'temp/b232', 'temp/b233', 'temp/b234', 'temp/b235', 'temp/b236', 'temp/b237', 'temp/b238', 'temp/b239', 'temp/b240', 'temp/b241', 'temp/b242', 'temp/b243', 'temp/b244', 'temp/b245', 'temp/b246', 'temp/b247', 'temp/b248', 'temp/b249', 'temp/b250', 'temp/b251', 'temp/b252', 'temp/b253', 'temp/b254', 'temp/b255', 'temp/b256', 'temp/b257', 'temp/b258', 'temp/b259', 'temp/b260', 'temp/b261', 'temp/b262', 'temp/b263', 'temp/b264', 'temp/b265', 'temp/b266', 'temp/b267', 'temp/b268', 'temp/b269', 'temp/b270', 'temp/b271', 'temp/b272', 'temp/b273', 'temp/b274', 'temp/b275', 'temp/b276', 'temp/b277', 'temp/b278', 'temp/b279', 'temp/b280', 'temp/b281', 'temp/b282', 'temp/b283', 'temp/b284', 'temp/b285', 'temp/b286', 'temp/b287', 'temp/b288', 'temp/b289', 'temp/b290', 'temp/b291', 'temp/b292', 'temp/b293', 'temp/b294', 'temp/b295', 'temp/b296', 'temp/b297', 'temp/b298', 'temp/b299', 'temp/b300', 'temp/b301', 'temp/b302', 'temp/b303', 'temp/b304', 'temp/b305', 'temp/b306', 'temp/b307', 'temp/b308', 'temp/b309', 'temp/b310', 'temp/b311', 'temp/b312', 'temp/b313', 'temp/b314', 'temp/b315', 'temp/b316', 'temp/b317', 'temp/b318', 'temp/b319', 'temp/b320', 'temp/b321', 'temp/b322', 'temp/b323', 'temp/b324', 'temp/b325', 'temp/b326', 'temp/b327', 'temp/b328', 'temp/b329', 'temp/b330', 'temp/b331', 'temp/b332', 'temp/b333', 'temp/b334', 'temp/b335', 'temp/b336', 'temp/b337', 'temp/b338', 'temp/b339', 'temp/b340', 'temp/b341', 'temp/b342', 'temp/b343', 'temp/b344', 'temp/b345', 'temp/b346', 'temp/b347', 'temp/b348', 'temp/b349', 'temp/b350', 'temp/b351', 'temp/b352', 'temp/b353', 'temp/b354', 'temp/b355', 'temp/b356', 'temp/b357', 'temp/b358', 'temp/b359', 'temp/b360', 'temp/b361', 'temp/b362', 'temp/b363', 'temp/b364', 'temp/b365', 'temp/b366', 'temp/b367', 'temp/b368', 'temp/b369', 'temp/b370', 'temp/b371', 'temp/b372', 'temp/b373', 'temp/b374', 'temp/b375', 'temp/b376', 'temp/b377', 'temp/b378', 'temp/b379', 'temp/b380', 'temp/b381', 'temp/b382', 'temp/b383', 'temp/b384', 'temp/b385', 'temp/b386', 'temp/b387', 'temp/b388', 'temp/b389', 'temp/b390', 'temp/b391', 'temp/b392', 'temp/b393', 'temp/b394', 'temp/b395', 'temp/b396', 'temp/b397', 'temp/b398', 'temp/b399', 'temp/b400', 'temp/b401', 'temp/b402', 'temp/b403', 'temp/b404', 'temp/b405', 'temp/b406', 'temp/b407', 'temp/b408', 'temp/b409', 'temp/b410', 'temp/b411', 'temp/b412', 'temp/b413', 'temp/b414', 'temp/b415', 'temp/b416', 'temp/b417', 'temp/b418', 'temp/b419', 'temp/b420', 'temp/b421', 'temp/b422', 'temp/b423', 'temp/b424', 'temp/b425', 'temp/b426', 'temp/b427', 'temp/b428', 'temp/b429', 'temp/b430', 'temp/b431', 'temp/b432', 'temp/b433', 'temp/b434', 'temp/b435', 'temp/b436', 'temp/b437', 'temp/b438', 'temp/b439', 'temp/b440', 'temp/b441', 'temp/b442', 'temp/b443', 'temp/b444', 'temp/b445', 'temp/b446', 'temp/b447', 'temp/b448', 'temp/b449', 'temp/b450', 'temp/b451', 'temp/b452', 'temp/b453', 'temp/b454', 'temp/b455', 'temp/b456', 'temp/b457', 'temp/b458', 'temp/b459', 'temp/b460', 'temp/b461', 'temp/b462', 'temp/b463', 'temp/b464', 'temp/b465', 'temp/b466', 'temp/b467', 'temp/b468', 'temp/b469', 'temp/b470', 'temp/b471', 'temp/b472', 'temp/b473', 'temp/b474', 'temp/b475', 'temp/b476', 'temp/b477', 'temp/b478', 'temp/b479', 'temp/b480', 'temp/b481', 'temp/b482', 'temp/b483', 'temp/b484', 'temp/b485', 'temp/b486', 'temp/b487', 'temp/b488', 'temp/b489', 'temp/b490', 'temp/b491', 'temp/b492', 'temp/b493', 'temp/b494', 'temp/b495', 'temp/b496', 'temp/b497', 'temp/b498', 'temp/b499']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from pycausal import search as s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def discrete_gauss(low, high, samples, std = 20):\n",
    "    x = np.arange(low, high)\n",
    "    xU, xL = x + 0.5, x - 0.5 \n",
    "    prob = ss.norm.cdf(xU, scale = std) - ss.norm.cdf(xL, scale = std)\n",
    "    prob = prob / prob.sum() #normalize the probabilities so their sum is 1\n",
    "    nums = np.random.choice(x, size = samples, p = prob)\n",
    "    return nums\n",
    "\n",
    "\n",
    "\n",
    "def bar_plot(x_ax, val1, val1std, val2, val2std):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ## the data\n",
    "    N = len(x_ax)\n",
    "\n",
    "    ## necessary variables\n",
    "    ind = np.arange(N)                # the x locations for the groups\n",
    "    width = 0.35                      # the width of the bars\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    ## the bars\n",
    "    rects1 = ax.bar(ind, val1, width,\n",
    "                    color='gray',\n",
    "                    yerr=val1std,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='blue'))\n",
    "\n",
    "    rects2 = ax.bar(ind+width, val2, width,\n",
    "                        color='blue',\n",
    "                        #yerr=val2std,\n",
    "                        error_kw=dict(elinewidth=2,ecolor='gray'))\n",
    "\n",
    "    # axes and labels\n",
    "    ax.set_xlim(-width,len(ind)+width)\n",
    "    #ax.set_ylim(0,45)\n",
    "    ax.set_ylabel('Percentage')\n",
    "    ax.set_title('')\n",
    "    plt.xticks(ind + width / 2, x_ax, rotation=75, size = 14)\n",
    "    ## add a legend\n",
    "    ax.legend( (rects1[0], rects2[0]), ('Accuracy', '% Violations') )\n",
    "    fig.savefig(\"violations.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 5000):\n",
    "    a = np.random.gumbel(mean, var, SIZE)\n",
    "    b = np.random.gumbel(mean, var, SIZE)\n",
    "    c = np.random.gumbel(mean, var, SIZE)\n",
    "    d = np.random.gumbel(mean, var, SIZE)\n",
    "    e = np.random.gumbel(mean, var, SIZE)\n",
    "    f= a + b + c + d + e + np.random.gumbel(mean, var, SIZE)\n",
    "    g = f + np.random.gumbel(mean,var, SIZE)\n",
    "    g = np.rint(g)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 5000):\n",
    "    a = np.random.gumbel(mean, var, SIZE)\n",
    "    b = np.random.gumbel(mean, var, SIZE)\n",
    "    c = np.random.gumbel(mean, var, SIZE)\n",
    "    d = np.random.gumbel(mean, var, SIZE)\n",
    "\n",
    "    f= a + b + c + d + np.random.gumbel(mean, var, SIZE)\n",
    "    g = f + np.random.gumbel(mean,var, SIZE)\n",
    "    \n",
    "    \n",
    "    g = np.rint(g)\n",
    "    e = g + np.random.gumbel(mean,var,SIZE)\n",
    "    \n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 400000):\n",
    "    f = np.random.normal(mean, var, SIZE)\n",
    "    a = f + np.random.normal(mean, var, SIZE)\n",
    "    b = f + np.random.normal(mean, var, SIZE)\n",
    "    c = f + np.random.normal(mean, var, SIZE)\n",
    "    d = f + np.random.normal(mean, var, SIZE)\n",
    "    e = f + np.random.normal(mean, var, SIZE)\n",
    "    g = a + b + c + d  + e + np.random.normal(mean, var, SIZE)\n",
    "\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def gen_data(mean = 0, var = 2, SIZE = 20000):\n",
    "    a = np.random.normal(mean, var, SIZE)\n",
    "    b = np.random.normal(mean, var, SIZE)\n",
    "    c = np.random.normal(mean, var, SIZE)\n",
    "    d = np.random.normal(mean, var, SIZE)\n",
    "    e = np.random.normal(mean, var, SIZE)\n",
    "    f= a + b + c + d + e + np.random.normal(mean, var, SIZE)\n",
    "    g = f + np.random.normal(mean,var, SIZE)\n",
    "    #g = np.rint(g)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "\n",
    "\n",
    "def get_CG(df, tetrad):\n",
    "    tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "    #tetrad.run(algoId = 'fges-mb', targetName = 'g', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "    #       structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "    #       completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "def get_MB(graph, var, pc):\n",
    "    parents = set()\n",
    "    children = set()\n",
    "    for i in pc.extractTetradGraphEdges(graph):\n",
    "        if i[-1] == var and i[3:5] == '->':\n",
    "            parents.add(i[0])\n",
    "        if i[0] == var and i[3:5] == '->':\n",
    "            children.add(i[-1])\n",
    "    return parents, children\n",
    "\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from collections import defaultdict\n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '5000M')\n",
    "tetrad = s.tetradrunner()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "verbosity = 0\n",
    "\n",
    "'''\n",
    "models = [#[8,8,4],\n",
    "          #[16,16,8],\n",
    "          #LogisticRegression(), \n",
    "          #Perceptron(),  \n",
    "          #DecisionTreeClassifier(),\n",
    "          #LinearSVC(),\n",
    "          #GaussianNB(),\n",
    "          #[32,32,16],\n",
    "          #[64,64,32],\n",
    "          #[128, 128, 64],\n",
    "          #[256, 256, 128],\n",
    "          #[512, 512, 256],\n",
    "          #[1024, 1024, 512],\n",
    "          [2048, 2048, 2048, 1024],[2048, 2048, 2048, 1024],[2048, 2048, 2048, 1024], \n",
    "            [2048, 2048, 2048, 1024],[2048, 2048, 2048, 1024],[2048, 2048, 2048, 1024],\n",
    "            [2048, 2048, 2048, 1024],[2048, 2048, 2048, 1024],[2048, 2048, 2048, 1024],\n",
    "          #[2048, 2048, 1024],\n",
    "         ]\n",
    "model_names = ['n1', \n",
    "               'n2',\n",
    "               'n3',\n",
    "                'n4', \n",
    "               'n5',\n",
    "               'n6',\n",
    "                'n7', \n",
    "               'n8',\n",
    "               'n9',\n",
    "\n",
    "               #'m7', \n",
    "               #'m8',\n",
    "               #'m9',\n",
    "               #'m10', \n",
    "               #'m11a',\n",
    "               #'m11b',\n",
    "               #'m11c',\n",
    "              ]\n",
    "'''\n",
    "\n",
    "\n",
    "models = []\n",
    "model_names = []\n",
    "\n",
    "num_models = 500\n",
    "model_layers = [256, 256]\n",
    "for i in range(num_models):\n",
    "    models.append(model_layers)\n",
    "    model_names.append('temp/b' + str(i))\n",
    "\n",
    "print(models, model_names)\n",
    "\n",
    "\n",
    "df = gen_data()\n",
    "X = df[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "y = df['g'].values\n",
    "\n",
    "val_df = gen_data(SIZE = 2000)\n",
    "x_val = df[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "y_val = df['g'].values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp/b0\n",
      "temp/b1\n",
      "temp/b2\n",
      "temp/b3\n",
      "temp/b4\n",
      "temp/b5\n",
      "temp/b6\n",
      "temp/b7\n",
      "temp/b8\n",
      "temp/b9\n",
      "temp/b10\n",
      "temp/b11\n",
      "temp/b12\n",
      "temp/b13\n",
      "temp/b14\n",
      "temp/b15\n",
      "temp/b16\n",
      "temp/b17\n",
      "temp/b18\n",
      "temp/b19\n",
      "temp/b20\n",
      "temp/b21\n",
      "temp/b22\n",
      "temp/b23\n",
      "temp/b24\n",
      "temp/b25\n",
      "temp/b26\n",
      "temp/b27\n",
      "temp/b28\n",
      "temp/b29\n",
      "temp/b30\n",
      "temp/b31\n",
      "temp/b32\n",
      "temp/b33\n",
      "temp/b34\n",
      "temp/b35\n",
      "temp/b36\n",
      "temp/b37\n",
      "temp/b38\n",
      "temp/b39\n",
      "temp/b40\n",
      "temp/b41\n",
      "temp/b42\n",
      "temp/b43\n",
      "temp/b44\n",
      "temp/b45\n",
      "temp/b46\n",
      "temp/b47\n",
      "temp/b48\n",
      "temp/b49\n",
      "temp/b50\n",
      "temp/b51\n",
      "temp/b52\n",
      "temp/b53\n",
      "temp/b54\n",
      "temp/b55\n",
      "temp/b56\n",
      "temp/b57\n",
      "temp/b58\n",
      "temp/b59\n",
      "temp/b60\n",
      "temp/b61\n",
      "temp/b62\n",
      "temp/b63\n",
      "temp/b64\n",
      "temp/b65\n",
      "temp/b66\n",
      "temp/b67\n",
      "temp/b68\n",
      "temp/b69\n",
      "temp/b70\n",
      "temp/b71\n",
      "temp/b72\n",
      "temp/b73\n",
      "temp/b74\n",
      "temp/b75\n",
      "temp/b76\n",
      "temp/b77\n",
      "temp/b78\n",
      "temp/b79\n",
      "temp/b80\n",
      "temp/b81\n",
      "temp/b82\n",
      "temp/b83\n",
      "temp/b84\n",
      "temp/b85\n",
      "temp/b86\n",
      "temp/b87\n",
      "temp/b88\n",
      "temp/b89\n",
      "temp/b90\n",
      "temp/b91\n",
      "temp/b92\n",
      "temp/b93\n",
      "temp/b94\n",
      "temp/b95\n",
      "temp/b96\n",
      "temp/b97\n",
      "temp/b98\n",
      "temp/b99\n",
      "temp/b100\n",
      "temp/b101\n",
      "temp/b102\n",
      "temp/b103\n",
      "temp/b104\n",
      "temp/b105\n",
      "temp/b106\n",
      "temp/b107\n",
      "temp/b108\n",
      "temp/b109\n",
      "temp/b110\n",
      "temp/b111\n",
      "temp/b112\n",
      "temp/b113\n",
      "temp/b114\n",
      "temp/b115\n",
      "temp/b116\n",
      "temp/b117\n",
      "temp/b118\n",
      "temp/b119\n",
      "temp/b120\n",
      "temp/b121\n",
      "temp/b122\n",
      "temp/b123\n",
      "temp/b124\n",
      "temp/b125\n",
      "temp/b126\n",
      "temp/b127\n",
      "temp/b128\n",
      "temp/b129\n",
      "temp/b130\n",
      "temp/b131\n",
      "temp/b132\n",
      "temp/b133\n",
      "temp/b134\n",
      "temp/b135\n",
      "temp/b136\n",
      "temp/b137\n",
      "temp/b138\n",
      "temp/b139\n",
      "temp/b140\n",
      "temp/b141\n",
      "temp/b142\n",
      "temp/b143\n",
      "temp/b144\n",
      "temp/b145\n",
      "temp/b146\n",
      "temp/b147\n",
      "temp/b148\n",
      "temp/b149\n",
      "temp/b150\n",
      "temp/b151\n",
      "temp/b152\n",
      "temp/b153\n",
      "temp/b154\n",
      "temp/b155\n",
      "temp/b156\n",
      "temp/b157\n",
      "temp/b158\n",
      "temp/b159\n",
      "temp/b160\n",
      "temp/b161\n",
      "temp/b162\n",
      "temp/b163\n",
      "temp/b164\n",
      "temp/b165\n",
      "temp/b166\n",
      "temp/b167\n",
      "temp/b168\n",
      "temp/b169\n",
      "temp/b170\n",
      "temp/b171\n",
      "temp/b172\n",
      "temp/b173\n",
      "temp/b174\n",
      "temp/b175\n",
      "temp/b176\n",
      "temp/b177\n",
      "temp/b178\n",
      "temp/b179\n",
      "temp/b180\n",
      "temp/b181\n",
      "temp/b182\n",
      "temp/b183\n",
      "temp/b184\n",
      "temp/b185\n",
      "temp/b186\n",
      "temp/b187\n",
      "temp/b188\n",
      "temp/b189\n",
      "temp/b190\n",
      "temp/b191\n",
      "temp/b192\n",
      "temp/b193\n",
      "temp/b194\n",
      "temp/b195\n",
      "temp/b196\n",
      "temp/b197\n",
      "temp/b198\n",
      "temp/b199\n",
      "temp/b200\n",
      "temp/b201\n",
      "temp/b202\n",
      "temp/b203\n",
      "temp/b204\n",
      "temp/b205\n",
      "temp/b206\n",
      "temp/b207\n",
      "temp/b208\n",
      "temp/b209\n",
      "temp/b210\n",
      "temp/b211\n",
      "temp/b212\n",
      "temp/b213\n",
      "temp/b214\n",
      "temp/b215\n",
      "temp/b216\n",
      "temp/b217\n",
      "temp/b218\n",
      "temp/b219\n",
      "temp/b220\n",
      "temp/b221\n",
      "temp/b222\n",
      "temp/b223\n",
      "temp/b224\n",
      "temp/b225\n",
      "temp/b226\n",
      "temp/b227\n",
      "temp/b228\n",
      "temp/b229\n",
      "temp/b230\n",
      "temp/b231\n",
      "temp/b232\n",
      "temp/b233\n",
      "temp/b234\n",
      "temp/b235\n",
      "temp/b236\n",
      "temp/b237\n",
      "temp/b238\n",
      "temp/b239\n",
      "temp/b240\n",
      "temp/b241\n",
      "temp/b242\n",
      "temp/b243\n",
      "temp/b244\n",
      "temp/b245\n",
      "temp/b246\n",
      "temp/b247\n",
      "temp/b248\n",
      "temp/b249\n",
      "temp/b250\n",
      "temp/b251\n",
      "temp/b252\n",
      "temp/b253\n",
      "temp/b254\n",
      "temp/b255\n",
      "temp/b256\n",
      "temp/b257\n",
      "temp/b258\n",
      "temp/b259\n",
      "temp/b260\n",
      "temp/b261\n",
      "temp/b262\n",
      "temp/b263\n",
      "temp/b264\n",
      "temp/b265\n",
      "temp/b266\n",
      "temp/b267\n",
      "temp/b268\n",
      "temp/b269\n",
      "temp/b270\n",
      "temp/b271\n",
      "temp/b272\n",
      "temp/b273\n",
      "temp/b274\n",
      "temp/b275\n",
      "temp/b276\n",
      "temp/b277\n",
      "temp/b278\n",
      "temp/b279\n",
      "temp/b280\n",
      "temp/b281\n",
      "temp/b282\n",
      "temp/b283\n",
      "temp/b284\n",
      "temp/b285\n",
      "temp/b286\n",
      "temp/b287\n",
      "temp/b288\n",
      "temp/b289\n",
      "temp/b290\n",
      "temp/b291\n",
      "temp/b292\n",
      "temp/b293\n",
      "temp/b294\n",
      "temp/b295\n",
      "temp/b296\n",
      "temp/b297\n",
      "temp/b298\n",
      "temp/b299\n",
      "temp/b300\n",
      "temp/b301\n",
      "temp/b302\n",
      "temp/b303\n",
      "temp/b304\n",
      "temp/b305\n",
      "temp/b306\n",
      "temp/b307\n",
      "temp/b308\n",
      "temp/b309\n",
      "temp/b310\n",
      "temp/b311\n",
      "temp/b312\n",
      "temp/b313\n",
      "temp/b314\n",
      "temp/b315\n",
      "temp/b316\n",
      "temp/b317\n",
      "temp/b318\n",
      "temp/b319\n",
      "temp/b320\n",
      "temp/b321\n",
      "temp/b322\n",
      "temp/b323\n",
      "temp/b324\n",
      "temp/b325\n",
      "temp/b326\n",
      "temp/b327\n",
      "temp/b328\n",
      "temp/b329\n",
      "temp/b330\n",
      "temp/b331\n",
      "temp/b332\n",
      "temp/b333\n",
      "temp/b334\n",
      "temp/b335\n",
      "temp/b336\n",
      "temp/b337\n",
      "temp/b338\n",
      "temp/b339\n",
      "temp/b340\n",
      "temp/b341\n",
      "temp/b342\n",
      "temp/b343\n",
      "temp/b344\n",
      "temp/b345\n",
      "temp/b346\n",
      "temp/b347\n",
      "temp/b348\n",
      "temp/b349\n",
      "temp/b350\n",
      "temp/b351\n",
      "temp/b352\n",
      "temp/b353\n",
      "temp/b354\n",
      "temp/b355\n",
      "temp/b356\n",
      "temp/b357\n",
      "temp/b358\n",
      "temp/b359\n",
      "temp/b360\n",
      "temp/b361\n",
      "temp/b362\n",
      "temp/b363\n",
      "temp/b364\n",
      "temp/b365\n",
      "temp/b366\n",
      "temp/b367\n",
      "temp/b368\n",
      "temp/b369\n",
      "temp/b370\n",
      "temp/b371\n",
      "temp/b372\n",
      "temp/b373\n",
      "temp/b374\n",
      "temp/b375\n",
      "temp/b376\n",
      "temp/b377\n"
     ]
    }
   ],
   "source": [
    "for idx, model_name in enumerate(model_names):\n",
    "    print(model_name)\n",
    "\n",
    "    if type(models[idx]) is list:\n",
    "        #clear session\n",
    "        keras.backend.clear_session() \n",
    "        #get model according to specification\n",
    "        model = get_model(models[idx], [0.2] * len(models), 6)\n",
    "        callbacks = [ModelCheckpoint(model_name, verbose= verbosity, monitor='val_loss',save_best_only=True), \n",
    "                     EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose= verbosity, mode='auto')]\n",
    "        model.compile(optimizer = optimizers.SGD(lr = 0.001, momentum = 0.9, ), loss='mean_squared_error', metrics = ['mse'])\n",
    "        #print(len(X), len(y))\n",
    "        model.fit(X, y, epochs = 20, validation_data = (x_val, y_val), callbacks = callbacks, batch_size = 32, verbose = verbosity)\n",
    "    else:\n",
    "        models[idx].fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the size of the test set\n",
    "nb_test = 2000\n",
    "\n",
    "\n",
    "metrics_dicts = []\n",
    "for m in models:\n",
    "    metrics_dicts.append(defaultdict(list))\n",
    "\n",
    "\n",
    "#means = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "#variances = [1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0]\n",
    "means = [0,1,2]\n",
    "variances = [1, 2,3]\n",
    "\n",
    "\n",
    "# ok at this point we need to check the model on various variances and means\n",
    "for m in means:\n",
    "    for v in variances:\n",
    "        print(m,v)\n",
    "        #t0 = time.time()\n",
    "        perturbed_df = gen_data(mean =m, var = v, SIZE = nb_test)\n",
    "        y_test2 = perturbed_df['g']\n",
    "        x_test2 = perturbed_df[['a', 'b', 'c', 'd', 'e', 'f']]\n",
    "        #t1 = time.time()\n",
    "        #print(\"Time for gen_data = \", t1 - t0)\n",
    "        for idx, model_name in enumerate(model_names):\n",
    "            #t0 = time.time()\n",
    "            if type(models[idx]) is list:\n",
    "                keras.backend.clear_session()\n",
    "                model = load_model(model_name)\n",
    "            else:\n",
    "                model = models[idx]\n",
    "            #t1 = time.time()\n",
    "            #print(\"Time to load model = \", t1 - t0)\n",
    "            \n",
    "            y_pred2 = model.predict(x_test2)\n",
    "            metrics_dicts[idx][str(m) + '_' + str(v)].append(mean_squared_error(y_test2, y_pred2))\n",
    "\n",
    "            test_df2 = pd.DataFrame(x_test2, columns = ['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "            test_targets2 = pd.DataFrame(model.predict(x_test2), columns = ['g'])\n",
    "            test_df2 = test_df2.join(test_targets2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the number of times to sample \n",
    "times = 10\n",
    "\n",
    "\n",
    "violations = np.zeros(len(models))\n",
    "violation_mean = np.zeros((len(models), times))\n",
    "violation_mean2 = np.zeros((len(models), times))\n",
    "mean = np.zeros((len(models), times))\n",
    "\n",
    "\n",
    "#metrics_dicts = []\n",
    "causal_dicts = []\n",
    "for m in models:\n",
    "#    metrics_dicts.append(defaultdict(list))\n",
    "    causal_dicts.append(defaultdict(list))\n",
    "\n",
    "\n",
    "\n",
    "for t in range(times):\n",
    "    print(\"Times = \", t)\n",
    "    df_test = gen_data(SIZE = nb_test)\n",
    "    x_test = df_test[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "    y_test = df_test['g'].values\n",
    "    setAp, setAc = get_MB(get_CG(df_test, tetrad), 'g', pc)\n",
    "    setA = set(tetrad.getEdges())\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        if type(models[idx]) is list:\n",
    "            keras.backend.clear_session()\n",
    "            model = load_model(model_name)\n",
    "        else:\n",
    "            model = models[idx]\n",
    "        test_df = pd.DataFrame(x_test, columns = ['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "        test_targets = pd.DataFrame(model.predict(x_test), columns = ['g'])\n",
    "        test_df = test_df.join(test_targets)\n",
    "        mean[idx][t] = mean_squared_error(y_test, model.predict(x_test))  \n",
    "        setBp, setBc = get_MB(get_CG(test_df, tetrad), 'g', pc)\n",
    "        setB = set(tetrad.getEdges())\n",
    "        print(setAp, setAc, setBp, setBc)\n",
    "        if setAp != {'f'}:\n",
    "            print(\"Error in SETAp markov blanket\")\n",
    "        print(setAp, setAc, setBp, setBc)\n",
    "        \n",
    "        \n",
    "        #print(\"Set violations = \", len(setA.difference(setB)) + len(setB.difference(setA)))\n",
    "        #print(\"Violations = \",len(setAp.difference(setBp)) + len(setBp.difference(setAp)) + len(setAc.difference(setBc)) + len(setBc.difference(setAc)))\n",
    "        violation_mean[idx][t] = len(setAp.difference(setBp)) + len(setBp.difference(setAp)) + len(setAc.difference(setBc)) + len(setBc.difference(setAc))\n",
    "        violation_mean2[idx][t] = len(setA.difference(setB)) + len(setB.difference(setA))\n",
    "        if setAp != setBp or setAc != setBc:\n",
    "            #print(\"Violation:\", model_names[idx], setA , setB)\n",
    "            violations[idx] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = []\n",
    "metric_err = []\n",
    "viol = []\n",
    "viol_err = []\n",
    "\n",
    "#normalize the violations for prettier graphing.\n",
    "#also violations are always positive, so just divide by max.\n",
    "\n",
    "#TMK\n",
    "#violation_mean = violation_mean / np.max(violation_mean)\n",
    "\n",
    "for i in range(len(violations)):\n",
    "    print(\"Model_name = \", model_names[i], \"Violations = \", violations[i])\n",
    "    print(\"Average_violations = \", np.mean(violation_mean[i]), np.std(violation_mean[i]))\n",
    "    print(\"MSE = \", np.mean(mean[i]), np.std(mean[i]))\n",
    "    #print(\"mean = \", mean[i])\n",
    "    metric.append(np.mean(mean[i]))\n",
    "    metric_err.append(np.std(mean[i]))\n",
    "    viol.append(np.mean(violation_mean[i]))\n",
    "    #viol.append(violations[i]/times)\n",
    "    viol_err.append(np.std(violation_mean[i]))\n",
    "print(np.array(metric), \n",
    "         np.array(metric_err), \n",
    "         np.array(viol), \n",
    "         np.array(viol_err))    \n",
    "\n",
    "bar_plot(model_names, \n",
    "         np.array(metric), \n",
    "         np.array(metric_err), \n",
    "         np.array(viol), \n",
    "         np.array(viol_err))\n",
    "\n",
    "\n",
    "def heat_plot(x,y,z, xlab = 'Mean', ylab = 'Variance', clim_low = 0, clim_high = 1):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    cax = ax.scatter(x, y, c=z, s=450, edgecolor='')\n",
    "    cax.set_clim(clim_low, clim_high)\n",
    "    ax.set_xlabel(xlab)\n",
    "    ax.set_ylabel(ylab)\n",
    "    plt.colorbar(cax)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "MSE = []\n",
    "VIO = []\n",
    "VIO2 = []\n",
    "AUS = []\n",
    "for i, m in enumerate(models):\n",
    "    print(model_names[i])\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "    \n",
    "    \n",
    "    rectangular_approx = 0\n",
    "    for k, v in metrics_dicts[i].items():\n",
    "        x.append(float(k.split('_')[0]))\n",
    "        y.append(float(k.split('_')[-1]))\n",
    "        z.append(np.mean(v))\n",
    "        rectangular_approx += np.mean(v)\n",
    "    print(\"Area under surface (rectangular approx) = \", rectangular_approx)\n",
    "    print(\"Violations = \", violations[i])\n",
    "    print(\"Average_violations = \", np.mean(violation_mean[i]))\n",
    "    print(\"MSE = \", np.mean(mean[i]))   \n",
    "    MSE.append(np.mean(mean[i]))\n",
    "    VIO.append(np.mean(violation_mean[i]))\n",
    "    VIO2.append(np.mean(violation_mean2[i]))\n",
    "    #VIO.append(violations[i]/times)\n",
    "    AUS.append(rectangular_approx)\n",
    "    \n",
    "    heat_plot(x,y,z, clim_low = 0, clim_high = 10)\n",
    "    \n",
    "#heat_plot(MSE,VIO,AUS, xlab = 'MSE', ylab='Violations', clim_low = np.min(AUS), clim_high = np.max(AUS))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from numpy.polynomial.polynomial import polyfit  \n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(VIO,AUS, 1)\n",
    "print(b,m)\n",
    "ax.plot(VIO,AUS, '.')\n",
    "\n",
    "plt.plot(VIO, b + m * np.array(VIO), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"Violations\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(VIO2,AUS, 1)\n",
    "print(b,m)\n",
    "ax.plot(VIO2,AUS, '.')\n",
    "plt.plot(VIO2, b + m * np.array(VIO2), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"Violations2\")\n",
    "ax.set_ylabel(\"AUS\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(VIO,MSE, 1)\n",
    "print(b,m)\n",
    "ax.plot(VIO,MSE, '.')\n",
    "\n",
    "plt.plot(VIO, b + m * np.array(VIO), '-')\n",
    "    #cax = ax.scatter(VIO,AUS)\n",
    "ax.set_xlabel(\"Violations\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"AUS vs Violation 1\")\n",
    "bar_plot(model_names, \n",
    "         np.array(AUS/np.max(AUS)), \n",
    "         np.array(metric_err), \n",
    "         np.array(VIO/np.max(VIO)), \n",
    "         np.array(viol_err))\n",
    "\n",
    "print(\"AUS vs Violation 2\")\n",
    "bar_plot(model_names, \n",
    "         np.array(AUS/np.max(AUS)), \n",
    "         np.array(metric_err), \n",
    "         np.array(VIO2/np.max(VIO2)), \n",
    "         np.array(viol_err))\n",
    "\n",
    "print(\"Violations vs Violation2\")\n",
    "bar_plot(model_names, \n",
    "         np.array(VIO/np.max(VIO)), \n",
    "         np.array(metric_err), \n",
    "         np.array(VIO2/np.max(VIO2)), \n",
    "         np.array(viol_err))\n",
    "'''\n",
    "\n",
    "\n",
    "sorted_aus = [AUS for _,AUS in sorted(zip(VIO,AUS))]\n",
    "sorted_aus2 = [AUS for _,AUS in sorted(zip(VIO2,AUS))]\n",
    "sorted_mse = [MSE for _,MSE in sorted(zip(VIO2,MSE))]\n",
    "\n",
    "low = []\n",
    "low_mse = []\n",
    "high = []\n",
    "high_mse = []\n",
    "\n",
    "split = 8\n",
    "low = sorted_aus[:split]\n",
    "high = sorted_aus[split:]\n",
    "low2 = sorted_aus2[:split]\n",
    "high2 = sorted_aus2[split:]\n",
    "low_mse = sorted_mse[:split]\n",
    "high_mse = sorted_mse[split:]\n",
    "        \n",
    "        \n",
    "print(\"Low Violations = \", np.mean(low), \"for\", len(low))\n",
    "print(\"High Violations = \", np.mean(high), \"for\", len(high))\n",
    "print(\"Low Violations = \", np.mean(low2), \"for\", len(low2))\n",
    "print(\"High Violations = \", np.mean(high2), \"for\", len(high2))\n",
    "print(\"Low MSE = \", np.mean(low_mse), \"for\", len(low_mse))\n",
    "print(\"High MSE = \", np.mean(high_mse), \"for\", len(high_mse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
