{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes \n",
    "### Required installing Oracle JAVA 8 to get javabridge installed\n",
    "### Then, I was able to install py-causal from https://bd2kccd.github.io/docs/py-causal/\n",
    "### GFCI is slower than RFCI, but more accurate (SPIRTES), GFCI and RFCI account for unobserved variables, FGES assumes no unobserved variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure Learning Performance Guarantees If the assumptions in the previous section hold, then in the large sample limit, the CBN structure output by GFCId will contain an edge of one of four kinds between Xand Y   if and only if Xand Yare not independent conditional on any subset of the other measured variables of less than or equal to a specified size. In addition, there is (1) an arc X->Y   if and only if Xdirectly or indirectly causes Y, and Y   does not directly or indirectly cause X; (2) an edge X <-->Y   if and only if X   is not a direct or indirect cause of Yand Y   is not a direct or indirect cause of X(which can only occur if there are latent confounders of Xand some other variable or Yand some other variable; (3) an edge Xo->Y   only if Yis not a direct or indirect cause of X, but Xmay or may not be an indirect cause of Y; (4) an edge X oâ€“o Y   indicates that Xand Y   are dependent no matter what subset of observed variables is conditioned on, but contains no orientation information (X   may be a direct or indirect cause of Y, and Ymay be an indirect cause of X, or there may be a latent common cause of Xand Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for continous data.\n",
    "# generate some toy data:\n",
    "SIZE = 20000\n",
    "a = np.random.normal(size=SIZE, scale = 1)\n",
    "b = np.random.normal(size=SIZE, scale = 1)\n",
    "c = np.random.normal(size=SIZE, scale = 1)\n",
    "d = np.random.normal(size=SIZE, scale = 1)\n",
    "e = np.random.normal(size=SIZE, scale = 1)\n",
    "\n",
    "f= a + b + c + d + e + np.random.normal(size=SIZE, scale = 1)\n",
    "g = f + np.random.normal(size=SIZE, scale = 1)\n",
    "\n",
    "# load the data into a dataframe:\n",
    "df = pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "import pandas as pd\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from pycausal import search as s\n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '500M')\n",
    "from pycausal import prior as p\n",
    "\n",
    "tetrad = s.tetradrunner()\n",
    "#GFCI = Greedy Fast Causal Interference (GFCI) \n",
    "# bdeu = Bayesian Dirichlet likelihood equivalence and uniform\n",
    "tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "dot_str = pc.tetradGraphToDot(tetrad.getTetradGraph())\n",
    "graphs = pydot.graph_from_dot_data(dot_str)\n",
    "svg_str = graphs[0].create_svg()\n",
    "SVG(svg_str)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try and predict D HERE\n",
    "\n",
    "# This is for continous data.\n",
    "# generate some toy data:\n",
    "SIZE = 100000\n",
    "a = np.random.normal(size=SIZE, scale = 1)\n",
    "b = np.random.normal(size=SIZE, scale = 1)\n",
    "c = np.random.normal(size=SIZE, scale = 1)\n",
    "d = a + b + c + np.random.normal(size=SIZE, scale = 1)\n",
    "e = d + np.random.normal(size=SIZE, scale = 1)\n",
    "f= d + np.random.normal(size=SIZE, scale = 1)\n",
    "g = d + np.random.normal(size=SIZE, scale = 1)\n",
    "\n",
    "# load the data into a dataframe:\n",
    "df = pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "import pandas as pd\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from pycausal import search as s\n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '2500M')\n",
    "from pycausal import prior as p\n",
    "\n",
    "tetrad = s.tetradrunner()\n",
    "#GFCI = Greedy Fast Causal Interference (GFCI) \n",
    "# bdeu = Bayesian Dirichlet likelihood equivalence and uniform\n",
    "tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "dot_str = pc.tetradGraphToDot(tetrad.getTetradGraph())\n",
    "graphs = pydot.graph_from_dot_data(dot_str)\n",
    "svg_str = graphs[0].create_svg()\n",
    "SVG(svg_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '500M')\n",
    "from pycausal import prior as p\n",
    "\n",
    "tetrad = s.tetradrunner()\n",
    "#GFCI = Greedy Fast Causal Interference (GFCI) \n",
    "# bdeu = Bayesian Dirichlet likelihood equivalence and uniform\n",
    "tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "dot_str = pc.tetradGraphToDot(tetrad.getTetradGraph())\n",
    "graphs = pydot.graph_from_dot_data(dot_str)\n",
    "print(len(graphs))\n",
    "svg_str = graphs[0].create_svg()\n",
    "SVG(svg_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying some various ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Synthetic data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "\n",
    "def gen_data():\n",
    "    SIZE = 10000\n",
    "    a = np.random.binomial(2, 0.5, size=SIZE)\n",
    "    b = np.random.binomial(2, 0.5, size=SIZE)\n",
    "    c = np.random.binomial(2, 0.5, size=SIZE)\n",
    "    d = np.random.binomial(2, 0.5, size=SIZE)\n",
    "    e = np.random.binomial(2, 0.5, size=SIZE)\n",
    "    f= a + b + c + d + e + np.random.binomial(2, 0.5, size=SIZE)\n",
    "    g = f + np.random.binomial(2, 0.5, size=SIZE)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "\n",
    "import scipy.stats as ss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def discrete_gauss(low, high, samples, std = 20):\n",
    "    x = np.arange(low, high)\n",
    "    xU, xL = x + 0.5, x - 0.5 \n",
    "    prob = ss.norm.cdf(xU, scale = std) - ss.norm.cdf(xL, scale = std)\n",
    "    prob = prob / prob.sum() #normalize the probabilities so their sum is 1\n",
    "    nums = np.random.choice(x, size = samples, p = prob)\n",
    "    return nums\n",
    "\n",
    "def gen_data():\n",
    "    SIZE = 40000\n",
    "    a = discrete_gauss(-3,3, SIZE)\n",
    "    b = discrete_gauss(-3,3, SIZE)\n",
    "    c = discrete_gauss(-3,3, SIZE)\n",
    "    d = discrete_gauss(-3,3, SIZE)\n",
    "    e = discrete_gauss(-3,3, SIZE)\n",
    "    f= a + b + c + d + e + np.random.binomial(2, 0.5, size=SIZE)\n",
    "    g = f + discrete_gauss(-3,3, SIZE)\n",
    "\n",
    "    #g[g < 0] = 0\n",
    "    #g[g > 0] = 1\n",
    "    #g[(g <= 3) & (g >= -3)] = 1\n",
    "    #g[g < -3] = 0\n",
    "    #g[g > 3] = 2\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def gen_data():\n",
    "    #SIZE = 40000\n",
    "    SIZE = 40000\n",
    "    a = np.random.normal(0, 1, SIZE)\n",
    "    b = np.random.normal(0, 1, SIZE)\n",
    "    c = np.random.normal(0, 1, SIZE)\n",
    "    d = np.random.normal(0, 1, SIZE)\n",
    "    e = np.random.normal(0, 1, SIZE)\n",
    "    f= a + b + c + d + e + np.random.normal(0, 1, SIZE)\n",
    "    g = f + np.random.normal(0, 1, SIZE)\n",
    "    g = np.rint(g)\n",
    "    #m = np.mean(g)\n",
    "    #g[g < m] = 0\n",
    "    #g[g >= m] = 1\n",
    "\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "df = gen_data()\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def bar_plot(x_ax, val1, val1std, val2, val2std):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ## the data\n",
    "    N = len(x_ax)\n",
    "\n",
    "    ## necessary variables\n",
    "    ind = np.arange(N)                # the x locations for the groups\n",
    "    width = 0.35                      # the width of the bars\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    ## the bars\n",
    "    rects1 = ax.bar(ind, val1, width,\n",
    "                    color='gray',\n",
    "                    yerr=val1std,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='blue'))\n",
    "\n",
    "    rects2 = ax.bar(ind+width, val2, width,\n",
    "                        color='blue',\n",
    "                        #yerr=val2std,\n",
    "                        error_kw=dict(elinewidth=2,ecolor='gray'))\n",
    "\n",
    "    # axes and labels\n",
    "    ax.set_xlim(-width,len(ind)+width)\n",
    "    #ax.set_ylim(0,45)\n",
    "    ax.set_ylabel('Percentage')\n",
    "    ax.set_title('')\n",
    "    plt.xticks(rotation=75)\n",
    "    ax.set_xticks(ind+width, x_ax)\n",
    "\n",
    "\n",
    "    ## add a legend\n",
    "    ax.legend( (rects1[0], rects2[0]), ('Accuracy', '% Violations') )\n",
    "    fig.savefig(\"violations.pdf\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=4.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: LogisticRegression() 0.2613545816733068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Perceptron() 0.2147410358565737\n",
      "Error: DecisionTreeClassifier() 0.27091633466135456\n",
      "Error: LinearSVC() 0.2948207171314741\n",
      "Error: GaussianNB() 0.3597609561752988\n",
      "Violation: {'f'} {'b', 'e', 'a', 'd', 'f', 'c'} g <pycausal.pycausal.pycausal object at 0x7fd1281503c8>\n",
      "Error: BernoulliNB() 0.26733067729083665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: LinearDiscriminantAnalysis() 0.34860557768924305\n",
      "Error: RandomForestClassifier() 0.30278884462151395\n",
      "Error: ExtraTreesClassifier() 0.2896414342629482\n",
      "Error: AdaBoostClassifier() 0.27091633466135456\n",
      "Error: BaggingClassifier() 0.3035856573705179\n",
      "Error: GradientBoostingClassifier() 0.34103585657370517\n",
      "Error: MLPClassifier() 0.35179282868525896\n",
      "Error: LogisticRegression() 0.2762894842063175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Perceptron() 0.17752898840463815\n",
      "Violation: {'f'} {'f', 'd'} g <pycausal.pycausal.pycausal object at 0x7fd1281503c8>\n",
      "Error: DecisionTreeClassifier() 0.2818872451019592\n",
      "Error: LinearSVC() 0.29788084766093564\n",
      "Error: GaussianNB() 0.3634546181527389\n",
      "Violation: {'f'} {'b', 'e', 'a', 'd', 'f', 'c'} g <pycausal.pycausal.pycausal object at 0x7fd1281503c8>\n",
      "Error: BernoulliNB() 0.24830067972810876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: LinearDiscriminantAnalysis() 0.3750499800079968\n",
      "Error: RandomForestClassifier() 0.29788084766093564\n",
      "Error: ExtraTreesClassifier() 0.3006797281087565\n",
      "Error: AdaBoostClassifier() 0.258296681327469\n",
      "Error: BaggingClassifier() 0.3078768492602959\n",
      "Error: GradientBoostingClassifier() 0.34106357457017195\n",
      "Error: MLPClassifier() 0.3714514194322271\n",
      "Error: LogisticRegression() 0.2683219863836604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Perceptron() 0.16419703644373249\n",
      "Violation: {'f'} {'a', 'b'} g <pycausal.pycausal.pycausal object at 0x7fd1281503c8>\n",
      "Error: DecisionTreeClassifier() 0.263516219463356\n",
      "Error: LinearSVC() 0.289146976371646\n",
      "Error: GaussianNB() 0.3592310772927513\n",
      "Violation: {'f'} {'b', 'e', 'a', 'd', 'f', 'c'} g <pycausal.pycausal.pycausal object at 0x7fd1281503c8>\n",
      "Error: BernoulliNB() 0.24028834601521826\n",
      "Violation: {'f'} {'a', 'f'} g <pycausal.pycausal.pycausal object at 0x7fd1281503c8>\n",
      "Error: LinearDiscriminantAnalysis() 0.3628354024829796\n",
      "Error: RandomForestClassifier() 0.2867440929114938\n",
      "Error: ExtraTreesClassifier() 0.29555466559871846\n",
      "Error: AdaBoostClassifier() 0.27472967561073286\n",
      "Error: BaggingClassifier() 0.30837004405286345\n",
      "Error: GradientBoostingClassifier() 0.33079695634761713\n",
      "Error: MLPClassifier() 0.35042050460552665\n",
      "Error: LogisticRegression() 0.27447833065810595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Perceptron() 0.2102728731942215\n",
      "Violation: {'f'} {'f', 'e', 'd'} g <pycausal.pycausal.pycausal object at 0x7fd1281503c8>\n",
      "Error: DecisionTreeClassifier() 0.28370786516853935\n",
      "Error: LinearSVC() 0.3021669341894061\n",
      "Error: GaussianNB() 0.3683788121990369\n",
      "Violation: {'f'} {'b', 'e', 'a', 'd', 'f', 'c'} g <pycausal.pycausal.pycausal object at 0x7fd1281503c8>\n",
      "Error: BernoulliNB() 0.24558587479935795\n",
      "Error: LinearDiscriminantAnalysis() 0.3707865168539326\n",
      "Error: RandomForestClassifier() 0.29935794542536115\n",
      "Error: ExtraTreesClassifier() 0.3025682182985554\n",
      "Error: AdaBoostClassifier() 0.2712680577849117\n",
      "Error: BaggingClassifier() 0.31380417335473515\n",
      "Error: GradientBoostingClassifier() 0.3342696629213483\n",
      "Error: MLPClassifier() 0.3828250401284109\n",
      "Violations =  [0. 3. 0. 0. 4. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Model_name =  LogisticRegression() Violations =  0.0\n",
      "Average_violations =  0.0 0.0\n",
      "Accuracy =  0.27011109573034764 0.005854969593426323 MSE =  2.117482189926859 0.02399665049959431\n",
      "Model_name =  Perceptron() Violations =  3.0\n",
      "Average_violations =  0.30000000000000004 0.22360679774997896\n",
      "Accuracy =  0.19168498347479146 0.02140718701780834 MSE =  4.396599287011185 0.9246006431850574\n",
      "Model_name =  DecisionTreeClassifier() Violations =  0.0\n",
      "Average_violations =  0.0 0.0\n",
      "Accuracy =  0.2750069160988023 0.008243397472676071 MSE =  2.1350130413630417 0.03432673893384494\n",
      "Model_name =  LinearSVC() Violations =  0.0\n",
      "Average_violations =  0.0 0.0\n",
      "Accuracy =  0.2960038688383655 0.0047413934208007024 MSE =  1.8504097524143366 0.017336119357165527\n",
      "Model_name =  GaussianNB() Violations =  4.0\n",
      "Average_violations =  1.0 0.0\n",
      "Accuracy =  0.3627063659549564 0.0036568291523949885 MSE =  1.2086861165637155 0.028487874782390354\n",
      "Model_name =  BernoulliNB() Violations =  1.0\n",
      "Average_violations =  0.05 0.08660254037844388\n",
      "Accuracy =  0.25037639445838045 0.010203848514156975 MSE =  2.8600604706438584 0.059835594571590545\n",
      "Model_name =  LinearDiscriminantAnalysis() Violations =  0.0\n",
      "Average_violations =  0.0 0.0\n",
      "Accuracy =  0.36431936925853803 0.010075904524547966 MSE =  1.1768483669687033 0.03734349868085191\n",
      "Model_name =  RandomForestClassifier() Violations =  0.0\n",
      "Average_violations =  0.0 0.0\n",
      "Accuracy =  0.29669293265482616 0.006013588572411902 MSE =  1.7832513997936479 0.02860979504862751\n",
      "Model_name =  ExtraTreesClassifier() Violations =  0.0\n",
      "Average_violations =  0.0 0.0\n",
      "Accuracy =  0.2971110115672446 0.005018307507031927 MSE =  1.9056669439269025 0.01621484634433548\n",
      "Model_name =  AdaBoostClassifier() Violations =  0.0\n",
      "Average_violations =  0.0 0.0\n",
      "Accuracy =  0.268802687346117 0.006246016905325651 MSE =  2.8567185268350395 0.05401236200414827\n",
      "Model_name =  BaggingClassifier() Violations =  0.0\n",
      "Average_violations =  0.0 0.0\n",
      "Accuracy =  0.30840918100960313 0.003628265132730463 MSE =  1.7034509070859853 0.020727302297836626\n",
      "Model_name =  GradientBoostingClassifier() Violations =  0.0\n",
      "Average_violations =  0.0 0.0\n",
      "Accuracy =  0.3367915126032106 0.00443168702947351 MSE =  1.51037258778851 0.011426545464049644\n",
      "Model_name =  MLPClassifier() Violations =  0.0\n",
      "Average_violations =  0.0 0.0\n",
      "Accuracy =  0.3641224482128559 0.013631430040607306 MSE =  1.223514470955248 0.0477881217833128\n",
      "[0.2701111  0.19168498 0.27500692 0.29600387 0.36270637 0.25037639\n",
      " 0.36431937 0.29669293 0.29711101 0.26880269 0.30840918 0.33679151\n",
      " 0.36412245] [0.00585497 0.02140719 0.0082434  0.00474139 0.00365683 0.01020385\n",
      " 0.0100759  0.00601359 0.00501831 0.00624602 0.00362827 0.00443169\n",
      " 0.01363143] [0.   0.3  0.   0.   1.   0.05 0.   0.   0.   0.   0.   0.   0.  ] [0.         0.2236068  0.         0.         0.         0.08660254\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.        ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "\n",
    "from pycausal import search as s\n",
    "\n",
    "def get_CG(df, tetrad):\n",
    "    tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "    #tetrad.run(algoId = 'gfci', dfs = df, testId = 'bdeu', scoreId = 'bdeu', dataType = 'discrete',\n",
    "    #       structurePrior = 1.0, samplePrior = 1.0, maxDegree = 3, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "def get_MB(graph, var, pc):\n",
    "    parents = set()\n",
    "    for i in pc.extractTetradGraphEdges(graph):\n",
    "        if i[-1] == var and i[3:5] == '->':\n",
    "            parents.add(i[0])\n",
    "        if i[0] == var and i[3:5] == '->':\n",
    "            parents.add(i[-1])\n",
    "    return parents\n",
    "\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '5000M')\n",
    "tetrad = s.tetradrunner()\n",
    "def run_models(models, model_names, x, y, num_folds=4):\n",
    "    violations = np.zeros(len(models))\n",
    "    violation_mean = np.zeros((len(models), num_folds))\n",
    "    mean = np.zeros((len(models), num_folds))\n",
    "    mean2 = np.zeros((len(models), num_folds)) # used to store our secondary metric\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=num_folds)\n",
    "    fold = 0\n",
    "    for train_index, test_index in skf.split(x, y):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        for idx, model in enumerate(models):\n",
    "            model.fit(x_train, y_train)\n",
    "            original_df = pd.DataFrame(x_test, columns = ['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "\n",
    "            original_targets = pd.DataFrame(y_test, columns = ['g'])\n",
    "            original_df = original_df.join(original_targets)\n",
    "            test_df = pd.DataFrame(x_test, columns = ['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "            test_targets = pd.DataFrame(model.predict(x_test), columns = ['g'])\n",
    "            test_df = test_df.join(test_targets)\n",
    "            #print(get_MB(get_CG(test_df), 'g'))\n",
    "            print(\"Error:\", model_names[idx], accuracy_score(y_test, model.predict(x_test)))\n",
    "            \n",
    "            \n",
    "            mean[idx][fold] = accuracy_score(y_test, model.predict(x_test))\n",
    "            mean2[idx][fold] = mean_squared_error(y_test, model.predict(x_test))\n",
    "            \n",
    "            setA = get_MB(get_CG(original_df, tetrad), 'g', pc)\n",
    "            setB = get_MB(get_CG(test_df, tetrad), 'g', pc)\n",
    "            assert(setA == {'f'})\n",
    "            violation_mean[idx][fold] = len(setA.difference(setB)) + len(setB.difference(setA))\n",
    "            if setA != setB:\n",
    "                print(\"Violation:\", setA , setB, 'g', pc)\n",
    "                violations[idx] += 1\n",
    "            \n",
    "            #pc.stop_vm()\n",
    "        fold += 1\n",
    "    \n",
    "    print(\"Violations = \", violations)\n",
    "    \n",
    "    metric = []\n",
    "    metric_err = []\n",
    "    viol = []\n",
    "    viol_err = []\n",
    "    \n",
    "    #normalize the violations for prettier graphing.\n",
    "    #also violations are always positive, so just divide by max.\n",
    "    violation_mean = violation_mean / np.max(violation_mean)\n",
    "    \n",
    "    for i in range(len(violations)):\n",
    "        print(\"Model_name = \", model_names[i], \"Violations = \", violations[i])\n",
    "        print(\"Average_violations = \", np.mean(violation_mean[i]), np.std(violation_mean[i]))\n",
    "        print(\"Accuracy = \", np.mean(mean[i]), np.std(mean[i]), \"MSE = \", np.mean(mean2[i]), np.std(mean2[i]),)\n",
    "        metric.append(np.mean(mean[i]))\n",
    "        metric_err.append(np.std(mean[i]))\n",
    "        viol.append(np.mean(violation_mean[i]))\n",
    "        \n",
    "        viol_err.append(np.std(violation_mean[i]))\n",
    "    print(np.array(metric), \n",
    "             np.array(metric_err), \n",
    "             np.array(viol), \n",
    "             np.array(viol_err))    \n",
    "    \n",
    "    bar_plot(model_names, \n",
    "             np.array(metric), \n",
    "             np.array(metric_err), \n",
    "             np.array(viol), \n",
    "             np.array(viol_err))\n",
    "    return \n",
    "\n",
    "X = df[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "y = df['g'].values\n",
    "\n",
    "models = [LogisticRegression(), \n",
    "          Perceptron(),  \n",
    "          DecisionTreeClassifier(),\n",
    "          LinearSVC(),\n",
    "          GaussianNB(),\n",
    "          BernoulliNB(),\n",
    "          LinearDiscriminantAnalysis(),\n",
    "          RandomForestClassifier(),\n",
    "          ExtraTreesClassifier(),\n",
    "          AdaBoostClassifier(),\n",
    "          BaggingClassifier(),\n",
    "          GradientBoostingClassifier(),\n",
    "          MLPClassifier()\n",
    "         ]\n",
    "model_names = ['LogisticRegression()', \n",
    "          'Perceptron()',  \n",
    "          'DecisionTreeClassifier()',\n",
    "          'LinearSVC()',\n",
    "          'GaussianNB()',\n",
    "          'BernoulliNB()',\n",
    "          'LinearDiscriminantAnalysis()',\n",
    "          'RandomForestClassifier()',\n",
    "          'ExtraTreesClassifier()', \n",
    "          'AdaBoostClassifier()',\n",
    "          'BaggingClassifier()',\n",
    "          'GradientBoostingClassifier()',\n",
    "          'MLPClassifier()'\n",
    "         ]\n",
    "\n",
    "run_models(models,model_names, X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "test.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
