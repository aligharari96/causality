{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes \n",
    "### Required installing Oracle JAVA 8 to get javabridge installed\n",
    "### Then, I was able to install py-causal from https://bd2kccd.github.io/docs/py-causal/\n",
    "### GFCI is slower than RFCI, but more accurate (SPIRTES), GFCI and RFCI account for unobserved variables, FGES assumes no unobserved variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure Learning Performance Guarantees If the assumptions in the previous section hold, then in the large sample limit, the CBN structure output by GFCId will contain an edge of one of four kinds between Xand Y   if and only if Xand Yare not independent conditional on any subset of the other measured variables of less than or equal to a specified size. In addition, there is (1) an arc X->Y   if and only if Xdirectly or indirectly causes Y, and Y   does not directly or indirectly cause X; (2) an edge X <-->Y   if and only if X   is not a direct or indirect cause of Yand Y   is not a direct or indirect cause of X(which can only occur if there are latent confounders of Xand some other variable or Yand some other variable; (3) an edge Xo->Y   only if Yis not a direct or indirect cause of X, but Xmay or may not be an indirect cause of Y; (4) an edge X oâ€“o Y   indicates that Xand Y   are dependent no matter what subset of observed variables is conditioned on, but contains no orientation information (X   may be a direct or indirect cause of Y, and Ymay be an indirect cause of X, or there may be a latent common cause of Xand Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for continous data.\n",
    "# generate some toy data:\n",
    "SIZE = 20000\n",
    "a = np.random.normal(size=SIZE, scale = 1)\n",
    "b = np.random.normal(size=SIZE, scale = 1)\n",
    "c = np.random.normal(size=SIZE, scale = 1)\n",
    "d = np.random.normal(size=SIZE, scale = 1)\n",
    "e = np.random.normal(size=SIZE, scale = 1)\n",
    "\n",
    "f= a + b + c + d + e + np.random.normal(size=SIZE, scale = 1)\n",
    "g = f + np.random.normal(size=SIZE, scale = 1)\n",
    "\n",
    "# load the data into a dataframe:\n",
    "df = pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "import pandas as pd\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from pycausal import search as s\n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '500M')\n",
    "from pycausal import prior as p\n",
    "\n",
    "tetrad = s.tetradrunner()\n",
    "#GFCI = Greedy Fast Causal Interference (GFCI) \n",
    "# bdeu = Bayesian Dirichlet likelihood equivalence and uniform\n",
    "tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "dot_str = pc.tetradGraphToDot(tetrad.getTetradGraph())\n",
    "graphs = pydot.graph_from_dot_data(dot_str)\n",
    "svg_str = graphs[0].create_svg()\n",
    "SVG(svg_str)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try and predict D HERE\n",
    "\n",
    "# This is for continous data.\n",
    "# generate some toy data:\n",
    "SIZE = 100000\n",
    "a = np.random.normal(size=SIZE, scale = 1)\n",
    "b = np.random.normal(size=SIZE, scale = 1)\n",
    "c = np.random.normal(size=SIZE, scale = 1)\n",
    "d = a + b + c + np.random.normal(size=SIZE, scale = 1)\n",
    "e = d + np.random.normal(size=SIZE, scale = 1)\n",
    "f= d + np.random.normal(size=SIZE, scale = 1)\n",
    "g = d + np.random.normal(size=SIZE, scale = 1)\n",
    "\n",
    "# load the data into a dataframe:\n",
    "df = pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "import pandas as pd\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from pycausal import search as s\n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '2500M')\n",
    "from pycausal import prior as p\n",
    "\n",
    "tetrad = s.tetradrunner()\n",
    "#GFCI = Greedy Fast Causal Interference (GFCI) \n",
    "# bdeu = Bayesian Dirichlet likelihood equivalence and uniform\n",
    "tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "dot_str = pc.tetradGraphToDot(tetrad.getTetradGraph())\n",
    "graphs = pydot.graph_from_dot_data(dot_str)\n",
    "svg_str = graphs[0].create_svg()\n",
    "SVG(svg_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '500M')\n",
    "from pycausal import prior as p\n",
    "\n",
    "tetrad = s.tetradrunner()\n",
    "#GFCI = Greedy Fast Causal Interference (GFCI) \n",
    "# bdeu = Bayesian Dirichlet likelihood equivalence and uniform\n",
    "tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "dot_str = pc.tetradGraphToDot(tetrad.getTetradGraph())\n",
    "graphs = pydot.graph_from_dot_data(dot_str)\n",
    "print(len(graphs))\n",
    "svg_str = graphs[0].create_svg()\n",
    "SVG(svg_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying some various ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Synthetic data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#binomial???\n",
    "# This is for continous data.\n",
    "# generate some toy data:\n",
    "SIZE = 100000\n",
    "a = np.random.binomial(2, 0.2, size=SIZE)\n",
    "b = np.random.binomial(2, 0.3, size=SIZE)\n",
    "c = np.random.binomial(2, 0.4, size=SIZE)\n",
    "d = np.random.binomial(2, 0.5, size=SIZE)\n",
    "e = np.random.binomial(2, 0.6, size=SIZE)\n",
    "\n",
    "f= a + b + c + d + e + np.random.binomial(2, 0.5, size=SIZE)\n",
    "\n",
    "##### NOTE THAT RANDOM NORMAL IS HERE....\n",
    "#g = f + np.random.normal(2, 0.5, size=SIZE)\n",
    "g = f + np.random.binomial(2, 0.5, size=SIZE)\n",
    "\n",
    "df = pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "import pandas as pd\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from pycausal import search as s\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: LogisticRegression() 0.8778805298675331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Perceptron() 2.776005998500375\n",
      "Violation: {'f'} {'f', 'd'}\n",
      "Error: DecisionTreeClassifier() 0.5174706323419145\n",
      "Violation: {'f'} {'a', 'd'}\n",
      "Error: LinearSVC() 0.8417895526118471\n",
      "Violation: {'f'} {'c', 'f'}\n",
      "Error: GaussianNB() 0.5231692076980755\n",
      "Violation: {'f'} {'a', 'f'}\n",
      "Error: BernoulliNB() 1.6689827543114222\n",
      "Violation: {'f'} {'b', 'a', 'c', 'd', 'e'}\n",
      "Error: LinearDiscriminantAnalysis() 0.5049237690577356\n",
      "Violation: {'f'} {'b', 'a', 'c', 'd', 'e'}\n",
      "Error: RandomForestClassifier() 0.5192201949512621\n",
      "Violation: {'f'} {'b', 'a', 'c', 'd', 'e'}\n",
      "Error: ExtraTreesClassifier() 0.5174206448387904\n",
      "Violation: {'f'} {'a', 'd'}\n",
      "Error: LogisticRegression() 0.8771745650869825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Perceptron() 3.6105278944211157\n",
      "Violation: {'f'} {'f', 'd', 'e'}\n",
      "Error: DecisionTreeClassifier() 0.5184463107378524\n",
      "Violation: {'f'} {'b', 'f'}\n",
      "Error: LinearSVC() 0.8517296540691862\n",
      "Violation: {'f'} {'c', 'a', 'f'}\n",
      "Error: GaussianNB() 0.5249450109978004\n",
      "Violation: {'f'} {'c', 'a', 'b', 'd'}\n",
      "Error: BernoulliNB() 1.6755648870225954\n",
      "Violation: {'f'} {'b', 'a', 'c', 'd', 'e'}\n",
      "Error: LinearDiscriminantAnalysis() 0.503249350129974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: RandomForestClassifier() 0.5216956608678265\n",
      "Error: ExtraTreesClassifier() 0.5184463107378524\n",
      "Violation: {'f'} {'b', 'f'}\n",
      "Error: LogisticRegression() 0.8822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Perceptron() 2.54155\n",
      "Violation: {'f'} set()\n",
      "Error: DecisionTreeClassifier() 0.5122\n",
      "Violation: {'f'} {'a', 'f'}\n",
      "Error: LinearSVC() 0.85275\n",
      "Violation: {'f'} {'c', 'f'}\n",
      "Error: GaussianNB() 0.515\n",
      "Violation: {'f'} {'c', 'a', 'b', 'd'}\n",
      "Error: BernoulliNB() 1.671\n",
      "Violation: {'f'} {'b', 'a', 'c', 'd', 'e'}\n",
      "Error: LinearDiscriminantAnalysis() 0.4964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Violation: {'f'} {'b', 'a', 'c', 'd', 'e'}\n",
      "Error: RandomForestClassifier() 0.5126\n",
      "Error: ExtraTreesClassifier() 0.512\n",
      "Violation: {'f'} {'a', 'f'}\n",
      "Error: LogisticRegression() 0.8843384338433843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Perceptron() 3.044004400440044\n",
      "Violation: {'f'} {'c', 'f', 'd'}\n",
      "Error: DecisionTreeClassifier() 0.5109010901090109\n",
      "Violation: {'f'} {'b', 'f'}\n",
      "Error: LinearSVC() 0.8477347734773477\n",
      "Violation: {'f'} {'c', 'f'}\n",
      "Error: GaussianNB() 0.5227022702270226\n",
      "Violation: {'f'} {'c', 'a', 'b', 'd'}\n",
      "Error: BernoulliNB() 1.6837183718371838\n",
      "Violation: {'f'} {'b', 'a', 'c', 'd', 'e'}\n",
      "Error: LinearDiscriminantAnalysis() 0.5012501250125012\n",
      "Error: RandomForestClassifier() 0.514951495149515\n",
      "Error: ExtraTreesClassifier() 0.5110511051105111\n",
      "Violation: {'f'} {'b', 'f'}\n",
      "Error: LogisticRegression() 0.8808583004051418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Perceptron() 13.942930025508929\n",
      "Violation: {'f'} {'f', 'd'}\n",
      "Error: DecisionTreeClassifier() 0.5151302956034612\n",
      "Violation: {'f'} {'c', 'd', 'e'}\n",
      "Error: LinearSVC() 0.8441954684139449\n",
      "Violation: {'f'} {'f', 'd'}\n",
      "Error: GaussianNB() 0.525433901865653\n",
      "Violation: {'f'} {'c', 'a', 'b', 'd'}\n",
      "Error: BernoulliNB() 1.6744860701245436\n",
      "Violation: {'f'} {'b', 'a', 'c', 'd', 'e'}\n",
      "Error: LinearDiscriminantAnalysis() 0.5014254989246236\n",
      "Error: RandomForestClassifier() 0.516430750762767\n",
      "Violation: {'f'} {'c', 'a', 'd', 'e'}\n",
      "Error: ExtraTreesClassifier() 0.5151302956034612\n",
      "Violation: {'f'} {'c', 'd', 'e'}\n",
      "Violations =  [0. 5. 5. 5. 5. 5. 2. 2. 5.]\n",
      "Model_name =  LogisticRegression() Violations =  0.0\n",
      "Model_name =  Perceptron() Violations =  5.0\n",
      "Model_name =  DecisionTreeClassifier() Violations =  5.0\n",
      "Model_name =  LinearSVC() Violations =  5.0\n",
      "Model_name =  GaussianNB() Violations =  5.0\n",
      "Model_name =  BernoulliNB() Violations =  5.0\n",
      "Model_name =  LinearDiscriminantAnalysis() Violations =  2.0\n",
      "Model_name =  RandomForestClassifier() Violations =  2.0\n",
      "Model_name =  ExtraTreesClassifier() Violations =  5.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.2395781244593154, 2.053879133606337)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "\n",
    "from pycausal import search as s\n",
    "\n",
    "def get_CG(df, tetrad):\n",
    "\n",
    "    tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "def get_MB(graph, var, pc):\n",
    "    parents = set()\n",
    "    for i in pc.extractTetradGraphEdges(graph):\n",
    "        if i[-1] == var and i[3:5] == '->':\n",
    "            parents.add(i[0])\n",
    "    return parents\n",
    "\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '5000M')\n",
    "tetrad = s.tetradrunner()\n",
    "def mse_sklearn(models, model_names, x, y, num_folds=5):\n",
    "    '''\n",
    "    given an sklearn model, data, and number of folds returns mean and std dev of n-fold cross validated test auroc.\n",
    "    '''\n",
    "    results = []\n",
    "    violations = np.zeros(len(models))\n",
    "    mean = np.zeros((len(models), num_folds))\n",
    "    skf = StratifiedKFold(n_splits=num_folds)\n",
    "    fold = 0\n",
    "    for train_index, test_index in skf.split(x, y):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        for idx, model in enumerate(models):\n",
    "            model.fit(x_train, y_train)\n",
    "            #results.append(mean_squared_error(y_test, model.predict(x_test)))\n",
    "\n",
    "            original_df = pd.DataFrame(x_test, columns = ['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "            original_targets = pd.DataFrame(y_test, columns = ['g'])\n",
    "            original_df = original_df.join(original_targets)\n",
    "            #print(get_MB(get_CG(original_df), 'g'))\n",
    "\n",
    "            test_df = pd.DataFrame(x_test, columns = ['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "            test_targets = pd.DataFrame(model.predict(x_test), columns = ['g'])\n",
    "            test_df = test_df.join(test_targets)\n",
    "            #print(get_MB(get_CG(test_df), 'g'))\n",
    "            print(\"Error:\", model_names[idx], mean_squared_error(y_test, model.predict(x_test)))\n",
    "            mean[idx][fold] = mean_squared_error(y_test, model.predict(x_test))\n",
    "            \n",
    "            if get_MB(get_CG(original_df, tetrad), 'g', pc) != get_MB(get_CG(test_df, tetrad), 'g', pc):\n",
    "                print(\"Violation:\", get_MB(get_CG(original_df, tetrad), 'g', pc) , get_MB(get_CG(test_df, tetrad), 'g', pc))\n",
    "                violations[idx] += 1\n",
    "            #pc.stop_vm()\n",
    "        fold += 1\n",
    "    print(\"Violations = \", violations)\n",
    "    for i in range(len(violations)):\n",
    "        print(\"Model_name = \", model_names[i], \"Violations = \", violations[i])\n",
    "        print(\"MSE = \", np.mean(mean[i]), np.s)\n",
    "    return np.mean(results), np.std(results)\n",
    "\n",
    "X = df[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "y = df['g'].values\n",
    "\n",
    "models = [LogisticRegression(), \n",
    "          Perceptron(),  \n",
    "          DecisionTreeClassifier(),\n",
    "          LinearSVC(),\n",
    "          GaussianNB(),\n",
    "          BernoulliNB(),\n",
    "          LinearDiscriminantAnalysis(),\n",
    "          RandomForestClassifier(),\n",
    "          ExtraTreesClassifier()\n",
    "         ]\n",
    "model_names = ['LogisticRegression()', \n",
    "          'Perceptron()',  \n",
    "          'DecisionTreeClassifier()',\n",
    "          'LinearSVC()',\n",
    "          'GaussianNB()',\n",
    "          'BernoulliNB()',\n",
    "          'LinearDiscriminantAnalysis()',\n",
    "          'RandomForestClassifier()',\n",
    "          'ExtraTreesClassifier()'\n",
    "         ]\n",
    "\n",
    "mse_sklearn(models,model_names, X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 8 members, which is too few. The minimum number of members in any class cannot be less than n_splits=15.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: LogisticRegression() 0.8766671661921175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Perceptron() 6.427693690993556\n",
      "Violation: {'f'} {'f', 'e'}\n",
      "Error: DecisionTreeClassifier() 0.5134122583545632\n",
      "Violation: {'f'} {'b', 'a', 'c', 'd', 'e'}\n",
      "Error: LinearSVC() 0.8535890903641541\n",
      "Violation: {'f'} {'c', 'f'}\n",
      "Error: GaussianNB() 0.5280983066087217\n",
      "Error: BernoulliNB() 1.6461861231829762\n",
      "Violation: {'f'} {'b', 'a', 'c', 'd', 'e'}\n",
      "Error: LinearDiscriminantAnalysis() 0.5065188071332234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Violation: {'f'} {'b', 'a', 'c', 'd', 'e'}\n",
      "Error: RandomForestClassifier() 0.5137119736250562\n",
      "Violation: {'f'} {'b', 'a', 'c', 'd', 'e'}\n",
      "Error: ExtraTreesClassifier() 0.5132624007193166\n",
      "Violation: {'f'} {'b', 'a', 'c', 'd', 'e'}\n",
      "Error: LogisticRegression() 0.8851918465227818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Perceptron() 3.466576738609113\n",
      "Violation: {'f'} {'f', 'e'}\n",
      "Error: DecisionTreeClassifier() 0.5143884892086331\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b25ed5d5b66d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmse_sklearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-9087f4f88351>\u001b[0m in \u001b[0;36mmse_sklearn\u001b[0;34m(models, model_names, x, y, num_folds)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             self.loss, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"crammer_singer\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         epsilon, sample_weight)\n\u001b[0m\u001b[1;32m    891\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mse_sklearn(models,model_names, X,y, num_folds = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "print('log reg', mse_sklearn(logreg,X,y))\n",
    "perc = Perceptron()\n",
    "print('perceptron', mse_sklearn(perc,X,y))\n",
    "neigh = KNeighborsClassifier(n_neighbors=2)\n",
    "print('knn', mse_sklearn(neigh,X,y))\n",
    "tree = DecisionTreeClassifier()\n",
    "print('decision tree',mse_sklearn(tree,X,y))\n",
    "svc = LinearSVC()\n",
    "print('linear svm',mse_sklearn(svc,X,y))\n",
    "gnb = GaussianNB()\n",
    "print('gnb',mse_sklearn(gnb,X,y))\n",
    "bnb = BernoulliNB()\n",
    "print('bnb', mse_sklearn(bnb,X,y))\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "print('lda', mse_sklearn(lda,X,y))\n",
    "rfc = RandomForestClassifier()\n",
    "print('random forest',mse_sklearn(rfc,X,y))\n",
    "etc = ExtraTreesClassifier()\n",
    "print('ext. rand. trees',mse_sklearn(etc,X,y))\n",
    "ada = AdaBoostClassifier()\n",
    "print('ada',mse_sklearn(ada,X,y))\n",
    "bc = BaggingClassifier()\n",
    "print('baggging',mse_sklearn(bc,X,y))\n",
    "gdc = GradientBoostingClassifier()\n",
    "print('grad boost',mse_sklearn(gdc,X,y))\n",
    "#xgb = XGBClassifier()\n",
    "#print('xgb', mse_sklearn(xgb,X,y))\n",
    "mlp = MLPClassifier()\n",
    "print('mlp',mse_sklearn(mlp,X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
