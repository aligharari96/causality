{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes \n",
    "### Required installing Oracle JAVA 8 to get javabridge installed\n",
    "### Then, I was able to install py-causal from https://bd2kccd.github.io/docs/py-causal/\n",
    "### GFCI is slower than RFCI, but more accurate (SPIRTES), GFCI and RFCI account for unobserved variables, FGES assumes no unobserved variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure Learning Performance Guarantees If the assumptions in the previous section hold, then in the large sample limit, the CBN structure output by GFCId will contain an edge of one of four kinds between Xand Y   if and only if Xand Yare not independent conditional on any subset of the other measured variables of less than or equal to a specified size. In addition, there is (1) an arc X->Y   if and only if Xdirectly or indirectly causes Y, and Y   does not directly or indirectly cause X; (2) an edge X <-->Y   if and only if X   is not a direct or indirect cause of Yand Y   is not a direct or indirect cause of X(which can only occur if there are latent confounders of Xand some other variable or Yand some other variable; (3) an edge Xo->Y   only if Yis not a direct or indirect cause of X, but Xmay or may not be an indirect cause of Y; (4) an edge X oâ€“o Y   indicates that Xand Y   are dependent no matter what subset of observed variables is conditioned on, but contains no orientation information (X   may be a direct or indirect cause of Y, and Ymay be an indirect cause of X, or there may be a latent common cause of Xand Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for continous data.\n",
    "# generate some toy data:\n",
    "SIZE = 20000\n",
    "a = np.random.normal(size=SIZE, scale = 1)\n",
    "b = np.random.normal(size=SIZE, scale = 1)\n",
    "c = np.random.normal(size=SIZE, scale = 1)\n",
    "d = np.random.normal(size=SIZE, scale = 1)\n",
    "e = np.random.normal(size=SIZE, scale = 1)\n",
    "\n",
    "f= a + b + c + d + e + np.random.normal(size=SIZE, scale = 1)\n",
    "g = f + np.random.normal(size=SIZE, scale = 1)\n",
    "\n",
    "# load the data into a dataframe:\n",
    "df = pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "import pandas as pd\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from pycausal import search as s\n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '500M')\n",
    "from pycausal import prior as p\n",
    "\n",
    "tetrad = s.tetradrunner()\n",
    "#GFCI = Greedy Fast Causal Interference (GFCI) \n",
    "# bdeu = Bayesian Dirichlet likelihood equivalence and uniform\n",
    "tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "dot_str = pc.tetradGraphToDot(tetrad.getTetradGraph())\n",
    "graphs = pydot.graph_from_dot_data(dot_str)\n",
    "svg_str = graphs[0].create_svg()\n",
    "SVG(svg_str)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try and predict D HERE\n",
    "\n",
    "# This is for continous data.\n",
    "# generate some toy data:\n",
    "SIZE = 100000\n",
    "a = np.random.normal(size=SIZE, scale = 1)\n",
    "b = np.random.normal(size=SIZE, scale = 1)\n",
    "c = np.random.normal(size=SIZE, scale = 1)\n",
    "d = a + b + c + np.random.normal(size=SIZE, scale = 1)\n",
    "e = d + np.random.normal(size=SIZE, scale = 1)\n",
    "f= d + np.random.normal(size=SIZE, scale = 1)\n",
    "g = d + np.random.normal(size=SIZE, scale = 1)\n",
    "\n",
    "# load the data into a dataframe:\n",
    "df = pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "import pandas as pd\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from pycausal import search as s\n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '2500M')\n",
    "from pycausal import prior as p\n",
    "\n",
    "tetrad = s.tetradrunner()\n",
    "#GFCI = Greedy Fast Causal Interference (GFCI) \n",
    "# bdeu = Bayesian Dirichlet likelihood equivalence and uniform\n",
    "tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "dot_str = pc.tetradGraphToDot(tetrad.getTetradGraph())\n",
    "graphs = pydot.graph_from_dot_data(dot_str)\n",
    "svg_str = graphs[0].create_svg()\n",
    "SVG(svg_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '500M')\n",
    "from pycausal import prior as p\n",
    "\n",
    "tetrad = s.tetradrunner()\n",
    "#GFCI = Greedy Fast Causal Interference (GFCI) \n",
    "# bdeu = Bayesian Dirichlet likelihood equivalence and uniform\n",
    "tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "dot_str = pc.tetradGraphToDot(tetrad.getTetradGraph())\n",
    "graphs = pydot.graph_from_dot_data(dot_str)\n",
    "print(len(graphs))\n",
    "svg_str = graphs[0].create_svg()\n",
    "SVG(svg_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying some various ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Synthetic data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "\n",
    "def gen_data():\n",
    "    SIZE = 10000\n",
    "    a = np.random.binomial(2, 0.2, size=SIZE)\n",
    "    b = np.random.binomial(2, 0.3, size=SIZE)\n",
    "    c = np.random.binomial(2, 0.4, size=SIZE)\n",
    "    d = np.random.binomial(2, 0.5, size=SIZE)\n",
    "    e = np.random.binomial(2, 0.6, size=SIZE)\n",
    "    f= a + b + c + d + e + np.random.binomial(2, 0.5, size=SIZE)\n",
    "    g = f + np.random.binomial(2, 0.5, size=SIZE)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "\n",
    "import scipy.stats as ss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def discrete_gauss(low, high, samples):\n",
    "    x = np.arange(low, high)\n",
    "    xU, xL = x + 0.5, x - 0.5 \n",
    "    prob = ss.norm.cdf(xU, scale = 3) - ss.norm.cdf(xL, scale = 3)\n",
    "    prob = prob / prob.sum() #normalize the probabilities so their sum is 1\n",
    "    nums = np.random.choice(x, size = samples, p = prob)\n",
    "    return nums\n",
    "\n",
    "def gen_data():\n",
    "    SIZE = 10000\n",
    "    a = discrete_gauss(-3,3, SIZE)\n",
    "    b = discrete_gauss(-3,3, SIZE)\n",
    "    c = discrete_gauss(-3,3, SIZE)\n",
    "    d = discrete_gauss(-3,3, SIZE)\n",
    "    e = discrete_gauss(-3,3, SIZE)\n",
    "    f= a + b + c + d + e + discrete_gauss(-3,3, SIZE)\n",
    "    g = f + discrete_gauss(-3,3, SIZE)\n",
    "    #for idx, i in enumerate(g):\n",
    "    #    if i > np.mean(g):\n",
    "    #        g[idx] = 1\n",
    "    #    else:\n",
    "    #        g[idx] = 0\n",
    "    g[(g <= 3) & (g >= -3)] = 1\n",
    "    g[g < -3] = 0\n",
    "\n",
    "    g[g > 3] = 2\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "df = gen_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: LogisticRegression() 0.816591704147926\n",
      "Error: Perceptron() 0.7161419290354822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: DecisionTreeClassifier() 0.7706146926536732\n",
      "Error: LinearSVC() 0.8035982008995503\n",
      "Error: GaussianNB() 0.8285857071464268\n",
      "Error: BernoulliNB() 0.697151424287856\n",
      "Error: LinearDiscriminantAnalysis() 0.8260869565217391\n",
      "Error: RandomForestClassifier() 0.8025987006496752\n",
      "Error: ExtraTreesClassifier() 0.7956021989005497\n",
      "Error: LogisticRegression() 0.8045977011494253\n",
      "Error: Perceptron() 0.7241379310344828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: DecisionTreeClassifier() 0.7531234382808596\n",
      "Error: LinearSVC() 0.7981009495252374\n",
      "Error: GaussianNB() 0.8185907046476761\n",
      "Error: BernoulliNB() 0.6956521739130435\n",
      "Error: LinearDiscriminantAnalysis() 0.8170914542728636\n",
      "Error: RandomForestClassifier() 0.782608695652174\n",
      "Error: ExtraTreesClassifier() 0.7856071964017991\n",
      "Error: LogisticRegression() 0.7925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Perceptron() 0.749\n",
      "Violation: {'f'} {'a', 'f'}\n",
      "Error: DecisionTreeClassifier() 0.7465\n",
      "Error: LinearSVC() 0.79\n",
      "Error: GaussianNB() 0.817\n",
      "Error: BernoulliNB() 0.702\n",
      "Error: LinearDiscriminantAnalysis() 0.813\n",
      "Error: RandomForestClassifier() 0.7935\n",
      "Error: ExtraTreesClassifier() 0.7765\n",
      "Error: LogisticRegression() 0.8285\n",
      "Error: Perceptron() 0.751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Violation: {'f'} {'f', 'a', 'd'}\n",
      "Error: DecisionTreeClassifier() 0.7705\n",
      "Error: LinearSVC() 0.8215\n",
      "Error: GaussianNB() 0.8405\n",
      "Error: BernoulliNB() 0.72\n",
      "Error: LinearDiscriminantAnalysis() 0.8465\n",
      "Error: RandomForestClassifier() 0.805\n",
      "Error: ExtraTreesClassifier() 0.794\n",
      "Error: LogisticRegression() 0.8168168168168168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Perceptron() 0.7702702702702703\n",
      "Error: DecisionTreeClassifier() 0.7537537537537538\n",
      "Error: LinearSVC() 0.8083083083083084\n",
      "Error: GaussianNB() 0.8173173173173173\n",
      "Error: BernoulliNB() 0.6866866866866866\n",
      "Error: LinearDiscriminantAnalysis() 0.8268268268268268\n",
      "Error: RandomForestClassifier() 0.7952952952952953\n",
      "Error: ExtraTreesClassifier() 0.7812812812812813\n",
      "Violations =  [0. 2. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Model_name =  LogisticRegression() Violations =  0.0\n",
      "Accuracy =  0.8118012444228336 0.012258727345077854\n",
      "Model_name =  Perceptron() Violations =  2.0\n",
      "Accuracy =  0.742110026068047 0.01957979057196889\n",
      "Model_name =  DecisionTreeClassifier() Violations =  0.0\n",
      "Accuracy =  0.7588983769376573 0.009852984588645293\n",
      "Model_name =  LinearSVC() Violations =  0.0\n",
      "Accuracy =  0.8043014917466191 0.010538664745915019\n",
      "Model_name =  GaussianNB() Violations =  0.0\n",
      "Accuracy =  0.824398745822284 0.009114849893716805\n",
      "Model_name =  BernoulliNB() Violations =  0.0\n",
      "Accuracy =  0.7002980569775171 0.011025589297987191\n",
      "Model_name =  LinearDiscriminantAnalysis() Violations =  0.0\n",
      "Accuracy =  0.8259010475242858 0.011569394392520875\n",
      "Model_name =  RandomForestClassifier() Violations =  0.0\n",
      "Accuracy =  0.7958005383194289 0.007879309186862533\n",
      "Model_name =  ExtraTreesClassifier() Violations =  0.0\n",
      "Accuracy =  0.7865981353167261 0.007308679709130659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(nan, nan)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "\n",
    "from pycausal import search as s\n",
    "\n",
    "def get_CG(df, tetrad):\n",
    "    tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "    #tetrad.run(algoId = 'gfci', dfs = df, testId = 'bdeu', scoreId = 'bdeu', dataType = 'discrete',\n",
    "    #       structurePrior = 1.0, samplePrior = 1.0, maxDegree = 3, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "def get_MB(graph, var, pc):\n",
    "    parents = set()\n",
    "    for i in pc.extractTetradGraphEdges(graph):\n",
    "        if i[-1] == var and i[3:5] == '->':\n",
    "            parents.add(i[0])\n",
    "    return parents\n",
    "\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '5000M')\n",
    "tetrad = s.tetradrunner()\n",
    "def run_models(models, model_names, x, y, num_folds=5):\n",
    "    '''\n",
    "    given an sklearn model, data, and number of folds returns mean and std dev of n-fold cross validated test auroc.\n",
    "    '''\n",
    "    results = []\n",
    "    violations = np.zeros(len(models))\n",
    "    mean = np.zeros((len(models), num_folds))\n",
    "    skf = StratifiedKFold(n_splits=num_folds)\n",
    "    fold = 0\n",
    "    for train_index, test_index in skf.split(x, y):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        for idx, model in enumerate(models):\n",
    "            model.fit(x_train, y_train)\n",
    "            original_df = pd.DataFrame(x_test, columns = ['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "\n",
    "            original_targets = pd.DataFrame(y_test, columns = ['g'])\n",
    "            original_df = original_df.join(original_targets)\n",
    "            test_df = pd.DataFrame(x_test, columns = ['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "            test_targets = pd.DataFrame(model.predict(x_test), columns = ['g'])\n",
    "            test_df = test_df.join(test_targets)\n",
    "            #print(get_MB(get_CG(test_df), 'g'))\n",
    "            print(\"Error:\", model_names[idx], accuracy_score(y_test, model.predict(x_test)))\n",
    "            mean[idx][fold] = accuracy_score(y_test, model.predict(x_test))\n",
    "            if get_MB(get_CG(original_df, tetrad), 'g', pc) != get_MB(get_CG(test_df, tetrad), 'g', pc):\n",
    "                print(\"Violation:\", get_MB(get_CG(original_df, tetrad), 'g', pc) , get_MB(get_CG(test_df, tetrad), 'g', pc))\n",
    "                violations[idx] += 1\n",
    "            #pc.stop_vm()\n",
    "        fold += 1\n",
    "    \n",
    "    print(\"Violations = \", violations)\n",
    "    for i in range(len(violations)):\n",
    "        print(\"Model_name = \", model_names[i], \"Violations = \", violations[i])\n",
    "        print(\"Accuracy = \", np.mean(mean[i]), np.std(mean[i]))\n",
    "    return np.mean(results), np.std(results)\n",
    "\n",
    "X = df[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "y = df['g'].values\n",
    "\n",
    "models = [LogisticRegression(), \n",
    "          Perceptron(),  \n",
    "          DecisionTreeClassifier(),\n",
    "          LinearSVC(),\n",
    "          GaussianNB(),\n",
    "          BernoulliNB(),\n",
    "          LinearDiscriminantAnalysis(),\n",
    "          RandomForestClassifier(),\n",
    "          ExtraTreesClassifier(),\n",
    "          #AdaBoostClassifier(),\n",
    "          #BaggingClassifier(),\n",
    "          #GradientBoostingClassifier(),\n",
    "          #MLPClassifier()\n",
    "         ]\n",
    "model_names = ['LogisticRegression()', \n",
    "          'Perceptron()',  \n",
    "          'DecisionTreeClassifier()',\n",
    "          'LinearSVC()',\n",
    "          'GaussianNB()',\n",
    "          'BernoulliNB()',\n",
    "          'LinearDiscriminantAnalysis()',\n",
    "          'RandomForestClassifier()',\n",
    "          'ExtraTreesClassifier()', \n",
    "          #'AdaBoostClassifier()',\n",
    "          #'BaggingClassifier()',\n",
    "          #'GradientBoostingClassifier()',\n",
    "          #'MLPClassifier()'\n",
    "         ]\n",
    "\n",
    "run_models(models,model_names, X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_CG(df, tetrad)\n",
    "dot_str = pc.tetradGraphToDot(tetrad.getTetradGraph())\n",
    "graphs = pydot.graph_from_dot_data(dot_str)\n",
    "print(len(graphs))\n",
    "svg_str = graphs[0].create_svg()\n",
    "SVG(svg_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "[0 2 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 1 1]\n",
      "[ 985  987  989  990  991  992  993  994  995  996  998  999 1000 1001\n",
      " 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015\n",
      " 1016 1017]\n",
      "[0 0 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0]\n",
      "[1953 1955 1956 1957 1958 1961 1964 1965 1966 1968 1969 1972 1981 1983\n",
      " 1986 1990 1992 1993 1994 1995 2002 2003 2005 2007 2008 2010 2011 2012\n",
      " 2015 2017]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[2932 2968 2978 2980 2983 2984 2987 2988 2989 2990 2992 2994 2995 2999\n",
      " 3004 3006 3007 3009 3010 3011 3012 3013 3016 3018 3019 3021 3022 3023\n",
      " 3025 3026]\n",
      "[2 2 2 2 0 0 2 0 0 0 0 2 0 0 0 0 0 0 0 0 0 2 0 2 0 0 0 0 0 0]\n",
      "[3841 3848 3858 3874 3880 3897 3939 3982 3983 3985 3986 3988 3989 3992\n",
      " 3994 3996 3997 3998 3999 4000 4008 4010 4015 4016 4020 4021 4027 4029\n",
      " 4030 4031]\n",
      "[2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "[4947 4977 4982 4984 4989 4995 5008 5009 5010 5011 5012 5013 5014 5015\n",
      " 5016 5017 5018 5019 5020 5021 5022 5023 5024 5025 5026 5027 5028 5029\n",
      " 5030 5031]\n",
      "[2 2 2 2 2 2 1 1 0 1 1 1 1 1 1 0 1 1 2 1 0 1 0 1 1 1 1 0 1 1]\n",
      "[5977 5978 5979 5980 5982 5984 5985 5987 5988 5990 5991 5994 5995 5997\n",
      " 5998 5999 6001 6006 6011 6013 6015 6016 6018 6019 6020 6021 6022 6024\n",
      " 6025 6026]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 0]\n",
      "[6980 6983 6984 6986 6988 6991 6992 6993 6997 6999 7000 7002 7003 7004\n",
      " 7005 7006 7010 7012 7015 7017 7019 7022 7023 7025 7026 7027 7028 7029\n",
      " 7030 7031]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 1 1 0 0]\n",
      "[7985 7987 7988 7989 7991 7993 7994 7995 7997 7998 7999 8000 8001 8002\n",
      " 8003 8004 8005 8006 8007 8008 8009 8010 8011 8012 8013 8014 8015 8017\n",
      " 8018 8019]\n",
      "[1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0]\n",
      "[8975 8976 8977 8978 8979 8981 8982 8983 8984 8985 8986 8987 8988 8991\n",
      " 8994 8995 8996 8997 9001 9002 9003 9004 9006 9010 9017 9018 9023 9024\n",
      " 9025 9027]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# REMEMBER THAT STRATIFIED KFOLD PRESERVES SAMPLES\n",
    "X = df[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "y = df['g'].values\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    x_train, x_test = X[train_index], X[test_index]\n",
    "    y_train, Y_test = y[train_index], y[test_index]\n",
    "    print(test_index[:30])\n",
    "    print(Y_test[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[[3841, 3848]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "print('log reg', mse_sklearn(logreg,X,y))\n",
    "perc = Perceptron()\n",
    "print('perceptron', mse_sklearn(perc,X,y))\n",
    "neigh = KNeighborsClassifier(n_neighbors=2)\n",
    "print('knn', mse_sklearn(neigh,X,y))\n",
    "tree = DecisionTreeClassifier()\n",
    "print('decision tree',mse_sklearn(tree,X,y))\n",
    "svc = LinearSVC()\n",
    "print('linear svm',mse_sklearn(svc,X,y))\n",
    "gnb = GaussianNB()\n",
    "print('gnb',mse_sklearn(gnb,X,y))\n",
    "bnb = BernoulliNB()\n",
    "print('bnb', mse_sklearn(bnb,X,y))\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "print('lda', mse_sklearn(lda,X,y))\n",
    "rfc = RandomForestClassifier()\n",
    "print('random forest',mse_sklearn(rfc,X,y))\n",
    "etc = ExtraTreesClassifier()\n",
    "print('ext. rand. trees',mse_sklearn(etc,X,y))\n",
    "ada = AdaBoostClassifier()\n",
    "print('ada',mse_sklearn(ada,X,y))\n",
    "bc = BaggingClassifier()\n",
    "print('baggging',mse_sklearn(bc,X,y))\n",
    "gdc = GradientBoostingClassifier()\n",
    "print('grad boost',mse_sklearn(gdc,X,y))\n",
    "#xgb = XGBClassifier()\n",
    "#print('xgb', mse_sklearn(xgb,X,y))\n",
    "mlp = MLPClassifier()\n",
    "print('mlp',mse_sklearn(mlp,X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
