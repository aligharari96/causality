{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes \n",
    "### Required installing Oracle JAVA 8 to get javabridge installed\n",
    "### Then, I was able to install py-causal from https://bd2kccd.github.io/docs/py-causal/\n",
    "### GFCI is slower than RFCI, but more accurate (SPIRTES), GFCI and RFCI account for unobserved variables, FGES assumes no unobserved variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure Learning Performance Guarantees If the assumptions in the previous section hold, then in the large sample limit, the CBN structure output by GFCId will contain an edge of one of four kinds between Xand Y   if and only if Xand Yare not independent conditional on any subset of the other measured variables of less than or equal to a specified size. In addition, there is (1) an arc X->Y   if and only if Xdirectly or indirectly causes Y, and Y   does not directly or indirectly cause X; (2) an edge X <-->Y   if and only if X   is not a direct or indirect cause of Yand Y   is not a direct or indirect cause of X(which can only occur if there are latent confounders of Xand some other variable or Yand some other variable; (3) an edge Xo->Y   only if Yis not a direct or indirect cause of X, but Xmay or may not be an indirect cause of Y; (4) an edge X oâ€“o Y   indicates that Xand Y   are dependent no matter what subset of observed variables is conditioned on, but contains no orientation information (X   may be a direct or indirect cause of Y, and Ymay be an indirect cause of X, or there may be a latent common cause of Xand Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying some various ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import configparser\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, \\\n",
    "                        Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "import keras.optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "import glob, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "# select your GPU Here\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #Comment this line out if you want all GPUS (2 hehe)\n",
    "\n",
    "# python full-display web browser\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "def get_model(dense, dropouts, inputs):\n",
    "    # dense is an ordered list of the number of dense neurons like [1024, 2048, 1024]\n",
    "    # dropouts is an ordered list of the dropout masks like [0.2, 0.3, 0.4]\n",
    "    inputs = keras.Input(shape = (inputs,))\n",
    "\n",
    "    x = keras.layers.Dense(dense[0], activation = 'relu')(inputs)\n",
    "    x = keras.layers.Dropout(dropouts[0])(x, training=True)\n",
    "    for den, drop in zip(dense[1:], dropouts[1:]):\n",
    "        x = keras.layers.Dense(den, activation = 'relu')(x)\n",
    "        x = keras.layers.Dropout(drop)(x, training=True)\n",
    "    outputs = keras.layers.Dense(1, activation = 'linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from pycausal import search as s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def discrete_gauss(low, high, samples, std = 20):\n",
    "    x = np.arange(low, high)\n",
    "    xU, xL = x + 0.5, x - 0.5 \n",
    "    prob = ss.norm.cdf(xU, scale = std) - ss.norm.cdf(xL, scale = std)\n",
    "    prob = prob / prob.sum() #normalize the probabilities so their sum is 1\n",
    "    nums = np.random.choice(x, size = samples, p = prob)\n",
    "    return nums\n",
    "\n",
    "\n",
    "\n",
    "def bar_plot(x_ax, val1, val1std, val2, val2std):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ## the data\n",
    "    N = len(x_ax)\n",
    "\n",
    "    ## necessary variables\n",
    "    ind = np.arange(N)                # the x locations for the groups\n",
    "    width = 0.35                      # the width of the bars\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    ## the bars\n",
    "    rects1 = ax.bar(ind, val1, width,\n",
    "                    color='gray',\n",
    "                    yerr=val1std,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='blue'))\n",
    "\n",
    "    rects2 = ax.bar(ind+width, val2, width,\n",
    "                        color='blue',\n",
    "                        #yerr=val2std,\n",
    "                        error_kw=dict(elinewidth=2,ecolor='gray'))\n",
    "\n",
    "    # axes and labels\n",
    "    ax.set_xlim(-width,len(ind)+width)\n",
    "    #ax.set_ylim(0,45)\n",
    "    ax.set_ylabel('Percentage')\n",
    "    ax.set_title('')\n",
    "    plt.xticks(ind + width / 2, x_ax, rotation=75, size = 14)\n",
    "    ## add a legend\n",
    "    ax.legend( (rects1[0], rects2[0]), ('Accuracy', '% Violations') )\n",
    "    fig.savefig(\"violations.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 5000):\n",
    "    a = np.random.gumbel(mean, var, SIZE)\n",
    "    b = np.random.gumbel(mean, var, SIZE)\n",
    "    c = np.random.gumbel(mean, var, SIZE)\n",
    "    d = np.random.gumbel(mean, var, SIZE)\n",
    "    e = np.random.gumbel(mean, var, SIZE)\n",
    "    f= a + b + c + d + e + np.random.gumbel(mean, var, SIZE)\n",
    "    g = f + np.random.gumbel(mean,var, SIZE)\n",
    "    g = np.rint(g)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 5000):\n",
    "    a = np.random.gumbel(mean, var, SIZE)\n",
    "    b = np.random.gumbel(mean, var, SIZE)\n",
    "    c = np.random.gumbel(mean, var, SIZE)\n",
    "    d = np.random.gumbel(mean, var, SIZE)\n",
    "\n",
    "    f= a + b + c + d + np.random.gumbel(mean, var, SIZE)\n",
    "    g = f + np.random.gumbel(mean,var, SIZE)\n",
    "    \n",
    "    \n",
    "    g = np.rint(g)\n",
    "    e = g + np.random.gumbel(mean,var,SIZE)\n",
    "    \n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 40000):\n",
    "    a = np.random.normal(mean, var, SIZE)\n",
    "    b = np.random.normal(mean, var, SIZE)\n",
    "    c = np.random.normal(mean, var, SIZE)\n",
    "    d = np.random.normal(mean, var, SIZE)\n",
    "    e = np.random.normal(mean, var, SIZE)\n",
    "    f= a + b + c + d + e + np.random.normal(mean, var, SIZE)\n",
    "    g = f + np.random.normal(mean,var, SIZE)\n",
    "    #g = np.rint(g)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "def gen_data(mean = 0, var = 2, SIZE = 500000):\n",
    "    f = np.random.normal(mean, var, SIZE)\n",
    "    a = f + np.random.normal(mean, var, SIZE)\n",
    "    b = f + np.random.normal(mean, var, SIZE)\n",
    "    c = f + np.random.normal(mean, var, SIZE)\n",
    "    d = f + np.random.normal(mean, var, SIZE)\n",
    "    e = f + np.random.normal(mean, var, SIZE)\n",
    "    g = a + b + c + d  + e + np.random.normal(mean, var, SIZE)\n",
    "\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c, 'd' : d,'e' : e,'f':f, 'g':g})\n",
    "\n",
    "\n",
    "def get_CG(df, tetrad):\n",
    "    tetrad.run(algoId = 'gfci', dfs = df, testId = 'sem-bic', scoreId = 'sem-bic', dataType = 'continuous',\n",
    "           structurePrior = 1.0, samplePrior = 1.0, maxDegree = -1, maxPathLength = -1, \n",
    "           completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True)\n",
    "    #tetrad.run(algoId = 'gfci', dfs = df, testId = 'bdeu', scoreId = 'bdeu', dataType = 'discrete',\n",
    "    #       structurePrior = 1.0, samplePrior = 1.0, maxDegree = 3, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "def get_MB(graph, var, pc):\n",
    "    parents = set()\n",
    "    for i in pc.extractTetradGraphEdges(graph):\n",
    "        if i[-1] == var and i[3:5] == '->':\n",
    "            parents.add(i[0])\n",
    "        if i[0] == var and i[3:5] == '->':\n",
    "            parents.add(i[-1])\n",
    "    return parents\n",
    "\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from collections import defaultdict\n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '5000M')\n",
    "tetrad = s.tetradrunner()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "verbosity = 1\n",
    "\n",
    "\n",
    "models = [#[8,8,4],\n",
    "          #[16,16,8],\n",
    "          #LogisticRegression(), \n",
    "          #Perceptron(),  \n",
    "          #DecisionTreeClassifier(),\n",
    "          #LinearSVC(),\n",
    "          #GaussianNB(),\n",
    "          #[32,32,16],\n",
    "          #[64,64,32],\n",
    "          #[128, 128, 64],\n",
    "          #[256, 256, 128],\n",
    "          #[512, 512, 256],\n",
    "          #[1024, 1024, 512],\n",
    "          [2048, 2048, 2048, 1024],[2048, 2048, 2048, 1024],[2048, 2048, 2048, 1024],\n",
    "    [2048, 2048, 2048, 1024],[2048, 2048, 2048, 1024],[2048, 2048, 2048, 1024],\n",
    "     [2048, 2048, 2048, 1024],[2048, 2048, 2048, 1024],[2048, 2048, 2048, 1024],\n",
    "          #[2048, 2048, 1024],\n",
    "         ]\n",
    "model_names = ['m1', \n",
    "               'm2',\n",
    "               'm3',\n",
    "                'm4', \n",
    "               'm5',\n",
    "               'm6',\n",
    "\n",
    "               'm7', \n",
    "               'm8',\n",
    "               'm9',\n",
    "\n",
    "               #'m7', \n",
    "               #'m8',\n",
    "               #'m9',\n",
    "               #'m10', \n",
    "               #'m11a',\n",
    "               #'m11b',\n",
    "               #'m11c',\n",
    "              ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = gen_data()\n",
    "X = df[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "y = df['g'].values\n",
    "\n",
    "val_df = gen_data()\n",
    "x_val = df[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "y_val = df['g'].values\n",
    "\n",
    "\n",
    "model_list = []\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m1\n",
      "Train on 500000 samples, validate on 500000 samples\n",
      "Epoch 1/20\n",
      "500000/500000 [==============================] - 92s 184us/step - loss: 6.2068 - mean_squared_error: 6.2068 - val_loss: 5.3224 - val_mean_squared_error: 5.3224\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.32241, saving model to m1\n",
      "Epoch 2/20\n",
      "500000/500000 [==============================] - 91s 181us/step - loss: 5.4169 - mean_squared_error: 5.4169 - val_loss: 4.8647 - val_mean_squared_error: 4.8647\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.32241 to 4.86465, saving model to m1\n",
      "Epoch 3/20\n",
      "500000/500000 [==============================] - 86s 173us/step - loss: 5.1411 - mean_squared_error: 5.1411 - val_loss: 6.0539 - val_mean_squared_error: 6.0539\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 4.86465\n",
      "Epoch 4/20\n",
      "500000/500000 [==============================] - 85s 170us/step - loss: 5.0716 - mean_squared_error: 5.0716 - val_loss: 5.1800 - val_mean_squared_error: 5.1800\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 4.86465\n",
      "Epoch 5/20\n",
      "500000/500000 [==============================] - 84s 167us/step - loss: 5.0021 - mean_squared_error: 5.0021 - val_loss: 4.8605 - val_mean_squared_error: 4.8605\n",
      "\n",
      "Epoch 00005: val_loss improved from 4.86465 to 4.86049, saving model to m1\n",
      "Epoch 6/20\n",
      "500000/500000 [==============================] - 85s 169us/step - loss: 5.0042 - mean_squared_error: 5.0042 - val_loss: 4.9647 - val_mean_squared_error: 4.9647\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 4.86049\n",
      "Epoch 7/20\n",
      "500000/500000 [==============================] - 84s 169us/step - loss: 4.9732 - mean_squared_error: 4.9732 - val_loss: 4.8397 - val_mean_squared_error: 4.8397\n",
      "\n",
      "Epoch 00007: val_loss improved from 4.86049 to 4.83970, saving model to m1\n",
      "Epoch 8/20\n",
      "500000/500000 [==============================] - 84s 168us/step - loss: 4.9570 - mean_squared_error: 4.9570 - val_loss: 5.0819 - val_mean_squared_error: 5.0819\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 4.83970\n",
      "Epoch 9/20\n",
      "500000/500000 [==============================] - 84s 168us/step - loss: 4.9343 - mean_squared_error: 4.9343 - val_loss: 5.0399 - val_mean_squared_error: 5.0399\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 4.83970\n",
      "Epoch 10/20\n",
      "500000/500000 [==============================] - 84s 168us/step - loss: 4.9317 - mean_squared_error: 4.9317 - val_loss: 5.2799 - val_mean_squared_error: 5.2799\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 4.83970\n",
      "Epoch 00010: early stopping\n",
      "m2\n",
      "Train on 500000 samples, validate on 500000 samples\n",
      "Epoch 1/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 6.1742 - mean_squared_error: 6.1742 - val_loss: 5.3055 - val_mean_squared_error: 5.3055\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.30550, saving model to m2\n",
      "Epoch 2/20\n",
      "500000/500000 [==============================] - 85s 171us/step - loss: 5.4395 - mean_squared_error: 5.4395 - val_loss: 4.8835 - val_mean_squared_error: 4.8835\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.30550 to 4.88350, saving model to m2\n",
      "Epoch 3/20\n",
      "500000/500000 [==============================] - 85s 171us/step - loss: 5.1526 - mean_squared_error: 5.1526 - val_loss: 6.2463 - val_mean_squared_error: 6.2463\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 4.88350\n",
      "Epoch 4/20\n",
      "500000/500000 [==============================] - 85s 170us/step - loss: 5.0363 - mean_squared_error: 5.0363 - val_loss: 4.8002 - val_mean_squared_error: 4.8002\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.88350 to 4.80022, saving model to m2\n",
      "Epoch 5/20\n",
      "500000/500000 [==============================] - 85s 170us/step - loss: 5.0092 - mean_squared_error: 5.0092 - val_loss: 5.5413 - val_mean_squared_error: 5.5413\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 4.80022\n",
      "Epoch 6/20\n",
      "500000/500000 [==============================] - 85s 170us/step - loss: 5.0028 - mean_squared_error: 5.0028 - val_loss: 4.8906 - val_mean_squared_error: 4.8906\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 4.80022\n",
      "Epoch 7/20\n",
      "500000/500000 [==============================] - 85s 170us/step - loss: 4.9710 - mean_squared_error: 4.9710 - val_loss: 5.3797 - val_mean_squared_error: 5.3797\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 4.80022\n",
      "Epoch 00007: early stopping\n",
      "m3\n",
      "Train on 500000 samples, validate on 500000 samples\n",
      "Epoch 1/20\n",
      "500000/500000 [==============================] - 85s 171us/step - loss: 6.2504 - mean_squared_error: 6.2504 - val_loss: 5.3314 - val_mean_squared_error: 5.3314\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.33139, saving model to m3\n",
      "Epoch 2/20\n",
      "500000/500000 [==============================] - 85s 170us/step - loss: 5.3971 - mean_squared_error: 5.3971 - val_loss: 4.9853 - val_mean_squared_error: 4.9853\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.33139 to 4.98525, saving model to m3\n",
      "Epoch 3/20\n",
      "500000/500000 [==============================] - 85s 171us/step - loss: 5.1651 - mean_squared_error: 5.1651 - val_loss: 5.3042 - val_mean_squared_error: 5.3042\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 4.98525\n",
      "Epoch 4/20\n",
      "500000/500000 [==============================] - 86s 171us/step - loss: 5.0965 - mean_squared_error: 5.0965 - val_loss: 4.9252 - val_mean_squared_error: 4.9252\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.98525 to 4.92523, saving model to m3\n",
      "Epoch 5/20\n",
      "500000/500000 [==============================] - 85s 171us/step - loss: 5.0075 - mean_squared_error: 5.0075 - val_loss: 4.8865 - val_mean_squared_error: 4.8865\n",
      "\n",
      "Epoch 00005: val_loss improved from 4.92523 to 4.88653, saving model to m3\n",
      "Epoch 6/20\n",
      "500000/500000 [==============================] - 86s 171us/step - loss: 4.9892 - mean_squared_error: 4.9892 - val_loss: 4.8674 - val_mean_squared_error: 4.8674\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.88653 to 4.86738, saving model to m3\n",
      "Epoch 7/20\n",
      "500000/500000 [==============================] - 86s 171us/step - loss: 4.9714 - mean_squared_error: 4.9714 - val_loss: 4.7510 - val_mean_squared_error: 4.7510\n",
      "\n",
      "Epoch 00007: val_loss improved from 4.86738 to 4.75102, saving model to m3\n",
      "Epoch 8/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 4.9700 - mean_squared_error: 4.9700 - val_loss: 5.5992 - val_mean_squared_error: 5.5992\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 4.75102\n",
      "Epoch 9/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 4.9596 - mean_squared_error: 4.9596 - val_loss: 4.4788 - val_mean_squared_error: 4.4788\n",
      "\n",
      "Epoch 00009: val_loss improved from 4.75102 to 4.47881, saving model to m3\n",
      "Epoch 10/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 4.9223 - mean_squared_error: 4.9223 - val_loss: 4.8582 - val_mean_squared_error: 4.8582\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 4.47881\n",
      "Epoch 11/20\n",
      "500000/500000 [==============================] - 86s 171us/step - loss: 4.9349 - mean_squared_error: 4.9349 - val_loss: 5.1413 - val_mean_squared_error: 5.1413\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 4.47881\n",
      "Epoch 12/20\n",
      "500000/500000 [==============================] - 85s 169us/step - loss: 4.9276 - mean_squared_error: 4.9276 - val_loss: 4.7525 - val_mean_squared_error: 4.7525\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 4.47881\n",
      "Epoch 00012: early stopping\n",
      "m4\n",
      "Train on 500000 samples, validate on 500000 samples\n",
      "Epoch 1/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 6.1357 - mean_squared_error: 6.1357 - val_loss: 5.3089 - val_mean_squared_error: 5.3089\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.30887, saving model to m4\n",
      "Epoch 2/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 5.3798 - mean_squared_error: 5.3798 - val_loss: 5.7849 - val_mean_squared_error: 5.7849\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 5.30887\n",
      "Epoch 3/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 5.1543 - mean_squared_error: 5.1543 - val_loss: 4.6135 - val_mean_squared_error: 4.6135\n",
      "\n",
      "Epoch 00003: val_loss improved from 5.30887 to 4.61353, saving model to m4\n",
      "Epoch 4/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 5.0927 - mean_squared_error: 5.0927 - val_loss: 4.9292 - val_mean_squared_error: 4.9292\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 4.61353\n",
      "Epoch 5/20\n",
      "500000/500000 [==============================] - 86s 173us/step - loss: 5.0020 - mean_squared_error: 5.0020 - val_loss: 5.1583 - val_mean_squared_error: 5.1583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: val_loss did not improve from 4.61353\n",
      "Epoch 6/20\n",
      "500000/500000 [==============================] - 86s 173us/step - loss: 4.9744 - mean_squared_error: 4.9744 - val_loss: 4.8639 - val_mean_squared_error: 4.8639\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 4.61353\n",
      "Epoch 00006: early stopping\n",
      "m5\n",
      "Train on 500000 samples, validate on 500000 samples\n",
      "Epoch 1/20\n",
      "500000/500000 [==============================] - 86s 173us/step - loss: 6.2523 - mean_squared_error: 6.2523 - val_loss: 6.3306 - val_mean_squared_error: 6.3306\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.33063, saving model to m5\n",
      "Epoch 2/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 5.3676 - mean_squared_error: 5.3676 - val_loss: 5.6688 - val_mean_squared_error: 5.6688\n",
      "\n",
      "Epoch 00002: val_loss improved from 6.33063 to 5.66883, saving model to m5\n",
      "Epoch 3/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 5.1488 - mean_squared_error: 5.1488 - val_loss: 4.8320 - val_mean_squared_error: 4.8320\n",
      "\n",
      "Epoch 00003: val_loss improved from 5.66883 to 4.83203, saving model to m5\n",
      "Epoch 4/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 5.0586 - mean_squared_error: 5.0586 - val_loss: 5.1805 - val_mean_squared_error: 5.1805\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 4.83203\n",
      "Epoch 5/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 5.0077 - mean_squared_error: 5.0077 - val_loss: 4.5920 - val_mean_squared_error: 4.5920\n",
      "\n",
      "Epoch 00005: val_loss improved from 4.83203 to 4.59202, saving model to m5\n",
      "Epoch 6/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 4.9822 - mean_squared_error: 4.9822 - val_loss: 4.6583 - val_mean_squared_error: 4.6583\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 4.59202\n",
      "Epoch 7/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 4.9645 - mean_squared_error: 4.9645 - val_loss: 5.6025 - val_mean_squared_error: 5.6025\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 4.59202\n",
      "Epoch 8/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 4.9677 - mean_squared_error: 4.9677 - val_loss: 5.1242 - val_mean_squared_error: 5.1242\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 4.59202\n",
      "Epoch 00008: early stopping\n",
      "m6\n",
      "Train on 500000 samples, validate on 500000 samples\n",
      "Epoch 1/20\n",
      "500000/500000 [==============================] - 87s 173us/step - loss: 6.1382 - mean_squared_error: 6.1382 - val_loss: 6.3446 - val_mean_squared_error: 6.3446\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.34457, saving model to m6\n",
      "Epoch 2/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 5.4092 - mean_squared_error: 5.4092 - val_loss: 4.8639 - val_mean_squared_error: 4.8639\n",
      "\n",
      "Epoch 00002: val_loss improved from 6.34457 to 4.86391, saving model to m6\n",
      "Epoch 3/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 5.1540 - mean_squared_error: 5.1540 - val_loss: 5.2331 - val_mean_squared_error: 5.2331\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 4.86391\n",
      "Epoch 4/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 5.0701 - mean_squared_error: 5.0701 - val_loss: 4.7245 - val_mean_squared_error: 4.7245\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.86391 to 4.72446, saving model to m6\n",
      "Epoch 5/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 5.0296 - mean_squared_error: 5.0296 - val_loss: 5.0106 - val_mean_squared_error: 5.0106\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 4.72446\n",
      "Epoch 6/20\n",
      "500000/500000 [==============================] - 86s 173us/step - loss: 4.9515 - mean_squared_error: 4.9515 - val_loss: 5.0311 - val_mean_squared_error: 5.0311\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 4.72446\n",
      "Epoch 7/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 4.9376 - mean_squared_error: 4.9376 - val_loss: 5.4865 - val_mean_squared_error: 5.4865\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 4.72446\n",
      "Epoch 00007: early stopping\n",
      "m7\n",
      "Train on 500000 samples, validate on 500000 samples\n",
      "Epoch 1/20\n",
      "500000/500000 [==============================] - 87s 173us/step - loss: 6.1972 - mean_squared_error: 6.1972 - val_loss: 5.4174 - val_mean_squared_error: 5.4174\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.41745, saving model to m7\n",
      "Epoch 2/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 5.3669 - mean_squared_error: 5.3669 - val_loss: 4.7558 - val_mean_squared_error: 4.7558\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.41745 to 4.75578, saving model to m7\n",
      "Epoch 3/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 5.1663 - mean_squared_error: 5.1663 - val_loss: 5.0274 - val_mean_squared_error: 5.0274\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 4.75578\n",
      "Epoch 4/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 5.0525 - mean_squared_error: 5.0525 - val_loss: 5.7152 - val_mean_squared_error: 5.7152\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 4.75578\n",
      "Epoch 5/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 5.0187 - mean_squared_error: 5.0187 - val_loss: 5.2891 - val_mean_squared_error: 5.2891\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 4.75578\n",
      "Epoch 00005: early stopping\n",
      "m8\n",
      "Train on 500000 samples, validate on 500000 samples\n",
      "Epoch 1/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 6.2797 - mean_squared_error: 6.2797 - val_loss: 5.1930 - val_mean_squared_error: 5.1930\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.19301, saving model to m8\n",
      "Epoch 2/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 5.3503 - mean_squared_error: 5.3503 - val_loss: 5.1140 - val_mean_squared_error: 5.1140\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.19301 to 5.11396, saving model to m8\n",
      "Epoch 3/20\n",
      "500000/500000 [==============================] - 86s 173us/step - loss: 5.1298 - mean_squared_error: 5.1298 - val_loss: 4.8640 - val_mean_squared_error: 4.8640\n",
      "\n",
      "Epoch 00003: val_loss improved from 5.11396 to 4.86402, saving model to m8\n",
      "Epoch 4/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 5.0799 - mean_squared_error: 5.0799 - val_loss: 5.0698 - val_mean_squared_error: 5.0698\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 4.86402\n",
      "Epoch 5/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 4.9832 - mean_squared_error: 4.9832 - val_loss: 4.7558 - val_mean_squared_error: 4.7558\n",
      "\n",
      "Epoch 00005: val_loss improved from 4.86402 to 4.75579, saving model to m8\n",
      "Epoch 6/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 4.9702 - mean_squared_error: 4.9702 - val_loss: 4.6546 - val_mean_squared_error: 4.6546\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.75579 to 4.65456, saving model to m8\n",
      "Epoch 7/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 4.9510 - mean_squared_error: 4.9510 - val_loss: 5.2233 - val_mean_squared_error: 5.2233\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 4.65456\n",
      "Epoch 8/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 4.9579 - mean_squared_error: 4.9579 - val_loss: 4.6059 - val_mean_squared_error: 4.6059\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.65456 to 4.60591, saving model to m8\n",
      "Epoch 9/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 4.9285 - mean_squared_error: 4.9285 - val_loss: 4.8183 - val_mean_squared_error: 4.8183\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 4.60591\n",
      "Epoch 10/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 4.9321 - mean_squared_error: 4.9321 - val_loss: 4.5932 - val_mean_squared_error: 4.5932\n",
      "\n",
      "Epoch 00010: val_loss improved from 4.60591 to 4.59320, saving model to m8\n",
      "Epoch 11/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 4.9432 - mean_squared_error: 4.9432 - val_loss: 4.8874 - val_mean_squared_error: 4.8874\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 4.59320\n",
      "Epoch 12/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 4.9084 - mean_squared_error: 4.9084 - val_loss: 4.9952 - val_mean_squared_error: 4.9952\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 4.59320\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000/500000 [==============================] - 86s 172us/step - loss: 4.9098 - mean_squared_error: 4.9098 - val_loss: 4.6084 - val_mean_squared_error: 4.6084\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 4.59320\n",
      "Epoch 00013: early stopping\n",
      "m9\n",
      "Train on 500000 samples, validate on 500000 samples\n",
      "Epoch 1/20\n",
      "500000/500000 [==============================] - 86s 173us/step - loss: 6.1669 - mean_squared_error: 6.1669 - val_loss: 5.3692 - val_mean_squared_error: 5.3692\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.36920, saving model to m9\n",
      "Epoch 2/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 5.4020 - mean_squared_error: 5.4020 - val_loss: 4.8646 - val_mean_squared_error: 4.8646\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.36920 to 4.86459, saving model to m9\n",
      "Epoch 3/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 5.1374 - mean_squared_error: 5.1374 - val_loss: 4.8841 - val_mean_squared_error: 4.8841\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 4.86459\n",
      "Epoch 4/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 5.0576 - mean_squared_error: 5.0576 - val_loss: 5.0063 - val_mean_squared_error: 5.0063\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 4.86459\n",
      "Epoch 5/20\n",
      "500000/500000 [==============================] - 86s 172us/step - loss: 4.9991 - mean_squared_error: 4.9991 - val_loss: 5.5100 - val_mean_squared_error: 5.5100\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 4.86459\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "for idx, model_name in enumerate(model_names):\n",
    "    print(model_name)\n",
    "\n",
    "    if type(models[idx]) is list:\n",
    "        #clear session\n",
    "        keras.backend.clear_session() \n",
    "        #get model according to specification\n",
    "        model = get_model(models[idx], [0.2] * len(models), 6)\n",
    "        callbacks = [ModelCheckpoint(model_name, verbose= verbosity, monitor='val_loss',save_best_only=True), \n",
    "                     EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=3, verbose= verbosity, mode='auto')]\n",
    "        model.compile(optimizer = optimizers.SGD(lr = 0.001, momentum = 0.9, ), loss='mean_squared_error', metrics = ['mse'])\n",
    "        #print(len(X), len(y))\n",
    "        model.fit(X, y, epochs = 20, validation_data = (x_val, y_val), callbacks = callbacks, batch_size = 32, verbose = verbosity)\n",
    "    else:\n",
    "        models[idx].fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times =  0\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "Times =  1\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "Times =  2\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "Times =  3\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "Times =  4\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "Times =  5\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "Times =  6\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "Times =  7\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "Times =  8\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "Times =  9\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "Times =  10\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "Times =  11\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "Times =  12\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "Times =  13\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "Times =  14\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "Times =  15\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n",
      "{'d', 'b', 'c', 'e', 'a'} {'d', 'b', 'c', 'e', 'a'}\n",
      "Error in SETA markov blanket\n"
     ]
    }
   ],
   "source": [
    "#the number of times to sample \n",
    "times = 25\n",
    "\n",
    "## the size of the test set\n",
    "nb_test = 20000\n",
    "\n",
    "violations = np.zeros(len(models))\n",
    "violation_mean = np.zeros((len(models), times))\n",
    "mean = np.zeros((len(models), times))\n",
    "\n",
    "fold = 0\n",
    "\n",
    "#metrics_dicts = []\n",
    "causal_dicts = []\n",
    "for m in models:\n",
    "#    metrics_dicts.append(defaultdict(list))\n",
    "    causal_dicts.append(defaultdict(list))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for t in range(times):\n",
    "    print(\"Times = \", t)\n",
    "    df_test = gen_data(SIZE = nb_test)\n",
    "    x_test = df_test[['a', 'b', 'c', 'd', 'e', 'f']].values\n",
    "    y_test = df_test['g'].values\n",
    "\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        if type(models[idx]) is list:\n",
    "            keras.backend.clear_session()\n",
    "            model = load_model(model_name)\n",
    "        else:\n",
    "            model = models[idx]\n",
    "        test_df = pd.DataFrame(x_test, columns = ['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "        test_targets = pd.DataFrame(model.predict(x_test), columns = ['g'])\n",
    "        test_df = test_df.join(test_targets)\n",
    "        mean[idx][t] = mean_squared_error(y_test, model.predict(x_test))  \n",
    "        #print(mean_squared_error(y_test, model.predict(x_test)))\n",
    "\n",
    "        setA = get_MB(get_CG(df_test, tetrad), 'g', pc)\n",
    "        setB = get_MB(get_CG(test_df, tetrad), 'g', pc)\n",
    "\n",
    "        print(setA, setB)\n",
    "        if setA != {'f'}:\n",
    "            print(\"Error in SETA markov blanket\")\n",
    "            #setA = {'f'}\n",
    "\n",
    "        violation_mean[idx][t] = len(setA.difference(setB)) + len(setB.difference(setA))\n",
    "        if setA != setB:\n",
    "            #print(\"Violation:\", model_names[idx], setA , setB)\n",
    "            violations[idx] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dicts = []\n",
    "for m in models:\n",
    "    metrics_dicts.append(defaultdict(list))\n",
    "\n",
    "\n",
    "#means = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "#variances = [1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0]\n",
    "means = [0,1,2]\n",
    "variances = [1,  2, 3]\n",
    "\n",
    "\n",
    "# ok at this point we need to check the model on various variances and means\n",
    "for m in means:\n",
    "    for v in variances:\n",
    "        print(m,v)\n",
    "        #t0 = time.time()\n",
    "        perturbed_df = gen_data(mean =m, var = v, SIZE = nb_test)\n",
    "        y_test2 = perturbed_df['g']\n",
    "        x_test2 = perturbed_df[['a', 'b', 'c', 'd', 'e', 'f']]\n",
    "        #t1 = time.time()\n",
    "        #print(\"Time for gen_data = \", t1 - t0)\n",
    "        for idx, model_name in enumerate(model_names):\n",
    "            #t0 = time.time()\n",
    "            if type(models[idx]) is list:\n",
    "                keras.backend.clear_session()\n",
    "                model = load_model(model_name)\n",
    "            else:\n",
    "                model = models[idx]\n",
    "            #t1 = time.time()\n",
    "            #print(\"Time to load model = \", t1 - t0)\n",
    "            \n",
    "            y_pred2 = model.predict(x_test2)\n",
    "            metrics_dicts[idx][str(m) + '_' + str(v)].append(mean_squared_error(y_test2, y_pred2))\n",
    "\n",
    "            test_df2 = pd.DataFrame(x_test2, columns = ['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "            test_targets2 = pd.DataFrame(model.predict(x_test2), columns = ['g'])\n",
    "            test_df2 = test_df2.join(test_targets2)\n",
    "'''\n",
    "            setA = get_MB(get_CG(perturbed_df, tetrad), 'g', pc)\n",
    "            if setA != {'f'}:\n",
    "                print(\"Error in SETA markov blanket\")\n",
    "                #setA = {'f'}\n",
    "            setC = get_MB(get_CG(test_df2, tetrad), 'g', pc)\n",
    "\n",
    "            if setA != setC:\n",
    "                causal_dicts[idx][str(m) + '_' + str(v)].append(1)\n",
    "            else:\n",
    "                causal_dicts[idx][str(m) + '_' + str(v)].append(0)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "print(\"Violations = \", violations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = []\n",
    "metric_err = []\n",
    "viol = []\n",
    "viol_err = []\n",
    "\n",
    "#normalize the violations for prettier graphing.\n",
    "#also violations are always positive, so just divide by max.\n",
    "\n",
    "#TMK\n",
    "#violation_mean = violation_mean / np.max(violation_mean)\n",
    "\n",
    "for i in range(len(violations)):\n",
    "    print(\"Model_name = \", model_names[i], \"Violations = \", violations[i])\n",
    "    print(\"Average_violations = \", np.mean(violation_mean[i]), np.std(violation_mean[i]))\n",
    "    print(\"MSE = \", np.mean(mean[i]), np.std(mean[i]))\n",
    "    #print(\"mean = \", mean[i])\n",
    "    metric.append(np.mean(mean[i]))\n",
    "    metric_err.append(np.std(mean[i]))\n",
    "    viol.append(np.mean(violation_mean[i]))\n",
    "    #viol.append(violations[i]/times)\n",
    "    viol_err.append(np.std(violation_mean[i]))\n",
    "print(np.array(metric), \n",
    "         np.array(metric_err), \n",
    "         np.array(viol), \n",
    "         np.array(viol_err))    \n",
    "\n",
    "bar_plot(model_names, \n",
    "         np.array(metric), \n",
    "         np.array(metric_err), \n",
    "         np.array(viol), \n",
    "         np.array(viol_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def heat_plot(x,y,z, xlab = 'Mean', ylab = 'Variance', clim_low = 0, clim_high = 1):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    cax = ax.scatter(x, y, c=z, s=450, edgecolor='')\n",
    "    cax.set_clim(clim_low, clim_high)\n",
    "    ax.set_xlabel(xlab)\n",
    "    ax.set_ylabel(ylab)\n",
    "    plt.colorbar(cax)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "MSE = []\n",
    "VIO = []\n",
    "AUS = []\n",
    "for i, m in enumerate(models):\n",
    "    print(model_names[i])\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "    \n",
    "    \n",
    "    rectangular_approx = 0\n",
    "    for k, v in metrics_dicts[i].items():\n",
    "        x.append(float(k.split('_')[0]))\n",
    "        y.append(float(k.split('_')[-1]))\n",
    "        z.append(np.mean(v))\n",
    "        rectangular_approx += np.mean(v)\n",
    "    print(\"Area under surface (rectangular approx) = \", rectangular_approx)\n",
    "    print(\"Violations = \", violations[i])\n",
    "    print(\"Average_violations = \", np.mean(violation_mean[i]))\n",
    "    print(\"MSE = \", np.mean(mean[i]))   \n",
    "    MSE.append(np.mean(mean[i]))\n",
    "    VIO.append(np.mean(violation_mean[i]))\n",
    "    #VIO.append(violations[i]/times)\n",
    "    AUS.append(rectangular_approx)\n",
    "    \n",
    "    heat_plot(x,y,z, clim_low = 0, clim_high = 10)\n",
    "    \n",
    "heat_plot(MSE,VIO,AUS, xlab = 'MSE', ylab='Violations', clim_low = np.min(AUS), clim_high = np.max(AUS))\n",
    "    \n",
    "#for idx, m in enumerate(models):\n",
    "#    print(model_names[idx])      \n",
    "#    x = []\n",
    "#    y = []\n",
    "#    z = []\n",
    "#    for k, v in causal_dicts[idx].items():\n",
    "#        x.append(k.split('_')[0])\n",
    "#        y.append(k.split('_')[-1])\n",
    "#        z.append(np.sum(v) / len(v))\n",
    "#    print(\"Causal assurance\")\n",
    "#    heat_plot(x,y,z, clim_low = 0, clim_high = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
