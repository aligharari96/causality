{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import configparser\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, \\\n",
    "                        Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "import keras.optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "import glob, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_absolute_error, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from pycausal import search as s\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from collections import defaultdict\n",
    "from numpy.polynomial.polynomial import polyfit  \n",
    "from scipy.stats import pearsonr\n",
    "from pylab import text\n",
    "from pycausal import prior as p\n",
    "import itertools\n",
    "\n",
    "# select your GPU Here\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" #Comment this line out if you want all GPUS (2 hehe)\n",
    "\n",
    "# python full-display web browser\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 20000):\n",
    "    a = np.random.normal(mean, var, SIZE)\n",
    "    b = a + np.random.normal(mean, var, SIZE)\n",
    "    c =  a + b + np.random.normal(mean, var, SIZE)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c})\n",
    "\n",
    "def get_model(dense, dropouts, inputs):\n",
    "    # dense is an ordered list of the number of dense neurons like [1024, 2048, 1024]\n",
    "    # dropouts is an ordered list of the dropout masks like [0.2, 0.3, 0.4]\n",
    "    inputs = keras.Input(shape = (inputs,))\n",
    "    x = keras.layers.Dense(dense[0], activation = 'relu')(inputs)\n",
    "    x = keras.layers.Dropout(dropouts[0])(x, training=True)\n",
    "    for den, drop in zip(dense[1:], dropouts[1:]):\n",
    "        x = keras.layers.Dense(den, activation = 'relu')(x)\n",
    "        x = keras.layers.Dropout(drop)(x, training=True)\n",
    "    outputs = keras.layers.Dense(1, activation = 'linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def get_bic(df, prior, penalty = 2):\n",
    "\n",
    "    tetrad.run(algoId = 'fges', dfs = df, scoreId = 'cond-gauss-bic', \n",
    "           priorKnowledge = prior, dataType = 'mixed', numCategoriesToDiscretize = 4,\n",
    "           structurePrior = 1.0, maxDegree = 3, faithfulnessAssumed = True, verbose = True)\n",
    "    \n",
    "    BIC = tetrad.getTetradGraph().getAllAttributes().toString()\n",
    "    BIC = float(BIC.split('=')[-1].split('}')[0])\n",
    "    return BIC #/ len(df)\n",
    "\n",
    "def make_categorical(df, complete_df, categoricals):   \n",
    "    retval = None\n",
    "    for key in df.columns:\n",
    "        if retval is not None:\n",
    "            if key in categoricals:\n",
    "                retval = np.concatenate((retval, to_categorical(df[key], len(complete_df[key].unique()))), axis = 1)\n",
    "            else:\n",
    "                retval = np.concatenate((retval, df[key].values[...,np.newaxis]), axis = 1)\n",
    "        else:\n",
    "            if key in categoricals:\n",
    "                retval = to_categorical(df[key], len(complete_df[key].unique()))\n",
    "            else:\n",
    "                retval = df[key]\n",
    "    return retval\n",
    "num_models = 100          \n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '21000M')\n",
    "tetrad = s.tetradrunner()\n",
    "\n",
    "inputs = ['gender', 'race', 'lunch', 'test_preparation_course', 'education', 'reading_score', 'writing_score']\n",
    "#inputs = ['gender', 'race', 'lunch', 'test_preparation_course', 'education']#, 'math_score', 'writing_score']\n",
    "target = ['math_score']\n",
    "categoricals = ['gender', 'race', 'education', 'lunch', 'test_preparation_course'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test_preparation_course</th>\n",
       "      <th>math_score</th>\n",
       "      <th>reading_score</th>\n",
       "      <th>writing_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.711111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.922222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.481928</td>\n",
       "      <td>0.377778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  race  education  lunch  test_preparation_course  math_score  \\\n",
       "0       0     1          1      1                        1        0.72   \n",
       "1       0     2          4      1                        0        0.69   \n",
       "2       0     1          3      1                        1        0.90   \n",
       "3       1     0          0      0                        1        0.47   \n",
       "4       1     2          4      1                        1        0.76   \n",
       "\n",
       "   reading_score  writing_score  \n",
       "0       0.662651       0.711111  \n",
       "1       0.879518       0.866667  \n",
       "2       0.939759       0.922222  \n",
       "3       0.481928       0.377778  \n",
       "4       0.734940       0.722222  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "df = pd.read_csv('~/Desktop/Kaggle/students-performance-in-exams/StudentsPerformance.csv')\n",
    "# Prior knowledge knows that the test prep comes before the outcome.\n",
    "df = df.rename(index=str, columns={\"reading score\": \"reading_score\", \"writing score\": \"writing_score\",  \"math score\": \"math_score\",\n",
    "                              \"test preparation course\": \"test_preparation_course\", \"parental level of education\": \"education\", \"race/ethnicity\" : \"race\"\n",
    "                             })\n",
    "#return a list of encoders and an updated dataframe\n",
    "label_encoder_list = []\n",
    "#one_hot = ['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course']\n",
    "for i,col in enumerate(categoricals):\n",
    "    label_encoder_list.append(LabelEncoder())\n",
    "    df[col] = label_encoder_list[i].fit_transform(df[col].values)\n",
    "def normalize(a):\n",
    "    return (a - np.min(a)) / (np.max(a) - np.min(a))\n",
    "df['math_score'] = normalize(df['math_score'])\n",
    "df['writing_score'] = normalize(df['writing_score'])\n",
    "df['reading_score'] = normalize(df['reading_score'])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256]] ['temp/school0', 'temp/school1', 'temp/school2', 'temp/school3', 'temp/school4', 'temp/school5', 'temp/school6', 'temp/school7', 'temp/school8', 'temp/school9', 'temp/school10', 'temp/school11', 'temp/school12', 'temp/school13', 'temp/school14', 'temp/school15', 'temp/school16', 'temp/school17', 'temp/school18', 'temp/school19', 'temp/school20', 'temp/school21', 'temp/school22', 'temp/school23', 'temp/school24', 'temp/school25', 'temp/school26', 'temp/school27', 'temp/school28', 'temp/school29', 'temp/school30', 'temp/school31', 'temp/school32', 'temp/school33', 'temp/school34', 'temp/school35', 'temp/school36', 'temp/school37', 'temp/school38', 'temp/school39', 'temp/school40', 'temp/school41', 'temp/school42', 'temp/school43', 'temp/school44', 'temp/school45', 'temp/school46', 'temp/school47', 'temp/school48', 'temp/school49', 'temp/school50', 'temp/school51', 'temp/school52', 'temp/school53', 'temp/school54', 'temp/school55', 'temp/school56', 'temp/school57', 'temp/school58', 'temp/school59', 'temp/school60', 'temp/school61', 'temp/school62', 'temp/school63', 'temp/school64', 'temp/school65', 'temp/school66', 'temp/school67', 'temp/school68', 'temp/school69', 'temp/school70', 'temp/school71', 'temp/school72', 'temp/school73', 'temp/school74', 'temp/school75', 'temp/school76', 'temp/school77', 'temp/school78', 'temp/school79', 'temp/school80', 'temp/school81', 'temp/school82', 'temp/school83', 'temp/school84', 'temp/school85', 'temp/school86', 'temp/school87', 'temp/school88', 'temp/school89', 'temp/school90', 'temp/school91', 'temp/school92', 'temp/school93', 'temp/school94', 'temp/school95', 'temp/school96', 'temp/school97', 'temp/school98', 'temp/school99']\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"260pt\" viewBox=\"0.00 0.00 276.17 260.00\" width=\"276pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 256)\">\n",
       "<title>g</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-256 272.1679,-256 272.1679,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- test_preparation_course -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>test_preparation_course</title>\n",
       "<ellipse cx=\"134.1936\" cy=\"-90\" fill=\"none\" rx=\"94.7833\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134.1936\" y=\"-86.3\">test_preparation_course</text>\n",
       "</g>\n",
       "<!-- writing_score -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>writing_score</title>\n",
       "<ellipse cx=\"134.1936\" cy=\"-18\" fill=\"none\" rx=\"59.2899\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134.1936\" y=\"-14.3\">writing_score</text>\n",
       "</g>\n",
       "<!-- test_preparation_course&#45;&gt;writing_score -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>test_preparation_course-&gt;writing_score</title>\n",
       "<path d=\"M134.1936,-71.8314C134.1936,-64.131 134.1936,-54.9743 134.1936,-46.4166\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"137.6937,-46.4132 134.1936,-36.4133 130.6937,-46.4133 137.6937,-46.4132\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- reading_score -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>reading_score</title>\n",
       "<ellipse cx=\"73.1936\" cy=\"-162\" fill=\"none\" rx=\"59.5901\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"73.1936\" y=\"-158.3\">reading_score</text>\n",
       "</g>\n",
       "<!-- reading_score&#45;&gt;test_preparation_course -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>reading_score-&gt;test_preparation_course</title>\n",
       "<path d=\"M94.7552,-136.5502C102.7932,-127.0628 111.7397,-116.5031 119.077,-107.8425\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"92.066,-134.3099 88.2723,-144.2022 97.4069,-138.8348 92.066,-134.3099\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- lunch -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>lunch</title>\n",
       "<ellipse cx=\"73.1936\" cy=\"-234\" fill=\"none\" rx=\"30.5947\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"73.1936\" y=\"-230.3\">lunch</text>\n",
       "</g>\n",
       "<!-- lunch&#45;&gt;writing_score -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>lunch-&gt;writing_score</title>\n",
       "<path d=\"M49.2766,-222.5782C33.2829,-213.4998 13.5214,-199.1447 4.1936,-180 -2.8144,-165.6164 1.4352,-159.7604 4.1936,-144 10.0592,-110.4869 7.7799,-97.596 30.1936,-72 44.4683,-55.6986 64.8665,-43.6145 83.8222,-35.0601\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"85.4494,-38.171 93.2699,-31.0233 82.6989,-31.734 85.4494,-38.171\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- lunch&#45;&gt;reading_score -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>lunch-&gt;reading_score</title>\n",
       "<path d=\"M73.1936,-215.8314C73.1936,-208.131 73.1936,-198.9743 73.1936,-190.4166\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"76.6937,-190.4132 73.1936,-180.4133 69.6937,-190.4133 76.6937,-190.4132\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- math_score -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>math_score</title>\n",
       "<ellipse cx=\"203.1936\" cy=\"-162\" fill=\"none\" rx=\"51.9908\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"203.1936\" y=\"-158.3\">math_score</text>\n",
       "</g>\n",
       "<!-- lunch&#45;&gt;math_score -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>lunch-&gt;math_score</title>\n",
       "<path d=\"M95.5414,-221.6228C115.0243,-210.8323 143.6603,-194.9723 166.501,-182.3221\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"168.2033,-185.3803 175.2555,-177.4734 164.8118,-179.2567 168.2033,-185.3803\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- math_score&#45;&gt;test_preparation_course -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>math_score-&gt;test_preparation_course</title>\n",
       "<path d=\"M179.3659,-137.1363C170.105,-127.4727 159.706,-116.6217 151.2221,-107.7689\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"177.0446,-139.7726 186.4907,-144.5708 182.0985,-134.9292 177.0446,-139.7726\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- gender -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>gender</title>\n",
       "<ellipse cx=\"203.1936\" cy=\"-234\" fill=\"none\" rx=\"34.394\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"203.1936\" y=\"-230.3\">gender</text>\n",
       "</g>\n",
       "<!-- gender&#45;&gt;writing_score -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>gender-&gt;writing_score</title>\n",
       "<path d=\"M226.5585,-220.4601C240.3071,-211.0577 256.47,-197.1524 264.1936,-180 270.763,-165.4108 266.9521,-159.7604 264.1936,-144 258.328,-110.4869 260.6074,-97.596 238.1936,-72 223.919,-55.6986 203.5208,-43.6145 184.5651,-35.0601\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"185.6884,-31.734 175.1174,-31.0233 182.9379,-38.171 185.6884,-31.734\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- gender&#45;&gt;reading_score -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>gender-&gt;reading_score</title>\n",
       "<path d=\"M179.3496,-220.7941C160.1512,-210.1611 132.8153,-195.0213 110.6914,-182.768\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"112.3223,-179.6703 101.8786,-177.8871 108.9307,-185.7939 112.3223,-179.6703\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- gender&#45;&gt;math_score -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>gender-&gt;math_score</title>\n",
       "<path d=\"M203.1936,-215.8314C203.1936,-208.131 203.1936,-198.9743 203.1936,-190.4166\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"206.6937,-190.4132 203.1936,-180.4133 199.6937,-190.4133 206.6937,-190.4132\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydot\n",
    "from IPython.display import SVG\n",
    "def examine_graph_continuous(df, prior = None):\n",
    "    tetrad.run(algoId = 'fges', dfs = df,  scoreId = 'sem-bic', dataType = 'continuous',\n",
    "               structurePrior = 1.0, samplePrior = 1, maxDegree = -1, maxPathLength = -1, priorKnowledge = prior,\n",
    "               completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True,\n",
    "               )\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "def examine_graph_mixed(df, prior = None):\n",
    "    tetrad.run(algoId = 'fges', dfs = df, scoreId = 'cond-gauss-bic', \n",
    "           priorKnowledge = prior, dataType = 'mixed', numCategoriesToDiscretize = 5,\n",
    "           structurePrior = 1.0, maxDegree = -1, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "def examine_graph_discrete(df, prior = None):\n",
    "    tetrad.run(algoId = 'fges', dfs = df, scoreId = 'bdeu', priorKnowledge = prior, dataType = 'discrete',\n",
    "               structurePrior = 1.0, samplePrior = 1.0, maxDegree = 3, faithfulnessAssumed = True, verbose = True)\n",
    "    return tetrad.getTetradGraph()\n",
    "    \n",
    "    \n",
    "\n",
    "tempForbid = p.ForbiddenWithin(['reading_score', 'writing_score', 'math_score'])\n",
    "temporal = [[ 'race', 'lunch', 'test_preparation_course', 'gender', 'education'], tempForbid]\n",
    "prior = p.knowledge( addtemporal = temporal)\n",
    "\n",
    "g = examine_graph_mixed(df[inputs + target], prior = prior)\n",
    "dot_str = pc.tetradGraphToDot(g)\n",
    "graphs = pydot.graph_from_dot_data(dot_str)\n",
    "svg_str = graphs[0].create_svg()\n",
    "\n",
    "known_conx = set({})\n",
    "for i in tetrad.getEdges():\n",
    "    if ' --> ' in i:\n",
    "        known_conx.add((i.split(' --> ')[0], i.split(' --> ')[1]))\n",
    "known_conx\n",
    "\n",
    "prior = p.knowledge(requiredirect =  list(map(list, known_conx)),)\n",
    "models = []\n",
    "model_names = []\n",
    "\n",
    "\n",
    "\n",
    "randomize = False\n",
    "if randomize:\n",
    "    layers = [256, 512, 1024, 2048, 4096]\n",
    "    for i in range(num_models):\n",
    "        network = []\n",
    "        for j in range(3):\n",
    "            network.append(layers[random.randint(0,len(layers) -1)])\n",
    "        models.append(network)\n",
    "        model_names.append('temp/random' + str(i))\n",
    "    print(models, model_names)    \n",
    "else:\n",
    "    model_layers = [512,256]\n",
    "    for i in range(num_models):\n",
    "        models.append(model_layers)\n",
    "        model_names.append('temp/school' + str(i))\n",
    "\n",
    "print(models, model_names)\n",
    "\n",
    "SVG(svg_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 math_score\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bestMSE = []\n",
    "bestBIC = []\n",
    "bestCOMBO = []\n",
    "original_df = df.copy()\n",
    "for t in range(100):\n",
    "    # let's split our df into two by race.  Let's see what happens if we \n",
    "    df = original_df.copy()\n",
    "    \n",
    "    holdout = 200\n",
    "        #df_test = df[df['charges'] > 0.54].copy()\n",
    "    continuous = [\"reading_score\",  \"writing_score\", \"math_score\"]\n",
    "    continuous = [\"math_score\"]\n",
    "    \n",
    "    '''\n",
    "    end_idx = len(df) - holdout\n",
    "    cont = random.randint(0, len(continuous) - 1)\n",
    "    start_idx = random.randint(0, end_idx)\n",
    "    print(t, \"Doing range:\",start_idx, start_idx + holdout, \"and \", continuous[cont])\n",
    "    df_test = df.nlargest(len(df) - start_idx, continuous[cont]).nsmallest(holdout, continuous[cont])\n",
    "    '''\n",
    "    small = random.randint(0,1)\n",
    "    cont = random.randint(0, len(continuous) - 1)\n",
    "    if small == 0:\n",
    "        df_test = df.nsmallest(holdout, continuous[cont])\n",
    "    else:\n",
    "        df_test = df.nlargest(holdout, continuous[cont])\n",
    "    print(t, small, continuous[cont])\n",
    "    \n",
    "\n",
    "    df.drop(df_test.index, inplace = True)\n",
    "    df_test.reset_index(inplace = True)\n",
    "    df.sample(frac= 1).reset_index(inplace = True) # this will shuffle and reset index\n",
    "\n",
    "    x_test = df_test[inputs]\n",
    "    y_test = df_test[target]\n",
    "\n",
    "    causal_split = 0.2\n",
    "    val_split = 0.2\n",
    "    train_split = 1 - (causal_split + val_split)\n",
    "\n",
    "    x_causal = df[inputs][-int(causal_split * len(df)) :]\n",
    "    y_causal = df[target][-int(causal_split * len(df)) :]\n",
    "\n",
    "    x_val = df[inputs][int(train_split * len(df)):-int(causal_split * len(df))]\n",
    "    y_val = df[target][int(train_split * len(df)):-int(causal_split * len(df))]\n",
    "\n",
    "    x_train = df[inputs][:int(train_split * len(df))]\n",
    "    y_train = df[target][:int(train_split * len(df))]\n",
    "\n",
    "    x_test_NN = make_categorical(x_test, original_df, categoricals)\n",
    "    x_causal_NN = make_categorical(x_causal, original_df, categoricals)\n",
    "    x_val_NN = make_categorical(x_val, original_df, categoricals)\n",
    "    x_train_NN = make_categorical(x_train, original_df, categoricals)\n",
    "    verbosity = 0\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        if type(models[idx]) is list:\n",
    "            #clear session\n",
    "            keras.backend.clear_session() \n",
    "            #get model according to specification\n",
    "            model = get_model(models[idx], [0.2] * len(models), np.shape(x_train_NN)[1])\n",
    "            callbacks = [ModelCheckpoint(model_name, verbose= verbosity, monitor='val_loss',save_best_only=True), \n",
    "                         EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose= verbosity, mode='auto')]\n",
    "            model.compile(optimizer = optimizers.SGD(lr = 0.0001, momentum = 0.9, ), loss='mean_squared_error', metrics = ['mse'])\n",
    "            #print(len(X), len(y))\n",
    "            model.fit(x_train_NN, y_train, epochs = 20, validation_data = (x_val_NN, y_val), callbacks = callbacks, batch_size = 32, verbose = verbosity)\n",
    "        else:\n",
    "            models[idx].fit(X,y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    generalization = []\n",
    "    metrics = []\n",
    "    proposed = []\n",
    "    x_causal.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        #print(model_name)\n",
    "        if type(models[idx]) is list:\n",
    "            keras.backend.clear_session()\n",
    "            model = load_model(model_name)\n",
    "        else:\n",
    "            model = models[idx]\n",
    "\n",
    "        y_pred = model.predict(x_test_NN)\n",
    "        generalization.append(mean_absolute_error(y_pred, y_test))\n",
    "\n",
    "        #### CHECK FOR CAUSAL METRIC HERE\n",
    "        y_causal_pred = model.predict(x_causal_NN)\n",
    "        causal_targets = pd.DataFrame(y_causal_pred, columns = target)\n",
    "        causal_targets.reset_index(drop=True, inplace = True)\n",
    "        causal_df = x_causal.join(causal_targets)\n",
    "\n",
    "\n",
    "\n",
    "        metrics.append(mean_absolute_error(y_causal_pred, y_causal))\n",
    "        #print(x_causal.head)\n",
    "        bic_pred = get_bic(causal_df, prior)\n",
    "        #print(bic_pred, tetrad.getEdges())\n",
    "\n",
    "        found_conx = set({})\n",
    "        for i in tetrad.getEdges():\n",
    "            if ' --> ' in i:\n",
    "                found_conx.add((i.split(' --> ')[0], i.split(' --> ')[1]))\n",
    "        found_conx\n",
    "\n",
    "        if found_conx == known_conx:\n",
    "            proposed.append(bic_pred)\n",
    "        else:\n",
    "            print(\"******Found an error\")\n",
    "            # for now just remove bad model.  Will need to add it to distance metric.\n",
    "            metrics = metrics[:-1]\n",
    "            generalization = generalization[:-1]\n",
    "    nbest = 10\n",
    "    total = normalize(metrics) + normalize(proposed)\n",
    "    final = pd.DataFrame(np.stack((metrics, proposed, total, generalization), axis = 1), columns = ['metrics', 'proposed', 'combined', 'generalization'])\n",
    "    print(\"MSE = \", np.sum(final.nsmallest(nbest, 'metrics')['generalization']))\n",
    "    print(\"BIC = \", np.sum(final.nsmallest(nbest, 'proposed')['generalization']))\n",
    "    print(\"COMB = \",np.sum(final.nsmallest(nbest, 'combined')['generalization']))\n",
    "    bestMSE.append(final.nsmallest(nbest, 'metrics')['generalization'])\n",
    "    bestBIC.append(final.nsmallest(nbest, 'proposed')['generalization'])\n",
    "    bestCOMBO.append(final.nsmallest(nbest, 'combined')['generalization'])\n",
    "np.mean(bestMSE), np.mean(bestCOMBO), np.std(bestMSE), np.std(bestCOMBO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(bestMSE), np.mean(bestCOMBO), np.std(bestMSE), np.std(bestCOMBO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(proposed,generalization, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(proposed,generalization)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(proposed,generalization, '.')\n",
    "plt.plot(proposed, b + m * np.array(proposed), '-')\n",
    "ax.set_xlabel(\"BIC\")\n",
    "ax.set_ylabel(\"GEN\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(metrics,generalization, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(metrics,generalization)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(metrics,generalization, '.')\n",
    "plt.plot(metrics, b + m * np.array(metrics), '-')\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"GEN\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "total = normalize(metrics) + normalize(proposed)\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(total,generalization, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(total,generalization)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(total,generalization, '.')\n",
    "plt.plot(total, b + m * np.array(total), '-')\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"GEN\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbest = 10\n",
    "final = pd.DataFrame(np.stack((metrics, proposed, total, generalization), axis = 1), columns = ['metrics', 'proposed', 'combined', 'generalization'])\n",
    "print(\"MSE = \", np.sum(final.nsmallest(nbest, 'metrics')['generalization']))\n",
    "print(\"BIC = \", np.sum(final.nsmallest(nbest, 'proposed')['generalization']))\n",
    "print(\"COMB = \",np.sum(final.nsmallest(nbest, 'combined')['generalization']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
