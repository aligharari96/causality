{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import configparser\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, \\\n",
    "                        Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "import keras.optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "import glob, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_absolute_error, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from pycausal import search as s\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from collections import defaultdict\n",
    "from numpy.polynomial.polynomial import polyfit  \n",
    "from scipy.stats import pearsonr\n",
    "from pylab import text\n",
    "from pycausal import prior as p\n",
    "import itertools\n",
    "\n",
    "# select your GPU Here\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" #Comment this line out if you want all GPUS (2 hehe)\n",
    "\n",
    "# python full-display web browser\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "def gen_data(mean = 0, var = 1, SIZE = 20000):\n",
    "    a = np.random.normal(mean, var, SIZE)\n",
    "    b = a + np.random.normal(mean, var, SIZE)\n",
    "    c =  a + b + np.random.normal(mean, var, SIZE)\n",
    "    return pd.DataFrame({'a' : a,'b' : b, 'c' : c})\n",
    "\n",
    "def get_model(dense, dropouts, inputs):\n",
    "    # dense is an ordered list of the number of dense neurons like [1024, 2048, 1024]\n",
    "    # dropouts is an ordered list of the dropout masks like [0.2, 0.3, 0.4]\n",
    "    inputs = keras.Input(shape = (inputs,))\n",
    "    x = keras.layers.Dense(dense[0], activation = 'relu')(inputs)\n",
    "    x = keras.layers.Dropout(dropouts[0])(x, training=False)\n",
    "    for den, drop in zip(dense[1:], dropouts[1:]):\n",
    "        x = keras.layers.Dense(den, activation = 'relu')(x)\n",
    "        x = keras.layers.Dropout(drop)(x, training=False)\n",
    "    outputs = keras.layers.Dense(1, activation = 'linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def get_bic(df, prior, penalty = 2):\n",
    "\n",
    "    tetrad.run(algoId = 'fges', dfs = df, scoreId = 'cond-gauss-bic', \n",
    "           priorKnowledge = prior, dataType = 'mixed', numCategoriesToDiscretize = 4,\n",
    "           structurePrior = 1.0, maxDegree = 3, faithfulnessAssumed = True, verbose = True)\n",
    "    \n",
    "    BIC = tetrad.getTetradGraph().getAllAttributes().toString()\n",
    "    BIC = float(BIC.split('=')[-1].split('}')[0])\n",
    "    return BIC #/ len(df)\n",
    "\n",
    "def make_categorical(df, complete_df, categoricals):   \n",
    "    retval = None\n",
    "    for key in df.columns:\n",
    "        if retval is not None:\n",
    "            if key in categoricals:\n",
    "                retval = np.concatenate((retval, to_categorical(df[key], len(complete_df[key].unique()))), axis = 1)\n",
    "            else:\n",
    "                retval = np.concatenate((retval, df[key].values[...,np.newaxis]), axis = 1)\n",
    "        else:\n",
    "            if key in categoricals:\n",
    "                retval = to_categorical(df[key], len(complete_df[key].unique()))\n",
    "            else:\n",
    "                retval = df[key]\n",
    "    return retval\n",
    "num_models = 100       \n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '21000M')\n",
    "tetrad = s.tetradrunner()\n",
    "\n",
    "inputs = ['Sex', 'Equipment', 'Age',  'Deadlift1Kg', 'Bench1Kg']\n",
    "#inputs = ['gender', 'race', 'lunch', 'test_preparation_course', 'education']#, 'math_score', 'writing_score']\n",
    "target = ['Squat1Kg']\n",
    "categoricals = ['Sex', 'Equipment'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (35) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "df = pd.read_csv('~/Desktop/Kaggle/openpowerlifting.csv')\n",
    "df = df[['Sex', 'Equipment', 'Age', 'Squat1Kg', 'Deadlift1Kg', 'Bench1Kg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)\n",
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "label_encoder_list = []\n",
    "#one_hot = ['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course']\n",
    "for i,col in enumerate(categoricals):\n",
    "    label_encoder_list.append(LabelEncoder())\n",
    "    df[col] = label_encoder_list[i].fit_transform(df[col].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(a):\n",
    "    return (a - np.min(a)) / (np.max(a) - np.min(a))\n",
    "df['Age'] = normalize(df['Age'])\n",
    "df['Squat1Kg'] = normalize(df['Squat1Kg'])\n",
    "df['Deadlift1Kg'] = normalize(df['Deadlift1Kg'])\n",
    "df['Bench1Kg'] = normalize(df['Bench1Kg'])\n",
    "#df['TotalKg'] = normalize(df['TotalKg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Equipment</th>\n",
       "      <th>Age</th>\n",
       "      <th>Squat1Kg</th>\n",
       "      <th>Deadlift1Kg</th>\n",
       "      <th>Bench1Kg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.303665</td>\n",
       "      <td>0.572072</td>\n",
       "      <td>0.620482</td>\n",
       "      <td>0.554252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.303665</td>\n",
       "      <td>0.590090</td>\n",
       "      <td>0.644578</td>\n",
       "      <td>0.565982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.240838</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>0.647590</td>\n",
       "      <td>0.580645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.471204</td>\n",
       "      <td>0.608108</td>\n",
       "      <td>0.668675</td>\n",
       "      <td>0.583578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.387435</td>\n",
       "      <td>0.614865</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>0.586510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sex  Equipment       Age  Squat1Kg  Deadlift1Kg  Bench1Kg\n",
       "0    0          3  0.303665  0.572072     0.620482  0.554252\n",
       "1    0          3  0.303665  0.590090     0.644578  0.565982\n",
       "2    0          3  0.240838  0.405405     0.647590  0.580645\n",
       "3    0          3  0.471204  0.608108     0.668675  0.583578\n",
       "4    0          3  0.387435  0.614865     0.638554  0.586510"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.head()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256]] ['temp/school0', 'temp/school1', 'temp/school2', 'temp/school3', 'temp/school4', 'temp/school5', 'temp/school6', 'temp/school7', 'temp/school8', 'temp/school9', 'temp/school10', 'temp/school11', 'temp/school12', 'temp/school13', 'temp/school14', 'temp/school15', 'temp/school16', 'temp/school17', 'temp/school18', 'temp/school19', 'temp/school20', 'temp/school21', 'temp/school22', 'temp/school23', 'temp/school24', 'temp/school25', 'temp/school26', 'temp/school27', 'temp/school28', 'temp/school29', 'temp/school30', 'temp/school31', 'temp/school32', 'temp/school33', 'temp/school34', 'temp/school35', 'temp/school36', 'temp/school37', 'temp/school38', 'temp/school39', 'temp/school40', 'temp/school41', 'temp/school42', 'temp/school43', 'temp/school44', 'temp/school45', 'temp/school46', 'temp/school47', 'temp/school48', 'temp/school49', 'temp/school50', 'temp/school51', 'temp/school52', 'temp/school53', 'temp/school54', 'temp/school55', 'temp/school56', 'temp/school57', 'temp/school58', 'temp/school59', 'temp/school60', 'temp/school61', 'temp/school62', 'temp/school63', 'temp/school64', 'temp/school65', 'temp/school66', 'temp/school67', 'temp/school68', 'temp/school69', 'temp/school70', 'temp/school71', 'temp/school72', 'temp/school73', 'temp/school74', 'temp/school75', 'temp/school76', 'temp/school77', 'temp/school78', 'temp/school79', 'temp/school80', 'temp/school81', 'temp/school82', 'temp/school83', 'temp/school84', 'temp/school85', 'temp/school86', 'temp/school87', 'temp/school88', 'temp/school89', 'temp/school90', 'temp/school91', 'temp/school92', 'temp/school93', 'temp/school94', 'temp/school95', 'temp/school96', 'temp/school97', 'temp/school98', 'temp/school99']\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"332pt\" viewBox=\"0.00 0.00 248.65 332.00\" width=\"249pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 328)\">\n",
       "<title>g</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-328 244.6482,-328 244.6482,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- Age -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>Age</title>\n",
       "<ellipse cx=\"66.0734\" cy=\"-306\" fill=\"none\" rx=\"27\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"66.0734\" y=\"-302.3\">Age</text>\n",
       "</g>\n",
       "<!-- Bench1Kg -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>Bench1Kg</title>\n",
       "<ellipse cx=\"149.0734\" cy=\"-234\" fill=\"none\" rx=\"48.1917\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"149.0734\" y=\"-230.3\">Bench1Kg</text>\n",
       "</g>\n",
       "<!-- Age&#45;&gt;Bench1Kg -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>Age-&gt;Bench1Kg</title>\n",
       "<path d=\"M82.8697,-291.4297C94.1671,-281.6295 109.3049,-268.498 122.2537,-257.2653\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"124.719,-259.7601 129.9794,-250.5634 120.132,-254.4724 124.719,-259.7601\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- Deadlift1Kg -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>Deadlift1Kg</title>\n",
       "<ellipse cx=\"66.0734\" cy=\"-162\" fill=\"none\" rx=\"54.6905\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"66.0734\" y=\"-158.3\">Deadlift1Kg</text>\n",
       "</g>\n",
       "<!-- Age&#45;&gt;Deadlift1Kg -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>Age-&gt;Deadlift1Kg</title>\n",
       "<path d=\"M66.0734,-287.7623C66.0734,-263.201 66.0734,-219.2474 66.0734,-190.3541\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"69.5735,-190.0896 66.0734,-180.0896 62.5735,-190.0897 69.5735,-190.0896\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- Squat1Kg -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>Squat1Kg</title>\n",
       "<ellipse cx=\"76.0734\" cy=\"-18\" fill=\"none\" rx=\"46.2923\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"76.0734\" y=\"-14.3\">Squat1Kg</text>\n",
       "</g>\n",
       "<!-- Age&#45;&gt;Squat1Kg -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>Age-&gt;Squat1Kg</title>\n",
       "<path d=\"M52.8456,-290.0217C30.9174,-261.6002 -9.4652,-199.9772 2.0734,-144 9.1071,-109.8774 15.1751,-101.8911 33.0734,-72 39.1069,-61.9237 46.8224,-51.6487 54.048,-42.8036\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"56.7398,-45.0407 60.4837,-35.1294 51.3762,-40.5427 56.7398,-45.0407\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- Bench1Kg&#45;&gt;Deadlift1Kg -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>Bench1Kg-&gt;Deadlift1Kg</title>\n",
       "<path d=\"M129.8229,-217.3008C116.5098,-205.7521 98.8596,-190.4411 85.507,-178.8581\" fill=\"none\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- Equipment -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>Equipment</title>\n",
       "<ellipse cx=\"171.0734\" cy=\"-90\" fill=\"none\" rx=\"50.0912\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"171.0734\" y=\"-86.3\">Equipment</text>\n",
       "</g>\n",
       "<!-- Bench1Kg&#45;&gt;Equipment -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>Bench1Kg-&gt;Equipment</title>\n",
       "<path d=\"M157.3289,-205.7721C159.4853,-197.5251 161.6119,-188.4549 163.0734,-180 167.3484,-155.2682 169.4047,-126.2949 170.3457,-108.0428\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"153.8841,-205.1012 154.6294,-215.6698 160.6374,-206.9432 153.8841,-205.1012\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- Bench1Kg&#45;&gt;Squat1Kg -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>Bench1Kg-&gt;Squat1Kg</title>\n",
       "<path d=\"M168.8061,-217.4475C202.625,-187.0577 264.812,-120.9934 230.0734,-72 216.6767,-53.106 157.5753,-36.3194 116.6144,-26.6553\" fill=\"none\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- Sex -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>Sex</title>\n",
       "<ellipse cx=\"76.0734\" cy=\"-90\" fill=\"none\" rx=\"27\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"76.0734\" y=\"-86.3\">Sex</text>\n",
       "</g>\n",
       "<!-- Bench1Kg&#45;&gt;Sex -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>Bench1Kg-&gt;Sex</title>\n",
       "<path d=\"M147.2578,-205.5632C145.0985,-187.095 140.3793,-163.0125 130.0734,-144 121.3542,-127.9148 106.3436,-113.4797 94.3583,-103.5898\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"143.7935,-206.0984 148.2562,-215.7075 150.7598,-205.4127 143.7935,-206.0984\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- Deadlift1Kg&#45;&gt;Equipment -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>Deadlift1Kg-&gt;Equipment</title>\n",
       "<path d=\"M98.4042,-139.8303C114.3368,-128.9051 133.1922,-115.9757 147.7032,-106.0252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"96.1675,-137.1202 89.8995,-145.6621 100.1262,-142.8933 96.1675,-137.1202\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- Deadlift1Kg&#45;&gt;Squat1Kg -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>Deadlift1Kg-&gt;Squat1Kg</title>\n",
       "<path d=\"M55.0253,-144.2555C49.3343,-134.063 42.9902,-120.7818 40.0734,-108 36.5136,-92.401 35.4368,-87.3135 40.0734,-72 44.1874,-58.4123 52.9726,-45.1186 60.8342,-35.121\" fill=\"none\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- Deadlift1Kg&#45;&gt;Sex -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>Deadlift1Kg-&gt;Sex</title>\n",
       "<path d=\"M69.9825,-133.8545C71.1693,-125.3097 72.4422,-116.1442 73.516,-108.4133\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"66.5059,-133.4449 68.5968,-143.8314 73.4393,-134.408 66.5059,-133.4449\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- Equipment&#45;&gt;Squat1Kg -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>Equipment-&gt;Squat1Kg</title>\n",
       "<path d=\"M149.5164,-73.6621C136.4283,-63.7427 119.5495,-50.9503 105.2739,-40.1309\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"107.3016,-37.2761 97.2178,-34.0252 103.0734,-42.8549 107.3016,-37.2761\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- Sex&#45;&gt;Squat1Kg -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>Sex-&gt;Squat1Kg</title>\n",
       "<path d=\"M76.0734,-71.8314C76.0734,-64.131 76.0734,-54.9743 76.0734,-46.4166\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"79.5735,-46.4132 76.0734,-36.4133 72.5735,-46.4133 79.5735,-46.4132\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydot\n",
    "from IPython.display import SVG\n",
    "def examine_graph_continuous(df, prior = None):\n",
    "    tetrad.run(algoId = 'fges', dfs = df,  scoreId = 'sem-bic', dataType = 'continuous',\n",
    "               structurePrior = 1.0, samplePrior = 1, maxDegree = -1, maxPathLength = -1, priorKnowledge = prior,\n",
    "               completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True,\n",
    "               )\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "def examine_graph_mixed(df, prior = None):\n",
    "    tetrad.run(algoId = 'fges', dfs = df, scoreId = 'cond-gauss-bic', \n",
    "           priorKnowledge = prior, dataType = 'mixed', numCategoriesToDiscretize = 5,\n",
    "           structurePrior = 1.0, maxDegree = -1, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "def examine_graph_discrete(df, prior = None):\n",
    "    tetrad.run(algoId = 'fges', dfs = df, scoreId = 'bdeu', priorKnowledge = prior, dataType = 'discrete',\n",
    "               structurePrior = 1.0, samplePrior = 1.0, maxDegree = 3, faithfulnessAssumed = True, verbose = True)\n",
    "    return tetrad.getTetradGraph()\n",
    "    \n",
    "    \n",
    "\n",
    "tempForbid = p.ForbiddenWithin(['Sex', 'Age', 'Equipment'])\n",
    "temporal = [tempForbid, ['Squat1Kg',  'Deadlift1Kg', 'Bench1Kg']]\n",
    "prior = p.knowledge( addtemporal = temporal)\n",
    "\n",
    "g = examine_graph_mixed(df[inputs + target], prior = prior)\n",
    "dot_str = pc.tetradGraphToDot(g)\n",
    "graphs = pydot.graph_from_dot_data(dot_str)\n",
    "svg_str = graphs[0].create_svg()\n",
    "\n",
    "known_conx = set({})\n",
    "for i in tetrad.getEdges():\n",
    "    if ' --> ' in i:\n",
    "        known_conx.add((i.split(' --> ')[0], i.split(' --> ')[1]))\n",
    "known_conx\n",
    "\n",
    "prior = p.knowledge(requiredirect =  list(map(list, known_conx)),)\n",
    "models = []\n",
    "model_names = []\n",
    "\n",
    "\n",
    "\n",
    "randomize = False\n",
    "if randomize:\n",
    "    layers = [256, 512, 1024, 2048, 4096]\n",
    "    for i in range(num_models):\n",
    "        network = []\n",
    "        for j in range(3):\n",
    "            network.append(layers[random.randint(0,len(layers) -1)])\n",
    "        models.append(network)\n",
    "        model_names.append('temp/random' + str(i))\n",
    "    print(models, model_names)    \n",
    "else:\n",
    "    model_layers = [512,256]\n",
    "    for i in range(num_models):\n",
    "        models.append(model_layers)\n",
    "        model_names.append('temp/school' + str(i))\n",
    "\n",
    "print(models, model_names)\n",
    "\n",
    "SVG(svg_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 Deadlift1Kg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bestMSE = []\n",
    "bestBIC = []\n",
    "bestCOMBO = []\n",
    "original_df = df.copy()\n",
    "for t in range(10):\n",
    "    # let's split our df into two by race.  Let's see what happens if we \n",
    "    df = original_df.copy()\n",
    "    \n",
    "    holdout = 5000\n",
    "        #df_test = df[df['charges'] > 0.54].copy()\n",
    "    continuous = ['Squat1Kg',  'Deadlift1Kg', 'Bench1Kg']\n",
    "    #continuous = [\"math_score\"]\n",
    "    \n",
    "    '''\n",
    "    end_idx = len(df) - holdout\n",
    "    cont = random.randint(0, len(continuous) - 1)\n",
    "    start_idx = random.randint(0, end_idx)\n",
    "    print(t, \"Doing range:\",start_idx, start_idx + holdout, \"and \", continuous[cont])\n",
    "    df_test = df.nlargest(len(df) - start_idx, continuous[cont]).nsmallest(holdout, continuous[cont])\n",
    "    '''\n",
    "\n",
    "    small = random.randint(0,1)\n",
    "    cont = random.randint(0, len(continuous) - 1)\n",
    "    if small == 0:\n",
    "        df_test = df.nsmallest(holdout, continuous[cont])\n",
    "    else:\n",
    "        df_test = df.nlargest(holdout, continuous[cont])\n",
    "    print(t, small, continuous[cont])\n",
    "\n",
    "    \n",
    "\n",
    "    df.drop(df_test.index, inplace = True)\n",
    "    df_test.reset_index(inplace = True)\n",
    "    df.sample(frac= 1).reset_index(inplace = True) # this will shuffle and reset index\n",
    "\n",
    "    x_test = df_test[inputs]\n",
    "    y_test = df_test[target]\n",
    "\n",
    "    causal_split = 0.2\n",
    "    val_split = 0.2\n",
    "    train_split = 1 - (causal_split + val_split)\n",
    "\n",
    "    x_causal = df[inputs][-int(causal_split * len(df)) :]\n",
    "    y_causal = df[target][-int(causal_split * len(df)) :]\n",
    "\n",
    "    x_val = df[inputs][int(train_split * len(df)):-int(causal_split * len(df))]\n",
    "    y_val = df[target][int(train_split * len(df)):-int(causal_split * len(df))]\n",
    "\n",
    "    x_train = df[inputs][:int(train_split * len(df))]\n",
    "    y_train = df[target][:int(train_split * len(df))]\n",
    "\n",
    "    x_test_NN = make_categorical(x_test, original_df, categoricals)\n",
    "    x_causal_NN = make_categorical(x_causal, original_df, categoricals)\n",
    "    x_val_NN = make_categorical(x_val, original_df, categoricals)\n",
    "    x_train_NN = make_categorical(x_train, original_df, categoricals)\n",
    "    verbosity = 0\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        if type(models[idx]) is list:\n",
    "            #clear session\n",
    "            keras.backend.clear_session() \n",
    "            #get model according to specification\n",
    "            model = get_model(models[idx], [0.2] * len(models), np.shape(x_train_NN)[1])\n",
    "            callbacks = [ModelCheckpoint(model_name, verbose= verbosity, monitor='val_loss',save_best_only=True), \n",
    "                         EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose= verbosity, mode='auto')]\n",
    "            model.compile(optimizer = optimizers.SGD(lr = 0.0001, momentum = 0.9, ), loss='mean_squared_error', metrics = ['mse'])\n",
    "            #print(len(X), len(y))\n",
    "            model.fit(x_train_NN, y_train, epochs = 20, validation_data = (x_val_NN, y_val), callbacks = callbacks, batch_size = 32, verbose = verbosity)\n",
    "        else:\n",
    "            models[idx].fit(X,y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    generalization = []\n",
    "    metrics = []\n",
    "    proposed = []\n",
    "    x_causal.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        #print(model_name)\n",
    "        if type(models[idx]) is list:\n",
    "            keras.backend.clear_session()\n",
    "            model = load_model(model_name)\n",
    "        else:\n",
    "            model = models[idx]\n",
    "\n",
    "        y_pred = model.predict(x_test_NN)\n",
    "        generalization.append(mean_absolute_error(y_pred, y_test))\n",
    "\n",
    "        #### CHECK FOR CAUSAL METRIC HERE\n",
    "        y_causal_pred = model.predict(x_causal_NN)\n",
    "        causal_targets = pd.DataFrame(y_causal_pred, columns = target)\n",
    "        causal_targets.reset_index(drop=True, inplace = True)\n",
    "        causal_df = x_causal.join(causal_targets)\n",
    "\n",
    "\n",
    "\n",
    "        metrics.append(mean_absolute_error(y_causal_pred, y_causal))\n",
    "        #print(x_causal.head)\n",
    "        bic_pred = get_bic(causal_df, prior)\n",
    "        #print(bic_pred, tetrad.getEdges())\n",
    "\n",
    "        found_conx = set({})\n",
    "        for i in tetrad.getEdges():\n",
    "            if ' --> ' in i:\n",
    "                found_conx.add((i.split(' --> ')[0], i.split(' --> ')[1]))\n",
    "        found_conx\n",
    "\n",
    "        if found_conx == known_conx:\n",
    "            proposed.append(bic_pred)\n",
    "        else:\n",
    "            print(\"******Found an error\")\n",
    "            # for now just remove bad model.  Will need to add it to distance metric.\n",
    "            metrics = metrics[:-1]\n",
    "            generalization = generalization[:-1]\n",
    "    nbest = 10\n",
    "    total = normalize(metrics) + normalize(proposed)\n",
    "    final = pd.DataFrame(np.stack((metrics, proposed, total, generalization), axis = 1), columns = ['metrics', 'proposed', 'combined', 'generalization'])\n",
    "    print(\"MSE = \", np.sum(final.nsmallest(nbest, 'metrics')['generalization']))\n",
    "    print(\"BIC = \", np.sum(final.nsmallest(nbest, 'proposed')['generalization']))\n",
    "    print(\"COMB = \",np.sum(final.nsmallest(nbest, 'combined')['generalization']))\n",
    "    bestMSE.append(final.nsmallest(nbest, 'metrics')['generalization'])\n",
    "    bestBIC.append(final.nsmallest(nbest, 'proposed')['generalization'])\n",
    "    bestCOMBO.append(final.nsmallest(nbest, 'combined')['generalization'])\n",
    "np.mean(bestMSE), np.mean(bestCOMBO), np.std(bestMSE), np.std(bestCOMBO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_improvement(df1, df2):\n",
    "    ret = []\n",
    "    for i, j in zip(df1,df2):\n",
    "        ret.append(np.sum(j.values - i.values))\n",
    "    return ret\n",
    "\n",
    "improvement = get_average_improvement(bestMSE, bestCOMBO)\n",
    "np.mean(improvement), np.std(improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(bestMSE), np.mean(bestCOMBO), np.std(bestMSE), np.std(bestCOMBO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(proposed,generalization, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(proposed,generalization)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(proposed,generalization, '.')\n",
    "plt.plot(proposed, b + m * np.array(proposed), '-')\n",
    "ax.set_xlabel(\"BIC\")\n",
    "ax.set_ylabel(\"GEN\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(metrics,generalization, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(metrics,generalization)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(metrics,generalization, '.')\n",
    "plt.plot(metrics, b + m * np.array(metrics), '-')\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"GEN\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "total = normalize(metrics) + normalize(proposed)\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(total,generalization, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(total,generalization)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(total,generalization, '.')\n",
    "plt.plot(total, b + m * np.array(total), '-')\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"GEN\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbest = 10\n",
    "final = pd.DataFrame(np.stack((metrics, proposed, total, generalization), axis = 1), columns = ['metrics', 'proposed', 'combined', 'generalization'])\n",
    "print(\"MSE = \", np.sum(final.nsmallest(nbest, 'metrics')['generalization']))\n",
    "print(\"BIC = \", np.sum(final.nsmallest(nbest, 'proposed')['generalization']))\n",
    "print(\"COMB = \",np.sum(final.nsmallest(nbest, 'combined')['generalization']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
