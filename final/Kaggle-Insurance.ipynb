{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256]] ['temp/insurance0', 'temp/insurance1', 'temp/insurance2', 'temp/insurance3', 'temp/insurance4', 'temp/insurance5', 'temp/insurance6', 'temp/insurance7', 'temp/insurance8', 'temp/insurance9', 'temp/insurance10', 'temp/insurance11', 'temp/insurance12', 'temp/insurance13', 'temp/insurance14', 'temp/insurance15', 'temp/insurance16', 'temp/insurance17', 'temp/insurance18', 'temp/insurance19', 'temp/insurance20', 'temp/insurance21', 'temp/insurance22', 'temp/insurance23', 'temp/insurance24', 'temp/insurance25', 'temp/insurance26', 'temp/insurance27', 'temp/insurance28', 'temp/insurance29', 'temp/insurance30', 'temp/insurance31', 'temp/insurance32', 'temp/insurance33', 'temp/insurance34', 'temp/insurance35', 'temp/insurance36', 'temp/insurance37', 'temp/insurance38', 'temp/insurance39', 'temp/insurance40', 'temp/insurance41', 'temp/insurance42', 'temp/insurance43', 'temp/insurance44', 'temp/insurance45', 'temp/insurance46', 'temp/insurance47', 'temp/insurance48', 'temp/insurance49', 'temp/insurance50', 'temp/insurance51', 'temp/insurance52', 'temp/insurance53', 'temp/insurance54', 'temp/insurance55', 'temp/insurance56', 'temp/insurance57', 'temp/insurance58', 'temp/insurance59', 'temp/insurance60', 'temp/insurance61', 'temp/insurance62', 'temp/insurance63', 'temp/insurance64', 'temp/insurance65', 'temp/insurance66', 'temp/insurance67', 'temp/insurance68', 'temp/insurance69', 'temp/insurance70', 'temp/insurance71', 'temp/insurance72', 'temp/insurance73', 'temp/insurance74', 'temp/insurance75', 'temp/insurance76', 'temp/insurance77', 'temp/insurance78', 'temp/insurance79', 'temp/insurance80', 'temp/insurance81', 'temp/insurance82', 'temp/insurance83', 'temp/insurance84', 'temp/insurance85', 'temp/insurance86', 'temp/insurance87', 'temp/insurance88', 'temp/insurance89', 'temp/insurance90', 'temp/insurance91', 'temp/insurance92', 'temp/insurance93', 'temp/insurance94', 'temp/insurance95', 'temp/insurance96', 'temp/insurance97', 'temp/insurance98', 'temp/insurance99']\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"260pt\" viewBox=\"0.00 0.00 167.84 260.00\" width=\"168pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 256)\">\n",
       "<title>g</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-256 163.8437,-256 163.8437,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- age -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>age</title>\n",
       "<ellipse cx=\"64.6967\" cy=\"-234\" fill=\"none\" rx=\"27\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"64.6967\" y=\"-230.3\">age</text>\n",
       "</g>\n",
       "<!-- charges -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>charges</title>\n",
       "<ellipse cx=\"37.6967\" cy=\"-90\" fill=\"none\" rx=\"37.8943\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"37.6967\" y=\"-86.3\">charges</text>\n",
       "</g>\n",
       "<!-- age&#45;&gt;charges -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>age-&gt;charges</title>\n",
       "<path d=\"M61.3317,-216.0535C56.7237,-191.4774 48.403,-147.1008 42.9649,-118.0974\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"46.3871,-117.3564 41.104,-108.1727 39.507,-118.6465 46.3871,-117.3564\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- bmi -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>bmi</title>\n",
       "<ellipse cx=\"92.6967\" cy=\"-162\" fill=\"none\" rx=\"27\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92.6967\" y=\"-158.3\">bmi</text>\n",
       "</g>\n",
       "<!-- age&#45;&gt;bmi -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>age-&gt;bmi</title>\n",
       "<path d=\"M71.4747,-216.5708C74.6838,-208.3187 78.5902,-198.2738 82.1724,-189.0623\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"85.5215,-190.1069 85.884,-179.5182 78.9975,-187.5697 85.5215,-190.1069\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- smoker -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>smoker</title>\n",
       "<ellipse cx=\"37.6967\" cy=\"-18\" fill=\"none\" rx=\"37.0935\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"37.6967\" y=\"-14.3\">smoker</text>\n",
       "</g>\n",
       "<!-- charges&#45;&gt;smoker -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>charges-&gt;smoker</title>\n",
       "<path d=\"M37.6967,-61.5726C37.6967,-53.1084 37.6967,-44.0593 37.6967,-36.4133\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"34.1968,-61.8313 37.6967,-71.8314 41.1968,-61.8314 34.1968,-61.8313\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- bmi&#45;&gt;charges -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>bmi-&gt;charges</title>\n",
       "<path d=\"M80.2163,-145.6621C73.3328,-136.6509 64.6381,-125.2688 56.9075,-115.1488\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"59.5533,-112.8466 50.7015,-107.0245 53.9906,-117.0959 59.5533,-112.8466\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- region -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>region</title>\n",
       "<ellipse cx=\"126.6967\" cy=\"-90\" fill=\"none\" rx=\"33.2948\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"126.6967\" y=\"-86.3\">region</text>\n",
       "</g>\n",
       "<!-- bmi&#45;&gt;region -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>bmi-&gt;region</title>\n",
       "<path d=\"M100.9271,-144.5708C104.8654,-136.2309 109.6684,-126.0598 114.0559,-116.7686\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"117.3189,-118.0553 118.4242,-107.5182 110.9892,-115.0662 117.3189,-118.0553\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, \\\n",
    "                        Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "import keras.optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "import glob, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_absolute_error, mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from pycausal import search as s\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from collections import defaultdict\n",
    "from numpy.polynomial.polynomial import polyfit  \n",
    "from scipy.stats import pearsonr\n",
    "from pylab import text\n",
    "from pycausal import prior as p\n",
    "import itertools\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# select your GPU Here\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #Comment this line out if you want all GPUS (2 hehe)\n",
    "\n",
    "# python full-display web browser\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "def examine_graph_continuous(df, prior = None):\n",
    "    tetrad.run(algoId = 'fges', dfs = df,  scoreId = 'sem-bic', dataType = 'continuous',\n",
    "               structurePrior = 1.0, samplePrior = 1, maxDegree = -1, maxPathLength = -1, priorKnowledge = prior,\n",
    "               completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True,\n",
    "               )\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "def examine_graph_mixed(df, prior = None):\n",
    "    tetrad.run(algoId = 'fges', dfs = df, scoreId = 'cond-gauss-bic', \n",
    "           priorKnowledge = prior, dataType = 'mixed', numCategoriesToDiscretize = 5,\n",
    "           structurePrior = 1.0, maxDegree = -1, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "def examine_graph_discrete(df, prior = None):\n",
    "    tetrad.run(algoId = 'fges', dfs = df, scoreId = 'bdeu', priorKnowledge = prior, dataType = 'discrete',\n",
    "               structurePrior = 1.0, samplePrior = 1.0, maxDegree = 3, faithfulnessAssumed = True, verbose = True)\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "def normalize(a):\n",
    "    return (a - np.min(a)) / (np.max(a) - np.min(a))\n",
    "\n",
    "def make_categorical(df, complete_df, categoricals):   \n",
    "    retval = None\n",
    "    for key in df.columns:\n",
    "        if retval is not None:\n",
    "            if key in categoricals:\n",
    "                retval = np.concatenate((retval, to_categorical(df[key], len(complete_df[key].unique()))), axis = 1)\n",
    "            else:\n",
    "                retval = np.concatenate((retval, df[key].values[...,np.newaxis]), axis = 1)\n",
    "        else:\n",
    "            if key in categoricals:\n",
    "                retval = to_categorical(df[key], len(complete_df[key].unique()))\n",
    "            else:\n",
    "                retval = np.expand_dims(df[key], axis = 1)\n",
    "    return retval\n",
    "\n",
    "def get_model(dense, dropouts, inputs):\n",
    "    # dense is an ordered list of the number of dense neurons like [1024, 2048, 1024]\n",
    "    # dropouts is an ordered list of the dropout masks like [0.2, 0.3, 0.4]\n",
    "    inputs = keras.Input(shape = (inputs,))\n",
    "    x = keras.layers.Dense(dense[0], activation = 'relu')(inputs)\n",
    "    #x = keras.layers.Dropout(dropouts[0])(x, training=True)\n",
    "    for den, drop in zip(dense[1:], dropouts[1:]):\n",
    "        x = keras.layers.Dense(den, activation = 'relu')(x)\n",
    "        #x = keras.layers.Dropout(drop)(x, training=True)\n",
    "    outputs = keras.layers.Dense(1, activation = 'linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def get_bic(df, prior, penalty = 2):\n",
    "\n",
    "    tetrad.run(algoId = 'fges', dfs = df, scoreId = 'cond-gauss-bic', \n",
    "           priorKnowledge = prior, dataType = 'mixed', numCategoriesToDiscretize = 4,\n",
    "           structurePrior = 1.0, maxDegree = 3, faithfulnessAssumed = True, verbose = True)\n",
    "    \n",
    "    BIC = tetrad.getTetradGraph().getAllAttributes().toString()\n",
    "    BIC = float(BIC.split('=')[-1].split('}')[0])\n",
    "    return BIC\n",
    "\n",
    "num_models = 100 \n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '15000M')\n",
    "tetrad = s.tetradrunner()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('~/Desktop/Kaggle/insurance.csv')\n",
    "inputs = [\"age\", \"sex\", \"bmi\", \"children\", \"smoker\", \"region\"]\n",
    "target =  [\"charges\"]\n",
    "categoricals = ['sex','smoker','region'] \n",
    "\n",
    "label_encoder_list = []\n",
    "#one_hot = ['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course']\n",
    "for i,col in enumerate(['sex', 'smoker', 'region']):\n",
    "    label_encoder_list.append(LabelEncoder())\n",
    "    df[col] = label_encoder_list[i].fit_transform(df[col].values)\n",
    "#tempForbid = p.ForbiddenWithin(['Diameter', 'Height', 'Length'])\n",
    "temporal = [['age','sex'], ['bmi', 'children', 'smoker', 'region'], target]\n",
    "prior = p.knowledge(#requiredirect= [('Sex', 'Height'),('Sex', 'Diameter'), ('Sex', 'Length')],\n",
    "                   addtemporal = temporal\n",
    "                   )\n",
    "\n",
    "g = examine_graph_mixed(df[inputs + target], prior = prior)\n",
    "dot_str = pc.tetradGraphToDot(g)\n",
    "graphs = pydot.graph_from_dot_data(dot_str)\n",
    "svg_str = graphs[0].create_svg()\n",
    "\n",
    "known_conx = set({})\n",
    "for i in tetrad.getEdges():\n",
    "    if ' --> ' in i:\n",
    "        known_conx.add((i.split(' --> ')[0], i.split(' --> ')[1]))\n",
    "\n",
    "\n",
    "prior = p.knowledge(requiredirect =  list(map(list, known_conx)),)\n",
    "\n",
    "n_holdout = 1000\n",
    "\n",
    "df['age'] = normalize(df['age'])\n",
    "df['bmi'] = normalize(df['bmi'])\n",
    "df['charges'] = normalize(df['charges'])\n",
    "\n",
    "models = []\n",
    "model_names = []\n",
    "randomize = False\n",
    "if randomize:\n",
    "    layers = [256, 512, 1024, 2048, 4096]\n",
    "    for i in range(num_models):\n",
    "        network = []\n",
    "        for j in range(3):\n",
    "            network.append(layers[random.randint(0,len(layers) -1)])\n",
    "        models.append(network)\n",
    "        model_names.append('temp/random' + str(i))\n",
    "    print(models, model_names)    \n",
    "else:\n",
    "    model_layers = [512,256]\n",
    "    for i in range(num_models):\n",
    "        models.append(model_layers)\n",
    "        model_names.append('temp/insurance' + str(i))\n",
    "\n",
    "print(models, model_names)\n",
    "SVG(svg_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Doing range: 126 392 and  age\n",
      "MSE =  1.2077781758683221\n",
      "BIC =  1.309003491738493\n",
      "COMB =  1.2266237388662298\n",
      "1 Doing range: 992 1258 and  charges\n",
      "MSE =  2.0352411795962198\n",
      "BIC =  2.090337112836843\n",
      "COMB =  2.0152745210050917\n",
      "2 Doing range: 646 912 and  age\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a4d9368933df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_causal_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m#print(x_causal.head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mbic_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcausal_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;31m#print(bic_pred, tetrad.getEdges())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-b025b6355fa9>\u001b[0m in \u001b[0;36mget_bic\u001b[0;34m(df, prior, penalty)\u001b[0m\n\u001b[1;32m    109\u001b[0m     tetrad.run(algoId = 'fges', dfs = df, scoreId = 'cond-gauss-bic', \n\u001b[1;32m    110\u001b[0m            \u001b[0mpriorKnowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataType\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mixed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumCategoriesToDiscretize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m            structurePrior = 1.0, maxDegree = 3, faithfulnessAssumed = True, verbose = True)\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mBIC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtetrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetTetradGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetAllAttributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pycausal/search.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, algoId, dfs, testId, scoreId, priorKnowledge, dataType, numCategoriesToDiscretize, **parameters)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0mtetradData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadDiscreteData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                 \u001b[0mtetradData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadMixedData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumCategoriesToDiscretize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pycausal/pycausal.py\u001b[0m in \u001b[0;36mloadMixedData\u001b[0;34m(self, df, numCategoriesToDiscretize)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdisc_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0mcat_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjavabridge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJClassWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.Integer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                     \u001b[0mmixedDataBox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/javabridge/wrappers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, class_name)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mJ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"getModifiers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"()I\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mSTATIC\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mSTATIC\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmethods\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/javabridge/jutil.py\u001b[0m in \u001b[0;36mget_method_wrapper\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m     '''\n\u001b[0;32m-> 1998\u001b[0;31m     \u001b[0;32mclass\u001b[0m \u001b[0mMethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bestAUC = []\n",
    "bestBIC = []\n",
    "bestCOMBO = []\n",
    "original_df = df.copy()\n",
    "for t in range(10):\n",
    "    \n",
    "\n",
    "    df = original_df.copy()\n",
    "\n",
    "    holdout = 266 #20% of patients\n",
    "    \n",
    "    #df_test = df[df['charges'] > 0.54].copy()\n",
    "    continuous = [\"age\",  \"bmi\",  \"charges\"]\n",
    "    end_idx = len(df) - holdout\n",
    "    cont = random.randint(0, len(continuous) - 1)\n",
    "    start_idx = random.randint(0, end_idx)\n",
    "    print(t, \"Doing range:\",start_idx, start_idx + holdout, \"and \", continuous[cont])\n",
    "    df_test = df.nlargest(len(df) - start_idx, continuous[cont]).nsmallest(holdout, continuous[cont])\n",
    "    '''\n",
    "    small = random.randint(0,1)\n",
    "    cont = random.randint(0, len(continuous) - 1)\n",
    "    if small == 0:\n",
    "        df_test = df.nsmallest(holdout, continuous[cont])\n",
    "    else:\n",
    "        df_test = df.nlargest(holdout, continuous[cont])\n",
    "    print(t, continuous[cont])\n",
    "    '''\n",
    "    \n",
    "    df.drop(df_test.index, inplace = True)\n",
    "    df_test.reset_index(inplace = True)\n",
    "    df.sample(frac= 1).reset_index(inplace = True) # this will shuffle and reset index\n",
    "\n",
    "    x_test = df_test[inputs]\n",
    "    y_test = df_test[target]\n",
    "    causal_split = 0.2\n",
    "    val_split = 0.2\n",
    "    train_split = 1 - (causal_split + val_split)\n",
    "    x_causal = df[inputs][-int(causal_split * len(df)) :]\n",
    "    y_causal = df[target][-int(causal_split * len(df)) :]\n",
    "    x_val = df[inputs][int(train_split * len(df)):-int(causal_split * len(df))]\n",
    "    y_val = df[target][int(train_split * len(df)):-int(causal_split * len(df))]\n",
    "    x_train = df[inputs][:int(train_split * len(df))]\n",
    "    y_train = df[target][:int(train_split * len(df))]\n",
    "    x_test_NN = make_categorical(x_test, original_df, categoricals)\n",
    "    x_causal_NN = make_categorical(x_causal, original_df, categoricals)\n",
    "    x_val_NN = make_categorical(x_val, original_df, categoricals)\n",
    "    x_train_NN = make_categorical(x_train, original_df, categoricals)\n",
    "    verbosity = 0\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        #print(model_name)\n",
    "\n",
    "        if type(models[idx]) is list:\n",
    "            #clear session\n",
    "            keras.backend.clear_session() \n",
    "            #get model according to specification\n",
    "            model = get_model(models[idx], [0.2] * len(models), np.shape(x_train_NN)[1])\n",
    "            callbacks = [ModelCheckpoint(model_name, verbose= verbosity, monitor='val_loss',save_best_only=True), \n",
    "                         EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose= verbosity, mode='auto')]\n",
    "            model.compile(optimizer = optimizers.SGD(lr = 0.0001, momentum = 0.9, ), loss='mean_squared_error', metrics = ['mse'])\n",
    "            #print(len(X), len(y))\n",
    "            model.fit(x_train_NN, y_train, epochs = 20, validation_data = (x_val_NN, y_val), callbacks = callbacks, batch_size = 32, verbose = verbosity)\n",
    "        else:\n",
    "            models[idx].fit(X,y)\n",
    "\n",
    "    generalization = []\n",
    "    metrics = []\n",
    "    proposed = []\n",
    "    x_causal.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        \n",
    "        #print(model_name)\n",
    "        if type(models[idx]) is list:\n",
    "            keras.backend.clear_session()\n",
    "            model = load_model(model_name)\n",
    "        else:\n",
    "            model = models[idx]\n",
    "\n",
    "        y_pred = model.predict(x_test_NN)\n",
    "        generalization.append(mean_absolute_error(y_pred, y_test))\n",
    "\n",
    "        #### CHECK FOR CAUSAL METRIC HERE\n",
    "        y_causal_pred = model.predict(x_causal_NN)\n",
    "        causal_targets = pd.DataFrame(y_causal_pred, columns = target)\n",
    "        causal_targets.reset_index(drop=True, inplace = True)\n",
    "        causal_df = x_causal.join(causal_targets)\n",
    "\n",
    "\n",
    "\n",
    "        metrics.append(mean_absolute_error(y_causal_pred, y_causal))\n",
    "        #print(x_causal.head)\n",
    "        bic_pred = get_bic(causal_df, prior)\n",
    "        #print(bic_pred, tetrad.getEdges())\n",
    "\n",
    "        found_conx = set({})\n",
    "        for i in tetrad.getEdges():\n",
    "            if ' --> ' in i:\n",
    "                found_conx.add((i.split(' --> ')[0], i.split(' --> ')[1]))\n",
    "        found_conx\n",
    "\n",
    "        if found_conx == known_conx:\n",
    "            proposed.append(bic_pred)\n",
    "        else:\n",
    "            print(\"******Found an error\")\n",
    "            # for now just remove bad model.  Will need to add it to distance metric.\n",
    "            metrics = metrics[:-1]\n",
    "            generalization = generalization[:-1]\n",
    "            \n",
    "    nbest = 10\n",
    "    total = normalize(metrics) + normalize(proposed)\n",
    "    final = pd.DataFrame(np.stack((metrics, proposed, total, generalization), axis = 1), columns = ['metrics', 'proposed', 'combined', 'generalization'])\n",
    "    print(\"MSE = \", np.sum(final.nsmallest(nbest, 'metrics')['generalization']))\n",
    "    print(\"BIC = \", np.sum(final.nsmallest(nbest, 'proposed')['generalization']))\n",
    "    print(\"COMB = \",np.sum(final.nsmallest(nbest, 'combined')['generalization']))\n",
    "    bestAUC.append(final.nsmallest(nbest, 'metrics')['generalization'])\n",
    "    bestBIC.append(final.nsmallest(nbest, 'proposed')['generalization'])\n",
    "    bestCOMBO.append(final.nsmallest(nbest, 'combined')['generalization'])\n",
    "        \n",
    "np.mean(bestAUC), np.mean(bestBIC), np.mean(bestCOMBO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_improvement(df1, df2):\n",
    "    ret = []\n",
    "    for i, j in zip(df1,df2):\n",
    "        ret.append(np.sum(j.values - i.values))\n",
    "    return ret\n",
    "\n",
    "improvement = get_average_improvement(bestAUC, bestCOMBO)\n",
    "np.mean(improvement), np.std(improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(bestAUC), np.std(bestCOMBO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(proposed,generalization, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(proposed,generalization)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(proposed,generalization, '.')\n",
    "plt.plot(proposed, b + m * np.array(proposed), '-')\n",
    "ax.set_xlabel(\"BIC\")\n",
    "ax.set_ylabel(\"GEN\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(metrics,generalization, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(metrics,generalization)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(metrics,generalization, '.')\n",
    "plt.plot(metrics, b + m * np.array(metrics), '-')\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"GEN\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "total = normalize(metrics) + normalize(proposed)\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(total,generalization, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(total,generalization)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(total,generalization, '.')\n",
    "plt.plot(total, b + m * np.array(total), '-')\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"GEN\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbest = 5\n",
    "final = pd.DataFrame(np.stack((metrics, proposed, total, generalization), axis = 1), columns = ['metrics', 'proposed', 'combined', 'generalization'])\n",
    "print(\"MSE = \", np.sum(final.nsmallest(nbest, 'metrics')['generalization']))\n",
    "print(\"BIC = \", np.sum(final.nsmallest(nbest, 'proposed')['generalization']))\n",
    "print(\"COMB = \",np.sum(final.nsmallest(nbest, 'combined')['generalization']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df.nsmallest(1000, 'Height')['Sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
